[
  {
    "objectID": "posts/2025-02-10-summerizer/index.html",
    "href": "posts/2025-02-10-summerizer/index.html",
    "title": "Summarization Task",
    "section": "",
    "text": "In the Deeplearning.ai NLP specialization, we implemented both Q&A and Summarization tasks. However, we only looked at these tasks in terms of the attention models. It is now time to dive deeper into the summarization task. In this article I will review a talk covering a body of research on summarization, which has many ideas about features. Consider some of these and see what aspects are relevant to a modern task implementation.\nSome of the tasks here can be done with a simple LSTM or Transformer. However it seems that RL may be useful if we want to plan the summary in a non-linear fashion or to make use of a dynamic programming approach. For instance to identify the narrative structure might require many more steps then we typically use in a summerization task. For a speeches we may want to capture a gist of the rhetoric."
  },
  {
    "objectID": "posts/2025-02-10-summerizer/index.html#motivation",
    "href": "posts/2025-02-10-summerizer/index.html#motivation",
    "title": "Summarization Task",
    "section": "",
    "text": "In the Deeplearning.ai NLP specialization, we implemented both Q&A and Summarization tasks. However, we only looked at these tasks in terms of the attention models. It is now time to dive deeper into the summarization task. In this article I will review a talk covering a body of research on summarization, which has many ideas about features. Consider some of these and see what aspects are relevant to a modern task implementation.\nSome of the tasks here can be done with a simple LSTM or Transformer. However it seems that RL may be useful if we want to plan the summary in a non-linear fashion or to make use of a dynamic programming approach. For instance to identify the narrative structure might require many more steps then we typically use in a summerization task. For a speeches we may want to capture a gist of the rhetoric."
  },
  {
    "objectID": "posts/2025-02-10-summerizer/index.html#ideas",
    "href": "posts/2025-02-10-summerizer/index.html#ideas",
    "title": "Summarization Task",
    "section": "Ideas",
    "text": "Ideas\nSeveral ideas surfaced while working on the assignment for building a hybrid generative/abstractive summarizer based on GPT21. Many ideas came from prior work developing search engines. I had noticed that some issues were anathema to summarization from its inception.\n1 a Generative Pre-training Transformer or a decoder transformer\nCoverage ranking content by importance\nThe first issue is coverage, which is how much of the original document to include. Coverage is a much more difficult problem than it appears.\nConsider summerizing a long novel a single paragraph summary may provide a good idea of the main plot points of the story. On the other hand summerising a similar sized encyclopedia might require listing all the top level headings and subheadings i.e. a one paragraph summary would not be able to provide a good idea of the content of the document. On the other hand it should be much easier to generate a summary for the encyclopedia than for the novel as the encyclopedia has a highly structured document with clear headings and subheadings and writing conventions that provide cues to the reader about the importance of the content. Summerizing a novel is more challanging as we would actualy want to ommit alsmost all the details and so deciding which parts to keep is a major challange with a very sparse signal.\nFor example, technical documents, like patents, headings, academic writing cues, and even IR statistics, may serve as features that we may feed into a regression that can guide us in discarding the chuff from the grain. On the other hand, for movie scripts or novels, we can’t use all that, and we need to decide what is essential based on the narrative structure, which requires sophisticated processing and extensive domain knowledge to extract. So, When we want to create a synopsis of a book like War and Peace, specific details are part of the narrative structure that stands out as essential to us, and I can say that even in 2025, LLM is not good at picking these out of a large document. For attentive readers with a suitable background, if we double the length of the text, they might pick additional plot points and then less significant subplots. If again we double we would, they would include more details concerning characters, their motivations, etc.\nTo conclude, different documents can have radically different structures and cues. These suggest different strategies for selecting and ranking the material to be included in a summary of a given length. More so, when we consider summaries of different lengths, one can see that we are talking about a hierarchal structure."
  },
  {
    "objectID": "posts/2025-02-10-summerizer/index.html#avoiding-repetition",
    "href": "posts/2025-02-10-summerizer/index.html#avoiding-repetition",
    "title": "Summarization Task",
    "section": "Avoiding repetition",
    "text": "Avoiding repetition\nA second challenge that is pervasive is avoiding repetition. In the generative settings we have a tougher challenge since sentences that are generated may well be equivalent to sentences we have already generated, and we want the model to redirect it attention from material already covered. Luckily we have learned to detect similar sentences in the Q&A task2\n2 using Siamese networks for oneshot similarity detection.check if it is similar to any sentence in the summary\nAlg1: similarity detection\nsummary = []\ntokenized_doc = tokenize(doc)\nembeddings_doc= embed(tokenized_doc)\nwhile len(summary) &lt; doc_tokens * summary_ratio: \n    a = gen_a_sentence(embeddings_doc,summary)\n    for s in summary:\n        if sim(a,s) &gt; threshold:\n            continue\n    else:\n        summary.append(a)\ns -&gt; gen a sentence,"
  },
  {
    "objectID": "posts/2025-02-10-summerizer/index.html#context-window-constraints",
    "href": "posts/2025-02-10-summerizer/index.html#context-window-constraints",
    "title": "Summarization Task",
    "section": "Context window constraints",
    "text": "Context window constraints\nThe third challenge is technical and is related to the length of the context window. as transformer models have a limit on the number of tokens that they can process in a context window. A second issue here is that the summary itself needs to also fit in the context window. If the structure is linear and incremental we can chunk it into many smaller pieced say using sections. However in reality we are often dealing with trees of graphs structures and chunking can erode the model’s ability to inspect said hierarchy of the structure it is tasked with summarizing. In Q&A or IR tasks since we can process each chunk separately and then combine the results.\nIf we are not using a generative model one imagines a tree data structure that can efficiently track what what part of the document has been summarized. If we can use that to work with the attention mechanism we may be able to reduce the effective window size as we progress with the summary. A more nuanced idea would come from bayesian search where we may want to keep moving the probability mass for the attention mechanism to regions that are significant but have not been covered.\nAnother challenge is that we may want to grow our summary in a non-linear fashion. We may consider the main narrative structure then add in the subplots and then the character motivations. This suggests two approaches - a top down planning based approach3 and a bottom up approach where we start with the most important details, then add in the less important ones. In the second case we may want to revise the initial sentences to efficiently incorporate more details as we progress.\n3 using rl or dynamic programming4 reformer layersI also interviewed with a company that told me they often worked with patents and that these documents were frequently in excess of a hundred pages and they were having lots of problems with the summarization task. This suggest that an efficient transformer4 would be needed if we are to support context windows that can span hundreds of pages. But that besides the context window we need some good way to ensure that the summary is balanced and that it is not too repetitive."
  },
  {
    "objectID": "posts/2025-02-10-summerizer/index.html#compassion",
    "href": "posts/2025-02-10-summerizer/index.html#compassion",
    "title": "Summarization Task",
    "section": "Compassion",
    "text": "Compassion\nMy experience with summarization by hand is that most texts can be compressed by 5-10% without losing any information simply by rephrasing with more concise language. So another idea that should be obvious is that the generated summary should be compressive, i.e., once we generate a good sentence we should consider if we can make it shorter. This should be fairly easy as we are reducing the length of a single sentence. We may however be able to do even better by considering a section and trying to see if we can merge two sentences or if two sentences have partial overlap that can be merged or referenced via an anaphora. This is another area where we diverge from Q&A, where we do not have a length constraint and may often want to include all relevant information. Implementation of this idea cam be easily embodied in a loss function that penalizes summaries for each token or character. This same idea can also be used in a RL summarizer."
  },
  {
    "objectID": "posts/2025-02-10-summerizer/index.html#grounding",
    "href": "posts/2025-02-10-summerizer/index.html#grounding",
    "title": "Summarization Task",
    "section": "Grounding",
    "text": "Grounding\nGenerative summarization is a case where we can require that all generated text be grounded in the original document. In reality generative models have many glitches, due to tokenization issues, or failures of the attention mechanism, or out of distribution generation, due to bias learned from the corpus, due to negative recency bias and and due to emergent contradictions (where a contradiction is introduced into the context window and cannot be removed.) And even if we can avoid all these there is still nothing in the model that makes it prefer truthy generations. So it seems essential that the generated text is grounded in the original document. This seems to be verifiable by visually inspecting the using the attention mechanism. In reality there may be multiple heads and multiple layers. This means we need to access the attention mechanism programmatically and store annotations from the source document for each generated token. We may also want to make abstracting summarization explicit. c.f. [Extrinsic Hallucinations in LLMs in ](https://lilianweng.github.io/posts/2024-07-07-hallucination/) by Lilian Weng"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NLP course notes and research notebooks",
    "section": "",
    "text": "Domination Games\n\n\nEstablishing hierarchies\n\n\n\nGame Theory\n\n\nAgent Based Models\n\n\nSocial Structures\n\n\nHierarchies\n\n\n\n\n\n\n\n\n\nTuesday, February 18, 2025\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nA Samurai’s World\n\n\nInducing politeness, honor, into an Emergent language\n\n\n\nEmergent Language\n\n\nGame Theory\n\n\nSocial Structure\n\n\n\nIn this post, we will explore the emergence of face, politeness, and formality via multi-agent signaling language game. Specifically I consider the complex states and a framing game for a signaling game for which the requisite linguistics aspects of politeness and formality would emerge out of language evolution\n\n\n\n\n\nMonday, February 17, 2025\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nKnights and Knaves world\n\n\nInducing logic into an Emergent language\n\n\n\n\n\n\n\n\nMonday, February 17, 2025\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nFurther Desiderata for Emergent Language\n\n\n\n\n\n\n\n\n\n\n\nMonday, February 17, 2025\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nA Convolutional Attention Network for Extreme Summarization of Source Code\n\n\nreview\n\n\n\nNLP\n\n\nPaper\n\n\nAttention\n\n\nDeep learning\n\n\nReview\n\n\nStub\n\n\n\n\n\n\n\n\n\nThursday, February 13, 2025\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nFloating Constraints in Lexical Choice\n\n\nreview\n\n\n\n\n\n\n\n\nWednesday, February 12, 2025\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Morphological Analysis: Encoding-Decoding Canonical Segments\n\n\nReview\n\n\n\nPaper\n\n\nReview\n\n\n\n\n\n\n\n\n\nWednesday, February 12, 2025\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nCoverage Embedding Models for Neural Machine Translation\n\n\nReview\n\n\n\nPaper\n\n\nReview\n\n\n\n\n\n\n\n\n\nWednesday, February 12, 2025\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nSummarization Task\n\n\n\n\n\n\nNLP\n\n\nNotes\n\n\nLiterature review\n\n\nSummarization task\n\n\nRelevance\n\n\n\nThought on engineering an Automatic text Summarizer\n\n\n\n\n\nMonday, February 10, 2025\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nMorphological Word Embeddings\n\n\nReview\n\n\n\nPaper\n\n\nReview\n\n\nPodcast\n\n\nNLP\n\n\nMorphology\n\n\n\n\n\n\n\n\n\nFriday, February 7, 2025\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nPractical and Optimal LSH for Angular Distance\n\n\nReview\n\n\n\nPaper\n\n\nReview\n\n\n\n\n\n\n\n\n\nWednesday, February 5, 2025\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nxLSTM: Extended Long Short-Term Memory\n\n\nReview\n\n\n\nPaper\n\n\nReview\n\n\nNLP\n\n\nLSTM\n\n\nSeq2Seqs\n\n\nRNN\n\n\nPodcast\n\n\n\n\n\n\n\n\n\nMonday, February 3, 2025\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nThe Secret Life of Pronouns What Our Words Say About Us\n\n\nReview\n\n\n\nReview\n\n\nBook\n\n\nNLP\n\n\nSentiment Analysis\n\n\nSentiment Analysis\n\n\nPodcast\n\n\n\n\n\n\n\n\n\nThursday, January 30, 2025\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nWhen and Why are Pre-trained Word Embeddings Useful for Neural Machine Translation?\n\n\nReview\n\n\n\nAttention\n\n\nBidirectional LSTM\n\n\nDeep learning\n\n\nEmbeddings\n\n\nLLM\n\n\nNLP\n\n\nPaper\n\n\nReview\n\n\nPodcast\n\n\n\n\n\n\n\n\n\nSunday, February 11, 2024\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Machine Translation by Jointly Learning to Align and Translate\n\n\nReview\n\n\n\nAttention\n\n\nBidirectional LSTM\n\n\nDeep learning\n\n\nEmbeddings\n\n\nLLM\n\n\nNLP\n\n\nNMT\n\n\nPaper\n\n\nReview\n\n\nPodcast\n\n\nTranslation task\n\n\n\nthe paper that introduced the attention for NMT\n\n\n\n\n\nSunday, February 11, 2024\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nSyntax and Parsing\n\n\nCMU CS11-737: Multilingual NLP\n\n\n\nSyntax\n\n\nDependency parsing\n\n\nMultilingual NLP\n\n\nNLP\n\n\nNotes\n\n\n\nThis week we’re delve into syntax in general but dependency parsing in particular. We’ll also discuss the Universal Dependencies treebank and the differences between syntactic and semantic dependencies.\n\n\n\n\n\nTuesday, March 29, 2022\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nAutomatic Speech Recognition\n\n\nCMU CS11-737: Multilingual NLP\n\n\n\nAttention\n\n\nMultilingual NLP\n\n\nNLP\n\n\nNotes\n\n\n\n\n\n\n\n\n\nThursday, March 3, 2022\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nSpeech\n\n\nCMU CS11-737: Multilingual NLP\n\n\n\nAttention\n\n\nMultilingual NLP\n\n\nNLP\n\n\nNotes\n\n\n\n\n\n\n\n\n\nTuesday, March 1, 2022\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nModels for multilingual ASR and TTS\n\n\nCMU CS11-737: Multilingual NLP\n\n\n\nAttention\n\n\nMultilingual NLP\n\n\nNLP\n\n\nNotes\n\n\n\n\n\n\n\n\n\nTuesday, March 1, 2022\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nMorphology\n\n\nCMU CS11-737: Multilingual NLP\n\n\n\nAttention\n\n\nMultilingual NLP\n\n\nNLP\n\n\nNotes\n\n\n\n\n\n\n\n\n\nTuesday, March 1, 2022\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nSpeech\n\n\nCMU CS11-737: Multilingual NLP\n\n\n\nAttention\n\n\nMultilingual NLP\n\n\nNLP\n\n\nNotes\n\n\n\n\n\n\n\n\n\nTuesday, March 1, 2022\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nSpeech\n\n\nCMU CS11-737: Multilingual NLP\n\n\n\nAttention\n\n\nMultilingual NLP\n\n\nNLP\n\n\nNotes\n\n\n\nThis week we will be discussing speech recognition and the various methods used to achieve it.\n\n\n\n\n\nTuesday, March 1, 2022\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nTypology: The Space of Languages\n\n\nCMU CS11-737: Multilingual NLP\n\n\n\nAttention\n\n\nMultilingual NLP\n\n\nNLP\n\n\nNotes\n\n\n\nThis week we will cover Typology and how to structure the space of languages. We will discuss how to quantify similarity between languages, language families, linguistic typology, and typological databases.\n\n\n\n\n\nFriday, February 25, 2022\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nCode Switching, Pidgins, Creoles\n\n\nCMU CS11-737: Multilingual NLP\n\n\n\nAttention\n\n\nMultilingual NLP\n\n\nNLP\n\n\nNotes\n\n\n\nThis week we will explores code-switching, pidgins, and creoles as linguistic phenomena. Code-switching involves fluent speakers mixing languages, often driven by convenience, social signaling, or semantic nuance. Pidgins emerge as simplified lingua francas between groups with different native languages, while creoles develop when pidgins acquire native speakers.\n\n\n\n\n\nThursday, February 24, 2022\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nUnsupervised Machine Translation\n\n\nCMU CS11-737: Multilingual NLP\n\n\n\nAttention\n\n\nMultilingual NLP\n\n\nNLP\n\n\nNotes\n\n\n\nThis week we will cover text classification and sequence labeling. We will start with the basics of text classification, and then move on to more advanced topics like sequence labeling.\n\n\n\n\n\nThursday, February 17, 2022\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment 1: Multilingual POS Tagging\n\n\nCMU CS11-737: Multilingual NLP\n\n\n\nMultilingual NLP\n\n\nNLP\n\n\nNotes\n\n\n\nThis week we will cover text classification and sequence labeling. We will start with the basics of text classification, and then move on to more advanced topics like sequence labeling.\n\n\n\n\n\nTuesday, February 15, 2022\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nTranslation and Translation Data\n\n\nCMU CS11-737: Multilingual NLP\n\n\n\nAttention\n\n\nMultilingual NLP\n\n\nNLP\n\n\nNotes\n\n\n\nThis week we will cover text classification and sequence labeling. We will start with the basics of text classification, and then move on to more advanced topics like sequence labeling.\n\n\n\n\n\nTuesday, February 15, 2022\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nMultilingual Q&A\n\n\nCMU CS11-737: Multilingual NLP\n\n\n\nAttention\n\n\nMultilingual NLP\n\n\nNLP\n\n\nNotes\n\n\n\nThis week we will cover multilingual question answering.\n\n\n\n\n\nTuesday, February 15, 2022\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nLanguage Contact and Change\n\n\nCMU CS11-737: Multilingual NLP\n\n\n\nMultilingual NLP\n\n\nNLP\n\n\nNotes\n\n\nLanguage Contact\n\n\nLanguage Change\n\n\n\nThis week we will cover language contact and change, including dialects, nonnative writing, cognates, and borrowing. We will also discuss computational models of transliteration, cognates, and borrowing, as well as bilingual embedding and lexicon induction.\n\n\n\n\n\nThursday, February 10, 2022\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nData-driven Strategies for NMT\n\n\nCMU CS11-737: Multilingual NLP\n\n\n\nAttention\n\n\nMultilingual NLP\n\n\nNLP\n\n\nNotes\n\n\nNMT\n\n\n\nThis week we will cover …\n\n\n\n\n\nTuesday, February 8, 2022\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nTranslation Models\n\n\nCMU CS11-737: Multilingual NLP\n\n\n\nAttention\n\n\nMultilingual NLP\n\n\nNLP\n\n\nNotes\n\n\nseq2seq\n\n\nNMT\n\n\n\nThis week cover MT and seq2seq models, including language models that calculate the probability of text and conditional language models that generate text based on specifications. We will delve into calculating probabilities of a sentence using autoregressive models, implemented with recurrent neural networks, and how to generate sentences using methods such as sampling and argmax. We also introduce attention mechanisms and the transformer model, which uses self-attention and other techniques like positional encodings, layer normalization and specialized training schedules.\n\n\n\n\n\nThursday, February 3, 2022\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nTranslation and Translation Data\n\n\nCMU CS11-737: Multilingual NLP\n\n\n\nAttention\n\n\nMultilingual NLP\n\n\nNLP\n\n\nNotes\n\n\n\nThis week we will cover the practice of translation, machine translation, translation evaluation metrics, translation data sources, and bi-text extraction & eoderedfiltering.\n\n\n\n\n\nTuesday, February 1, 2022\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nWords, Parts of Speech, Morphology\n\n\nCMU CS11-737: Multilingual NLP\n\n\n\nAttention\n\n\nMultilingual NLP\n\n\nNLP\n\n\nNotes\n\n\n\nThis week we will cover text classification and sequence labeling. We will start with the basics of text classification, and then move on to more advanced topics like sequence labeling.\n\n\n\n\n\nMonday, January 24, 2022\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nSequence Labeling\n\n\nCMU CS11-737: Multilingual NLP\n\n\n\nMultilingual NLP\n\n\nNLP\n\n\nNotes\n\n\n\nThis week we will cover text classification and sequence labeling. We will start with the basics of text classification, and then move on to more advanced topics like sequence labeling.\n\n\n\n\n\nThursday, January 20, 2022\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nText Classification\n\n\nCMU CS11-737: Multilingual NLP\n\n\n\nAttention\n\n\nMultilingual NLP\n\n\nNLP\n\n\nNotes\n\n\n\nThis week we will cover text classification and sequence labeling. We will start with the basics of text classification, and then move on to more advanced topics like sequence labeling.\n\n\n\n\n\nTuesday, January 18, 2022\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nData Augmentation for Low-Resource Neural Machine Translation\n\n\nreview\n\n\n\nNLP\n\n\nPaper\n\n\nNMT\n\n\nDeep learning\n\n\nReview\n\n\nStub\n\n\n\n\n\n\n\n\n\nFriday, May 14, 2021\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nRestoring Hebrew Diacritics Without a Dictionary\n\n\nreview\n\n\n\nNLP\n\n\nPaper\n\n\nLSTM\n\n\nHebrew\n\n\nMorphology\n\n\nDeep learning\n\n\nReview\n\n\nStub\n\n\n\n\n\n\n\n\n\nThursday, May 13, 2021\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nIt’s MBR All the Way Down: Modern Generation Techniques Through the Lens of Minimum Bayes Risk\n\n\nReview\n\n\n\nNLP\n\n\nPaper\n\n\nReview\n\n\nMBR\n\n\nMinimum Bayes Risk\n\n\nASR task\n\n\nAutomatic speech recognition\n\n\nMT task\n\n\nSyntactic Parsing task\n\n\nAMR parsing task\n\n\nQuestion answering task\n\n\nSummarization task\n\n\n\n\n\n\n\n\n\nMonday, May 10, 2021\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nGenerating Diverse and High-Quality Texts by Minimum Bayes Risk Decoding\n\n\nReview\n\n\n\nNLP\n\n\nPaper\n\n\nReview\n\n\nMBR\n\n\nMinimum Bayes Risk\n\n\nText Generation\n\n\nDecoding Algorithms\n\n\nEncoder-Decoder Models\n\n\nLanguage Models\n\n\nMachine Translation\n\n\nImage Captioning\n\n\nQuestion Generation\n\n\nCommon Sense Reasoning\n\n\nText Summarization\n\n\n\n\n\n\n\n\n\nMonday, May 10, 2021\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n\n\nReview\n\n\n\nAttention\n\n\nBidirectional LSTM\n\n\nDeep learning\n\n\nEmbeddings\n\n\nLLM\n\n\nNLP\n\n\nPaper\n\n\nReview\n\n\nTransformer\n\n\nPodcast\n\n\n\nreview of the paper 2018 paper introducing BERT, a pre-trained language model\n\n\n\n\n\nSunday, May 9, 2021\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nELMo - Deep contextualized word representations\n\n\nReview\n\n\n\nAttention\n\n\nBidirectional LSTM\n\n\nDeep learning\n\n\nEmbeddings\n\n\nLLM\n\n\nNLP\n\n\nPaper\n\n\nReview\n\n\nStub\n\n\nPodcast\n\n\n\nreview of the paper “Deep contextualized word representations” on dot product attention\n\n\n\n\n\nSunday, May 9, 2021\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nExposing Attention Glitches with Flip-Flop Language Modeling\n\n\nReview\n\n\n\nAttention\n\n\nLSTM\n\n\nDeep learning\n\n\nLLM\n\n\nNLP\n\n\nPaper\n\n\nPodcast\n\n\nReview\n\n\n\nreview\n\n\n\n\n\nSunday, May 9, 2021\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nAttention Is All You Need\n\n\nreview\n\n\n\nNLP\n\n\nPaper\n\n\nAttention\n\n\nDeep learning\n\n\nReview\n\n\nStub\n\n\n\nReview of the 2017 paper “Attention Is All You Need” on the transformer architecture.\n\n\n\n\n\nSaturday, May 8, 2021\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nEffective Approaches to Attention-based Neural Machine Translation\n\n\nReview\n\n\n\nNLP\n\n\nPaper\n\n\nAttention\n\n\nDeep learning\n\n\nReview\n\n\nPodcast\n\n\nTranslation task\n\n\n\nReview of the landmark 2015 paper innovating attention-based mechanisms for Neural Machine Translation\n\n\n\n\n\nSaturday, May 8, 2021\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nPutting the “Re” in Reformer: Ungraded Lab\n\n\n\n\n\n\n\n\n\n\n\nThursday, April 29, 2021\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nReformer Efficient Attention: Ungraded Lab\n\n\n\n\n\n\n\n\n\n\n\nWednesday, April 28, 2021\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nChat Bots\n\n\nNLP with Attention Models\n\n\n\nAttention\n\n\nChat bot development\n\n\nCoursera\n\n\nIntelligent agents\n\n\nLocality sensitive hashing\n\n\nNLP with Attention Models\n\n\nNeural Machine Translation\n\n\nNLP\n\n\nNotes\n\n\nPositional encoding\n\n\nQuestion answering task\n\n\nReversible layers\n\n\nTeacher forcing\n\n\nTransformer\n\n\n\nThis week of the NLP Specialization, we explore Chatbot. We will be building Reformer model, an efficient transformer to create intelligent conversational agents.We will learn how to train this model on dialogue datasets and generate responses that mimic human-like conversations. Through hands-on exercises using JAX, we will gain practical experience in building chatbot that can understand and respond to user queries. We will master the skills to develop sophisticated chatbot applications using state-of-the-art NLP techniques\n\n\n\n\n\nTuesday, April 27, 2021\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nAutomatic Summarization Task\n\n\n\n\n\n\nNLP\n\n\nNotes\n\n\nLiterature review\n\n\nSummarization task\n\n\nRelevance\n\n\nConference talk\n\n\n\nConcepts, slide commentaries and Lecture notes on Automatic text Summarization by Masa Nekic\n\n\n\n\n\nSaturday, April 24, 2021\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment 3 Ungraded Sections - Part 2: T5 SQuAD Model\n\n\nNLP with Attention Models\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nNLP with Attention Models\n\n\n\n\n\n\n\n\n\nTuesday, April 13, 2021\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment 3 Ungraded Sections - Part 1: BERT Loss Model\n\n\nNLP with Attention Models\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nNLP with Attention Models\n\n\n\n\n\n\n\n\n\nMonday, April 12, 2021\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nSentencePiece and Byte Pair Encoding\n\n\nNLP with Attention Models\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nNLP with Attention Models\n\n\n\n\n\n\n\n\n\nSunday, April 11, 2021\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion Answering\n\n\nNLP with Attention Models\n\n\n\nAttention\n\n\nCoursera\n\n\nDeep Learning Algorithms\n\n\nNLP\n\n\nNotes\n\n\nNLP with Attention Models\n\n\nNeural Machine Translation\n\n\nTransformer\n\n\nTeacher forcing\n\n\nPositional encoding\n\n\nQuestion answering task\n\n\n\nThis week we will dive into Neural Question Answering. We will build advanced models like T5 and BERT to accurately answer questions based on given contexts. We will fine-tune these models to optimize their performance. We will gain practical experience in building question-answering systems.\n\n\n\n\n\nSaturday, April 10, 2021\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nThe Transformer Decoder: Ungraded Lab Notebook\n\n\n\n\n\n\n\n\n\n\n\nSaturday, April 3, 2021\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nThe Three Ways of Attention and Dot Product Attention: Ungraded Lab Notebook\n\n\n\n\n\n\n\n\n\n\n\nFriday, April 2, 2021\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 2 - Text Summarization\n\n\nNLP with Attention Models\n\n\n\nNLP with Attention Models\n\n\nNeural Machine Translation\n\n\nCoursera\n\n\nNotes\n\n\nDeep Learning Algorithms\n\n\nTransformer\n\n\nTeacher forcing\n\n\nPositional encoding\n\n\nGPT2\n\n\nTransformer decoder\n\n\nAttention\n\n\nDot product attention\n\n\nSelf attention\n\n\nCausal attention\n\n\nMulti-head attention\n\n\nSummarization task\n\n\n\nThis week we unlock the secrets of Neural Text Summarization. We will building a powerful Transformer model to extract crucial information and create concise summaries. Through hands-on exercises using JAX, we will learn techniques like beam search and length normalization to enhance the quality of our summaries. Through hands-on exercises we will train our model on a dataset of articles.\n\n\n\n\n\nThursday, April 1, 2021\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nCalculating the Bilingual Evaluation Understudy (BLEU) score: Ungraded Lab\n\n\nSequence Models\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nSequence Models\n\n\n\n\n\n\n\n\n\nTuesday, March 23, 2021\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nStack Semantics in Trax: Ungraded Lab\n\n\nSequence Models\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nSequence Models\n\n\n\n\n\n\n\n\n\nMonday, March 22, 2021\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Machine Translation\n\n\nNLP with Attention Models\n\n\n\nAttention\n\n\nBeam search\n\n\nBLEU\n\n\nROUGE\n\n\nCoursera\n\n\nNLP with Attention Models\n\n\nNotes\n\n\nMachine translation task\n\n\nMBR\n\n\nNLP\n\n\nPositional encoding\n\n\nSeq2Seq\n\n\nTransformer\n\n\nTeacher forcing\n\n\nTranslation task\n\n\nWord alignment\n\n\n\nThis week we dive deep into the Neural Machine Translation. We’ll learn about the encoder-decoder architecture, explore attention mechanisms that enable the model to focus on different parts of the input sequence during translation. In the hands-on exercises, we’ll implement an attention model for English to German translation, train it on a dataset of sentence pairs, and evaluate its performance.\n\n\n\n\n\nSaturday, March 20, 2021\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nMachine Translation Course 2020 - Lecture 1 - Introduction\n\n\n\n\n\n\nNMT\n\n\nBIU\n\n\nNLP\n\n\nNotes\n\n\n\n\n\n\n\n\n\nMonday, March 1, 2021\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluate a Siamese model: Ungraded Lecture Notebook\n\n\nSequence Models\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nSequence Models\n\n\n\n\n\n\n\n\n\nSaturday, November 21, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nModified Triplet Loss\n\n\nSequence Models\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nSequence Models\n\n\n\n\n\n\n\n\n\nFriday, November 20, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nCreating a Siamese model using Trax: Ungraded Lecture Notebook\n\n\nSequence Models\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nSequence Models\n\n\n\n\n\n\n\n\n\nThursday, November 19, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nSiamese Networks\n\n\nSequence Models\n\n\n\nNLP\n\n\nCoursera\n\n\nnotes\n\n\nSequence Models\n\n\nLSTM\n\n\nSiamese networks\n\n\nOne shot learning\n\n\nTriplet loss\n\n\n\nwe cover Neural networks for deep learning, then build a tweet classifier that places tweets into positive or negative sentiment categories, using a DNN.\n\n\n\n\n\nWednesday, November 18, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nVanishing Gradients\n\n\nSequence Models\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nSequence Models\n\n\n\n\n\n\n\n\n\nTuesday, November 17, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nLSTMs and Named Entity Recognition\n\n\nSequence Models\n\n\n\nNLP\n\n\nCoursera\n\n\nNotes\n\n\nSequence Models\n\n\n\nwe cover Neural networks for deep learning, then build a tweet classifier that places tweets into positive or negative sentiment categories, using a DNN.\n\n\n\n\n\nMonday, November 16, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nCreating a GRU model using Trax: Ungraded Lecture Notebook\n\n\nSequence Models\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nSequence Models\n\n\n\n\n\n\n\n\n\nSaturday, November 14, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nVanilla RNNs, GRUs and the scan function\n\n\nSequence Models\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nSequence Models\n\n\n\n\n\n\n\n\n\nFriday, November 13, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with JAX numpy and calculating perplexity: Ungraded Lecture Notebook\n\n\nSequence Models\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nSequence Models\n\n\n\n\n\n\n\n\n\nThursday, November 12, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nHidden State Activation : Ungraded Lecture Notebook\n\n\nSequence Models\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nSequence Models\n\n\n\n\n\n\n\n\n\nWednesday, November 11, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nRecurrent Neural Networks for Language Modeling\n\n\nSequence Models\n\n\n\nNLP\n\n\nCoursera\n\n\nNotes\n\n\nSequence Models\n\n\n\n\n\n\n\n\n\nTuesday, November 10, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nData generators\n\n\nSequence Models\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nSequence Models\n\n\n\n\n\n\n\n\n\nMonday, November 9, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nClasses and subclasses\n\n\nSequence Models\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nSequence Models\n\n\n\n\n\n\n\n\n\nSunday, November 8, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nTrax : Ungraded Lecture Notebook\n\n\nSequence Models\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nSequence Models\n\n\n\n\n\n\n\n\n\nSaturday, November 7, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Networks for Sentiment Analysis\n\n\nSequence Models\n\n\n\nNLP\n\n\nCoursera\n\n\nNotes\n\n\nSequence Models\n\n\nSentiment analysis task\n\n\n\nwe cover Neural networks for deep learning, then build a tweet classifier that places tweets into positive or negative sentiment categories, using a DNN.\n\n\n\n\n\nFriday, November 6, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nWord Embeddings: Hands On\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nProbabilistic Models\n\n\n\n\n\n\n\n\n\nWednesday, November 4, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nWord Embeddings: Ungraded Practice Notebook\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nProbabilistic Models\n\n\n\n\n\n\n\n\n\nTuesday, November 3, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nWord Embeddings: Training the CBOW model\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nProbabilistic Models\n\n\n\n\n\n\n\n\n\nMonday, November 2, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nWord Embeddings: Intro to CBOW model, activation functions and working with Numpy\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nProbabilistic Models\n\n\n\n\n\n\n\n\n\nSunday, November 1, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nWord Embeddings First Steps: Data Preparation\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nProbabilistic Models\n\n\n\n\n\n\n\n\n\nFriday, October 30, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nWord embeddings with neural networks\n\n\nProbabilistic Models\n\n\n\nNLP\n\n\nCoursera\n\n\nnotes\n\n\nProbabilistic Models\n\n\n\nExtract features from text into numerical vectors, then build a binary classifier for tweets using logistic regression!\n\n\n\n\n\nThursday, October 29, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nOut of vocabulary words (OOV)\n\n\nProbabilistic Models\n\n\n\nNLP\n\n\nCoursera\n\n\nNotes\n\n\nProbabilistic Models\n\n\n\n\n\n\n\n\n\nTuesday, October 27, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding the language model\n\n\nProbabilistic Models\n\n\n\nNLP\n\n\nCoursera\n\n\nNotes\n\n\nProbabilistic Models\n\n\n\n\n\n\n\n\n\nMonday, October 26, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nN-grams Corpus preprocessing\n\n\nProbabilistic Models\n\n\n\nNLP\n\n\nCoursera\n\n\nNotes\n\n\nProbabilistic Models\n\n\n\n\n\n\n\n\n\nSunday, October 25, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nAutocomplete and Language Models\n\n\nProbabilistic Models\n\n\n\nNLP\n\n\nCoursera\n\n\nNotes\n\n\nProbabilistic Models\n\n\n\nExtract features from text into numerical vectors, then build a binary classifier for tweets using logistic regression!\n\n\n\n\n\nSaturday, October 24, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nParts-of-Speech Tagging - Working with tags and Numpy\n\n\nProbabilistic Models\n\n\n\nNLP\n\n\nCoursera\n\n\nlab\n\n\nProbabilistic Models\n\n\n\nIn this lab we will create a matrix using some tag information and then modify it using different approaches. This will serve as hands-on experience working with Numpy\n\n\n\n\n\nThursday, October 22, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nParts-of-Speech Tagging - First Steps: Working with text files, Creating a Vocabulary and Handling Unknown Words\n\n\nProbabilistic Models\n\n\n\nNLP\n\n\nCoursera\n\n\nlab\n\n\nProbabilistic Models\n\n\n\nExtract features from text into numerical vectors, then build a binary classifier for tweets using logistic regression!\n\n\n\n\n\nWednesday, October 21, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nPart of Speech Tagging and Hidden Markov Models\n\n\nProbabilistic Models\n\n\n\nNLP\n\n\nCoursera\n\n\nNotes\n\n\nProbabilistic Models\n\n\n\nExtract features from text into numerical vectors, then build a binary classifier for tweets using logistic regression!\n\n\n\n\n\nTuesday, October 20, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment 1: Auto Correct\n\n\n\n\n\n\n\n\n\n\n\nSunday, October 18, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nCandidates from String Edits\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nLogistic regression\n\n\nSentiment analysis task\n\n\nClassification & Vector Spaces\n\n\n\n\n\n\n\n\n\nSaturday, October 17, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding the vocabulary\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nLogistic regression\n\n\nSentiment analysis task\n\n\nClassification & Vector Spaces\n\n\n\n\n\n\n\n\n\nFriday, October 16, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nAutocorrect and minimum edit distance\n\n\nProbabilistic Models\n\n\n\nNLP\n\n\nCoursera\n\n\nNotes\n\n\nProbabilistic Models\n\n\n\nAuto-correct text with minimum edit distances\n\n\n\n\n\nThursday, October 15, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nHash functions and multiplanes\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nWord Embeddings\n\n\nTranslation task\n\n\nSearch Task\n\n\n\n\n\n\n\n\n\nTuesday, October 13, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nVector manipulation in Python\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nWord Embeddings\n\n\nTranslation task\n\n\nSearch Task\n\n\n\n\n\n\n\n\n\nMonday, October 12, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nMachine Translation and Document Search\n\n\nClassification & Vector Spaces\n\n\n\nNLP\n\n\nCoursera\n\n\nnotes\n\n\nWord Embeddings\n\n\nTranslation task\n\n\nSearch Task\n\n\n\nConcepts, code snippets, and slide commentaries for this week’s lesson of the Course notes from the deeplearning.ai natural language programming specialization.\n\n\n\n\n\nSunday, October 11, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nManipulating word embeddings\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nLogistic regression\n\n\nSentiment analysis task\n\n\nClassification & Vector Spaces\n\n\n\n\n\n\n\n\n\nFriday, October 9, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nLinear algebra in Python with NumPy\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nLogistic regression\n\n\nSentiment analysis task\n\n\nClassification & Vector Spaces\n\n\n\n\n\n\n\n\n\nThursday, October 8, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nVector Space Models\n\n\nClassification & Vector Spaces\n\n\n\nNLP\n\n\nCoursera\n\n\nNotes\n\n\nClassification & Vector Spaces\n\n\n\nVector space models capture semantic meaning and relationships between words. You’ll learn how to create word vectors that capture dependencies between words, then visualize their relationships in two dimensions using PCA.\n\n\n\n\n\nWednesday, October 7, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing Naive Bayes\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nLogistic regression\n\n\nSentiment analysis task\n\n\nClassification & Vector Spaces\n\n\n\n\n\n\n\n\n\nTuesday, October 6, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nProbability and Bayes Rule\n\n\nClassification & Vector Spaces\n\n\n\nNLP\n\n\nCoursera\n\n\nCode\n\n\nConditional Probability\n\n\nBayes rule\n\n\nNaïve Bayes\n\n\nLaplace smoothing\n\n\nLog-likelihood\n\n\nClassification\n\n\nSentiment analysis task\n\n\n\nConcepts, code snippets, and slide commentaries for this week’s lesson of the Course notes from the deeplearning.ai natural language programming specialization.\n\n\n\n\n\nTuesday, October 6, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nProbability and Bayes Rule\n\n\nClassification & Vector Spaces\n\n\n\nNLP\n\n\nCoursera\n\n\nClassification & Vector Spaces\n\n\nNotes\n\n\nNaïve Bayes\n\n\nSentiment analysis task\n\n\n\nThe theory behind Bayes’ rule for conditional probabilities, and its application toward building a Naive Bayes tweet classifier\n\n\n\n\n\nMonday, October 5, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nLab: Visualizing tweets & the Logistic Regression model\n\n\nNLP with Classification & Vector Spaces\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nLogistic regression\n\n\nSentiment analysis task\n\n\nClassification & Vector Spaces\n\n\nNLTK\n\n\n\n\n\n\n\n\n\nSunday, October 4, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nLab: Building and Visualizing word frequencies\n\n\nNLP with Classification & Vector Spaces\n\n\n\nClassification & Vector Spaces\n\n\nCoursera\n\n\nLab\n\n\nNLP\n\n\nNLTK\n\n\nSentiment analysis task\n\n\nWord frequencies\n\n\n\n\n\n\n\n\n\nSaturday, October 3, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nLab: Preprocessing\n\n\nNLP with Classification & Vector Spaces\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nLogistic regression\n\n\nSentiment analysis task\n\n\nClassification & Vector Spaces\n\n\nNLTK\n\n\n\n\n\n\n\n\n\nFriday, October 2, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nSentiment Analysis with Logistic Regression\n\n\nNLP with Classification & Vector Spaces\n\n\n\nClassification & Vector Spaces\n\n\nCoursera\n\n\nLogistic regression\n\n\nNLP\n\n\nNotes\n\n\nSentiment analysis task\n\n\n\nExtract features from text into numerical vectors, then build a binary classifier for tweets using logistic regression!\n\n\n\n\n\nThursday, October 1, 2020\n\n\nOren Bochman\n\n\n\n\n\n\nNo matching items\n\n\n  \n\nCitationBibTeX citation:@online{bochman2025,\n  author = {Bochman, Oren},\n  title = {NLP Course Notes and Research Notebooks},\n  date = {2025-02-11},\n  url = {https://orenbochman.github.io/notes-nlp/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2025. “NLP Course Notes and Research Notebooks\n.” February 11, 2025. https://orenbochman.github.io/notes-nlp/."
  },
  {
    "objectID": "notes/c1w1/lab02.html",
    "href": "notes/c1w1/lab02.html",
    "title": "Lab: Building and Visualizing word frequencies",
    "section": "",
    "text": "Figure 1: course banner\nIn this lab, we will focus on the build_freqs() helper function and visualizing a dataset fed into it. In our goal of tweet sentiment analysis, this function will build a dictionary where we can lookup how many times a word appears in the lists of positive or negative tweets. This will be very helpful when extracting the features of the dataset in the week’s programming assignment. Let’s see how this function is implemented under the hood in this notebook.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "L2 - Frequencies"
    ]
  },
  {
    "objectID": "notes/c1w1/lab02.html#setup",
    "href": "notes/c1w1/lab02.html#setup",
    "title": "Lab: Building and Visualizing word frequencies",
    "section": "Setup",
    "text": "Setup\nLet’s import the required libraries for this lab:\n\nimport nltk                                  # Python library for NLP\nfrom nltk.corpus import twitter_samples      # sample Twitter dataset from NLTK\nimport matplotlib.pyplot as plt              # visualization library\nimport numpy as np                           # library for scientific computing and matrix operations\n\n\nImport some helper functions that we provided in the utils.py file:\n\nprocess_tweet(): Cleans the text, tokenizes it into separate words, removes stopwords, and converts words to stems.\nbuild_freqs(): This counts how often a word in the ‘corpus’ (the entire set of tweets) was associated with a positive label 1 or a negative label 0. It then builds the freqs dictionary, where each key is a (word,label) tuple, and the value is the count of its frequency within the corpus of tweets.\n\n\n# download the stopwords for the process_tweet function\nnltk.download('stopwords')\n\n# import our convenience functions\nfrom utils import process_tweet, build_freqs\n\n[nltk_data] Downloading package stopwords to /home/oren/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\nTrue",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "L2 - Frequencies"
    ]
  },
  {
    "objectID": "notes/c1w1/lab02.html#load-the-nltk-sample-dataset",
    "href": "notes/c1w1/lab02.html#load-the-nltk-sample-dataset",
    "title": "Lab: Building and Visualizing word frequencies",
    "section": "Load the NLTK sample dataset",
    "text": "Load the NLTK sample dataset\nAs in the previous lab, we will be using the Twitter dataset from NLTK.\n\n# select the lists of positive and negative tweets\nall_positive_tweets = twitter_samples.strings('positive_tweets.json')\nall_negative_tweets = twitter_samples.strings('negative_tweets.json')\n\n# concatenate the lists, 1st part is the positive tweets followed by the negative\ntweets = all_positive_tweets + all_negative_tweets\n\n# let's see how many tweets we have\nprint(\"Number of tweets: \", len(tweets))\n\nNumber of tweets:  10000\n\n\nNext, we will build a labels array that matches the sentiments of our tweets. This data type works pretty much like a regular list but is optimized for computations and manipulation. The labels array will be composed of 10000 elements. The first 5000 will be filled with 1 labels denoting positive sentiments, and the next 5000 will be 0 labels denoting the opposite. We can do this easily with a series of operations provided by the numpy library:\n\nnp.ones() - create an array of 1’s\nnp.zeros() - create an array of 0’s\nnp.append() - concatenate arrays\n\n\n# make a numpy array representing labels of the tweets\nlabels = np.append(np.ones((len(all_positive_tweets))), np.zeros((len(all_negative_tweets))))",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "L2 - Frequencies"
    ]
  },
  {
    "objectID": "notes/c1w1/lab02.html#dictionaries",
    "href": "notes/c1w1/lab02.html#dictionaries",
    "title": "Lab: Building and Visualizing word frequencies",
    "section": "Dictionaries",
    "text": "Dictionaries\nIn Python, a dictionary is a mutable and indexed collection. It stores items as key-value pairs and uses hash tables underneath to allow practically constant time lookups. In NLP, dictionaries are essential because it enables fast retrieval of items or containment checks even with thousands of entries in the collection.\n\nDefinition\nA dictionary in Python is declared using curly brackets. Look at the next example:\n\ndictionary = {'key1': 1, 'key2': 2}\n\nThe former line defines a dictionary with two entries. Keys and values can be almost any type (with a few restriction on keys), and in this case, we used strings. We can also use floats, integers, tuples, etc.\n\n\nAdding or editing entries\nNew entries can be inserted into dictionaries using square brackets. If the dictionary already contains the specified key, its value is overwritten.\n\n# Add a new entry\ndictionary['key3'] = -5\n\n# Overwrite the value of key1\ndictionary['key1'] = 0\n\nprint(dictionary)\n\n{'key1': 0, 'key2': 2, 'key3': -5}\n\n\n\n\nAccessing values and lookup keys\nPerforming dictionary lookups and retrieval are common tasks in NLP. There are two ways to do this:\n\nUsing square bracket notation: This form is allowed if the lookup key is in the dictionary. It produces an error otherwise.\nUsing the get() method: This allows us to set a default value if the dictionary key does not exist.\n\nLet us see these in action:\n\n# Square bracket lookup when the key exist\nprint(dictionary['key2'])\n\n2\n\n\nHowever, if the key is missing, the operation produce an error\n\n# The output of this line is intended to produce a KeyError\nprint(dictionary['key8'])\n\n\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nCell In[8], line 2\n      1 # The output of this line is intended to produce a KeyError\n----&gt; 2 print(dictionary['key8'])\n\nKeyError: 'key8'\n\n\n\nWhen using a square bracket lookup, it is common to use an if-else block to check for containment first (with the keyword in) before getting the item. On the other hand, we can use the .get() method if we want to set a default value when the key is not found. Let’s compare these in the cells below:\n\n# This prints a value\nif 'key1' in dictionary:\n    print(\"item found: \", dictionary['key1'])\nelse:\n    print('key1 is not defined')\n\n# Same as what we get with get\nprint(\"item found: \", dictionary.get('key1', -1))\n\nitem found:  0\nitem found:  0\n\n\n\n# This prints a message because the key is not found\nif 'key7' in dictionary:\n    print(dictionary['key7'])\nelse:\n    print('key does not exist!')\n\n# This prints -1 because the key is not found and we set the default to -1\nprint(dictionary.get('key7', -1))\n\nkey does not exist!\n-1",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "L2 - Frequencies"
    ]
  },
  {
    "objectID": "notes/c1w1/lab02.html#word-frequency-dictionary",
    "href": "notes/c1w1/lab02.html#word-frequency-dictionary",
    "title": "Lab: Building and Visualizing word frequencies",
    "section": "Word frequency dictionary",
    "text": "Word frequency dictionary\nNow that we know the building blocks, let’s finally take a look at the build_freqs() function in utils.py. This is the function that creates the dictionary containing the word counts from each corpus.\ndef build_freqs(tweets, ys):\n    \"\"\"Build frequencies.\n    Input:\n        tweets: a list of tweets\n        ys: an m x 1 array with the sentiment label of each tweet\n            (either 0 or 1)\n    Output:\n        freqs: a dictionary mapping each (word, sentiment) pair to its\n        frequency\n    \"\"\"\n    # Convert np array to list since zip needs an iterable.\n    # The squeeze is necessary or the list ends up with one element.\n    # Also note that this is just a NOP if ys is already a list.\n    yslist = np.squeeze(ys).tolist()\n\n    # Start with an empty dictionary and populate it by looping over all tweets\n    # and over all processed words in each tweet.\n    freqs = {}\n    for y, tweet in zip(yslist, tweets):\n        for word in process_tweet(tweet):\n            pair = (word, y)\n            if pair in freqs:\n                freqs[pair] += 1\n            else:\n                freqs[pair] = 1    \n    return freqs\nWe can also do the for loop like this to make it a bit more compact:\n    for y, tweet in zip(yslist, tweets):\n        for word in process_tweet(tweet):\n            pair = (word, y)\n            freqs[pair] = freqs.get(pair, 0) + 1\nAs shown above, each key is a 2-element tuple containing a (word, y) pair. The word is an element in a processed tweet while y is an integer representing the corpus: 1 for the positive tweets and 0 for the negative tweets. The value associated with this key is the number of times that word appears in the specified corpus. For example:\n# \"folowfriday\" appears 25 times in the positive tweets\n('followfriday', 1.0): 25\n\n# \"shame\" appears 19 times in the negative tweets\n('shame', 0.0): 19 \nNow, it is time to use the dictionary returned by the build_freqs() function. First, let us feed our tweets and labels lists then print a basic report:\n\n# create frequency dictionary\nfreqs = build_freqs(tweets, labels)\n\n# check data type\nprint(f'type(freqs) = {type(freqs)}')\n\n# check length of the dictionary\nprint(f'len(freqs) = {len(freqs)}')\n\ntype(freqs) = &lt;class 'dict'&gt;\nlen(freqs) = 13065\n\n\nNow print the frequency of each word depending on its class.\n\nprint(freqs)\n\n{('followfriday', 1.0): 25, ('top', 1.0): 32, ('engag', 1.0): 7, ('member', 1.0): 16, ('commun', 1.0): 33, ('week', 1.0): 83, (':)', 1.0): 3568, ('hey', 1.0): 76, ('jame', 1.0): 7, ('odd', 1.0): 2, (':/', 1.0): 5, ('pleas', 1.0): 97, ('call', 1.0): 37, ('contact', 1.0): 7, ('centr', 1.0): 2, ('02392441234', 1.0): 1, ('abl', 1.0): 8, ('assist', 1.0): 1, ('mani', 1.0): 33, ('thank', 1.0): 620, ('listen', 1.0): 16, ('last', 1.0): 47, ('night', 1.0): 68, ('bleed', 1.0): 2, ('amaz', 1.0): 51, ('track', 1.0): 5, ('scotland', 1.0): 2, ('congrat', 1.0): 21, ('yeaaah', 1.0): 1, ('yipppi', 1.0): 1, ('accnt', 1.0): 2, ('verifi', 1.0): 2, ('rqst', 1.0): 1, ('succeed', 1.0): 1, ('got', 1.0): 69, ('blue', 1.0): 9, ('tick', 1.0): 1, ('mark', 1.0): 1, ('fb', 1.0): 6, ('profil', 1.0): 2, ('15', 1.0): 5, ('day', 1.0): 246, ('one', 1.0): 129, ('irresist', 1.0): 2, ('flipkartfashionfriday', 1.0): 17, ('like', 1.0): 233, ('keep', 1.0): 68, ('love', 1.0): 400, ('custom', 1.0): 4, ('wait', 1.0): 70, ('long', 1.0): 36, ('hope', 1.0): 141, ('enjoy', 1.0): 75, ('happi', 1.0): 211, ('friday', 1.0): 116, ('lwwf', 1.0): 1, ('second', 1.0): 10, ('thought', 1.0): 29, ('’', 1.0): 21, ('enough', 1.0): 18, ('time', 1.0): 127, ('dd', 1.0): 1, ('new', 1.0): 143, ('short', 1.0): 7, ('enter', 1.0): 9, ('system', 1.0): 2, ('sheep', 1.0): 1, ('must', 1.0): 18, ('buy', 1.0): 11, ('jgh', 1.0): 4, ('go', 1.0): 148, ('bayan', 1.0): 1, (':d', 1.0): 629, ('bye', 1.0): 7, ('act', 1.0): 8, ('mischiev', 1.0): 1, ('etl', 1.0): 1, ('layer', 1.0): 1, ('in-hous', 1.0): 1, ('wareh', 1.0): 1, ('app', 1.0): 16, ('katamari', 1.0): 1, ('well', 1.0): 81, ('…', 1.0): 38, ('name', 1.0): 18, ('impli', 1.0): 1, (':p', 1.0): 138, ('influenc', 1.0): 18, ('big', 1.0): 33, ('...', 1.0): 289, ('juici', 1.0): 3, ('selfi', 1.0): 12, ('follow', 1.0): 381, ('perfect', 1.0): 24, ('alreadi', 1.0): 28, ('know', 1.0): 145, (\"what'\", 1.0): 17, ('great', 1.0): 171, ('opportun', 1.0): 23, ('junior', 1.0): 2, ('triathlet', 1.0): 1, ('age', 1.0): 2, ('12', 1.0): 5, ('13', 1.0): 6, ('gatorad', 1.0): 1, ('seri', 1.0): 5, ('get', 1.0): 206, ('entri', 1.0): 4, ('lay', 1.0): 4, ('greet', 1.0): 5, ('card', 1.0): 8, ('rang', 1.0): 3, ('print', 1.0): 3, ('today', 1.0): 108, ('job', 1.0): 41, (':-)', 1.0): 692, (\"friend'\", 1.0): 3, ('lunch', 1.0): 5, ('yummm', 1.0): 1, ('nostalgia', 1.0): 1, ('tb', 1.0): 2, ('ku', 1.0): 1, ('id', 1.0): 8, ('conflict', 1.0): 1, ('help', 1.0): 41, (\"here'\", 1.0): 25, ('screenshot', 1.0): 3, ('work', 1.0): 110, ('hi', 1.0): 173, ('liv', 1.0): 2, ('hello', 1.0): 59, ('need', 1.0): 78, ('someth', 1.0): 28, ('u', 1.0): 175, ('fm', 1.0): 2, ('twitter', 1.0): 29, ('—', 1.0): 27, ('sure', 1.0): 58, ('thing', 1.0): 69, ('dm', 1.0): 39, ('x', 1.0): 72, (\"i'v\", 1.0): 35, ('heard', 1.0): 9, ('four', 1.0): 5, ('season', 1.0): 9, ('pretti', 1.0): 20, ('dope', 1.0): 2, ('penthous', 1.0): 1, ('obv', 1.0): 1, ('gobigorgohom', 1.0): 1, ('fun', 1.0): 58, (\"y'all\", 1.0): 3, ('yeah', 1.0): 47, ('suppos', 1.0): 7, ('lol', 1.0): 64, ('chat', 1.0): 13, ('bit', 1.0): 20, ('youth', 1.0): 19, ('💅🏽', 1.0): 1, ('💋', 1.0): 2, ('seen', 1.0): 10, ('year', 1.0): 43, ('rest', 1.0): 12, ('goe', 1.0): 7, ('quickli', 1.0): 3, ('bed', 1.0): 16, ('music', 1.0): 21, ('fix', 1.0): 10, ('dream', 1.0): 20, ('spiritu', 1.0): 1, ('ritual', 1.0): 1, ('festiv', 1.0): 8, ('népal', 1.0): 1, ('begin', 1.0): 4, ('line-up', 1.0): 4, ('left', 1.0): 13, ('see', 1.0): 184, ('sarah', 1.0): 4, ('send', 1.0): 22, ('us', 1.0): 109, ('email', 1.0): 26, ('bitsy@bitdefender.com', 1.0): 1, (\"we'll\", 1.0): 20, ('asap', 1.0): 5, ('kik', 1.0): 22, ('hatessuc', 1.0): 1, ('32429', 1.0): 1, ('kikm', 1.0): 1, ('lgbt', 1.0): 2, ('tinder', 1.0): 1, ('nsfw', 1.0): 1, ('akua', 1.0): 1, ('cumshot', 1.0): 1, ('come', 1.0): 70, ('hous', 1.0): 7, ('nsn_supplement', 1.0): 1, ('effect', 1.0): 4, ('press', 1.0): 1, ('releas', 1.0): 11, ('distribut', 1.0): 1, ('result', 1.0): 2, ('link', 1.0): 18, ('remov', 1.0): 3, ('pressreleas', 1.0): 1, ('newsdistribut', 1.0): 1, ('bam', 1.0): 44, ('bestfriend', 1.0): 50, ('lot', 1.0): 87, ('warsaw', 1.0): 44, ('&lt;3', 1.0): 134, ('x46', 1.0): 1, ('everyon', 1.0): 58, ('watch', 1.0): 46, ('documentari', 1.0): 1, ('earthl', 1.0): 2, ('youtub', 1.0): 13, ('support', 1.0): 27, ('buuut', 1.0): 1, ('oh', 1.0): 53, ('look', 1.0): 137, ('forward', 1.0): 29, ('visit', 1.0): 30, ('next', 1.0): 48, ('letsgetmessi', 1.0): 1, ('jo', 1.0): 1, ('make', 1.0): 99, ('feel', 1.0): 46, ('better', 1.0): 52, ('never', 1.0): 36, ('anyon', 1.0): 11, ('kpop', 1.0): 1, ('flesh', 1.0): 1, ('good', 1.0): 238, ('girl', 1.0): 44, ('best', 1.0): 65, ('wish', 1.0): 37, ('reason', 1.0): 13, ('epic', 1.0): 2, ('soundtrack', 1.0): 1, ('shout', 1.0): 12, ('ad', 1.0): 14, ('video', 1.0): 34, ('playlist', 1.0): 5, ('would', 1.0): 84, ('dear', 1.0): 17, ('jordan', 1.0): 1, ('okay', 1.0): 39, ('fake', 1.0): 2, ('gameplay', 1.0): 2, (';)', 1.0): 27, ('haha', 1.0): 53, ('im', 1.0): 51, ('kid', 1.0): 18, ('stuff', 1.0): 13, ('exactli', 1.0): 6, ('product', 1.0): 12, ('line', 1.0): 6, ('etsi', 1.0): 1, ('shop', 1.0): 16, ('check', 1.0): 52, ('vacat', 1.0): 6, ('recharg', 1.0): 1, ('normal', 1.0): 6, ('charger', 1.0): 2, ('asleep', 1.0): 9, ('talk', 1.0): 45, ('sooo', 1.0): 6, ('someon', 1.0): 34, ('text', 1.0): 18, ('ye', 1.0): 77, ('bet', 1.0): 6, (\"he'll\", 1.0): 4, ('fit', 1.0): 3, ('hear', 1.0): 33, ('speech', 1.0): 1, ('piti', 1.0): 3, ('green', 1.0): 3, ('garden', 1.0): 7, ('midnight', 1.0): 1, ('sun', 1.0): 6, ('beauti', 1.0): 50, ('canal', 1.0): 1, ('dasvidaniya', 1.0): 1, ('till', 1.0): 18, ('scout', 1.0): 1, ('sg', 1.0): 1, ('futur', 1.0): 13, ('wlan', 1.0): 1, ('pro', 1.0): 5, ('confer', 1.0): 1, ('asia', 1.0): 1, ('chang', 1.0): 24, ('lollipop', 1.0): 1, ('🍭', 1.0): 1, ('nez', 1.0): 1, ('agnezmo', 1.0): 1, ('oley', 1.0): 1, ('mama', 1.0): 1, ('stand', 1.0): 8, ('stronger', 1.0): 1, ('god', 1.0): 20, ('misti', 1.0): 1, ('babi', 1.0): 20, ('cute', 1.0): 26, ('woohoo', 1.0): 3, (\"can't\", 1.0): 43, ('sign', 1.0): 11, ('yet', 1.0): 13, ('still', 1.0): 48, ('think', 1.0): 63, ('mka', 1.0): 5, ('liam', 1.0): 8, ('access', 1.0): 3, ('welcom', 1.0): 73, ('stat', 1.0): 60, ('arriv', 1.0): 67, ('1', 1.0): 75, ('unfollow', 1.0): 63, ('via', 1.0): 69, ('surpris', 1.0): 10, ('figur', 1.0): 5, ('happybirthdayemilybett', 1.0): 1, ('sweet', 1.0): 19, ('talent', 1.0): 5, ('2', 1.0): 58, ('plan', 1.0): 27, ('drain', 1.0): 1, ('gotta', 1.0): 5, ('timezon', 1.0): 1, ('parent', 1.0): 5, ('proud', 1.0): 12, ('least', 1.0): 16, ('mayb', 1.0): 18, ('sometim', 1.0): 13, ('grade', 1.0): 4, ('al', 1.0): 4, ('grand', 1.0): 4, ('manila_bro', 1.0): 2, ('chosen', 1.0): 1, ('let', 1.0): 68, ('around', 1.0): 17, ('..', 1.0): 128, ('side', 1.0): 15, ('world', 1.0): 27, ('eh', 1.0): 2, ('take', 1.0): 43, ('care', 1.0): 18, ('final', 1.0): 30, ('fuck', 1.0): 26, ('weekend', 1.0): 75, ('real', 1.0): 21, ('x45', 1.0): 1, ('join', 1.0): 23, ('hushedcallwithfraydo', 1.0): 1, ('gift', 1.0): 8, ('yeahhh', 1.0): 1, ('hushedpinwithsammi', 1.0): 2, ('event', 1.0): 8, ('might', 1.0): 27, ('luv', 1.0): 6, ('realli', 1.0): 79, ('appreci', 1.0): 31, ('share', 1.0): 46, ('wow', 1.0): 22, ('tom', 1.0): 5, ('gym', 1.0): 4, ('monday', 1.0): 9, ('invit', 1.0): 17, ('scope', 1.0): 5, ('friend', 1.0): 61, ('nude', 1.0): 2, ('sleep', 1.0): 45, ('birthday', 1.0): 74, ('want', 1.0): 96, ('t-shirt', 1.0): 3, ('cool', 1.0): 38, ('haw', 1.0): 1, ('phela', 1.0): 1, ('mom', 1.0): 10, ('obvious', 1.0): 2, ('princ', 1.0): 1, ('charm', 1.0): 1, ('stage', 1.0): 2, ('luck', 1.0): 30, ('tyler', 1.0): 2, ('hipster', 1.0): 1, ('glass', 1.0): 5, ('marti', 1.0): 2, ('glad', 1.0): 43, ('done', 1.0): 54, ('afternoon', 1.0): 10, ('read', 1.0): 34, ('kahfi', 1.0): 1, ('finish', 1.0): 17, ('ohmyg', 1.0): 1, ('yaya', 1.0): 3, ('dub', 1.0): 2, ('stalk', 1.0): 2, ('ig', 1.0): 3, ('gondooo', 1.0): 1, ('moo', 1.0): 2, ('tologooo', 1.0): 1, ('becom', 1.0): 10, ('detail', 1.0): 10, ('zzz', 1.0): 1, ('xx', 1.0): 42, ('physiotherapi', 1.0): 1, ('hashtag', 1.0): 5, ('💪', 1.0): 1, ('monica', 1.0): 1, ('miss', 1.0): 27, ('sound', 1.0): 23, ('morn', 1.0): 101, (\"that'\", 1.0): 67, ('x43', 1.0): 1, ('definit', 1.0): 23, ('tri', 1.0): 44, ('tonight', 1.0): 20, ('took', 1.0): 8, ('advic', 1.0): 6, ('treviso', 1.0): 1, ('concert', 1.0): 24, ('citi', 1.0): 27, ('countri', 1.0): 23, (\"i'll\", 1.0): 90, ('start', 1.0): 61, ('fine', 1.0): 10, ('gorgeou', 1.0): 12, ('xo', 1.0): 2, ('oven', 1.0): 3, ('roast', 1.0): 2, ('garlic', 1.0): 1, ('oliv', 1.0): 1, ('oil', 1.0): 4, ('dri', 1.0): 5, ('tomato', 1.0): 1, ('basil', 1.0): 1, ('centuri', 1.0): 1, ('tuna', 1.0): 1, ('right', 1.0): 47, ('back', 1.0): 98, ('atchya', 1.0): 1, ('even', 1.0): 35, ('almost', 1.0): 10, ('chanc', 1.0): 6, ('cheer', 1.0): 20, ('po', 1.0): 4, ('ice', 1.0): 6, ('cream', 1.0): 6, ('agre', 1.0): 16, ('100', 1.0): 8, ('heheheh', 1.0): 2, ('that', 1.0): 13, ('point', 1.0): 13, ('stay', 1.0): 25, ('home', 1.0): 31, ('soon', 1.0): 47, ('promis', 1.0): 6, ('web', 1.0): 4, ('whatsapp', 1.0): 5, ('volta', 1.0): 1, ('funcionar', 1.0): 1, ('com', 1.0): 2, ('iphon', 1.0): 7, ('jailbroken', 1.0): 1, ('later', 1.0): 16, ('34', 1.0): 3, ('min', 1.0): 9, ('leia', 1.0): 1, ('appear', 1.0): 3, ('hologram', 1.0): 1, ('r2d2', 1.0): 1, ('w', 1.0): 18, ('messag', 1.0): 10, ('obi', 1.0): 1, ('wan', 1.0): 3, ('sit', 1.0): 8, ('luke', 1.0): 6, ('inter', 1.0): 1, ('3', 1.0): 31, ('ucl', 1.0): 1, ('arsen', 1.0): 2, ('small', 1.0): 4, ('team', 1.0): 29, ('pass', 1.0): 12, ('🚂', 1.0): 1, ('dewsburi', 1.0): 2, ('railway', 1.0): 1, ('station', 1.0): 4, ('dew', 1.0): 1, ('west', 1.0): 3, ('yorkshir', 1.0): 2, ('430', 1.0): 1, ('smh', 1.0): 2, ('9:25', 1.0): 1, ('live', 1.0): 26, ('strang', 1.0): 4, ('imagin', 1.0): 5, ('megan', 1.0): 1, ('masaantoday', 1.0): 6, ('a4', 1.0): 3, ('shweta', 1.0): 1, ('tripathi', 1.0): 1, ('5', 1.0): 17, ('20', 1.0): 6, ('kurta', 1.0): 3, ('half', 1.0): 7, ('number', 1.0): 13, ('wsalelov', 1.0): 16, ('ah', 1.0): 13, ('larri', 1.0): 3, ('anyway', 1.0): 16, ('kinda', 1.0): 13, ('goood', 1.0): 4, ('life', 1.0): 49, ('enn', 1.0): 1, ('could', 1.0): 32, ('warmup', 1.0): 1, ('15th', 1.0): 2, ('bath', 1.0): 7, ('dum', 1.0): 2, ('andar', 1.0): 1, ('ram', 1.0): 1, ('sampath', 1.0): 1, ('sona', 1.0): 1, ('mohapatra', 1.0): 1, ('samantha', 1.0): 1, ('edward', 1.0): 1, ('mein', 1.0): 1, ('tulan', 1.0): 1, ('razi', 1.0): 2, ('wah', 1.0): 2, ('josh', 1.0): 1, ('alway', 1.0): 67, ('smile', 1.0): 62, ('pictur', 1.0): 12, ('16.20', 1.0): 1, ('giveitup', 1.0): 1, ('given', 1.0): 3, ('ga', 1.0): 3, ('subsidi', 1.0): 1, ('initi', 1.0): 4, ('propos', 1.0): 3, ('delight', 1.0): 7, ('yesterday', 1.0): 7, ('x42', 1.0): 1, ('lmaoo', 1.0): 2, ('song', 1.0): 22, ('ever', 1.0): 23, ('shall', 1.0): 6, ('littl', 1.0): 31, ('throwback', 1.0): 3, ('outli', 1.0): 1, ('island', 1.0): 5, ('cheung', 1.0): 1, ('chau', 1.0): 1, ('mui', 1.0): 1, ('wo', 1.0): 1, ('total', 1.0): 9, ('differ', 1.0): 11, ('kfckitchentour', 1.0): 2, ('kitchen', 1.0): 4, ('clean', 1.0): 1, (\"i'm\", 1.0): 183, ('cusp', 1.0): 1, ('test', 1.0): 7, ('water', 1.0): 8, ('reward', 1.0): 1, ('arummzz', 1.0): 2, (\"let'\", 1.0): 23, ('drive', 1.0): 11, ('travel', 1.0): 20, ('yogyakarta', 1.0): 3, ('jeep', 1.0): 3, ('indonesia', 1.0): 4, ('instamood', 1.0): 3, ('wanna', 1.0): 30, ('skype', 1.0): 3, ('may', 1.0): 22, ('nice', 1.0): 98, ('friendli', 1.0): 2, ('pretend', 1.0): 2, ('film', 1.0): 9, ('congratul', 1.0): 15, ('winner', 1.0): 4, ('cheesydelight', 1.0): 1, ('contest', 1.0): 6, ('address', 1.0): 10, ('guy', 1.0): 60, ('market', 1.0): 5, ('24/7', 1.0): 1, ('14', 1.0): 1, ('hour', 1.0): 27, ('leav', 1.0): 12, ('without', 1.0): 12, ('delay', 1.0): 2, ('actual', 1.0): 19, ('easi', 1.0): 9, ('guess', 1.0): 14, ('train', 1.0): 10, ('wd', 1.0): 1, ('shift', 1.0): 5, ('engin', 1.0): 2, ('etc', 1.0): 2, ('sunburn', 1.0): 1, ('peel', 1.0): 2, ('blog', 1.0): 31, ('huge', 1.0): 11, ('warm', 1.0): 6, ('☆', 1.0): 3, ('complet', 1.0): 11, ('triangl', 1.0): 2, ('northern', 1.0): 1, ('ireland', 1.0): 2, ('sight', 1.0): 1, ('smthng', 1.0): 2, ('fr', 1.0): 3, ('hug', 1.0): 13, ('xoxo', 1.0): 3, ('uu', 1.0): 1, ('jaann', 1.0): 1, ('topnewfollow', 1.0): 2, ('connect', 1.0): 14, ('wonder', 1.0): 35, ('made', 1.0): 53, ('fluffi', 1.0): 1, ('insid', 1.0): 8, ('pirouett', 1.0): 1, ('moos', 1.0): 1, ('trip', 1.0): 14, ('philli', 1.0): 1, ('decemb', 1.0): 3, (\"i'd\", 1.0): 20, ('dude', 1.0): 6, ('x41', 1.0): 1, ('question', 1.0): 17, ('flaw', 1.0): 1, ('pain', 1.0): 9, ('negat', 1.0): 1, ('strength', 1.0): 3, ('went', 1.0): 12, ('solo', 1.0): 4, ('move', 1.0): 12, ('fav', 1.0): 13, ('nirvana', 1.0): 1, ('smell', 1.0): 2, ('teen', 1.0): 3, ('spirit', 1.0): 3, ('rip', 1.0): 3, ('ami', 1.0): 4, ('winehous', 1.0): 1, ('coupl', 1.0): 9, ('tomhiddleston', 1.0): 1, ('elizabetholsen', 1.0): 1, ('yaytheylookgreat', 1.0): 1, ('goodnight', 1.0): 24, ('vid', 1.0): 11, ('wake', 1.0): 12, ('gonna', 1.0): 21, ('shoot', 1.0): 6, ('itti', 1.0): 2, ('bitti', 1.0): 2, ('teeni', 1.0): 2, ('bikini', 1.0): 3, ('much', 1.0): 89, ('4th', 1.0): 4, ('togeth', 1.0): 7, ('end', 1.0): 20, ('xfile', 1.0): 1, ('content', 1.0): 4, ('rain', 1.0): 21, ('fabul', 1.0): 5, ('fantast', 1.0): 13, ('♡', 1.0): 20, ('jb', 1.0): 1, ('forev', 1.0): 5, ('belieb', 1.0): 3, ('nighti', 1.0): 1, ('bug', 1.0): 3, ('bite', 1.0): 1, ('bracelet', 1.0): 2, ('idea', 1.0): 26, ('foundri', 1.0): 1, ('game', 1.0): 27, ('sens', 1.0): 7, ('pic', 1.0): 27, ('ef', 1.0): 1, ('phone', 1.0): 19, ('woot', 1.0): 2, ('derek', 1.0): 1, ('use', 1.0): 44, ('parkshar', 1.0): 1, ('gloucestershir', 1.0): 1, ('aaaahhh', 1.0): 1, ('man', 1.0): 23, ('traffic', 1.0): 2, ('stress', 1.0): 8, ('reliev', 1.0): 1, (\"how'r\", 1.0): 1, ('arbeloa', 1.0): 1, ('turn', 1.0): 15, ('17', 1.0): 4, ('omg', 1.0): 15, ('say', 1.0): 61, ('europ', 1.0): 1, ('rise', 1.0): 2, ('find', 1.0): 23, ('hard', 1.0): 12, ('believ', 1.0): 9, ('uncount', 1.0): 1, ('coz', 1.0): 3, ('unlimit', 1.0): 1, ('cours', 1.0): 18, ('teamposit', 1.0): 1, ('aldub', 1.0): 2, ('☕', 1.0): 3, ('rita', 1.0): 2, ('info', 1.0): 13, (\"we'd\", 1.0): 4, ('way', 1.0): 46, ('boy', 1.0): 21, ('x40', 1.0): 1, ('true', 1.0): 22, ('sethi', 1.0): 2, ('high', 1.0): 7, ('exe', 1.0): 1, ('skeem', 1.0): 1, ('saam', 1.0): 1, ('peopl', 1.0): 48, ('polit', 1.0): 2, ('izzat', 1.0): 1, ('wese', 1.0): 1, ('trust', 1.0): 9, ('khawateen', 1.0): 1, ('k', 1.0): 9, ('sath', 1.0): 2, ('mana', 1.0): 1, ('kar', 1.0): 1, ('deya', 1.0): 1, ('sort', 1.0): 9, ('smart', 1.0): 5, ('hair', 1.0): 12, ('tbh', 1.0): 5, ('jacob', 1.0): 2, ('g', 1.0): 10, ('upgrad', 1.0): 6, ('tee', 1.0): 2, ('famili', 1.0): 19, ('person', 1.0): 19, ('two', 1.0): 22, ('convers', 1.0): 6, ('onlin', 1.0): 7, ('mclaren', 1.0): 1, ('fridayfeel', 1.0): 5, ('tgif', 1.0): 10, ('squar', 1.0): 1, ('enix', 1.0): 1, ('bissmillah', 1.0): 1, ('ya', 1.0): 23, ('allah', 1.0): 3, (\"we'r\", 1.0): 29, ('socent', 1.0): 1, ('startup', 1.0): 2, ('drop', 1.0): 9, ('your', 1.0): 3, ('arnd', 1.0): 1, ('town', 1.0): 5, ('basic', 1.0): 4, ('piss', 1.0): 3, ('cup', 1.0): 4, ('also', 1.0): 35, ('terribl', 1.0): 2, ('complic', 1.0): 1, ('discuss', 1.0): 3, ('snapchat', 1.0): 36, ('lynettelow', 1.0): 1, ('kikmenow', 1.0): 3, ('snapm', 1.0): 2, ('hot', 1.0): 24, ('amazon', 1.0): 1, ('kikmeguy', 1.0): 3, ('defin', 1.0): 2, ('grow', 1.0): 7, ('sport', 1.0): 4, ('rt', 1.0): 12, ('rakyat', 1.0): 1, ('write', 1.0): 13, ('sinc', 1.0): 15, ('mention', 1.0): 24, ('fli', 1.0): 5, ('fish', 1.0): 3, ('promot', 1.0): 5, ('post', 1.0): 21, ('cyber', 1.0): 1, ('ourdaughtersourprid', 1.0): 5, ('mypapamyprid', 1.0): 2, ('papa', 1.0): 2, ('coach', 1.0): 2, ('posit', 1.0): 8, ('kha', 1.0): 1, ('atleast', 1.0): 2, ('x39', 1.0): 1, ('mango', 1.0): 1, (\"lassi'\", 1.0): 1, (\"monty'\", 1.0): 1, ('marvel', 1.0): 2, ('though', 1.0): 19, ('suspect', 1.0): 3, ('meant', 1.0): 3, ('24', 1.0): 4, ('hr', 1.0): 2, ('touch', 1.0): 15, ('kepler', 1.0): 4, ('452b', 1.0): 5, ('chalna', 1.0): 1, ('hai', 1.0): 11, ('thankyou', 1.0): 14, ('hazel', 1.0): 1, ('food', 1.0): 6, ('brooklyn', 1.0): 1, ('pta', 1.0): 2, ('awak', 1.0): 10, ('okayi', 1.0): 2, ('awww', 1.0): 15, ('ha', 1.0): 23, ('doc', 1.0): 1, ('splendid', 1.0): 1, ('spam', 1.0): 1, ('folder', 1.0): 1, ('amount', 1.0): 1, ('nigeria', 1.0): 1, ('claim', 1.0): 1, ('rted', 1.0): 1, ('leg', 1.0): 5, ('hurt', 1.0): 8, ('bad', 1.0): 18, ('mine', 1.0): 14, ('saturday', 1.0): 8, ('thaaank', 1.0): 1, ('puhon', 1.0): 1, ('happinesss', 1.0): 1, ('tnc', 1.0): 1, ('prior', 1.0): 1, ('notif', 1.0): 2, ('fat', 1.0): 1, ('co', 1.0): 1, ('probabl', 1.0): 9, ('ate', 1.0): 4, ('yuna', 1.0): 2, ('tamesid', 1.0): 1, ('´', 1.0): 3, ('googl', 1.0): 6, ('account', 1.0): 19, ('scouser', 1.0): 1, ('everyth', 1.0): 13, ('zoe', 1.0): 2, ('mate', 1.0): 7, ('liter', 1.0): 6, (\"they'r\", 1.0): 12, ('samee', 1.0): 1, ('edgar', 1.0): 1, ('updat', 1.0): 13, ('log', 1.0): 4, ('bring', 1.0): 17, ('abe', 1.0): 1, ('meet', 1.0): 34, ('x38', 1.0): 1, ('sigh', 1.0): 3, ('dreamili', 1.0): 1, ('pout', 1.0): 1, ('eye', 1.0): 14, ('quacketyquack', 1.0): 7, ('funni', 1.0): 19, ('happen', 1.0): 16, ('phil', 1.0): 1, ('em', 1.0): 3, ('del', 1.0): 1, ('rodder', 1.0): 1, ('els', 1.0): 10, ('play', 1.0): 46, ('newest', 1.0): 1, ('gamejam', 1.0): 1, ('irish', 1.0): 2, ('literatur', 1.0): 2, ('inaccess', 1.0): 2, (\"kareena'\", 1.0): 2, ('fan', 1.0): 30, ('brain', 1.0): 13, ('dot', 1.0): 11, ('braindot', 1.0): 11, ('fair', 1.0): 5, ('rush', 1.0): 1, ('either', 1.0): 11, ('brandi', 1.0): 1, ('18', 1.0): 5, ('carniv', 1.0): 1, ('men', 1.0): 10, ('put', 1.0): 17, ('mask', 1.0): 3, ('xavier', 1.0): 1, ('forneret', 1.0): 1, ('jennif', 1.0): 1, ('site', 1.0): 9, ('free', 1.0): 37, ('50.000', 1.0): 3, ('8', 1.0): 10, ('ball', 1.0): 7, ('pool', 1.0): 5, ('coin', 1.0): 5, ('edit', 1.0): 7, ('trish', 1.0): 1, ('♥', 1.0): 19, ('grate', 1.0): 5, ('three', 1.0): 10, ('comment', 1.0): 8, ('wakeup', 1.0): 1, ('besid', 1.0): 2, ('dirti', 1.0): 2, ('sex', 1.0): 6, ('lmaooo', 1.0): 1, ('😤', 1.0): 2, ('loui', 1.0): 4, (\"he'\", 1.0): 11, ('throw', 1.0): 3, ('caus', 1.0): 15, ('inspir', 1.0): 7, ('ff', 1.0): 48, ('twoof', 1.0): 3, ('gr8', 1.0): 1, ('wkend', 1.0): 3, ('kind', 1.0): 24, ('exhaust', 1.0): 2, ('word', 1.0): 20, ('cheltenham', 1.0): 1, ('area', 1.0): 4, ('kale', 1.0): 1, ('crisp', 1.0): 1, ('ruin', 1.0): 5, ('x37', 1.0): 1, ('open', 1.0): 12, ('worldwid', 1.0): 2, ('outta', 1.0): 1, ('sfvbeta', 1.0): 1, ('vantast', 1.0): 1, ('xcylin', 1.0): 1, ('bundl', 1.0): 1, ('show', 1.0): 28, ('internet', 1.0): 2, ('price', 1.0): 4, ('realisticli', 1.0): 1, ('pay', 1.0): 8, ('net', 1.0): 1, ('educ', 1.0): 1, ('power', 1.0): 7, ('weapon', 1.0): 1, ('nelson', 1.0): 1, ('mandela', 1.0): 1, ('recent', 1.0): 9, ('j', 1.0): 3, ('chenab', 1.0): 1, ('flow', 1.0): 5, ('pakistan', 1.0): 2, ('incredibleindia', 1.0): 1, ('teenchoic', 1.0): 10, ('choiceinternationalartist', 1.0): 9, ('superjunior', 1.0): 9, ('caught', 1.0): 4, ('first', 1.0): 50, ('salmon', 1.0): 3, ('super-blend', 1.0): 1, ('project', 1.0): 6, ('youth@bipolaruk.org.uk', 1.0): 1, ('awesom', 1.0): 42, ('stream', 1.0): 14, ('alma', 1.0): 1, ('mater', 1.0): 1, ('highschoolday', 1.0): 1, ('clientvisit', 1.0): 1, ('faith', 1.0): 3, ('christian', 1.0): 1, ('school', 1.0): 9, ('lizaminnelli', 1.0): 1, ('upcom', 1.0): 2, ('uk', 1.0): 4, ('😄', 1.0): 5, ('singl', 1.0): 6, ('hill', 1.0): 4, ('everi', 1.0): 26, ('beat', 1.0): 10, ('wrong', 1.0): 10, ('readi', 1.0): 25, ('natur', 1.0): 1, ('pefumeri', 1.0): 1, ('workshop', 1.0): 3, ('neal', 1.0): 1, ('yard', 1.0): 1, ('covent', 1.0): 1, ('tomorrow', 1.0): 40, ('fback', 1.0): 27, ('indo', 1.0): 1, ('harmo', 1.0): 1, ('americano', 1.0): 1, ('rememb', 1.0): 16, ('aww', 1.0): 10, ('head', 1.0): 14, ('saw', 1.0): 19, ('dark', 1.0): 6, ('handshom', 1.0): 1, ('juga', 1.0): 1, ('hurray', 1.0): 1, ('hate', 1.0): 13, ('cant', 1.0): 15, ('decid', 1.0): 4, ('save', 1.0): 12, ('list', 1.0): 15, ('hiya', 1.0): 4, ('exec', 1.0): 1, ('loryn.good@lincs-chamber.co.uk', 1.0): 1, ('photo', 1.0): 19, ('thx', 1.0): 15, ('4', 1.0): 24, ('china', 1.0): 2, ('homosexu', 1.0): 1, ('hyungbot', 1.0): 1, ('give', 1.0): 48, ('fam', 1.0): 5, ('mind', 1.0): 23, ('timetunnel', 1.0): 1, ('1982', 1.0): 1, ('quit', 1.0): 13, ('radio', 1.0): 5, ('set', 1.0): 11, ('heart', 1.0): 11, ('hiii', 1.0): 2, ('jack', 1.0): 3, ('ili', 1.0): 5, ('✨', 1.0): 4, ('domino', 1.0): 1, ('pub', 1.0): 1, ('heat', 1.0): 1, ('prob', 1.0): 5, ('sorri', 1.0): 22, ('hastili', 1.0): 1, ('type', 1.0): 6, ('came', 1.0): 7, ('pakistani', 1.0): 1, ('x36', 1.0): 1, ('3point', 1.0): 1, ('dreamteam', 1.0): 1, ('gooo', 1.0): 2, ('bailey', 1.0): 2, ('pbb', 1.0): 4, ('737gold', 1.0): 3, ('drank', 1.0): 2, ('old', 1.0): 13, ('gotten', 1.0): 2, ('1/2', 1.0): 1, ('welsh', 1.0): 1, ('wale', 1.0): 3, ('yippe', 1.0): 1, ('💟', 1.0): 4, ('bro', 1.0): 24, ('lord', 1.0): 4, ('michael', 1.0): 4, (\"u'r\", 1.0): 1, ('ure', 1.0): 1, ('bigot', 1.0): 1, ('usual', 1.0): 6, ('front', 1.0): 4, ('squat', 1.0): 1, ('dobar', 1.0): 1, ('dan', 1.0): 5, ('brand', 1.0): 8, ('heavi', 1.0): 2, ('musicolog', 1.0): 1, ('2015', 1.0): 16, ('spend', 1.0): 2, ('marathon', 1.0): 1, ('iflix', 1.0): 2, ('offici', 1.0): 10, ('graduat', 1.0): 3, ('cri', 1.0): 9, ('__', 1.0): 1, ('yep', 1.0): 9, ('expert', 1.0): 4, ('bisexu', 1.0): 1, ('minal', 1.0): 1, ('aidzin', 1.0): 1, ('yo', 1.0): 7, ('pi', 1.0): 1, ('cook', 1.0): 2, ('book', 1.0): 21, ('dinner', 1.0): 7, ('tough', 1.0): 2, ('choic', 1.0): 8, ('other', 1.0): 12, ('chill', 1.0): 6, ('smu', 1.0): 1, ('oval', 1.0): 1, ('basketbal', 1.0): 1, ('player', 1.0): 4, ('whahahaha', 1.0): 1, ('soamaz', 1.0): 1, ('moment', 1.0): 12, ('onto', 1.0): 3, ('a5', 1.0): 1, ('wardrob', 1.0): 2, ('user', 1.0): 3, ('teamr', 1.0): 1, ('appar', 1.0): 6, ('depend', 1.0): 2, ('greatli', 1.0): 1, ('design', 1.0): 21, ('ahhh', 1.0): 1, ('7th', 1.0): 1, ('cinepambata', 1.0): 1, ('mechan', 1.0): 1, ('form', 1.0): 4, ('download', 1.0): 10, ('ur', 1.0): 38, ('swisher', 1.0): 1, ('cop', 1.0): 1, ('ducktail', 1.0): 1, ('surreal', 1.0): 3, ('exposur', 1.0): 1, ('sotw', 1.0): 1, ('halesowen', 1.0): 1, ('blackcountryfair', 1.0): 1, ('street', 1.0): 1, ('assess', 1.0): 1, ('mental', 1.0): 4, ('bodi', 1.0): 15, ('ooz', 1.0): 1, ('appeal', 1.0): 1, ('amassiveoverdoseofship', 1.0): 1, ('latest', 1.0): 5, ('isi', 1.0): 1, ('chan', 1.0): 1, ('c', 1.0): 9, ('note', 1.0): 6, ('pkwalasawa', 1.0): 1, ('gemma', 1.0): 1, ('orlean', 1.0): 1, ('fever', 1.0): 2, ('geskenya', 1.0): 1, ('obamainkenya', 1.0): 1, ('magicalkenya', 1.0): 1, ('greatkenya', 1.0): 1, ('allgoodthingsk', 1.0): 1, ('anim', 1.0): 6, ('umaru', 1.0): 1, ('singer', 1.0): 4, ('ship', 1.0): 8, ('order', 1.0): 17, ('room', 1.0): 5, ('car', 1.0): 6, ('gone', 1.0): 5, ('hahaha', 1.0): 14, ('stori', 1.0): 11, ('relat', 1.0): 4, ('label', 1.0): 1, ('worst', 1.0): 3, ('batch', 1.0): 1, ('princip', 1.0): 1, ('due', 1.0): 3, ('march', 1.0): 1, ('wooftast', 1.0): 2, ('receiv', 1.0): 8, ('necessari', 1.0): 1, ('regret', 1.0): 4, ('rn', 1.0): 4, ('whatev', 1.0): 5, ('hat', 1.0): 1, ('success', 1.0): 6, ('abstin', 1.0): 1, ('wtf', 1.0): 3, (\"there'\", 1.0): 11, ('thrown', 1.0): 1, ('middl', 1.0): 2, ('repeat', 1.0): 3, ('relentlessli', 1.0): 1, ('approxim', 1.0): 1, ('oldschool', 1.0): 1, ('runescap', 1.0): 1, ('daaay', 1.0): 1, ('jumma_mubarik', 1.0): 1, ('frnd', 1.0): 1, ('stay_bless', 1.0): 1, ('bless', 1.0): 12, ('pussycat', 1.0): 1, ('main', 1.0): 7, ('launch', 1.0): 4, ('pretoria', 1.0): 1, ('fahrinahmad', 1.0): 1, ('tengkuaaronshah', 1.0): 1, ('eksperimencinta', 1.0): 1, ('tykkäsin', 1.0): 1, ('videosta', 1.0): 1, ('month', 1.0): 13, ('hoodi', 1.0): 2, ('eeep', 1.0): 1, ('yay', 1.0): 16, ('sohappyrightnow', 1.0): 1, ('mmm', 1.0): 1, ('azz-set', 1.0): 1, ('babe', 1.0): 9, ('feedback', 1.0): 11, ('gain', 1.0): 6, ('valu', 1.0): 2, ('peac', 1.0): 8, ('refresh', 1.0): 5, ('manthan', 1.0): 1, ('tune', 1.0): 5, ('fresh', 1.0): 6, ('mother', 1.0): 5, ('determin', 1.0): 2, ('maxfreshmov', 1.0): 2, ('loneliest', 1.0): 1, ('tattoo', 1.0): 3, ('friday.and', 1.0): 1, ('magnific', 1.0): 2, ('e', 1.0): 5, ('achiev', 1.0): 2, ('rashmi', 1.0): 1, ('dedic', 1.0): 2, ('happyfriday', 1.0): 6, ('nearli', 1.0): 4, ('retweet', 1.0): 35, ('alert', 1.0): 1, ('da', 1.0): 5, ('dang', 1.0): 2, ('rad', 1.0): 2, ('fanart', 1.0): 1, ('massiv', 1.0): 1, ('niamh', 1.0): 1, ('fennel', 1.0): 1, ('journal', 1.0): 1, ('land', 1.0): 2, ('copi', 1.0): 5, ('past', 1.0): 7, ('tweet', 1.0): 61, ('yesss', 1.0): 5, ('ariana', 1.0): 2, ('selena', 1.0): 2, ('gomez', 1.0): 1, ('tomlinson', 1.0): 1, ('payn', 1.0): 1, ('caradelevingn', 1.0): 1, ('🌷', 1.0): 1, ('trade', 1.0): 3, ('tire', 1.0): 5, ('nope', 1.0): 7, ('appli', 1.0): 6, ('iamca', 1.0): 1, ('found', 1.0): 15, ('afti', 1.0): 1, ('goodmorn', 1.0): 8, ('prokabaddi', 1.0): 1, ('koel', 1.0): 1, ('mallick', 1.0): 1, ('recit', 1.0): 4, ('nation', 1.0): 3, ('anthem', 1.0): 1, ('6', 1.0): 23, ('yournaturallead', 1.0): 1, ('youngnaturallead', 1.0): 1, ('mon', 1.0): 3, ('27juli', 1.0): 1, ('cumbria', 1.0): 1, ('flockstar', 1.0): 1, ('thur', 1.0): 2, ('30juli', 1.0): 1, ('itv', 1.0): 1, ('sleeptight', 1.0): 1, ('haveagoodday', 1.0): 1, ('septemb', 1.0): 5, ('perhap', 1.0): 3, ('bb', 1.0): 4, ('full', 1.0): 19, ('album', 1.0): 6, ('fulli', 1.0): 2, ('intend', 1.0): 1, ('possibl', 1.0): 7, ('attack', 1.0): 3, ('&gt;:d', 1.0): 4, ('bird', 1.0): 4, ('teamadmicro', 1.0): 1, ('fridaydownpour', 1.0): 1, ('clear', 1.0): 4, ('rohit', 1.0): 1, ('queen', 1.0): 8, ('otwolgrandtrail', 1.0): 3, ('sheer', 1.0): 1, ('fact', 1.0): 8, ('obama', 1.0): 1, ('innumer', 1.0): 1, ('presid', 1.0): 2, ('ni', 1.0): 3, ('shauri', 1.0): 1, ('yako', 1.0): 1, ('memotohat', 1.0): 1, ('sunday', 1.0): 9, ('pamper', 1.0): 2, (\"t'wa\", 1.0): 1, ('cabincrew', 1.0): 1, ('interview', 1.0): 5, ('langkawi', 1.0): 1, ('1st', 1.0): 1, ('august', 1.0): 7, ('fulfil', 1.0): 5, ('fantasi', 1.0): 6, ('👉', 1.0): 6, ('ex-tweleb', 1.0): 1, ('apart', 1.0): 2, ('makeov', 1.0): 1, ('brilliantli', 1.0): 1, ('happyyi', 1.0): 1, ('birthdaaayyy', 1.0): 2, ('kill', 1.0): 3, ('interest', 1.0): 20, ('internship', 1.0): 3, ('program', 1.0): 5, ('sadli', 1.0): 1, ('career', 1.0): 3, ('page', 1.0): 9, ('issu', 1.0): 10, ('sad', 1.0): 5, ('overwhelmingli', 1.0): 1, ('aha', 1.0): 2, ('beaut', 1.0): 2, ('♬', 1.0): 2, ('win', 1.0): 16, ('deo', 1.0): 1, ('faaabul', 1.0): 1, ('freebiefriday', 1.0): 4, ('aluminiumfre', 1.0): 1, ('stayfresh', 1.0): 1, ('john', 1.0): 6, ('worri', 1.0): 18, ('navig', 1.0): 1, ('thnk', 1.0): 1, ('progrmr', 1.0): 1, ('9pm', 1.0): 1, ('9am', 1.0): 2, ('hardli', 1.0): 1, ('rose', 1.0): 4, ('emot', 1.0): 3, ('poetri', 1.0): 1, ('frequentfly', 1.0): 1, ('break', 1.0): 10, ('apolog', 1.0): 4, ('kb', 1.0): 1, ('londondairi', 1.0): 1, ('icecream', 1.0): 2, ('experi', 1.0): 7, ('cover', 1.0): 9, ('sin', 1.0): 1, ('excit', 1.0): 33, (\":')\", 1.0): 2, ('xxx', 1.0): 15, ('jim', 1.0): 1, ('chuckl', 1.0): 1, ('cake', 1.0): 10, ('doh', 1.0): 1, ('500', 1.0): 2, ('subscrib', 1.0): 2, ('reach', 1.0): 1, ('scorch', 1.0): 1, ('summer', 1.0): 17, ('younger', 1.0): 4, ('woman', 1.0): 4, ('stamina', 1.0): 1, ('expect', 1.0): 6, ('anyth', 1.0): 22, ('less', 1.0): 8, ('tweeti', 1.0): 1, ('fab', 1.0): 12, ('dont', 1.0): 13, ('--&gt;', 1.0): 2, ('10', 1.0): 16, ('loner', 1.0): 3, ('introduc', 1.0): 3, ('vs', 1.0): 4, ('alter', 1.0): 1, ('understand', 1.0): 6, ('spread', 1.0): 8, ('problem', 1.0): 19, ('supa', 1.0): 1, ('dupa', 1.0): 1, ('near', 1.0): 6, ('dartmoor', 1.0): 1, ('gold', 1.0): 7, ('colour', 1.0): 4, ('ok', 1.0): 38, ('someday', 1.0): 4, ('r', 1.0): 14, ('dii', 1.0): 1, ('n', 1.0): 17, ('forget', 1.0): 17, ('si', 1.0): 4, ('smf', 1.0): 1, ('ft', 1.0): 4, ('japanes', 1.0): 3, ('import', 1.0): 5, ('kitti', 1.0): 1, ('match', 1.0): 6, ('stationari', 1.0): 1, ('draw', 1.0): 6, ('close', 1.0): 14, ('broken', 1.0): 3, ('specialis', 1.0): 4, ('thermal', 1.0): 4, ('imag', 1.0): 6, ('survey', 1.0): 4, ('–', 1.0): 14, ('south', 1.0): 2, ('korea', 1.0): 3, ('scamper', 1.0): 1, ('slept', 1.0): 4, ('alarm', 1.0): 1, (\"ain't\", 1.0): 5, ('mad', 1.0): 4, ('chweina', 1.0): 1, ('xd', 1.0): 4, ('jotzh', 1.0): 1, ('wast', 1.0): 7, ('place', 1.0): 21, ('worth', 1.0): 11, ('coat', 1.0): 3, ('beforehand', 1.0): 1, ('tho', 1.0): 12, ('foh', 1.0): 2, ('outsid', 1.0): 5, ('holiday', 1.0): 11, ('menac', 1.0): 1, ('jojo', 1.0): 2, ('ta', 1.0): 2, ('accept', 1.0): 1, ('admin', 1.0): 2, ('lukri', 1.0): 1, ('😘', 1.0): 10, ('momma', 1.0): 2, ('bear', 1.0): 2, ('❤', 1.0): 29, ('️', 1.0): 20, ('redid', 1.0): 1, ('8th', 1.0): 1, ('v.ball', 1.0): 1, ('atm', 1.0): 4, ('build', 1.0): 8, ('pack', 1.0): 8, ('suitcas', 1.0): 2, ('hang-copi', 1.0): 1, ('translat', 1.0): 1, (\"dostoevsky'\", 1.0): 1, ('voucher', 1.0): 2, ('bugatti', 1.0): 1, ('bra', 1.0): 3, ('مطعم_هاشم', 1.0): 1, ('yummi', 1.0): 3, ('a7la', 1.0): 1, ('bdayt', 1.0): 1, ('mnwreeen', 1.0): 1, ('jazz', 1.0): 2, ('truck', 1.0): 1, ('x34', 1.0): 1, ('speak', 1.0): 8, ('pbevent', 1.0): 1, ('hq', 1.0): 1, ('add', 1.0): 22, ('yoona', 1.0): 1, ('hairpin', 1.0): 1, ('otp', 1.0): 1, ('collect', 1.0): 7, ('mastership', 1.0): 1, ('honey', 1.0): 4, ('paindo', 1.0): 1, ('await', 1.0): 1, ('report', 1.0): 3, ('manni', 1.0): 1, ('asshol', 1.0): 3, ('brijresid', 1.0): 1, ('structur', 1.0): 1, ('156', 1.0): 1, ('unit', 1.0): 3, ('encompass', 1.0): 1, ('bhk', 1.0): 1, ('flat', 1.0): 2, ('91', 1.0): 2, ('975-580-', 1.0): 1, ('444', 1.0): 1, ('honor', 1.0): 2, ('curri', 1.0): 2, ('clash', 1.0): 1, ('milano', 1.0): 1, ('👌', 1.0): 1, ('followback', 1.0): 6, (':-d', 1.0): 5, ('legit', 1.0): 1, ('loser', 1.0): 5, ('gass', 1.0): 1, ('dead', 1.0): 4, ('starsquad', 1.0): 4, ('⭐', 1.0): 3, ('news', 1.0): 25, ('utc', 1.0): 1, ('flume', 1.0): 1, ('kaytranada', 1.0): 1, ('alunageorg', 1.0): 1, ('ticket', 1.0): 12, ('km', 1.0): 1, ('certainti', 1.0): 1, ('solv', 1.0): 2, ('faster', 1.0): 3, ('👊', 1.0): 1, ('hurri', 1.0): 5, ('totem', 1.0): 1, ('somewher', 1.0): 5, ('alic', 1.0): 4, ('dog', 1.0): 6, ('cat', 1.0): 5, ('goodwynsgoodi', 1.0): 1, ('ugh', 1.0): 1, ('fade', 1.0): 2, ('moan', 1.0): 1, ('leed', 1.0): 1, ('jozi', 1.0): 1, ('wasnt', 1.0): 2, ('fifth', 1.0): 2, ('avail', 1.0): 10, ('tix', 1.0): 2, ('pa', 1.0): 2, ('ba', 1.0): 2, ('ng', 1.0): 2, ('atl', 1.0): 1, ('coldplay', 1.0): 1, ('favorit', 1.0): 14, ('scientist', 1.0): 1, ('yellow', 1.0): 2, ('atla', 1.0): 1, ('yein', 1.0): 1, ('selo', 1.0): 1, ('jabongatpumaurbanstamped', 1.0): 4, ('an', 1.0): 3, ('7', 1.0): 8, ('waiter', 1.0): 1, ('bill', 1.0): 5, ('sir', 1.0): 12, ('titl', 1.0): 2, ('pocket', 1.0): 1, ('wrip', 1.0): 1, ('jean', 1.0): 1, ('conni', 1.0): 2, ('crew', 1.0): 3, ('staff', 1.0): 2, ('sweetan', 1.0): 1, ('ask', 1.0): 37, ('mum', 1.0): 2, ('beg', 1.0): 2, ('soprano', 1.0): 1, ('ukrain', 1.0): 2, ('x33', 1.0): 1, ('olli', 1.0): 2, ('disney.art', 1.0): 1, ('elmoprinssi', 1.0): 1, ('salsa', 1.0): 1, ('danc', 1.0): 2, ('tell', 1.0): 25, ('truth', 1.0): 4, ('pl', 1.0): 8, ('4-6', 1.0): 1, ('2nd', 1.0): 5, ('blogiversari', 1.0): 1, ('review', 1.0): 9, ('cuti', 1.0): 6, ('bohol', 1.0): 1, ('briliant', 1.0): 1, ('v', 1.0): 9, ('key', 1.0): 3, ('annual', 1.0): 1, ('far', 1.0): 19, ('spin', 1.0): 2, ('voic', 1.0): 3, ('\\U000fe334', 1.0): 1, ('yeheyi', 1.0): 1, ('pinya', 1.0): 1, ('whoooah', 1.0): 1, ('tranc', 1.0): 1, ('lover', 1.0): 4, ('subject', 1.0): 7, ('physic', 1.0): 1, ('stop', 1.0): 15, ('ब', 1.0): 1, ('matter', 1.0): 6, ('jungl', 1.0): 1, ('accommod', 1.0): 1, ('secret', 1.0): 9, ('behind', 1.0): 3, ('sandroforceo', 1.0): 2, ('ceo', 1.0): 11, ('1month', 1.0): 11, ('swag', 1.0): 1, ('mia', 1.0): 1, ('workinprogress', 1.0): 1, ('choos', 1.0): 2, ('finnigan', 1.0): 1, ('loyal', 1.0): 2, ('royal', 1.0): 2, ('fotoset', 1.0): 1, ('reus', 1.0): 1, ('seem', 1.0): 10, ('somebodi', 1.0): 1, ('sell', 1.0): 1, ('young', 1.0): 3, ('muntu', 1.0): 1, ('anoth', 1.0): 23, ('gem', 1.0): 2, ('falco', 1.0): 1, ('supersmash', 1.0): 1, ('hotnsexi', 1.0): 1, ('friskyfriday', 1.0): 1, ('beach', 1.0): 4, ('movi', 1.0): 24, ('crop', 1.0): 2, ('nash', 1.0): 1, ('tissu', 1.0): 1, ('chocol', 1.0): 7, ('tea', 1.0): 6, ('hannib', 1.0): 3, ('episod', 1.0): 5, ('hotb', 1.0): 1, ('bush', 1.0): 2, ('classicassur', 1.0): 1, ('thrill', 1.0): 2, ('intern', 1.0): 2, ('assign', 1.0): 1, ('aerial', 1.0): 1, ('camera', 1.0): 6, ('oper', 1.0): 1, ('boom', 1.0): 3, ('hong', 1.0): 1, ('kong', 1.0): 1, ('ferri', 1.0): 1, ('central', 1.0): 2, ('girlfriend', 1.0): 4, ('after-work', 1.0): 1, ('drink', 1.0): 8, ('dj', 1.0): 3, ('resto', 1.0): 1, ('drinkt', 1.0): 1, ('koffi', 1.0): 1, ('a6', 1.0): 1, ('stargat', 1.0): 1, ('atlanti', 1.0): 1, ('muaahhh', 1.0): 1, ('ohh', 1.0): 3, ('hii', 1.0): 2, ('🙈', 1.0): 1, ('di', 1.0): 5, ('nagsend', 1.0): 1, ('yung', 1.0): 1, ('ko', 1.0): 4, ('&lt;/3', 1.0): 1, ('ulit', 1.0): 3, ('🎉', 1.0): 5, ('🎈', 1.0): 1, ('ugli', 1.0): 4, ('legget', 1.0): 1, ('qui', 1.0): 1, ('per', 1.0): 1, ('la', 1.0): 8, ('mar', 1.0): 1, ('encourag', 1.0): 3, ('employ', 1.0): 5, ('board', 1.0): 5, ('sticker', 1.0): 1, ('sponsor', 1.0): 4, ('prize', 1.0): 3, ('(:', 1.0): 1, ('milo', 1.0): 1, ('aurini', 1.0): 1, ('juicebro', 1.0): 1, ('pillar', 1.0): 2, ('respect', 1.0): 2, ('boii', 1.0): 1, ('smashingbook', 1.0): 1, ('bibl', 1.0): 2, ('ill', 1.0): 6, ('sick', 1.0): 4, ('lamo', 1.0): 1, ('fangirl', 1.0): 3, ('platon', 1.0): 1, ('scienc', 1.0): 5, ('resid', 1.0): 2, ('servicewithasmil', 1.0): 1, ('bloodlin', 1.0): 1, ('huski', 1.0): 1, ('obituari', 1.0): 1, ('advert', 1.0): 1, ('goofingaround', 1.0): 1, ('bollywood', 1.0): 1, ('giveaway', 1.0): 6, ('dah', 1.0): 2, ('noth', 1.0): 15, ('bitter', 1.0): 2, ('anger', 1.0): 1, ('hatr', 1.0): 2, ('toward', 1.0): 2, ('pure', 1.0): 2, ('indiffer', 1.0): 1, ('suit', 1.0): 5, ('zach', 1.0): 1, ('codi', 1.0): 2, ('deliv', 1.0): 3, ('ac', 1.0): 1, ('excel', 1.0): 6, ('produc', 1.0): 1, ('boggl', 1.0): 1, ('fatigu', 1.0): 1, ('baareeq', 1.0): 1, ('gamedev', 1.0): 2, ('hobbi', 1.0): 1, ('tweenie_fox', 1.0): 1, ('click', 1.0): 3, ('accessori', 1.0): 1, ('tamang', 1.0): 1, ('hinala', 1.0): 1, ('niam', 1.0): 1, ('selfiee', 1.0): 1, ('especi', 1.0): 4, ('lass', 1.0): 1, ('ale', 1.0): 1, ('swim', 1.0): 3, ('bout', 1.0): 3, ('goodby', 1.0): 5, ('feminist', 1.0): 1, ('fought', 1.0): 1, ('snobbi', 1.0): 1, ('bitch', 1.0): 3, ('carolin', 1.0): 2, ('mighti', 1.0): 1, ('🔥', 1.0): 1, ('threw', 1.0): 2, ('hbd', 1.0): 1, ('follback', 1.0): 19, ('jog', 1.0): 1, ('remot', 1.0): 2, ('newli', 1.0): 1, ('ebay', 1.0): 2, ('store', 1.0): 15, ('disneyinfin', 1.0): 1, ('starwar', 1.0): 1, ('charact', 1.0): 3, ('preorder', 1.0): 1, ('starter', 1.0): 1, ('hit', 1.0): 13, ('snap', 1.0): 4, ('homi', 1.0): 3, ('bought', 1.0): 4, ('skin', 1.0): 8, ('bday', 1.0): 11, ('chant', 1.0): 2, ('jai', 1.0): 1, ('itali', 1.0): 2, ('fast', 1.0): 4, ('heeeyyy', 1.0): 1, ('woah', 1.0): 3, ('★', 1.0): 5, ('😊', 1.0): 11, ('whenev', 1.0): 4, ('ang', 1.0): 2, ('kiss', 1.0): 4, ('philippin', 1.0): 2, ('packag', 1.0): 3, ('bruis', 1.0): 1, ('rib', 1.0): 2, ('😀', 1.0): 2, ('😁', 1.0): 6, ('😂', 1.0): 17, ('😃', 1.0): 1, ('😅', 1.0): 1, ('😉', 1.0): 2, ('tombraid', 1.0): 1, ('hype', 1.0): 1, ('thejuiceinthemix', 1.0): 1, ('rela', 1.0): 1, ('low', 1.0): 6, ('prioriti', 1.0): 1, ('harri', 1.0): 5, ('bc', 1.0): 9, ('collaps', 1.0): 2, ('chaotic', 1.0): 1, ('cosa', 1.0): 1, ('&lt;---', 1.0): 2, ('alliter', 1.0): 1, ('oppayaa', 1.0): 1, (\"how'\", 1.0): 4, ('natgeo', 1.0): 1, ('lick', 1.0): 1, ('elbow', 1.0): 2, ('. .', 1.0): 2, ('“', 1.0): 7, ('emu', 1.0): 1, ('stoke', 1.0): 1, ('woke', 1.0): 5, (\"people'\", 1.0): 3, ('approv', 1.0): 6, (\"god'\", 1.0): 2, ('jisung', 1.0): 1, ('sunshin', 1.0): 7, ('mm', 1.0): 6, ('nicola', 1.0): 1, ('brighten', 1.0): 2, ('helen', 1.0): 3, ('brian', 1.0): 3, ('2-3', 1.0): 1, ('australia', 1.0): 5, ('ol', 1.0): 2, ('bone', 1.0): 1, ('creak', 1.0): 1, ('abuti', 1.0): 1, ('tweetland', 1.0): 1, ('android', 1.0): 3, ('xma', 1.0): 2, ('skyblock', 1.0): 1, ('bcaus', 1.0): 1, ('2009', 1.0): 1, ('die', 1.0): 10, ('twitch', 1.0): 5, ('sympathi', 1.0): 1, ('laugh', 1.0): 5, ('unniee', 1.0): 1, ('nuka', 1.0): 1, ('penacova', 1.0): 1, ('djset', 1.0): 1, ('edm', 1.0): 1, ('kizomba', 1.0): 1, ('latinhous', 1.0): 1, ('housemus', 1.0): 3, ('portug', 1.0): 1, ('wild', 1.0): 2, ('ride', 1.0): 6, ('anytim', 1.0): 6, ('tast', 1.0): 5, ('yer', 1.0): 2, ('mtn', 1.0): 2, ('maganda', 1.0): 1, ('mistress', 1.0): 2, ('saphir', 1.0): 1, ('busi', 1.0): 19, ('4000', 1.0): 1, ('instagram', 1.0): 7, ('among', 1.0): 5, ('coconut', 1.0): 1, ('sambal', 1.0): 1, ('mussel', 1.0): 1, ('recip', 1.0): 5, ('kalin', 1.0): 1, ('mixcloud', 1.0): 1, ('sarcasm', 1.0): 2, ('chelsea', 1.0): 3, ('he', 1.0): 2, ('useless', 1.0): 2, ('thursday', 1.0): 2, ('hang', 1.0): 3, ('hehe', 1.0): 10, ('said', 1.0): 16, ('benson', 1.0): 1, ('facebook', 1.0): 5, ('solid', 1.0): 1, ('16/17', 1.0): 1, ('30', 1.0): 3, ('°', 1.0): 1, ('😜', 1.0): 2, ('maryhick', 1.0): 1, ('kikmeboy', 1.0): 7, ('photooftheday', 1.0): 4, ('musicbiz', 1.0): 2, ('sheskindahot', 1.0): 1, ('fleekil', 1.0): 1, ('mbalula', 1.0): 1, ('africa', 1.0): 1, ('mexican', 1.0): 1, ('scar', 1.0): 1, ('offic', 1.0): 8, ('donut', 1.0): 2, ('foiegra', 1.0): 2, ('despit', 1.0): 2, ('weather', 1.0): 9, ('wed', 1.0): 5, ('toni', 1.0): 2, ('stark', 1.0): 1, ('incred', 1.0): 7, ('poem', 1.0): 2, ('bubbl', 1.0): 3, ('dale', 1.0): 1, ('billion', 1.0): 1, ('magic', 1.0): 5, ('op', 1.0): 3, ('cast', 1.0): 1, ('vote', 1.0): 9, ('elect', 1.0): 1, ('jcreport', 1.0): 1, ('piggin', 1.0): 1, ('botan', 1.0): 2, ('soap', 1.0): 4, ('late', 1.0): 13, ('upload', 1.0): 5, ('freshli', 1.0): 1, ('3week', 1.0): 1, ('heal', 1.0): 1, ('tobi-bro', 1.0): 1, ('isp', 1.0): 1, ('steel', 1.0): 1, ('wednesday', 1.0): 1, ('swear', 1.0): 3, ('met', 1.0): 4, ('earlier', 1.0): 4, ('cam', 1.0): 3, ('😭', 1.0): 2, ('except', 1.0): 2, (\"masha'allah\", 1.0): 1, ('french', 1.0): 5, ('wwat', 1.0): 2, ('franc', 1.0): 5, ('yaaay', 1.0): 3, ('beirut', 1.0): 2, ('coffe', 1.0): 11, ('panda', 1.0): 6, ('eonni', 1.0): 2, ('favourit', 1.0): 13, ('soda', 1.0): 1, ('fuller', 1.0): 1, ('shit', 1.0): 13, ('healthi', 1.0): 2, ('💓', 1.0): 2, ('rettweet', 1.0): 3, ('mvg', 1.0): 1, ('valuabl', 1.0): 1, ('madrid', 1.0): 3, ('sore', 1.0): 6, ('bergerac', 1.0): 1, ('u21', 1.0): 1, ('individu', 1.0): 2, ('adam', 1.0): 1, (\"beach'\", 1.0): 1, ('suicid', 1.0): 1, ('squad', 1.0): 1, ('fond', 1.0): 1, ('christoph', 1.0): 2, ('cocki', 1.0): 1, ('prove', 1.0): 3, (\"attitude'\", 1.0): 1, ('improv', 1.0): 3, ('suggest', 1.0): 6, ('date', 1.0): 12, ('inde', 1.0): 10, ('intellig', 1.0): 3, ('strong', 1.0): 7, ('cs', 1.0): 2, ('certain', 1.0): 2, ('exam', 1.0): 5, ('forgot', 1.0): 3, ('home-bas', 1.0): 1, ('knee', 1.0): 4, ('sale', 1.0): 3, ('fleur', 1.0): 1, ('dress', 1.0): 10, ('readystock_hijabmart', 1.0): 1, ('idr', 1.0): 2, ('325.000', 1.0): 1, ('200.000', 1.0): 1, ('tompolo', 1.0): 1, ('aim', 1.0): 1, ('cannot', 1.0): 4, ('buyer', 1.0): 3, ('disappoint', 1.0): 1, ('paper', 1.0): 4, ('slack', 1.0): 1, ('crack', 1.0): 1, ('particularli', 1.0): 2, ('strike', 1.0): 1, ('31', 1.0): 1, ('mam', 1.0): 2, ('feytyaz', 1.0): 1, ('instant', 1.0): 1, ('stiffen', 1.0): 1, ('ricky_feb', 1.0): 1, ('grindea', 1.0): 1, ('courier', 1.0): 1, ('crypt', 1.0): 1, ('arma', 1.0): 1, ('record', 1.0): 5, ('gosh', 1.0): 2, ('limbo', 1.0): 1, ('orchard', 1.0): 1, ('art', 1.0): 10, ('super', 1.0): 15, ('karachi', 1.0): 2, ('ka', 1.0): 4, ('venic', 1.0): 1, ('sever', 1.0): 3, ('part', 1.0): 15, ('wit', 1.0): 2, ('accumul', 1.0): 1, ('maroon', 1.0): 1, ('cocktail', 1.0): 4, ('0-100', 1.0): 1, ('quick', 1.0): 7, ('1100d', 1.0): 1, ('auto-focu', 1.0): 1, ('manual', 1.0): 2, ('vein', 1.0): 1, ('crackl', 1.0): 1, ('glaze', 1.0): 1, ('layout', 1.0): 3, ('bomb', 1.0): 4, ('social', 1.0): 4, ('websit', 1.0): 8, ('pake', 1.0): 1, ('joim', 1.0): 1, ('feed', 1.0): 4, ('troop', 1.0): 1, ('mail', 1.0): 3, ('ladolcevitainluxembourg@hotmail.com', 1.0): 1, ('prrequest', 1.0): 1, ('journorequest', 1.0): 1, ('the_madstork', 1.0): 1, ('shaun', 1.0): 1, ('bot', 1.0): 4, ('chloe', 1.0): 2, ('actress', 1.0): 3, ('away', 1.0): 13, ('wick', 1.0): 9, ('hola', 1.0): 1, ('juan', 1.0): 1, ('houston', 1.0): 1, ('tx', 1.0): 2, ('jenni', 1.0): 1, (\"year'\", 1.0): 2, ('stumbl', 1.0): 1, ('upon', 1.0): 1, ('prob.nic', 1.0): 1, ('choker', 1.0): 1, ('btw', 1.0): 12, ('seouljin', 1.0): 1, ('photoset', 1.0): 3, ('sadomasochistsparadis', 1.0): 1, ('wynter', 1.0): 1, ('bottom', 1.0): 3, ('outtak', 1.0): 1, ('sadomasochist', 1.0): 1, ('paradis', 1.0): 1, ('ty', 1.0): 8, ('bbi', 1.0): 3, ('clip', 1.0): 1, ('lose', 1.0): 6, ('cypher', 1.0): 1, ('amen', 1.0): 2, ('x32', 1.0): 1, ('plant', 1.0): 4, ('allow', 1.0): 4, ('corner', 1.0): 3, ('addict', 1.0): 4, ('gurl', 1.0): 1, ('suck', 1.0): 9, ('special', 1.0): 8, ('owe', 1.0): 1, ('daniel', 1.0): 2, ('ape', 1.0): 1, ('saar', 1.0): 1, ('ahead', 1.0): 4, ('vers', 1.0): 1, ('butterfli', 1.0): 1, ('bonu', 1.0): 2, ('fill', 1.0): 5, ('tear', 1.0): 1, ('laughter', 1.0): 2, ('5so', 1.0): 6, ('yummmyyi', 1.0): 1, ('eat', 1.0): 6, ('dosa', 1.0): 1, ('easier', 1.0): 2, ('unless', 1.0): 3, ('achi', 1.0): 2, ('youuu', 1.0): 2, ('bawi', 1.0): 1, ('ako', 1.0): 1, ('queenesth', 1.0): 1, ('sharp', 1.0): 2, ('yess', 1.0): 1, ('poldi', 1.0): 1, ('cimbom', 1.0): 1, ('buddi', 1.0): 7, ('bruhhh', 1.0): 1, ('daddi', 1.0): 2, ('”', 1.0): 5, ('knowledg', 1.0): 2, ('attent', 1.0): 4, ('1tb', 1.0): 1, ('bank', 1.0): 1, ('credit', 1.0): 4, ('depart', 1.0): 2, ('anz', 1.0): 1, ('extrem', 1.0): 3, ('offshor', 1.0): 1, ('absolut', 1.0): 9, ('classic', 1.0): 3, ('gottolovebank', 1.0): 1, ('yup', 1.0): 6, ('in-shaa-allah', 1.0): 1, ('dua', 1.0): 1, ('thru', 1.0): 2, ('aameen', 1.0): 2, ('4/5', 1.0): 1, ('coca', 1.0): 1, ('cola', 1.0): 1, ('fanta', 1.0): 1, ('pepsi', 1.0): 1, ('sprite', 1.0): 1, ('all', 1.0): 1, ('sweeeti', 1.0): 1, (';-)', 1.0): 3, ('welcometweet', 1.0): 2, ('psygustokita', 1.0): 4, ('setup', 1.0): 1, ('wet', 1.0): 3, ('feet', 1.0): 3, ('carpet', 1.0): 1, ('judgment', 1.0): 1, ('hypocrit', 1.0): 1, ('narcissist', 1.0): 1, ('jumpsuit', 1.0): 1, ('bt', 1.0): 2, ('denim', 1.0): 1, ('verg', 1.0): 1, ('owl', 1.0): 1, ('constant', 1.0): 1, ('run', 1.0): 12, ('sia', 1.0): 1, ('count', 1.0): 7, ('brilliant', 1.0): 9, ('teacher', 1.0): 1, ('compar', 1.0): 2, ('religion', 1.0): 1, ('rant', 1.0): 1, ('student', 1.0): 6, ('bencher', 1.0): 1, ('1/5', 1.0): 1, ('porsch', 1.0): 1, ('paddock', 1.0): 1, ('budapestgp', 1.0): 1, ('johnyherbert', 1.0): 1, ('roll', 1.0): 5, ('porschesupercup', 1.0): 1, ('koyal', 1.0): 1, ('melodi', 1.0): 1, ('unexpect', 1.0): 4, ('creat', 1.0): 8, ('memori', 1.0): 3, ('35', 1.0): 1, ('ep', 1.0): 3, ('catch', 1.0): 10, ('wirh', 1.0): 1, ('arc', 1.0): 1, ('x31', 1.0): 1, ('wolv', 1.0): 2, ('desir', 1.0): 1, ('ameen', 1.0): 1, ('kca', 1.0): 1, ('votejkt', 1.0): 1, ('48id', 1.0): 1, ('helpinggroupdm', 1.0): 1, ('quot', 1.0): 6, ('weird', 1.0): 5, ('dp', 1.0): 1, ('wife', 1.0): 5, ('poor', 1.0): 4, ('chick', 1.0): 1, ('guid', 1.0): 3, ('zonzofox', 1.0): 3, ('bhaiya', 1.0): 1, ('brother', 1.0): 4, ('lucki', 1.0): 10, ('patti', 1.0): 1, ('elabor', 1.0): 1, ('kuch', 1.0): 1, ('rate', 1.0): 1, ('merdeka', 1.0): 1, ('palac', 1.0): 2, ('hotel', 1.0): 5, ('plusmil', 1.0): 1, ('servic', 1.0): 7, ('hahahaa', 1.0): 1, ('mean', 1.0): 25, ('nex', 1.0): 2, ('safe', 1.0): 5, ('gwd', 1.0): 1, ('she', 1.0): 2, ('okok', 1.0): 1, ('33', 1.0): 4, ('idiot', 1.0): 1, ('chaerin', 1.0): 1, ('unni', 1.0): 1, ('viabl', 1.0): 1, ('altern', 1.0): 3, ('nowaday', 1.0): 2, ('ip', 1.0): 1, ('tombow', 1.0): 1, ('abt', 1.0): 2, ('friyay', 1.0): 2, ('smug', 1.0): 1, ('marrickvil', 1.0): 1, ('public', 1.0): 3, ('ten', 1.0): 1, ('ago', 1.0): 8, ('eighteen', 1.0): 1, ('auvssscr', 1.0): 1, ('ncaaseason', 1.0): 1, ('slow', 1.0): 2, ('popsicl', 1.0): 1, ('soft', 1.0): 2, ('melt', 1.0): 1, ('mouth', 1.0): 2, ('thankyouuu', 1.0): 1, ('dianna', 1.0): 1, ('ngga', 1.0): 1, ('usah', 1.0): 1, ('dipikirin', 1.0): 1, ('elah', 1.0): 1, ('easili', 1.0): 1, (\"who'\", 1.0): 9, ('entp', 1.0): 1, ('killin', 1.0): 1, ('meme', 1.0): 1, ('worthi', 1.0): 1, ('shot', 1.0): 6, ('emon', 1.0): 1, ('decent', 1.0): 2, ('outdoor', 1.0): 1, ('rave', 1.0): 1, ('dv', 1.0): 1, ('aku', 1.0): 1, ('bakal', 1.0): 1, ('liat', 1.0): 1, ('kak', 1.0): 2, ('merri', 1.0): 1, ('tv', 1.0): 5, ('outfit', 1.0): 3, ('---&gt;', 1.0): 1, ('fashionfriday', 1.0): 1, ('angle.nelson', 1.0): 1, ('cheap', 1.0): 1, ('mymonsoonstori', 1.0): 2, ('tree', 1.0): 2, ('lotion', 1.0): 1, ('moistur', 1.0): 1, ('monsoon', 1.0): 1, ('whoop', 1.0): 6, ('romant', 1.0): 2, ('valencia', 1.0): 1, ('daaru', 1.0): 1, ('parti', 1.0): 12, ('chaddi', 1.0): 1, ('wonderful.great', 1.0): 1, ('trim', 1.0): 1, ('pube', 1.0): 1, ('es', 1.0): 2, ('mi', 1.0): 5, ('tio', 1.0): 1, ('sinaloa', 1.0): 1, ('arr', 1.0): 1, ('stylish', 1.0): 1, ('trendi', 1.0): 1, ('kim', 1.0): 5, ('fabfriday', 1.0): 2, ('facetim', 1.0): 4, ('calum', 1.0): 3, ('constantli', 1.0): 1, ('announc', 1.0): 1, ('filbarbarian', 1.0): 1, ('beer', 1.0): 3, ('arm', 1.0): 3, ('testicl', 1.0): 1, ('light', 1.0): 13, ('katerina', 1.0): 1, ('maniataki', 1.0): 1, ('ahh', 1.0): 5, ('alright', 1.0): 6, ('worthwhil', 1.0): 3, ('judg', 1.0): 2, ('tech', 1.0): 2, ('window', 1.0): 7, ('stupid', 1.0): 8, ('plugin', 1.0): 1, ('bass', 1.0): 1, ('slap', 1.0): 1, ('6pm', 1.0): 1, ('door', 1.0): 3, ('vip', 1.0): 1, ('gener', 1.0): 4, ('seat', 1.0): 2, ('earli', 1.0): 9, ('london', 1.0): 9, ('toptravelcentar', 1.0): 1, ('ttctop', 1.0): 1, ('lux', 1.0): 1, ('luxurytravel', 1.0): 1, ('beograd', 1.0): 1, ('srbija', 1.0): 1, ('putovanja', 1.0): 1, ('wendi', 1.0): 2, ('provid', 1.0): 4, ('drainag', 1.0): 1, ('homebound', 1.0): 1, ('hahahay', 1.0): 1, ('yeeeah', 1.0): 1, ('moar', 1.0): 2, ('kitteh', 1.0): 1, ('incom', 1.0): 1, ('tower', 1.0): 2, ('yippee', 1.0): 1, ('scrummi', 1.0): 1, ('bio', 1.0): 5, ('mcpe', 1.0): 1, ('-&gt;', 1.0): 1, ('vainglori', 1.0): 1, ('driver', 1.0): 1, ('6:01', 1.0): 1, ('lilydal', 1.0): 1, ('fss', 1.0): 1, ('rais', 1.0): 3, ('magicalmysterytour', 1.0): 1, ('chek', 1.0): 2, ('rule', 1.0): 2, ('weebli', 1.0): 1, ('donetsk', 1.0): 1, ('earth', 1.0): 7, ('personalis', 1.0): 1, ('wrap', 1.0): 2, ('stationeri', 1.0): 1, ('adrian', 1.0): 1, ('parcel', 1.0): 2, ('tuesday', 1.0): 7, ('pri', 1.0): 3, ('80', 1.0): 3, ('wz', 1.0): 1, ('pattern', 1.0): 1, ('cut', 1.0): 3, ('buttonhol', 1.0): 1, ('4mi', 1.0): 1, ('famou', 1.0): 1, ('client', 1.0): 1, ('p', 1.0): 3, ('aliv', 1.0): 2, ('trial', 1.0): 1, ('spm', 1.0): 1, ('dinooo', 1.0): 1, ('cardio', 1.0): 1, ('steak', 1.0): 1, ('cue', 1.0): 1, ('laptop', 1.0): 1, ('guinea', 1.0): 1, ('pig', 1.0): 1, ('salamat', 1.0): 1, ('sa', 1.0): 6, ('mga', 1.0): 1, ('nag.greet', 1.0): 1, ('guis', 1.0): 1, ('godbless', 1.0): 2, ('crush', 1.0): 3, ('appl', 1.0): 4, ('deserv', 1.0): 11, ('charl', 1.0): 1, ('workhard', 1.0): 1, ('model', 1.0): 7, ('forrit', 1.0): 1, ('bread', 1.0): 2, ('bacon', 1.0): 2, ('butter', 1.0): 2, ('afang', 1.0): 2, ('soup', 1.0): 2, ('semo', 1.0): 2, ('brb', 1.0): 1, ('forc', 1.0): 2, ('doesnt', 1.0): 5, ('tato', 1.0): 1, ('bulat', 1.0): 1, ('concern', 1.0): 1, ('snake', 1.0): 1, ('perform', 1.0): 3, ('con', 1.0): 1, ('todayyy', 1.0): 1, ('max', 1.0): 2, ('gaza', 1.0): 1, ('bbb', 1.0): 1, ('pc', 1.0): 3, ('22', 1.0): 2, ('legal', 1.0): 1, ('ditch', 1.0): 2, ('tori', 1.0): 1, ('bajrangibhaijaanhighestweek', 1.0): 6, (\"s'okay\", 1.0): 1, ('andi', 1.0): 2, ('you-and', 1.0): 1, ('return', 1.0): 3, ('tuitutil', 1.0): 1, ('bud', 1.0): 2, ('learn', 1.0): 8, ('takeaway', 1.0): 1, ('instead', 1.0): 7, ('1hr', 1.0): 1, ('genial', 1.0): 1, ('competit', 1.0): 1, ('yosh', 1.0): 1, ('procrastin', 1.0): 1, ('plu', 1.0): 4, ('kfc', 1.0): 2, ('itun', 1.0): 1, ('dedicatedfan', 1.0): 1, ('💜', 1.0): 7, ('daft', 1.0): 1, ('teeth', 1.0): 1, ('troubl', 1.0): 1, ('huxley', 1.0): 1, ('basket', 1.0): 2, ('ben', 1.0): 2, ('sent', 1.0): 8, ('gamer', 1.0): 3, ('activ', 1.0): 5, ('120', 1.0): 2, ('distanc', 1.0): 2, ('suitabl', 1.0): 1, ('stockholm', 1.0): 1, ('zack', 1.0): 1, ('destroy', 1.0): 1, ('heel', 1.0): 2, ('claw', 1.0): 1, ('q', 1.0): 2, ('blond', 1.0): 2, ('box', 1.0): 3, ('cheerio', 1.0): 1, ('seed', 1.0): 4, ('cutest', 1.0): 2, ('ffback', 1.0): 2, ('spotifi', 1.0): 3, (\"we'v\", 1.0): 7, ('vc', 1.0): 1, ('tgp', 1.0): 1, ('race', 1.0): 5, ('averag', 1.0): 2, (\"joe'\", 1.0): 1, ('bluejay', 1.0): 1, ('vinylbear', 1.0): 1, ('pal', 1.0): 1, ('furbabi', 1.0): 1, ('luff', 1.0): 1, ('mega', 1.0): 4, ('retail', 1.0): 4, ('boot', 1.0): 2, ('whsmith', 1.0): 1, ('ps3', 1.0): 1, ('shannon', 1.0): 1, ('na', 1.0): 9, ('redecor', 1.0): 1, ('bob', 1.0): 3, ('elli', 1.0): 4, ('mairi', 1.0): 1, ('workout', 1.0): 6, ('impair', 1.0): 1, ('uggghhh', 1.0): 1, ('dam', 1.0): 2, ('dun', 1.0): 2, ('eczema', 1.0): 1, ('suffer', 1.0): 4, ('ndee', 1.0): 1, ('pleasur', 1.0): 14, ('publiliu', 1.0): 1, ('syru', 1.0): 1, ('fear', 1.0): 1, ('death', 1.0): 3, ('dread', 1.0): 1, ('fell', 1.0): 3, ('fuk', 1.0): 1, ('unblock', 1.0): 1, ('tweak', 1.0): 2, ('php', 1.0): 1, ('fall', 1.0): 10, ('oomf', 1.0): 1, ('pippa', 1.0): 1, ('hschool', 1.0): 1, ('bu', 1.0): 3, ('cardi', 1.0): 1, ('everyday', 1.0): 3, ('everytim', 1.0): 3, ('hk', 1.0): 1, (\"why'd\", 1.0): 1, ('acorn', 1.0): 1, ('origin', 1.0): 7, ('c64', 1.0): 1, ('cpu', 1.0): 1, ('consider', 1.0): 1, ('advanc', 1.0): 1, ('onair', 1.0): 1, ('bay', 1.0): 1, ('hold', 1.0): 6, ('river', 1.0): 3, ('0878 0388', 1.0): 1, ('1033', 1.0): 1, ('0272 3306', 1.0): 1, ('70', 1.0): 5, ('rescu', 1.0): 1, ('mutt', 1.0): 1, ('confirm', 1.0): 3, ('deliveri', 1.0): 3, ('switch', 1.0): 2, ('lap', 1.0): 1, ('optim', 1.0): 1, ('lu', 1.0): 1, (':|', 1.0): 1, ('tweetofthedecad', 1.0): 1, ('class', 1.0): 5, ('happiest', 1.0): 2, ('bbmme', 1.0): 3, ('pin', 1.0): 4, ('7df9e60a', 1.0): 1, ('bbm', 1.0): 2, ('bbmpin', 1.0): 2, ('addmeonbbm', 1.0): 1, ('addm', 1.0): 1, (\"today'\", 1.0): 3, ('menu', 1.0): 1, ('marri', 1.0): 3, ('glenn', 1.0): 1, ('what', 1.0): 4, ('height', 1.0): 1, (\"sculptor'\", 1.0): 1, ('ti5', 1.0): 1, ('dota', 1.0): 3, ('nudg', 1.0): 1, ('spot', 1.0): 5, ('tasti', 1.0): 1, ('hilli', 1.0): 1, ('cycl', 1.0): 6, ('england', 1.0): 4, ('scotlandismass', 1.0): 1, ('gen', 1.0): 2, ('vikk', 1.0): 1, ('fna', 1.0): 1, ('mombasa', 1.0): 1, ('tukutanemombasa', 1.0): 1, ('100reasonstovisitmombasa', 1.0): 1, ('karibumombasa', 1.0): 1, ('hanbin', 1.0): 1, ('certainli', 1.0): 4, ('goosnight', 1.0): 1, ('kindli', 1.0): 4, ('familiar', 1.0): 2, ('jealou', 1.0): 4, ('tent', 1.0): 2, ('yea', 1.0): 2, ('cozi', 1.0): 1, ('phenomen', 1.0): 2, ('collab', 1.0): 2, ('gave', 1.0): 4, ('birth', 1.0): 1, ('behav', 1.0): 2, ('monster', 1.0): 1, ('spree', 1.0): 4, ('000', 1.0): 1, ('tank', 1.0): 6, ('outstand', 1.0): 1, ('donat', 1.0): 3, ('h', 1.0): 4, ('contestkiduniya', 1.0): 2, ('mfundo', 1.0): 1, ('och', 1.0): 1, ('hun', 1.0): 4, ('inner', 1.0): 2, ('nerd', 1.0): 2, ('tame', 1.0): 2, ('insidi', 1.0): 1, ('logic', 1.0): 1, ('math', 1.0): 1, ('channel', 1.0): 5, ('continu', 1.0): 4, ('doubt', 1.0): 3, ('300', 1.0): 2, ('sub', 1.0): 2, ('200', 1.0): 3, ('forgiven', 1.0): 1, ('manner', 1.0): 1, ('yhooo', 1.0): 1, ('ngi', 1.0): 1, ('mood', 1.0): 7, ('push', 1.0): 1, ('limit', 1.0): 6, ('obakeng', 1.0): 1, ('goat', 1.0): 1, ('alhamdullilah', 1.0): 1, ('pebbl', 1.0): 1, ('engross', 1.0): 1, ('bing', 1.0): 2, ('scream', 1.0): 2, ('whole', 1.0): 7, ('wide', 1.0): 2, ('🌎', 1.0): 2, ('😧', 1.0): 1, ('wat', 1.0): 2, ('muahhh', 1.0): 1, ('pausetim', 1.0): 1, ('drift', 1.0): 1, ('loos', 1.0): 3, ('campaign', 1.0): 4, ('kickstart', 1.0): 1, ('articl', 1.0): 9, ('jenna', 1.0): 1, ('bellybutton', 1.0): 5, ('inni', 1.0): 4, ('outi', 1.0): 4, ('havent', 1.0): 4, ('delish', 1.0): 1, ('joselito', 1.0): 1, ('freya', 1.0): 1, ('nth', 1.0): 1, ('latepost', 1.0): 1, ('lupet', 1.0): 1, ('mo', 1.0): 2, ('eric', 1.0): 3, ('askaman', 1.0): 1, ('150', 1.0): 1, ('0345', 1.0): 2, ('454', 1.0): 1, ('111', 1.0): 1, ('webz', 1.0): 1, ('oop', 1.0): 5, (\"they'll\", 1.0): 6, ('realis', 1.0): 2, ('anymor', 1.0): 3, ('carmel', 1.0): 1, ('decis', 1.0): 5, ('matt', 1.0): 6, ('@commoncultur', 1.0): 1, ('@connorfranta', 1.0): 1, ('honestli', 1.0): 3, ('explain', 1.0): 3, ('relationship', 1.0): 4, ('pick', 1.0): 15, ('tessnzach', 1.0): 1, ('paperboy', 1.0): 1, ('honest', 1.0): 3, ('reassur', 1.0): 1, ('guysss', 1.0): 3, ('mubank', 1.0): 2, (\"dongwoo'\", 1.0): 1, ('bright', 1.0): 2, ('tommorow', 1.0): 3, ('newyork', 1.0): 1, ('lolll', 1.0): 1, ('twinx', 1.0): 1, ('16', 1.0): 2, ('path', 1.0): 1, ('firmansyahbl', 1.0): 1, ('procedur', 1.0): 1, ('grim', 1.0): 1, ('fandango', 1.0): 1, ('ordinari', 1.0): 1, ('extraordinari', 1.0): 1, ('bo', 1.0): 2, ('birmingham', 1.0): 1, ('oracl', 1.0): 1, ('samosa', 1.0): 1, ('firebal', 1.0): 1, ('shoe', 1.0): 4, ('serv', 1.0): 1, ('sushi', 1.0): 2, ('shoeshi', 1.0): 1, ('�', 1.0): 2, ('lymond', 1.0): 1, ('philippa', 1.0): 2, ('novel', 1.0): 1, ('tara', 1.0): 3, ('. . .', 1.0): 2, ('aur', 1.0): 2, ('han', 1.0): 1, ('imran', 1.0): 3, ('khan', 1.0): 7, ('63', 1.0): 1, ('agaaain', 1.0): 1, ('doli', 1.0): 1, ('siregar', 1.0): 1, ('ninh', 1.0): 1, ('size', 1.0): 5, ('geekiest', 1.0): 1, ('geek', 1.0): 2, ('wallet', 1.0): 3, ('request', 1.0): 4, ('media', 1.0): 4, ('ralli', 1.0): 1, ('rotat', 1.0): 3, ('direct', 1.0): 3, ('eek', 1.0): 1, ('red', 1.0): 6, ('beij', 1.0): 1, ('meni', 1.0): 1, ('tebrik', 1.0): 1, ('etdi', 1.0): 1, ('700', 1.0): 1, ('💗', 1.0): 2, ('rod', 1.0): 1, ('embrac', 1.0): 1, ('actor', 1.0): 1, ('aplomb', 1.0): 1, ('foreveralon', 1.0): 2, ('mysumm', 1.0): 1, ('01482', 1.0): 1, ('333505', 1.0): 1, ('hahahaha', 1.0): 2, ('wear', 1.0): 6, ('uniform', 1.0): 1, ('evil', 1.0): 1, ('owww', 1.0): 1, ('choo', 1.0): 1, ('chweet', 1.0): 1, ('shorthair', 1.0): 1, ('oscar', 1.0): 1, ('realiz', 1.0): 7, ('harmoni', 1.0): 1, ('deneriveri', 1.0): 1, ('506', 1.0): 1, ('kiksext', 1.0): 5, ('kikkomansabor', 1.0): 2, ('killer', 1.0): 1, ('henessydiari', 1.0): 1, ('journey', 1.0): 4, ('band', 1.0): 4, ('plz', 1.0): 5, ('convo', 1.0): 3, ('11', 1.0): 5, ('vault', 1.0): 1, ('expand', 1.0): 2, ('vinni', 1.0): 1, ('money', 1.0): 9, ('hahahahaha', 1.0): 2, ('50cent', 1.0): 1, ('repay', 1.0): 1, ('debt', 1.0): 2, ('evet', 1.0): 1, ('wifi', 1.0): 3, ('lifestyl', 1.0): 1, ('qatarday', 1.0): 1, ('. ..', 1.0): 3, ('🌞', 1.0): 3, ('girli', 1.0): 1, ('india', 1.0): 4, ('innov', 1.0): 1, ('volunt', 1.0): 2, ('saran', 1.0): 1, ('drama', 1.0): 3, ('genr', 1.0): 1, ('romanc', 1.0): 1, ('comedi', 1.0): 1, ('leannerin', 1.0): 1, ('19', 1.0): 7, ('porno', 1.0): 1, ('l4l', 1.0): 3, ('weloveyounamjoon', 1.0): 1, ('homey', 1.0): 1, ('kenya', 1.0): 1, ('roller', 1.0): 2, ('coaster', 1.0): 1, ('aspect', 1.0): 1, ('najam', 1.0): 1, ('confess', 1.0): 2, ('pricelessantiqu', 1.0): 1, ('takesonetoknowon', 1.0): 1, ('extra', 1.0): 5, ('ucount', 1.0): 1, ('ji', 1.0): 3, ('turkish', 1.0): 1, ('knew', 1.0): 8, ('crap', 1.0): 1, ('burn', 1.0): 3, ('80x', 1.0): 1, ('airlin', 1.0): 1, ('sexi', 1.0): 10, ('yello', 1.0): 1, ('gail', 1.0): 1, ('yael', 1.0): 1, ('lesson', 1.0): 4, ('en', 1.0): 1, ('mano', 1.0): 1, ('hand', 1.0): 4, ('manag', 1.0): 6, ('prettiest', 1.0): 1, ('reader', 1.0): 4, ('dnt', 1.0): 1, ('ideal', 1.0): 2, ('weekli', 1.0): 2, ('idol', 1.0): 3, ('pose', 1.0): 2, ('shortlist', 1.0): 1, ('dominion', 1.0): 2, ('picnic', 1.0): 2, ('tmrw', 1.0): 3, ('nobodi', 1.0): 2, ('jummamubarak', 1.0): 1, ('shower', 1.0): 3, ('shalwarkameez', 1.0): 1, ('itter', 1.0): 1, ('offer', 1.0): 8, ('jummapray', 1.0): 1, ('af', 1.0): 8, ('display', 1.0): 1, ('enabl', 1.0): 1, ('compani', 1.0): 4, ('peep', 1.0): 4, ('tweep', 1.0): 2, ('folow', 1.0): 1, ('2k', 1.0): 1, ('ohhh', 1.0): 4, ('teaser', 1.0): 2, ('airec', 1.0): 1, ('009', 1.0): 1, ('acid', 1.0): 1, ('mous', 1.0): 2, ('31st', 1.0): 2, ('includ', 1.0): 5, ('robin', 1.0): 1, ('rough', 1.0): 4, ('control', 1.0): 1, ('remix', 1.0): 5, ('fave', 1.0): 3, ('toss', 1.0): 1, ('ladi', 1.0): 8, ('🐑', 1.0): 1, ('librari', 1.0): 3, ('mr2', 1.0): 1, ('climb', 1.0): 1, ('cuddl', 1.0): 1, ('jilla', 1.0): 1, ('headlin', 1.0): 1, ('2017', 1.0): 1, ('jumma', 1.0): 5, ('mubarik', 1.0): 2, ('spent', 1.0): 2, ('congratz', 1.0): 1, ('contribut', 1.0): 3, ('2.0', 1.0): 2, ('yuppiiee', 1.0): 1, ('alienthought', 1.0): 1, ('happyalien', 1.0): 1, ('crowd', 1.0): 2, ('loudest', 1.0): 2, ('gari', 1.0): 1, ('particular', 1.0): 1, ('attract', 1.0): 1, ('supprt', 1.0): 1, ('savag', 1.0): 1, ('cleans', 1.0): 1, ('scam', 1.0): 1, ('ridden', 1.0): 1, ('vyapam', 1.0): 2, ('renam', 1.0): 1, ('wave', 1.0): 2, ('couch', 1.0): 1, ('dodg', 1.0): 1, ('explan', 1.0): 2, ('bag', 1.0): 4, ('sanza', 1.0): 1, ('yaa', 1.0): 3, ('slr', 1.0): 1, ('som', 1.0): 1, ('honour', 1.0): 1, ('heheh', 1.0): 1, ('view', 1.0): 16, ('explor', 1.0): 2, ('wayanadan', 1.0): 1, ('forest', 1.0): 1, ('wayanad', 1.0): 1, ('srijith', 1.0): 1, ('whisper', 1.0): 1, ('lie', 1.0): 4, ('pokemon', 1.0): 1, ('dazzl', 1.0): 1, ('urself', 1.0): 2, ('doubl', 1.0): 2, ('flare', 1.0): 1, ('black', 1.0): 4, ('9', 1.0): 3, ('51', 1.0): 1, ('brows', 1.0): 1, ('bore', 1.0): 9, ('femal', 1.0): 2, ('tour', 1.0): 8, ('delv', 1.0): 2, ('muchhh', 1.0): 1, ('tmr', 1.0): 1, ('breakfast', 1.0): 4, ('gl', 1.0): 1, (\"tonight'\", 1.0): 2, ('):', 1.0): 7, ('litey', 1.0): 1, ('manuella', 1.0): 1, ('abhi', 1.0): 2, ('tak', 1.0): 2, ('nhi', 1.0): 2, ('dekhi', 1.0): 1, ('promo', 1.0): 3, ('se', 1.0): 4, ('xpax', 1.0): 1, ('lisa', 1.0): 2, ('aboard', 1.0): 3, ('institut', 1.0): 1, ('nc', 1.0): 2, ('chees', 1.0): 4, ('overload', 1.0): 1, ('pizza', 1.0): 1, ('•', 1.0): 3, ('mcfloat', 1.0): 1, ('fudg', 1.0): 3, ('sanda', 1.0): 1, ('munchkin', 1.0): 1, (\"d'd\", 1.0): 1, ('granni', 1.0): 1, ('baller', 1.0): 1, ('lil', 1.0): 4, ('chain', 1.0): 1, ('everybodi', 1.0): 1, ('ought', 1.0): 1, ('jay', 1.0): 3, ('events@breastcancernow.org', 1.0): 1, ('79x', 1.0): 1, ('champion', 1.0): 1, ('letter', 1.0): 2, ('uniqu', 1.0): 2, ('affaraid', 1.0): 1, ('dearslim', 1.0): 2, ('role', 1.0): 2, ('billi', 1.0): 2, ('lab', 1.0): 1, ('ovh', 1.0): 2, ('maxi', 1.0): 2, ('bunch', 1.0): 1, ('acc', 1.0): 2, ('sprit', 1.0): 1, ('you', 1.0): 1, ('til', 1.0): 2, ('hammi', 1.0): 1, ('freedom', 1.0): 2, ('pistol', 1.0): 1, ('unlock', 1.0): 1, ('bemeapp', 1.0): 1, ('thumb', 1.0): 1, ('beme', 1.0): 1, ('bemecod', 1.0): 1, ('proudtobem', 1.0): 1, ('round', 1.0): 2, ('calm', 1.0): 5, ('kepo', 1.0): 1, ('luckili', 1.0): 1, ('clearli', 1.0): 2, ('دعمم', 1.0): 1, ('للعودة', 1.0): 1, ('للحياة', 1.0): 1, ('heiyo', 1.0): 2, ('dudafti', 1.0): 1, ('breaktym', 1.0): 1, ('fatal', 1.0): 1, ('danger', 1.0): 1, ('term', 1.0): 2, ('health', 1.0): 2, ('outrag', 1.0): 1, ('645k', 1.0): 1, ('muna', 1.0): 1, ('magstart', 1.0): 1, ('salut', 1.0): 3, ('→', 1.0): 1, ('thq', 1.0): 1, ('contin', 1.0): 1, ('thalaivar', 1.0): 1, ('£', 1.0): 7, ('heiya', 1.0): 2, ('grab', 1.0): 3, ('30.000', 1.0): 2, ('av', 1.0): 1, ('gd', 1.0): 3, ('wknd', 1.0): 1, ('ear', 1.0): 12, (\"y'day\", 1.0): 1, ('hxh', 1.0): 1, ('badass', 1.0): 2, ('killua', 1.0): 1, ('scene', 1.0): 2, ('78x', 1.0): 1, ('unappreci', 1.0): 1, ('graciou', 1.0): 1, ('nailedit', 1.0): 1, ('ourdisneyinfin', 1.0): 1, ('mari', 1.0): 3, ('jillmil', 1.0): 1, ('webcam', 1.0): 2, ('elfindelmundo', 1.0): 1, ('mainli', 1.0): 1, ('favour', 1.0): 1, ('dancetast', 1.0): 1, ('satyajit', 1.0): 1, (\"ray'\", 1.0): 1, ('porosh', 1.0): 1, ('pathor', 1.0): 1, ('situat', 1.0): 3, ('goldbug', 1.0): 1, ('wine', 1.0): 3, ('bottl', 1.0): 2, ('spill', 1.0): 2, ('jazmin', 1.0): 3, ('bonilla', 1.0): 3, ('15000', 1.0): 1, ('star', 1.0): 9, ('hollywood', 1.0): 3, ('rofl', 1.0): 3, ('shade', 1.0): 1, ('grey', 1.0): 1, ('netsec', 1.0): 1, ('kev', 1.0): 1, ('sister', 1.0): 6, ('told', 1.0): 6, ('unlist', 1.0): 1, ('hickey', 1.0): 1, ('dad', 1.0): 5, ('hock', 1.0): 1, ('mamma', 1.0): 1, ('human', 1.0): 5, ('be', 1.0): 1, ('mere', 1.0): 1, ('holist', 1.0): 1, ('cosmovis', 1.0): 1, ('narrow-mind', 1.0): 1, ('charg', 1.0): 3, ('cess', 1.0): 1, ('alix', 1.0): 1, ('quan', 1.0): 1, ('tip', 1.0): 5, ('naaahhh', 1.0): 1, ('duh', 1.0): 2, ('emesh', 1.0): 1, ('hilari', 1.0): 4, ('kath', 1.0): 3, ('kia', 1.0): 1, ('@vauk', 1.0): 1, ('tango', 1.0): 1, ('tracerequest', 1.0): 2, ('dassi', 1.0): 1, ('fwm', 1.0): 1, ('selamat', 1.0): 1, ('nichola', 1.0): 2, ('malta', 1.0): 1, ('gto', 1.0): 1, ('tomorrowland', 1.0): 1, ('incal', 1.0): 1, ('shob', 1.0): 1, ('incomplet', 1.0): 1, ('barkada', 1.0): 1, ('silverston', 1.0): 1, ('pull', 1.0): 1, ('bookstor', 1.0): 1, ('ganna', 1.0): 1, ('hillari', 1.0): 1, ('clinton', 1.0): 1, ('court', 1.0): 2, ('notic', 1.0): 11, ('slice', 1.0): 2, ('life-so', 1.0): 1, ('hidden', 1.0): 1, ('untap', 1.0): 1, ('mca', 1.0): 2, ('gettin', 1.0): 1, ('hella', 1.0): 1, ('wana', 1.0): 1, ('bandz', 1.0): 1, ('hell', 1.0): 4, ('donington', 1.0): 1, ('park', 1.0): 8, ('24/25', 1.0): 1, ('x30', 1.0): 1, ('merci', 1.0): 1, ('bien', 1.0): 1, ('pitbul', 1.0): 1, ('777x', 1.0): 1, ('fri', 1.0): 3, ('annyeong', 1.0): 1, ('oppa', 1.0): 7, ('indonesian', 1.0): 1, ('elf', 1.0): 3, ('flight', 1.0): 2, ('bf', 1.0): 2, ('jennyjean', 1.0): 1, ('kikchat', 1.0): 1, ('sabadodeganarseguidor', 1.0): 1, ('sexysasunday', 1.0): 2, ('marseil', 1.0): 1, ('ganda', 1.0): 1, ('fnaf', 1.0): 5, ('steam', 1.0): 1, ('assur', 1.0): 2, ('current', 1.0): 7, ('goin', 1.0): 1, ('sweeti', 1.0): 4, ('strongest', 1.0): 1, (\"spot'\", 1.0): 1, ('barnstapl', 1.0): 1, ('bideford', 1.0): 1, ('abit', 1.0): 1, ('road', 1.0): 5, ('rocro', 1.0): 1, ('13glodyysbro', 1.0): 1, ('hire', 1.0): 1, ('2ne1', 1.0): 1, ('aspetti', 1.0): 1, ('chicken', 1.0): 4, ('chip', 1.0): 3, ('cupboard', 1.0): 1, ('empti', 1.0): 2, ('jami', 1.0): 2, ('ian', 1.0): 2, ('latin', 1.0): 5, ('asian', 1.0): 5, ('version', 1.0): 8, ('va', 1.0): 1, ('642', 1.0): 1, ('kikgirl', 1.0): 5, ('orgasm', 1.0): 1, ('phonesex', 1.0): 1, ('spacer', 1.0): 1, ('felic', 1.0): 1, ('smoak', 1.0): 1, ('👓', 1.0): 1, ('💘', 1.0): 3, ('children', 1.0): 3, ('psychopath', 1.0): 1, ('spoil', 1.0): 1, ('dimpl', 1.0): 1, ('contempl', 1.0): 1, ('indi', 1.0): 2, ('rout', 1.0): 4, ('jsl', 1.0): 1, ('76x', 1.0): 1, ('gotcha', 1.0): 1, ('kina', 1.0): 1, ('donna', 1.0): 3, ('reachabl', 1.0): 1, ('jk', 1.0): 1, ('s02e04', 1.0): 1, ('air', 1.0): 7, ('naggi', 1.0): 1, ('anal', 1.0): 1, ('child', 1.0): 3, ('vidcon', 1.0): 2, ('anxiou', 1.0): 1, ('shake', 1.0): 2, ('10:30', 1.0): 1, ('smoke', 1.0): 3, ('white', 1.0): 4, ('grandpa', 1.0): 4, ('prolli', 1.0): 1, ('stash', 1.0): 2, ('closer-chas', 1.0): 1, ('spec', 1.0): 1, ('leagu', 1.0): 3, ('chase', 1.0): 1, ('wall', 1.0): 3, ('angel', 1.0): 4, ('mochamichel', 1.0): 1, ('iph', 1.0): 4, ('0ne', 1.0): 4, ('simpli', 1.0): 3, ('bi0', 1.0): 8, ('x29', 1.0): 1, ('there', 1.0): 2, ('background', 1.0): 2, ('maggi', 1.0): 1, ('afraid', 1.0): 3, ('mull', 1.0): 1, ('nil', 1.0): 1, ('glasgow', 1.0): 2, ('netbal', 1.0): 1, ('thistl', 1.0): 1, ('thistlelov', 1.0): 1, ('minecraft', 1.0): 7, ('drew', 1.0): 3, ('delici', 1.0): 3, ('muddl', 1.0): 1, ('racket', 1.0): 2, ('isol', 1.0): 1, ('fa', 1.0): 1, ('particip', 1.0): 2, ('icecreammast', 1.0): 1, ('group', 1.0): 10, ('huhu', 1.0): 3, ('shet', 1.0): 1, ('desk', 1.0): 1, ('o_o', 1.0): 1, ('orz', 1.0): 1, ('problemmm', 1.0): 1, ('75x', 1.0): 1, ('english', 1.0): 4, ('yeeaayi', 1.0): 1, ('alhamdulillah', 1.0): 1, ('amin', 1.0): 1, ('weed', 1.0): 1, ('crowdfund', 1.0): 1, ('goal', 1.0): 2, ('walk', 1.0): 12, ('hellooo', 1.0): 2, ('select', 1.0): 1, ('lynn', 1.0): 1, ('buffer', 1.0): 2, ('button', 1.0): 2, ('compos', 1.0): 1, ('fridayfun', 1.0): 1, ('non-filipina', 1.0): 1, ('ejayst', 1.0): 1, ('state', 1.0): 2, ('le', 1.0): 2, ('stan', 1.0): 1, ('lee', 1.0): 2, ('discoveri', 1.0): 1, ('cousin', 1.0): 5, ('1400', 1.0): 1, ('yr', 1.0): 2, ('teleport', 1.0): 1, ('shahid', 1.0): 1, ('afridi', 1.0): 1, ('tou', 1.0): 1, ('mahnor', 1.0): 1, ('baloch', 1.0): 1, ('nikki', 1.0): 2, ('flower', 1.0): 4, ('blackfli', 1.0): 1, ('courgett', 1.0): 1, ('wont', 1.0): 5, ('affect', 1.0): 2, ('fruit', 1.0): 5, ('italian', 1.0): 1, ('netfilx', 1.0): 1, ('unmarri', 1.0): 1, ('finger', 1.0): 6, ('rock', 1.0): 10, ('wielli', 1.0): 1, ('paul', 1.0): 2, ('barcod', 1.0): 1, ('charlott', 1.0): 1, ('thta', 1.0): 1, ('trailblazerhonor', 1.0): 1, ('labour', 1.0): 3, ('leader', 1.0): 3, ('alot', 1.0): 2, ('agayhippiehippi', 1.0): 1, ('exercis', 1.0): 2, ('ginger', 1.0): 1, ('x28', 1.0): 1, ('teach', 1.0): 2, ('awar', 1.0): 1, ('::', 1.0): 4, ('portsmouth', 1.0): 1, ('sonal', 1.0): 1, ('hungri', 1.0): 2, ('hmmm', 1.0): 4, ('pedant', 1.0): 1, ('98', 1.0): 1, ('kit', 1.0): 2, ('ack', 1.0): 1, ('hih', 1.0): 1, ('choir', 1.0): 1, ('rosidbinr', 1.0): 1, ('duke', 1.0): 2, ('earl', 1.0): 1, ('tau', 1.0): 1, ('orayt', 1.0): 1, ('knw', 1.0): 1, ('block', 1.0): 3, ('dikha', 1.0): 1, ('reh', 1.0): 1, ('adolf', 1.0): 1, ('hitler', 1.0): 1, ('obstacl', 1.0): 1, ('exist', 1.0): 2, ('surrend', 1.0): 2, ('terrif', 1.0): 1, ('advaddict', 1.0): 1, ('_15', 1.0): 1, ('jimin', 1.0): 1, ('notanapolog', 1.0): 3, ('map', 1.0): 2, ('inform', 1.0): 5, ('0.7', 1.0): 1, ('motherfuck', 1.0): 1, (\"david'\", 1.0): 1, ('damn', 1.0): 3, ('colleg', 1.0): 2, ('24th', 1.0): 3, ('steroid', 1.0): 1, ('alansmithpart', 1.0): 1, ('servu', 1.0): 1, ('bonasio', 1.0): 1, (\"doido'\", 1.0): 1, ('task', 1.0): 2, ('deleg', 1.0): 1, ('aaahhh', 1.0): 1, ('jen', 1.0): 2, ('virgin', 1.0): 5, ('non-mapbox', 1.0): 1, ('restrict', 1.0): 1, ('mapbox', 1.0): 1, ('basemap', 1.0): 1, ('contractu', 1.0): 1, ('research', 1.0): 1, ('seafood', 1.0): 1, ('weltum', 1.0): 1, ('teh', 1.0): 1, ('deti', 1.0): 1, ('huh', 1.0): 2, ('=d', 1.0): 2, ('annoy', 1.0): 2, ('katmtan', 1.0): 1, ('swan', 1.0): 1, ('fandom', 1.0): 3, ('blurri', 1.0): 1, ('besok', 1.0): 1, ('b', 1.0): 8, ('urgent', 1.0): 3, ('within', 1.0): 4, ('dorset', 1.0): 1, ('goddess', 1.0): 1, ('blast', 1.0): 1, ('shitfac', 1.0): 1, ('soul', 1.0): 4, ('sing', 1.0): 5, ('disney', 1.0): 1, ('doug', 1.0): 3, ('28', 1.0): 2, ('bnte', 1.0): 1, ('hain', 1.0): 2, (';p', 1.0): 1, ('shiiitt', 1.0): 1, ('case', 1.0): 9, ('rm35', 1.0): 1, ('negooo', 1.0): 1, ('male', 1.0): 1, ('madelin', 1.0): 1, ('nun', 1.0): 1, ('mornin', 1.0): 2, ('yapster', 1.0): 1, ('pli', 1.0): 1, ('icon', 1.0): 2, ('alchemist', 1.0): 1, ('x27', 1.0): 1, ('dayz', 1.0): 1, ('preview', 1.0): 1, ('thug', 1.0): 1, ('lmao', 1.0): 3, ('sharethelov', 1.0): 2, ('highvalu', 1.0): 2, ('halsey', 1.0): 1, ('30th', 1.0): 1, ('anniversari', 1.0): 5, ('folk', 1.0): 10, ('bae', 1.0): 6, ('repli', 1.0): 5, ('complain', 1.0): 3, ('rude', 1.0): 3, ('bond', 1.0): 4, ('nigg', 1.0): 1, ('readingr', 1.0): 1, ('wordoftheweek', 1.0): 1, ('wotw', 1.0): 1, ('4:18', 1.0): 1, ('est', 1.0): 1, ('earn', 1.0): 1, ('jess', 1.0): 2, ('surri', 1.0): 1, ('botani', 1.0): 1, ('gel', 1.0): 1, ('alison', 1.0): 1, ('lsa', 1.0): 1, ('respons', 1.0): 7, ('fron', 1.0): 1, ('debbi', 1.0): 1, ('carol', 1.0): 2, ('patient', 1.0): 4, ('discharg', 1.0): 1, ('loung', 1.0): 1, ('walmart', 1.0): 1, ('balanc', 1.0): 2, ('studi', 1.0): 6, ('hayley', 1.0): 2, ('shoulder', 1.0): 1, ('pad', 1.0): 2, ('mount', 1.0): 1, ('inquisitor', 1.0): 1, ('cosplay', 1.0): 4, ('cosplayprogress', 1.0): 1, ('mike', 1.0): 3, ('dunno', 1.0): 2, ('insecur', 1.0): 2, ('nh', 1.0): 1, ('devolut', 1.0): 1, ('patriot', 1.0): 1, ('halla', 1.0): 1, ('ark', 1.0): 1, (\"jiyeon'\", 1.0): 1, ('buzz', 1.0): 2, ('burnt', 1.0): 1, ('mist', 1.0): 4, ('opi', 1.0): 1, ('avoplex', 1.0): 1, ('nail', 1.0): 3, ('cuticl', 1.0): 1, ('replenish', 1.0): 1, ('15ml', 1.0): 1, ('seriou', 1.0): 2, ('submiss', 1.0): 1, ('lb', 1.0): 2, ('cherish', 1.0): 2, ('flip', 1.0): 1, ('learnt', 1.0): 2, ('backflip', 1.0): 2, ('jumpgiant', 1.0): 1, ('foampit', 1.0): 1, ('usa', 1.0): 3, ('pamer', 1.0): 1, ('thk', 1.0): 1, ('actuallythough', 1.0): 1, ('craft', 1.0): 2, ('session', 1.0): 3, ('mehtab', 1.0): 1, ('aunti', 1.0): 1, ('gc', 1.0): 1, ('yeeew', 1.0): 1, ('pre', 1.0): 3, ('lan', 1.0): 1, ('yeey', 1.0): 1, ('arrang', 1.0): 1, ('doodl', 1.0): 2, ('comic', 1.0): 1, ('summon', 1.0): 1, ('none', 1.0): 1, ('🙅', 1.0): 1, ('lycra', 1.0): 1, ('vincent', 1.0): 1, ('couldnt', 1.0): 1, ('roy', 1.0): 1, ('bg', 1.0): 1, ('img', 1.0): 1, ('circl', 1.0): 1, ('font', 1.0): 1, ('deathofgrass', 1.0): 1, ('loan', 1.0): 2, ('lawnmow', 1.0): 1, ('popular', 1.0): 2, ('charismat', 1.0): 1, ('man.h', 1.0): 1, ('thrive', 1.0): 1, ('economi', 1.0): 1, ('burst', 1.0): 2, ('georgi', 1.0): 1, ('x26', 1.0): 1, ('million', 1.0): 4, ('fl', 1.0): 1, ('kindest', 1.0): 2, ('iceland', 1.0): 1, ('crazi', 1.0): 4, ('landscap', 1.0): 2, ('yok', 1.0): 1, ('lah', 1.0): 1, ('concordia', 1.0): 1, ('reunit', 1.0): 1, ('xxxibmchll', 1.0): 1, ('sea', 1.0): 4, ('prettier', 1.0): 2, ('imitatia', 1.0): 1, ('oe', 1.0): 1, ('michel', 1.0): 1, ('comeback', 1.0): 1, ('gross', 1.0): 1, ('treat', 1.0): 5, ('equal', 1.0): 2, ('injustic', 1.0): 1, ('femin', 1.0): 1, ('ineedfeminismbecaus', 1.0): 1, ('forgotten', 1.0): 3, ('stuck', 1.0): 4, ('recommend', 1.0): 4, ('redhead', 1.0): 1, ('wacki', 1.0): 1, ('rather', 1.0): 5, ('waytoliveahappylif', 1.0): 1, ('hoxton', 1.0): 1, ('holborn', 1.0): 1, ('karen', 1.0): 2, ('wag', 1.0): 2, ('bum', 1.0): 1, ('wwooo', 1.0): 1, ('nite', 1.0): 3, ('laiten', 1.0): 1, ('arond', 1.0): 1, ('1:30', 1.0): 1, ('consid', 1.0): 3, ('matur', 1.0): 3, ('journeyp', 1.0): 2, ('foam', 1.0): 1, (\"lady'\", 1.0): 1, ('mob', 1.0): 1, ('fals', 1.0): 1, ('bulletin', 1.0): 1, ('spring', 1.0): 1, ('fiesta', 1.0): 1, ('nois', 1.0): 2, ('awuuu', 1.0): 1, ('aich', 1.0): 1, ('sept', 1.0): 2, ('rudramadevi', 1.0): 1, ('anushka', 1.0): 1, ('gunashekar', 1.0): 1, ('harryxhood', 1.0): 1, ('upset', 1.0): 1, ('ooh', 1.0): 1, ('humanist', 1.0): 1, ('magazin', 1.0): 2, ('usernam', 1.0): 1, ('rape', 1.0): 1, ('csrrace', 1.0): 1, ('lack', 1.0): 6, ('hygien', 1.0): 1, ('tose', 1.0): 1, ('cloth', 1.0): 1, ('temperatur', 1.0): 1, ('planet', 1.0): 2, ('brave', 1.0): 2, ('ge', 1.0): 1, ('2015kenya', 1.0): 1, ('ryan', 1.0): 4, ('tidi', 1.0): 2, ('hagergang', 1.0): 1, ('chanhun', 1.0): 1, ('photoshoot', 1.0): 1, ('afteral', 1.0): 1, ('sadkaay', 1.0): 1, ('thark', 1.0): 1, ('peak', 1.0): 1, ('heatwav', 1.0): 1, ('lower', 1.0): 1, ('standard', 1.0): 2, ('x25', 1.0): 1, ('recruit', 1.0): 2, ('doom', 1.0): 1, ('nasti', 1.0): 1, ('affili', 1.0): 1, ('&gt;:)', 1.0): 2, ('64', 1.0): 2, ('74', 1.0): 1, ('40', 1.0): 4, ('00', 1.0): 1, ('hall', 1.0): 2, ('ted', 1.0): 3, ('pixgram', 1.0): 2, ('creativ', 1.0): 2, ('slideshow', 1.0): 1, ('nibbl', 1.0): 2, ('ivi', 1.0): 1, ('sho', 1.0): 1, ('superpow', 1.0): 2, ('obsess', 1.0): 2, ('oth', 1.0): 1, ('third', 1.0): 2, ('ngarepfollbackdarinabilahjkt', 1.0): 1, ('48', 1.0): 1, ('sunglass', 1.0): 1, ('jacki', 1.0): 2, ('sunni', 1.0): 6, ('style', 1.0): 5, ('jlo', 1.0): 1, ('jlover', 1.0): 1, ('turkey', 1.0): 1, ('goodafternoon', 1.0): 2, ('collag', 1.0): 2, ('furri', 1.0): 2, ('bruce', 1.0): 2, ('kunoriforceo', 1.0): 8, ('aayegi', 1.0): 1, ('tim', 1.0): 2, ('wiw', 1.0): 1, ('bip', 1.0): 1, ('zareen', 1.0): 1, ('daisi', 1.0): 1, (\"b'coz\", 1.0): 1, ('kart', 1.0): 1, ('mak', 1.0): 1, ('∗', 1.0): 2, ('lega', 1.0): 1, ('spag', 1.0): 1, ('boat', 1.0): 2, ('outboard', 1.0): 1, ('spell', 1.0): 4, ('reboard', 1.0): 1, ('fire', 1.0): 2, ('offboard', 1.0): 1, ('sn16', 1.0): 1, ('9dg', 1.0): 1, ('bnf', 1.0): 1, ('50', 1.0): 1, ('jason', 1.0): 1, ('rob', 1.0): 2, ('feb', 1.0): 1, ('victoriasecret', 1.0): 1, ('finland', 1.0): 1, ('helsinki', 1.0): 1, ('airport', 1.0): 3, ('plane', 1.0): 2, ('beyond', 1.0): 4, ('ont', 1.0): 1, ('tii', 1.0): 1, ('lng', 1.0): 2, ('yan', 1.0): 2, (\"u'll\", 1.0): 2, ('steve', 1.0): 2, ('bell', 1.0): 1, ('prescott', 1.0): 1, ('leadership', 1.0): 2, ('cartoon', 1.0): 1, ('upsid', 1.0): 2, ('statement', 1.0): 1, ('selamathariraya', 1.0): 1, ('lovesummertim', 1.0): 1, ('dumont', 1.0): 1, ('jax', 1.0): 1, ('jone', 1.0): 1, ('awesomee', 1.0): 1, ('x24', 1.0): 1, ('geoff', 1.0): 1, ('amazingli', 1.0): 1, ('talant', 1.0): 1, ('vsco', 1.0): 2, ('thanki', 1.0): 2, ('hash', 1.0): 1, ('tag', 1.0): 5, ('ifimeetanalien', 1.0): 1, ('bff', 1.0): 4, ('section', 1.0): 3, ('follbaaack', 1.0): 1, ('az', 1.0): 1, ('cauliflow', 1.0): 1, ('attempt', 1.0): 1, ('prinsesa', 1.0): 1, ('yaaah', 1.0): 2, ('law', 1.0): 3, ('toy', 1.0): 2, ('sonaaa', 1.0): 1, ('beautiful', 1.0): 2, (\"josephine'\", 1.0): 1, ('mirror', 1.0): 3, ('cretaperfect', 1.0): 2, ('4me', 1.0): 2, ('cretaperfectsuv', 1.0): 2, ('creta', 1.0): 1, ('load', 1.0): 1, ('telecom', 1.0): 2, ('judi', 1.0): 1, ('superb', 1.0): 1, ('slightli', 1.0): 1, ('rakna', 1.0): 1, ('ew', 1.0): 1, ('whose', 1.0): 1, ('fifa', 1.0): 1, ('lineup', 1.0): 1, ('surviv', 1.0): 2, ('p90x', 1.0): 1, ('p90', 1.0): 1, ('dishoom', 1.0): 2, ('rajnigandha', 1.0): 1, ('minju', 1.0): 1, ('rapper', 1.0): 1, ('lead', 1.0): 2, ('vocal', 1.0): 1, ('yujin', 1.0): 1, ('visual', 1.0): 2, ('makna', 1.0): 1, ('jane', 1.0): 2, ('hah', 1.0): 4, ('hawk', 1.0): 2, ('greatest', 1.0): 2, ('histori', 1.0): 2, ('along', 1.0): 6, ('talkback', 1.0): 1, ('process', 1.0): 4, ('featur', 1.0): 4, ('mostli', 1.0): 1, (\"cinema'\", 1.0): 1, ('defend', 1.0): 2, ('fashion', 1.0): 2, ('atroc', 1.0): 1, ('pandimension', 1.0): 1, ('manifest', 1.0): 1, ('argo', 1.0): 1, ('ring', 1.0): 4, ('640', 1.0): 1, ('nad', 1.0): 1, ('plezzz', 1.0): 1, ('asthma', 1.0): 1, ('inhal', 1.0): 1, ('breath', 1.0): 3, ('goodluck', 1.0): 1, ('hunger', 1.0): 1, ('mockingjay', 1.0): 1, ('thehungergam', 1.0): 1, ('ador', 1.0): 4, ('x23', 1.0): 1, ('reina', 1.0): 1, ('felt', 1.0): 3, ('excus', 1.0): 2, ('attend', 1.0): 2, ('whn', 1.0): 1, ('andr', 1.0): 1, ('mamayang', 1.0): 1, ('11pm', 1.0): 1, ('1d', 1.0): 2, ('89.9', 1.0): 1, ('powi', 1.0): 1, ('shropshir', 1.0): 1, ('border', 1.0): 1, (\"school'\", 1.0): 1, ('san', 1.0): 2, ('diego', 1.0): 1, ('jump', 1.0): 2, ('sourc', 1.0): 3, ('appeas', 1.0): 1, ('¦', 1.0): 1, ('aj', 1.0): 1, ('action', 1.0): 1, ('grunt', 1.0): 1, ('sc', 1.0): 1, ('anti-christ', 1.0): 1, ('m8', 1.0): 1, ('ju', 1.0): 1, ('halfway', 1.0): 1, ('ex', 1.0): 2, ('postiv', 1.0): 2, ('opinion', 1.0): 3, ('avi', 1.0): 1, ('dare', 1.0): 4, ('corridor', 1.0): 1, ('👯', 1.0): 2, ('neither', 1.0): 2, ('rundown', 1.0): 1, ('yah', 1.0): 4, ('leviboard', 1.0): 1, ('kleper', 1.0): 1, (':(', 1.0): 1, ('impecc', 1.0): 2, ('setokido', 1.0): 1, ('shoulda', 1.0): 3, ('hippo', 1.0): 1, ('materialist', 1.0): 1, ('showpo', 1.0): 1, ('cough', 1.0): 6, ('@artofsleepingin', 1.0): 1, ('x22', 1.0): 1, ('☺', 1.0): 5, ('makesm', 1.0): 1, ('santorini', 1.0): 1, ('escap', 1.0): 2, ('beatport', 1.0): 1, ('👊🏻', 1.0): 1, ('trmdhesit', 1.0): 2, ('manuel', 1.0): 1, ('vall', 1.0): 1, ('king', 1.0): 3, ('seven', 1.0): 2, ('kingdom', 1.0): 2, ('andal', 1.0): 1, ('taught', 1.0): 1, ('hide', 1.0): 3, ('privaci', 1.0): 1, ('wise', 1.0): 1, ('natsuki', 1.0): 1, ('often', 1.0): 2, ('catchi', 1.0): 1, ('neil', 1.0): 2, ('emir', 1.0): 2, ('brill', 1.0): 1, ('urquhart', 1.0): 1, ('castl', 1.0): 1, ('simpl', 1.0): 2, ('shatter', 1.0): 2, ('contrast', 1.0): 1, ('educampakl', 1.0): 1, ('rotorua', 1.0): 1, ('pehli', 1.0): 1, ('phir', 1.0): 1, ('somi', 1.0): 1, ('burfday', 1.0): 1, ('univers', 1.0): 3, ('santo', 1.0): 1, ('toma', 1.0): 1, ('norh', 1.0): 1, ('dialogu', 1.0): 2, ('chainsaw', 1.0): 2, ('amus', 1.0): 1, ('awe', 1.0): 1, ('protect', 1.0): 2, ('pop', 1.0): 5, ('2ish', 1.0): 1, ('fahad', 1.0): 1, ('bhai', 1.0): 3, ('iqrar', 1.0): 1, ('waseem', 1.0): 1, ('abroad', 1.0): 2, ('movie', 1.0): 1, ('chef', 1.0): 1, ('grogol', 1.0): 1, ('long-dist', 1.0): 1, ('rhi', 1.0): 1, ('pwrfl', 1.0): 1, ('benefit', 1.0): 2, ('b2b', 1.0): 1, ('b2c', 1.0): 1, (\"else'\", 1.0): 2, ('soo', 1.0): 2, ('enterprison', 1.0): 1, ('schoolsoutforsumm', 1.0): 1, ('fellow', 1.0): 4, ('juggl', 1.0): 1, ('purrtho', 1.0): 1, ('catho', 1.0): 1, ('catami', 1.0): 1, ('fourfivesecond', 1.0): 4, ('deaf', 1.0): 4, ('drug', 1.0): 1, ('alcohol', 1.0): 1, ('apexi', 1.0): 3, ('crystal', 1.0): 3, ('meth', 1.0): 1, ('champagn', 1.0): 1, ('fc', 1.0): 1, ('streamer', 1.0): 1, ('juic', 1.0): 1, ('correct', 1.0): 1, ('portrait', 1.0): 1, ('izumi', 1.0): 1, ('fugiwara', 1.0): 1, ('clonmel', 1.0): 1, ('vibrant', 1.0): 1, ('estim', 1.0): 1, ('server', 1.0): 2, ('quiet', 1.0): 1, ('yey', 1.0): 1, (\"insha'allah\", 1.0): 1, ('wil', 1.0): 1, ('x21', 1.0): 1, ('trend', 1.0): 3, ('akshaymostlovedsuperstarev', 1.0): 1, ('indirect', 1.0): 1, ('askurban', 1.0): 1, ('lyka', 1.0): 2, ('nap', 1.0): 4, ('aff', 1.0): 1, ('unam', 1.0): 1, ('jonginuh', 1.0): 1, ('forecast', 1.0): 2, ('10am', 1.0): 2, ('5am', 1.0): 1, ('sooth', 1.0): 1, ('vii', 1.0): 1, ('sweetheart', 1.0): 1, ('freak', 1.0): 3, ('zayn', 1.0): 3, ('fucker', 1.0): 1, ('pet', 1.0): 2, ('illustr', 1.0): 1, ('wohoo', 1.0): 1, ('gleam', 1.0): 1, ('paint', 1.0): 4, ('deal', 1.0): 2, ('prime', 1.0): 2, ('minist', 1.0): 2, ('sunjam', 1.0): 1, ('industri', 1.0): 1, ('present', 1.0): 7, ('practic', 1.0): 3, ('proactiv', 1.0): 1, ('environ', 1.0): 1, ('unreal', 1.0): 1, ('zain', 1.0): 1, ('zac', 1.0): 1, ('isaac', 1.0): 1, ('oss', 1.0): 1, ('frank', 1.0): 1, ('iero', 1.0): 1, ('phase', 1.0): 2, ('david', 1.0): 1, ('beginn', 1.0): 1, ('shine', 1.0): 3, ('sunflow', 1.0): 2, ('tommarow', 1.0): 1, ('yall', 1.0): 2, ('rank', 1.0): 2, ('birthdaymonth', 1.0): 1, ('vianey', 1.0): 1, ('juli', 1.0): 11, ('birthdaygirl', 1.0): 1, (\"town'\", 1.0): 1, ('andrew', 1.0): 2, ('checkout', 1.0): 2, ('otwol', 1.0): 1, ('awhil', 1.0): 1, ('x20', 1.0): 1, ('all-tim', 1.0): 1, ('julia', 1.0): 1, ('robert', 1.0): 1, ('awwhh', 1.0): 1, ('bulldog', 1.0): 1, ('unfortun', 1.0): 2, ('02079', 1.0): 1, ('490', 1.0): 1, ('132', 1.0): 1, ('born', 1.0): 2, ('fightstickfriday', 1.0): 1, ('extravag', 1.0): 2, ('tearout', 1.0): 1, ('selekt', 1.0): 1, ('yoot', 1.0): 1, ('cross', 1.0): 3, ('gudday', 1.0): 1, ('dave', 1.0): 5, ('haileyhelp', 1.0): 1, ('eid', 1.0): 2, ('mubarak', 1.0): 5, ('brotheeerrr', 1.0): 1, ('adventur', 1.0): 5, ('tokyo', 1.0): 2, ('kansai', 1.0): 1, ('l', 1.0): 4, ('upp', 1.0): 2, ('om', 1.0): 1, ('60', 1.0): 1, ('minut', 1.0): 7, ('data', 1.0): 1, ('jesu', 1.0): 5, ('amsterdam', 1.0): 2, ('3rd', 1.0): 3, ('nextweek', 1.0): 1, ('booti', 1.0): 2, ('bcuz', 1.0): 1, ('step', 1.0): 3, ('option', 1.0): 3, ('stabl', 1.0): 1, ('sturdi', 1.0): 1, ('lukkke', 1.0): 1, ('again.ensoi', 1.0): 1, ('tc', 1.0): 1, ('madam', 1.0): 1, ('siddi', 1.0): 1, ('unknown', 1.0): 2, ('roomi', 1.0): 1, ('gn', 1.0): 2, ('gf', 1.0): 2, ('consent', 1.0): 1, ('mister', 1.0): 2, ('vine', 1.0): 2, ('peyton', 1.0): 1, ('nagato', 1.0): 1, ('yuki-chan', 1.0): 1, ('shoushitsu', 1.0): 1, ('archdbanterburi', 1.0): 3, ('experttradesmen', 1.0): 1, ('banter', 1.0): 1, ('quiz', 1.0): 1, ('tradetalk', 1.0): 1, ('floof', 1.0): 1, ('face', 1.0): 13, ('muahah', 1.0): 1, ('x19', 1.0): 1, ('anticip', 1.0): 1, ('jd', 1.0): 1, ('laro', 1.0): 1, ('tayo', 1.0): 1, ('answer', 1.0): 8, ('ht', 1.0): 1, ('angelica', 1.0): 1, ('anghel', 1.0): 1, ('aa', 1.0): 3, ('kkk', 1.0): 1, ('macbook', 1.0): 1, ('rehears', 1.0): 1, ('youthcelebr', 1.0): 1, ('mute', 1.0): 1, ('29th', 1.0): 1, ('gohf', 1.0): 4, ('vegetarian', 1.0): 1, (\"she'll\", 1.0): 1, ('gooday', 1.0): 3, ('101', 1.0): 3, ('12000', 1.0): 1, ('oshieer', 1.0): 1, ('realreview', 1.0): 1, ('happycustom', 1.0): 1, ('realoshi', 1.0): 1, ('dealsuthaonotebachao', 1.0): 1, ('bigger', 1.0): 2, ('dime', 1.0): 1, ('uhuh', 1.0): 1, ('🎵', 1.0): 3, ('code', 1.0): 4, ('pleasant', 1.0): 2, ('on-board', 1.0): 1, ('raheel', 1.0): 1, ('flyhigh', 1.0): 1, ('bother', 1.0): 2, ('everett', 1.0): 1, ('taylor', 1.0): 1, ('ha-ha', 1.0): 1, ('peachyloan', 1.0): 1, ('fridayfreebi', 1.0): 1, ('noe', 1.0): 1, ('yisss', 1.0): 1, ('bindingofissac', 1.0): 1, ('xboxon', 1.0): 1, ('consol', 1.0): 1, ('justin', 1.0): 2, ('gladli', 1.0): 1, ('son', 1.0): 4, ('morocco', 1.0): 1, ('peru', 1.0): 1, ('nxt', 1.0): 1, ('bp', 1.0): 1, ('resort', 1.0): 1, ('x18', 1.0): 1, ('havuuulovey', 1.0): 1, ('uuu', 1.0): 1, ('possitv', 1.0): 1, ('hopey', 1.0): 1, ('throwbackfriday', 1.0): 1, ('christen', 1.0): 1, ('ki', 1.0): 1, ('yaad', 1.0): 1, ('gayi', 1.0): 1, ('opossum', 1.0): 1, ('belat', 1.0): 5, ('yeahh', 1.0): 2, ('kuffar', 1.0): 1, ('comput', 1.0): 5, ('cell', 1.0): 1, ('diarrhea', 1.0): 1, ('immigr', 1.0): 1, ('lice', 1.0): 1, ('goictiv', 1.0): 1, ('70685', 1.0): 1, ('tagsforlik', 1.0): 4, ('trapmus', 1.0): 1, ('hotmusicdeloco', 1.0): 1, ('kinick', 1.0): 1, ('01282', 1.0): 2, ('452096', 1.0): 1, ('shadi', 1.0): 1, ('reserv', 1.0): 3, ('tkt', 1.0): 1, ('likewis', 1.0): 4, ('overgener', 1.0): 1, ('ikr', 1.0): 1, ('😍', 1.0): 2, ('consumer', 1.0): 1, ('fic', 1.0): 2, ('ouch', 1.0): 2, ('slip', 1.0): 1, ('disc', 1.0): 1, ('thw', 1.0): 1, ('chute', 1.0): 1, ('chalut', 1.0): 1, ('replay', 1.0): 1, ('iplay', 1.0): 1, ('11am', 1.0): 3, ('unneed', 1.0): 1, ('megamoh', 1.0): 1, ('7/29', 1.0): 1, ('tool', 1.0): 2, ('zealand', 1.0): 1, ('pile', 1.0): 2, ('dump', 1.0): 1, ('couscou', 1.0): 3, (\"women'\", 1.0): 2, ('fiction', 1.0): 1, ('wahahaah', 1.0): 1, ('x17', 1.0): 1, ('orhan', 1.0): 1, ('pamuk', 1.0): 1, ('hero', 1.0): 3, ('canopi', 1.0): 1, ('mapl', 1.0): 2, ('syrup', 1.0): 1, ('farm', 1.0): 2, ('stephani', 1.0): 2, ('💖', 1.0): 2, ('congrtaualt', 1.0): 1, ('philea', 1.0): 1, ('club', 1.0): 4, ('inc', 1.0): 1, ('photograph', 1.0): 2, ('phonegraph', 1.0): 1, ('srsli', 1.0): 1, ('10:17', 1.0): 1, ('ripaaa', 1.0): 1, ('banat', 1.0): 1, ('ray', 1.0): 1, ('dept', 1.0): 1, ('hospit', 1.0): 3, ('grt', 1.0): 1, ('infograph', 1.0): 1, (\"o'clock\", 1.0): 2, ('habit', 1.0): 1, ('1dfor', 1.0): 1, ('roadtrip', 1.0): 1, ('19:30', 1.0): 1, ('ifc', 1.0): 1, ('whip', 1.0): 1, ('lilsisbro', 1.0): 1, ('pre-ord', 1.0): 2, (\"pixar'\", 1.0): 2, ('steelbook', 1.0): 1, ('hmm', 1.0): 2, ('pegel', 1.0): 1, ('lemess', 1.0): 1, ('kyle', 1.0): 2, ('paypal', 1.0): 1, ('oct', 1.0): 1, ('tud', 1.0): 1, ('jst', 1.0): 2, ('humphrey', 1.0): 1, ('yell', 1.0): 2, ('erm', 1.0): 1, ('breach', 1.0): 1, ('lemon', 1.0): 2, ('yogurt', 1.0): 2, ('pot', 1.0): 1, ('discov', 1.0): 2, ('liquoric', 1.0): 1, ('pud', 1.0): 1, ('cajun', 1.0): 1, ('spice', 1.0): 1, ('yum', 1.0): 2, ('cajunchicken', 1.0): 1, ('infinit', 1.0): 2, ('fight', 1.0): 4, ('gern', 1.0): 1, ('cikaaa', 1.0): 1, ('maaf', 1.0): 1, ('telat', 1.0): 1, ('ngucapinnya', 1.0): 1, ('maaay', 1.0): 1, ('x16', 1.0): 1, ('viparita', 1.0): 1, ('karani', 1.0): 1, ('legsupthewal', 1.0): 1, ('unwind', 1.0): 1, ('coco', 1.0): 3, ('comfi', 1.0): 1, ('jalulu', 1.0): 1, ('rosh', 1.0): 1, ('gla', 1.0): 1, ('pallavi', 1.0): 1, ('nairobi', 1.0): 1, ('hrdstellobama', 1.0): 1, ('region', 1.0): 2, ('civil', 1.0): 1, ('societi', 1.0): 2, ('globe', 1.0): 1, ('hajur', 1.0): 1, ('yayi', 1.0): 2, (\"must'v\", 1.0): 1, ('nerv', 1.0): 1, ('prelim', 1.0): 1, ('costacc', 1.0): 1, ('nwb', 1.0): 1, ('shud', 1.0): 1, ('cold', 1.0): 2, ('hmu', 1.0): 2, ('cala', 1.0): 1, ('brush', 1.0): 1, ('ego', 1.0): 1, ('wherev', 1.0): 1, ('interact', 1.0): 2, ('dongsaeng', 1.0): 1, ('chorong', 1.0): 1, ('friendship', 1.0): 1, ('impress', 1.0): 3, ('dragon', 1.0): 2, ('duck', 1.0): 5, ('mix', 1.0): 5, ('cheetah', 1.0): 1, ('wagga', 1.0): 2, ('coursework', 1.0): 1, ('lorna', 1.0): 1, ('scan', 1.0): 1, ('x12', 1.0): 2, ('canva', 1.0): 2, ('iqbal', 1.0): 1, ('ima', 1.0): 1, ('hon', 1.0): 1, ('aja', 1.0): 1, ('besi', 1.0): 1, ('chati', 1.0): 1, ('phulani', 1.0): 1, ('swasa', 1.0): 1, ('bahari', 1.0): 1, ('jiba', 1.0): 1, ('mumbai', 1.0): 1, ('gujarat', 1.0): 1, ('distrub', 1.0): 1, ('otherwis', 1.0): 5, ('190cr', 1.0): 1, ('inspit', 1.0): 1, ('highest', 1.0): 1, ('holder', 1.0): 1, ('threaten', 1.0): 1, ('daili', 1.0): 2, ('basi', 1.0): 1, ('vr', 1.0): 1, ('angelo', 1.0): 1, ('quezon', 1.0): 1, ('sweatpant', 1.0): 1, ('farbridg', 1.0): 1, ('segalakatakata', 1.0): 1, ('nixu', 1.0): 1, ('begun', 1.0): 1, ('flint', 1.0): 1, ('🍰', 1.0): 5, ('separ', 1.0): 1, ('criticis', 1.0): 1, ('gestur', 1.0): 1, ('pedal', 1.0): 1, ('stroke', 1.0): 1, ('caro', 1.0): 1, ('deposit', 1.0): 1, ('secur', 1.0): 2, ('shock', 1.0): 1, ('coff', 1.0): 2, ('tenerina', 1.0): 1, ('auguri', 1.0): 1, ('iso', 1.0): 1, ('certif', 1.0): 1, ('paralyz', 1.0): 1, ('anxieti', 1.0): 1, (\"it'd\", 1.0): 1, ('develop', 1.0): 3, ('spain', 1.0): 2, ('def', 1.0): 1, ('bantim', 1.0): 1, ('fail', 1.0): 5, ('2ban', 1.0): 1, ('x15', 1.0): 1, ('awkward', 1.0): 2, ('ab', 1.0): 1, ('gale', 1.0): 1, ('founder', 1.0): 1, ('loveyaaah', 1.0): 1, ('⅛', 1.0): 1, ('⅞', 1.0): 1, ('∞', 1.0): 1, ('specialist', 1.0): 1, ('aw', 1.0): 3, ('babyyi', 1.0): 1, ('djstruthmat', 1.0): 1, ('re-cap', 1.0): 1, ('flickr', 1.0): 1, ('tack', 1.0): 2, ('zephbot', 1.0): 1, ('hhahahahaha', 1.0): 1, ('blew', 1.0): 2, ('entir', 1.0): 2, ('vega', 1.0): 3, ('strip', 1.0): 1, ('hahahahahhaha', 1.0): 1, (\"callie'\", 1.0): 1, ('puppi', 1.0): 1, ('owner', 1.0): 2, ('callinganimalabusehotlineasap', 1.0): 1, ('gorefiend', 1.0): 1, ('mythic', 1.0): 1, ('remind', 1.0): 6, ('9:00', 1.0): 1, ('▪', 1.0): 2, ('️bea', 1.0): 1, ('miller', 1.0): 2, ('lockscreen', 1.0): 1, ('mbf', 1.0): 1, ('keesh', 1.0): 1, (\"yesterday'\", 1.0): 1, ('groupi', 1.0): 1, ('bebe', 1.0): 1, ('sizam', 1.0): 1, ('color', 1.0): 5, ('invoic', 1.0): 1, ('kanina', 1.0): 1, ('pong', 1.0): 1, ('umaga', 1.0): 1, ('browser', 1.0): 1, ('typic', 1.0): 2, ('pleass', 1.0): 5, ('leeteuk', 1.0): 1, ('pearl', 1.0): 1, ('thusi', 1.0): 1, ('pour', 1.0): 1, ('milk', 1.0): 2, ('tgv', 1.0): 1, ('pari', 1.0): 5, ('austerlitz', 1.0): 1, ('bloi', 1.0): 1, ('mile', 1.0): 3, ('chateau', 1.0): 1, ('de', 1.0): 1, ('marai', 1.0): 1, ('taxi', 1.0): 1, ('x14', 1.0): 1, ('nom', 1.0): 1, ('enji', 1.0): 1, ('hater', 1.0): 3, ('purchas', 1.0): 2, ('specially-mark', 1.0): 1, ('custard', 1.0): 1, ('sm', 1.0): 1, ('on-pack', 1.0): 1, ('instruct', 1.0): 1, ('tile', 1.0): 1, ('downstair', 1.0): 1, ('kelli', 1.0): 1, ('greek', 1.0): 2, ('petra', 1.0): 1, ('shadowplayloui', 1.0): 1, ('mutual', 1.0): 2, ('cuz', 1.0): 4, ('liveonstream', 1.0): 1, ('lani', 1.0): 1, ('graze', 1.0): 1, ('pride', 1.0): 1, ('bristolart', 1.0): 1, ('in-app', 1.0): 1, ('ensur', 1.0): 1, ('item', 1.0): 2, ('screw', 1.0): 1, ('amber', 1.0): 2, ('43', 1.0): 1, ('hpc', 1.0): 1, ('wip', 1.0): 2, ('sw', 1.0): 1, ('newsround', 1.0): 1, ('hound', 1.0): 1, ('7:40', 1.0): 1, ('ada', 1.0): 1, ('racist', 1.0): 1, ('hulk', 1.0): 1, ('tight', 1.0): 2, ('prayer', 1.0): 3, ('pardon', 1.0): 1, ('phl', 1.0): 1, ('abu', 1.0): 2, ('dhabi', 1.0): 1, ('hihihi', 1.0): 1, ('teamjanuaryclaim', 1.0): 1, ('godonna', 1.0): 1, ('msg', 1.0): 2, ('bowwowchicawowwow', 1.0): 1, ('settl', 1.0): 1, ('dkt', 1.0): 1, ('porch', 1.0): 1, ('uber', 1.0): 2, ('mobil', 1.0): 4, ('applic', 1.0): 3, ('giggl', 1.0): 2, ('bare', 1.0): 3, ('wind', 1.0): 2, ('kahlil', 1.0): 1, ('gibran', 1.0): 1, ('flash', 1.0): 1, ('stiff', 1.0): 1, ('upper', 1.0): 1, ('lip', 1.0): 1, ('britain', 1.0): 1, ('latmon', 1.0): 1, ('endeavour', 1.0): 1, ('ann', 1.0): 2, ('joy', 1.0): 4, ('os', 1.0): 1, ('exploit', 1.0): 1, ('ign', 1.0): 2, ('au', 1.0): 1, ('pubcast', 1.0): 1, ('tengaman', 1.0): 1, ('21', 1.0): 2, ('celebratio', 1.0): 1, ('women', 1.0): 1, ('instal', 1.0): 2, ('glorifi', 1.0): 1, ('infirm', 1.0): 1, ('silli', 1.0): 1, ('suav', 1.0): 1, ('gentlemen', 1.0): 1, ('monthli', 1.0): 1, ('mileag', 1.0): 1, ('target', 1.0): 2, ('samsung', 1.0): 1, ('qualiti', 1.0): 3, ('ey', 1.0): 1, ('beth', 1.0): 2, ('gangster', 1.0): 1, (\"athena'\", 1.0): 1, ('fanci', 1.0): 1, ('wellington', 1.0): 1, ('rich', 1.0): 2, ('christina', 1.0): 1, ('newslett', 1.0): 1, ('zy', 1.0): 1, ('olur', 1.0): 1, ('x13', 1.0): 1, ('flawless', 1.0): 1, ('reaction', 1.0): 2, ('hayli', 1.0): 1, ('edwin', 1.0): 1, ('elvena', 1.0): 1, ('emc', 1.0): 1, ('rubber', 1.0): 3, ('swearword', 1.0): 1, ('infect', 1.0): 1, ('10:16', 1.0): 1, ('wrote', 1.0): 3, ('gan', 1.0): 1, ('brotherhood', 1.0): 1, ('wolf', 1.0): 5, ('pill', 1.0): 1, ('nocturn', 1.0): 1, ('rrp', 1.0): 1, ('18.99', 1.0): 1, ('13.99', 1.0): 1, ('jah', 1.0): 1, ('wobbl', 1.0): 1, ('retard', 1.0): 1, ('50notif', 1.0): 1, ('check-up', 1.0): 1, ('pun', 1.0): 1, ('elit', 1.0): 1, ('camillu', 1.0): 1, ('pleasee', 1.0): 1, ('spare', 1.0): 1, ('tyre', 1.0): 2, ('joke', 1.0): 3, ('ahahah', 1.0): 1, ('shame', 1.0): 1, ('abandon', 1.0): 1, ('disagre', 1.0): 2, ('nowher', 1.0): 2, ('contradict', 1.0): 1, ('chao', 1.0): 1, ('contain', 1.0): 1, ('cranium', 1.0): 1, ('sneaker', 1.0): 1, ('nike', 1.0): 1, ('nikeorigin', 1.0): 1, ('nikeindonesia', 1.0): 1, ('pierojogg', 1.0): 1, ('skoy', 1.0): 1, ('winter', 1.0): 2, ('falkland', 1.0): 1, ('jamie-le', 1.0): 1, ('congraaat', 1.0): 1, ('hooh', 1.0): 1, ('chrome', 1.0): 1, ('storm', 1.0): 1, ('thunderstorm', 1.0): 1, ('circuscircu', 1.0): 1, ('omgg', 1.0): 1, ('tdi', 1.0): 1, ('(-:', 1.0): 2, ('peter', 1.0): 1, ('expel', 1.0): 2, ('boughi', 1.0): 1, ('kernel', 1.0): 1, ('paralysi', 1.0): 1, ('liza', 1.0): 1, ('lol.hook', 1.0): 1, ('vampir', 1.0): 2, ('diari', 1.0): 3, ('twice', 1.0): 1, ('thanq', 1.0): 2, ('goodwil', 1.0): 1, ('vandr', 1.0): 1, ('ash', 1.0): 1, ('debat', 1.0): 3, ('solar', 1.0): 1, ('6-5', 1.0): 1, ('shown', 1.0): 1, ('ek', 1.0): 1, ('taco', 1.0): 2, ('mexico', 1.0): 2, ('viva', 1.0): 1, ('méxico', 1.0): 1, ('burger', 1.0): 3, ('thebestangkapuso', 1.0): 1, ('lighter', 1.0): 1, ('tooth', 1.0): 2, ('korean', 1.0): 2, ('netizen', 1.0): 1, ('crueler', 1.0): 1, ('eleph', 1.0): 1, ('marula', 1.0): 1, ('tdif', 1.0): 1, ('shoutout', 1.0): 1, ('shortli', 1.0): 1, ('itsamarvelth', 1.0): 1, (\"japan'\", 1.0): 1, ('artist', 1.0): 1, ('homework', 1.0): 1, ('marco', 1.0): 1, ('herb', 1.0): 1, ('pm', 1.0): 3, ('self', 1.0): 1, ('esteem', 1.0): 1, ('patienc', 1.0): 1, ('sobtian', 1.0): 1, ('cowork', 1.0): 1, ('deathli', 1.0): 1, ('hallow', 1.0): 1, ('supernatur', 1.0): 1, ('consult', 1.0): 1, ('himach', 1.0): 1, ('2.25', 1.0): 1, ('asham', 1.0): 1, ('where.do.i.start', 1.0): 1, ('moviemarathon', 1.0): 1, ('skill', 1.0): 4, ('shadow', 1.0): 1, ('own', 1.0): 1, ('pair', 1.0): 3, (\"it'll\", 1.0): 6, ('cortez', 1.0): 1, ('superstar', 1.0): 1, ('tthank', 1.0): 1, ('colin', 1.0): 1, ('luxuou', 1.0): 1, ('tarryn', 1.0): 1, ('hbdme', 1.0): 1, ('yeeeyyy', 1.0): 1, ('barsostay', 1.0): 1, ('males', 1.0): 1, ('independ', 1.0): 1, ('sum', 1.0): 1, ('debacl', 1.0): 1, ('perfectli', 1.0): 1, ('longer', 1.0): 2, ('amyjackson', 1.0): 1, ('omegl', 1.0): 2, ('countrymus', 1.0): 1, ('five', 1.0): 2, (\"night'\", 1.0): 2, (\"freddy'\", 1.0): 2, ('demo', 1.0): 2, ('pump', 1.0): 2, ('fanboy', 1.0): 1, ('thegrandad', 1.0): 1, ('sidni', 1.0): 1, ('remarriag', 1.0): 1, ('occas', 1.0): 1, ('languag', 1.0): 1, ('java', 1.0): 1, (\"php'\", 1.0): 1, ('notion', 1.0): 1, ('refer', 1.0): 1, ('confus', 1.0): 3, ('ohioan', 1.0): 1, ('stick', 1.0): 2, ('doctor', 1.0): 3, ('offlin', 1.0): 1, ('thesim', 1.0): 1, ('mb', 1.0): 1, ('meaningless', 1.0): 1, ('common', 1.0): 1, ('celebr', 1.0): 9, ('muertosatfring', 1.0): 1, ('emul', 1.0): 1, ('brought', 1.0): 1, ('enemi', 1.0): 2, ('relax', 1.0): 3, ('ou', 1.0): 1, ('pink', 1.0): 2, ('cc', 1.0): 2, ('meooowww', 1.0): 1, ('barkkkiiidee', 1.0): 1, ('bark', 1.0): 1, ('x11', 1.0): 1, ('routin', 1.0): 4, ('alek', 1.0): 1, ('awh', 1.0): 2, ('kumpul', 1.0): 1, ('cantik', 1.0): 1, ('ganteng', 1.0): 1, ('kresna', 1.0): 1, ('jelli', 1.0): 1, ('simon', 1.0): 1, ('lesley', 1.0): 3, ('blood', 1.0): 2, ('panti', 1.0): 1, ('lion', 1.0): 1, ('artworkbyli', 1.0): 1, ('judo', 1.0): 1, ('daredevil', 1.0): 2, ('despond', 1.0): 1, ('re-watch', 1.0): 1, ('welcoma.hav', 1.0): 1, ('favor', 1.0): 5, ('tridon', 1.0): 1, ('21pic', 1.0): 1, ('master', 1.0): 3, ('nim', 1.0): 1, (\"there'r\", 1.0): 1, ('22pic', 1.0): 1, ('kebun', 1.0): 1, ('ubud', 1.0): 1, ('ladyposs', 1.0): 1, ('xoxoxo', 1.0): 1, ('sneak', 1.0): 3, ('peek', 1.0): 2, ('inbox', 1.0): 1, ('happyweekend', 1.0): 1, ('therealgolden', 1.0): 1, ('47', 1.0): 1, ('girlfriendsmya', 1.0): 1, ('ppl', 1.0): 2, ('closest', 1.0): 1, ('njoy', 1.0): 1, ('followingg', 1.0): 1, ('privat', 1.0): 1, ('pusher', 1.0): 1, ('stun', 1.0): 4, ('wooohooo', 1.0): 1, ('cuss', 1.0): 1, ('teenag', 1.0): 1, ('ace', 1.0): 1, ('sauc', 1.0): 3, ('livi', 1.0): 1, ('fowl', 1.0): 1, ('oliviafowl', 1.0): 1, ('891', 1.0): 1, ('burnout', 1.0): 1, ('johnforceo', 1.0): 1, ('matthew', 1.0): 1, ('provok', 1.0): 1, ('indiankultur', 1.0): 1, ('oppos', 1.0): 1, ('biker', 1.0): 1, ('lyk', 1.0): 1, ('gud', 1.0): 4, ('weight', 1.0): 6, ('bcu', 1.0): 1, ('rubbish', 1.0): 1, ('veggi', 1.0): 2, ('steph', 1.0): 1, ('nj', 1.0): 1, ('x10', 1.0): 1, ('cohes', 1.0): 1, ('gossip', 1.0): 2, ('alex', 1.0): 3, ('heswifi', 1.0): 1, ('7am', 1.0): 1, ('wub', 1.0): 1, ('cerbchan', 1.0): 1, ('jarraaa', 1.0): 1, ('morrrn', 1.0): 1, ('snooz', 1.0): 1, ('clicksco', 1.0): 1, ('gay', 1.0): 4, ('lesbian', 1.0): 2, ('rigid', 1.0): 1, ('theocrat', 1.0): 1, ('wing', 1.0): 1, ('fundamentalist', 1.0): 1, ('islamist', 1.0): 1, ('brianaaa', 1.0): 1, ('brianazabrocki', 1.0): 1, ('sky', 1.0): 2, ('batb', 1.0): 1, ('clap', 1.0): 3, ('whilst', 1.0): 1, ('aki', 1.0): 1, ('thencerest', 1.0): 2, ('547', 1.0): 2, ('indiemus', 1.0): 5, ('sexyjudi', 1.0): 3, ('pussi', 1.0): 4, ('sexo', 1.0): 3, ('humid', 1.0): 1, ('87', 1.0): 1, ('sloppi', 1.0): 1, (\"second'\", 1.0): 1, ('stock', 1.0): 3, ('marmit', 1.0): 2, ('x9', 1.0): 1, ('nic', 1.0): 3, ('taft', 1.0): 1, ('finalist', 1.0): 1, ('lotteri', 1.0): 1, ('award', 1.0): 3, ('usagi', 1.0): 1, ('looov', 1.0): 1, ('wowww', 1.0): 2, ('💙', 1.0): 8, ('💚', 1.0): 8, ('💕', 1.0): 12, ('lepa', 1.0): 1, ('sembuh', 1.0): 1, ('sibuk', 1.0): 1, ('balik', 1.0): 1, ('kin', 1.0): 1, ('gotham', 1.0): 1, ('sunnyday', 1.0): 1, ('dudett', 1.0): 1, ('cost', 1.0): 1, ('flippin', 1.0): 1, ('fortun', 1.0): 1, ('divinediscont', 1.0): 1, (';}', 1.0): 1, ('amnot', 1.0): 1, ('autofollow', 1.0): 3, ('teamfollowback', 1.0): 4, ('geer', 1.0): 1, ('bat', 1.0): 2, ('mz', 1.0): 1, ('yang', 1.0): 2, ('deennya', 1.0): 1, ('jehwan', 1.0): 1, ('11:00', 1.0): 1, ('ashton', 1.0): 1, ('✧', 1.0): 12, ('｡', 1.0): 4, ('chelni', 1.0): 2, ('datz', 1.0): 1, ('jeremi', 1.0): 1, ('fmt', 1.0): 1, ('dat', 1.0): 3, ('heartbeat', 1.0): 1, ('clutch', 1.0): 1, ('🐢', 1.0): 2, ('besteverdoctorwhoepisod', 1.0): 1, ('relev', 1.0): 1, ('puke', 1.0): 1, ('proper', 1.0): 1, ('x8', 1.0): 1, ('sublimin', 1.0): 1, ('eatmeat', 1.0): 1, ('brewproject', 1.0): 1, ('lovenafianna', 1.0): 1, ('mr', 1.0): 7, ('lewi', 1.0): 1, ('clock', 1.0): 1, ('3:02', 1.0): 2, ('muslim', 1.0): 1, ('prophet', 1.0): 1, ('غردلي', 1.0): 4, ('is.h', 1.0): 1, ('mistak', 1.0): 4, ('understood', 1.0): 1, ('politician', 1.0): 1, ('argu', 1.0): 1, ('intellect', 1.0): 1, ('shiva', 1.0): 1, ('mp3', 1.0): 1, ('standrew', 1.0): 1, ('sandcastl', 1.0): 1, ('ewok', 1.0): 1, ('nate', 1.0): 2, ('brawl', 1.0): 1, ('rear', 1.0): 1, ('nake', 1.0): 1, ('choke', 1.0): 1, ('heck', 1.0): 1, ('gun', 1.0): 2, ('associ', 1.0): 1, ('um', 1.0): 1, ('endow', 1.0): 1, ('ai', 1.0): 1, ('sikandar', 1.0): 1, ('pti', 1.0): 1, ('standwdik', 1.0): 1, ('westandwithik', 1.0): 1, ('starbuck', 1.0): 2, ('logo', 1.0): 2, ('renew', 1.0): 1, ('chariti', 1.0): 1, ('جمعة_مباركة', 1.0): 1, ('hoki', 1.0): 1, ('biz', 1.0): 1, ('non', 1.0): 1, ('america', 1.0): 1, ('california', 1.0): 1, ('01:16', 1.0): 1, ('45gameplay', 1.0): 2, ('ilovey', 1.0): 2, ('vex', 1.0): 1, ('iger', 1.0): 1, ('leicaq', 1.0): 1, ('leica', 1.0): 1, ('dudee', 1.0): 1, ('persona', 1.0): 1, ('yepp', 1.0): 1, ('5878e503', 1.0): 1, ('x7', 1.0): 1, ('greg', 1.0): 1, ('posey', 1.0): 1, ('miami', 1.0): 1, ('james_yammouni', 1.0): 1, ('breakdown', 1.0): 1, ('materi', 1.0): 2, ('thorin', 1.0): 1, ('hunt', 1.0): 1, ('choroo', 1.0): 1, ('nahi', 1.0): 2, ('aztec', 1.0): 1, ('princess', 1.0): 2, ('raini', 1.0): 1, ('kingfish', 1.0): 1, ('chinua', 1.0): 1, ('acheb', 1.0): 1, ('intellectu', 1.0): 2, ('liquid', 1.0): 1, ('melbournetrip', 1.0): 1, ('taxikitchen', 1.0): 1, ('nooow', 1.0): 2, ('mcdo', 1.0): 1, ('everywher', 1.0): 2, ('dreamer', 1.0): 1, ('tanisha', 1.0): 1, ('1nonli', 1.0): 1, ('attitud', 1.0): 1, ('kindl', 1.0): 2, ('flame', 1.0): 1, ('convict', 1.0): 1, ('bar', 1.0): 1, ('repath', 1.0): 2, ('adi', 1.0): 1, ('stefani', 1.0): 1, ('sg1', 1.0): 1, ('lightbox', 1.0): 1, ('ran', 1.0): 2, ('incorrect', 1.0): 1, ('apologist', 1.0): 1, ('x6', 1.0): 1, ('vuli', 1.0): 1, ('01:15', 1.0): 1, ('batman', 1.0): 1, ('pearson', 1.0): 1, ('reput', 1.0): 2, ('nikkei', 1.0): 1, ('woodford', 1.0): 1, ('vscocam', 1.0): 1, ('vscoph', 1.0): 1, ('vscogood', 1.0): 1, ('vscophil', 1.0): 1, ('vscocousin', 1.0): 1, ('yaap', 1.0): 1, ('urwelc', 1.0): 1, ('neon', 1.0): 1, ('pant', 1.0): 1, ('haaa', 1.0): 1, ('will', 1.0): 2, ('auspost', 1.0): 1, ('openfollow', 1.0): 1, ('rp', 1.0): 2, ('eng', 1.0): 1, ('yūjō-cosplay', 1.0): 1, ('luxembourg', 1.0): 1, ('bunni', 1.0): 1, ('broadcast', 1.0): 1, ('needa', 1.0): 1, ('gal', 1.0): 3, ('bend', 1.0): 3, ('heaven', 1.0): 2, ('score', 1.0): 2, ('januari', 1.0): 1, ('hanabutl', 1.0): 1, ('kikhorni', 1.0): 1, ('interraci', 1.0): 1, ('makeup', 1.0): 1, ('chu', 1.0): 1, (\"weekend'\", 1.0): 1, ('punt', 1.0): 1, ('horserac', 1.0): 1, ('hors', 1.0): 2, ('horseracingtip', 1.0): 1, ('guitar', 1.0): 1, ('cocoar', 1.0): 1, ('brief', 1.0): 1, ('introduct', 1.0): 1, ('earliest', 1.0): 1, ('indian', 1.0): 1, ('subcontin', 1.0): 1, ('bfr', 1.0): 1, ('maurya', 1.0): 1, ('jordanian', 1.0): 1, ('00962778381', 1.0): 1, ('838', 1.0): 1, ('tenyai', 1.0): 1, ('hee', 1.0): 2, ('ss', 1.0): 1, ('semi', 1.0): 1, ('atp', 1.0): 2, ('wimbledon', 1.0): 2, ('feder', 1.0): 1, ('nadal', 1.0): 1, ('monfil', 1.0): 1, ('handsom', 1.0): 2, ('cilic', 1.0): 3, ('firm', 1.0): 1, ('potenti', 1.0): 3, ('nyc', 1.0): 1, ('chillin', 1.0): 2, ('tail', 1.0): 2, ('kitten', 1.0): 1, ('garret', 1.0): 1, ('baz', 1.0): 1, ('leo', 1.0): 2, ('xst', 1.0): 1, ('centrifug', 1.0): 1, ('etern', 1.0): 3, ('forgiv', 1.0): 2, ('kangin', 1.0): 1, ('بندر', 1.0): 1, ('العنزي', 1.0): 1, ('kristin', 1.0): 1, ('cass', 1.0): 1, ('surajettan', 1.0): 1, ('kashi', 1.0): 1, ('ashwathi', 1.0): 1, ('mommi', 1.0): 2, ('tirth', 1.0): 1, ('brambhatt', 1.0): 1, ('snooker', 1.0): 1, ('compens', 1.0): 1, ('theoper', 1.0): 1, ('479', 1.0): 1, ('premiostumundo', 1.0): 2, ('philosoph', 1.0): 1, ('x5', 1.0): 1, ('graphic', 1.0): 2, ('level', 1.0): 1, ('aug', 1.0): 3, ('excl', 1.0): 1, ('raw', 1.0): 1, ('weeni', 1.0): 1, ('annoyingbabi', 1.0): 1, ('lazi', 1.0): 2, ('cosi', 1.0): 1, ('client_amends_edit', 1.0): 1, ('_5_final_final_fin', 1.0): 1, ('pdf', 1.0): 1, ('mauliat', 1.0): 1, ('ito', 1.0): 2, ('okkay', 1.0): 1, ('knock', 1.0): 3, (\"soloist'\", 1.0): 1, ('ryu', 1.0): 1, ('saera', 1.0): 1, ('pinkeu', 1.0): 1, ('angri', 1.0): 3, ('screencap', 1.0): 1, ('jonghyun', 1.0): 1, ('seungyeon', 1.0): 1, ('cnblue', 1.0): 1, ('mbc', 1.0): 1, ('wgm', 1.0): 1, ('masa', 1.0): 2, ('entrepreneurship', 1.0): 1, ('empow', 1.0): 1, ('limpopo', 1.0): 1, ('pict', 1.0): 1, ('norapowel', 1.0): 1, ('hornykik', 1.0): 2, ('livesex', 1.0): 1, ('pumpkin', 1.0): 1, ('thrice', 1.0): 1, ('patron', 1.0): 1, ('ventur', 1.0): 1, ('deathcur', 1.0): 1, ('boob', 1.0): 1, ('blame', 1.0): 1, ('dine', 1.0): 1, ('modern', 1.0): 1, ('grill', 1.0): 1, ('disk', 1.0): 1, ('nt4', 1.0): 1, ('iirc', 1.0): 1, ('ux', 1.0): 1, ('refin', 1.0): 1, ('zdp', 1.0): 1, ('didnt', 1.0): 2, ('justic', 1.0): 1, ('daw', 1.0): 1, ('tine', 1.0): 1, ('gensan', 1.0): 1, ('frightl', 1.0): 1, ('undead', 1.0): 1, ('plush', 1.0): 1, ('cushion', 1.0): 1, ('nba', 1.0): 3, ('2k15', 1.0): 3, ('mypark', 1.0): 3, ('chronicl', 1.0): 4, ('gryph', 1.0): 3, ('volum', 1.0): 3, ('ellen', 1.0): 1, ('degener', 1.0): 1, ('shirt', 1.0): 1, ('mint', 1.0): 1, ('superdri', 1.0): 1, ('berangkaat', 1.0): 1, ('lagiii', 1.0): 1, ('siguro', 1.0): 1, ('un', 1.0): 1, ('kesa', 1.0): 1, ('lotsa', 1.0): 2, ('organis', 1.0): 2, ('4am', 1.0): 1, ('fingers-cross', 1.0): 1, ('deep', 1.0): 1, ('htaccess', 1.0): 1, ('file', 1.0): 2, ('adf', 1.0): 1, ('womad', 1.0): 1, ('gran', 1.0): 1, ('canaria', 1.0): 1, ('gig', 1.0): 1, ('twist', 1.0): 1, ('youv', 1.0): 1, ('teamnatur', 1.0): 1, ('huni', 1.0): 1, ('yayayayay', 1.0): 1, ('yt', 1.0): 2, ('convent', 1.0): 1, ('brighton', 1.0): 1, ('slay', 1.0): 1, ('nicknam', 1.0): 1, ('babygirl', 1.0): 1, ('regard', 1.0): 2, ('himmat', 1.0): 1, ('karain', 1.0): 2, ('baat', 1.0): 1, ('meri', 1.0): 1, ('hotee-mi', 1.0): 1, ('uncl', 1.0): 1, ('tongu', 1.0): 1, ('pronounc', 1.0): 1, ('nativ', 1.0): 1, ('american', 1.0): 2, ('proverb', 1.0): 1, ('lovabl', 1.0): 1, ('yesha', 1.0): 1, ('montoya', 1.0): 1, ('eagerli', 1.0): 1, ('payment', 1.0): 1, ('suprem', 1.0): 1, ('leon', 1.0): 1, ('ks', 1.0): 2, ('randi', 1.0): 1, ('9bi', 1.0): 1, ('physiqu', 1.0): 1, ('shave', 1.0): 1, ('uncut', 1.0): 1, ('boi', 1.0): 1, ('cheapest', 1.0): 1, ('regular', 1.0): 3, ('printer', 1.0): 3, ('nz', 1.0): 1, ('larg', 1.0): 4, ('format', 1.0): 1, ('10/10', 1.0): 1, ('senior', 1.0): 1, ('raid', 1.0): 2, ('conserv', 1.0): 1, ('batteri', 1.0): 1, ('comfort', 1.0): 2, ('swt', 1.0): 1, ('reservations@sandsbeach.eu', 1.0): 1, ('localgaragederbi', 1.0): 1, ('campu', 1.0): 1, ('subgam', 1.0): 1, ('faceit', 1.0): 1, ('snpcaht', 1.0): 1, ('hakhakhak', 1.0): 1, ('t___t', 1.0): 1, (\"kyungsoo'\", 1.0): 1, ('3d', 1.0): 2, ('properti', 1.0): 2, ('agent', 1.0): 1, ('accur', 1.0): 1, ('descript', 1.0): 1, ('theori', 1.0): 1, ('x4', 1.0): 1, ('15.90', 1.0): 1, ('yvett', 1.0): 1, ('author', 1.0): 2, ('mwf', 1.0): 1, ('programm', 1.0): 1, ('taal', 1.0): 1, ('lake', 1.0): 1, ('2emt', 1.0): 1, ('«', 1.0): 2, ('scurri', 1.0): 1, ('agil', 1.0): 1, ('solut', 1.0): 1, ('sme', 1.0): 1, ('omar', 1.0): 1, ('biggest', 1.0): 5, ('kamaal', 1.0): 1, ('amm', 1.0): 1, ('3am', 1.0): 1, ('hopehousekid', 1.0): 1, ('pitmantrain', 1.0): 1, ('walkersmithway', 1.0): 1, ('keepitloc', 1.0): 2, ('sehun', 1.0): 1, ('se100lead', 1.0): 1, ('unev', 1.0): 1, ('sofa', 1.0): 1, ('surf', 1.0): 1, ('cunt', 1.0): 1, ('rescoop', 1.0): 1, ('multiraci', 1.0): 1, ('fk', 1.0): 1, ('narrow', 1.0): 1, ('warlock', 1.0): 1, ('balloon', 1.0): 3, ('mj', 1.0): 1, ('madison', 1.0): 1, ('beonknockknock', 1.0): 1, ('con-gradu', 1.0): 1, ('gent', 1.0): 1, ('bitchfac', 1.0): 1, ('😒', 1.0): 1, ('organ', 1.0): 1, ('12pm', 1.0): 2, ('york', 1.0): 2, ('nearest', 1.0): 1, ('lendal', 1.0): 1, ('pikami', 1.0): 1, ('captur', 1.0): 1, ('fulton', 1.0): 1, ('sheen', 1.0): 1, ('baloney', 1.0): 1, ('unvarnish', 1.0): 1, ('laid', 1.0): 2, ('thick', 1.0): 1, ('blarney', 1.0): 1, ('flatteri', 1.0): 1, ('thin', 1.0): 1, ('sachin', 1.0): 1, ('unimport', 1.0): 1, ('context', 1.0): 1, ('dampen', 1.0): 1, ('yu', 1.0): 1, ('rocket', 1.0): 1, ('narendra', 1.0): 1, ('modi', 1.0): 1, ('aaaand', 1.0): 1, (\"team'\", 1.0): 1, ('macauley', 1.0): 1, ('howev', 1.0): 3, ('x3', 1.0): 1, ('wheeen', 1.0): 1, ('heechul', 1.0): 1, ('toast', 1.0): 2, ('coffee-weekday', 1.0): 1, ('9-11', 1.0): 1, ('sail', 1.0): 1, (\"friday'\", 1.0): 1, ('commerci', 1.0): 1, ('insur', 1.0): 1, ('requir', 1.0): 2, ('lookfortheo', 1.0): 1, ('cl', 1.0): 1, ('thou', 1.0): 1, ('april', 1.0): 2, ('airforc', 1.0): 1, ('clark', 1.0): 1, ('field', 1.0): 1, ('pampanga', 1.0): 1, ('troll', 1.0): 1, ('⚡', 1.0): 1, ('brow', 1.0): 1, ('oili', 1.0): 1, ('maricarljanah', 1.0): 1, ('6:15', 1.0): 1, ('degre', 1.0): 3, ('fahrenheit', 1.0): 1, ('🍸', 1.0): 7, ('╲', 1.0): 4, ('─', 1.0): 8, ('╱', 1.0): 5, ('🍤', 1.0): 4, ('╭', 1.0): 4, ('╮', 1.0): 4, ('┓', 1.0): 2, ('┳', 1.0): 1, ('┣', 1.0): 1, ('╰', 1.0): 3, ('╯', 1.0): 3, ('┗', 1.0): 2, ('┻', 1.0): 1, ('stool', 1.0): 1, ('toppl', 1.0): 1, ('findyourfit', 1.0): 1, ('prefer', 1.0): 2, ('whomosexu', 1.0): 1, ('stack', 1.0): 1, ('pandora', 1.0): 3, ('digitalexet', 1.0): 1, ('digitalmarket', 1.0): 1, ('sociamedia', 1.0): 1, ('nb', 1.0): 1, ('bom', 1.0): 1, ('dia', 1.0): 1, ('todo', 1.0): 1, ('forklift', 1.0): 1, ('warehous', 1.0): 1, ('worker', 1.0): 1, ('lsceen', 1.0): 1, ('immatur', 1.0): 1, ('gandhi', 1.0): 1, ('grassi', 1.0): 1, ('feetblog', 1.0): 2, ('daughter', 1.0): 3, ('4yr', 1.0): 1, ('old-porridg', 1.0): 1, ('fiend', 1.0): 1, ('2nite', 1.0): 1, ('comp', 1.0): 1, ('vike', 1.0): 1, ('t20blast', 1.0): 1, ('np', 1.0): 1, ('tax', 1.0): 1, ('ooohh', 1.0): 1, ('petjam', 1.0): 1, ('virtual', 1.0): 2, ('pounc', 1.0): 1, ('bentek', 1.0): 1, ('agn', 1.0): 1, ('socialmedia@dpdgroup.co.uk', 1.0): 1, ('sam', 1.0): 3, ('fruiti', 1.0): 1, ('vodka', 1.0): 2, ('sellyourcarin', 1.0): 2, ('5word', 1.0): 2, ('chaloniklo', 1.0): 2, ('pic.twitter.com/jxz2lbv6o', 1.0): 1, (\"paperwhite'\", 1.0): 1, ('laser-lik', 1.0): 1, ('focu', 1.0): 1, ('ghost', 1.0): 3, ('tagsforlikesapp', 1.0): 2, ('instagood', 1.0): 2, ('tbt', 1.0): 1, ('socket', 1.0): 1, ('spanner', 1.0): 1, ('😴', 1.0): 1, ('pglcsgo', 1.0): 1, ('x2', 1.0): 1, ('tend', 1.0): 1, ('crave', 1.0): 1, ('slower', 1.0): 1, ('sjw', 1.0): 1, ('cakehamp', 1.0): 1, ('glow', 1.0): 2, ('yayyy', 1.0): 1, ('merced', 1.0): 1, ('hood', 1.0): 1, ('badg', 1.0): 1, ('host', 1.0): 1, ('drone', 1.0): 1, ('blow', 1.0): 1, ('ignor', 1.0): 1, ('retali', 1.0): 1, ('bolling', 1.0): 1, (\"where'\", 1.0): 1, ('denmark', 1.0): 1, ('whitey', 1.0): 1, ('cultur', 1.0): 2, ('course', 1.0): 1, ('intro', 1.0): 2, ('graphicdesign', 1.0): 1, ('videograph', 1.0): 1, ('space', 1.0): 2, (\"ted'\", 1.0): 1, ('bogu', 1.0): 1, ('1000', 1.0): 1, ('hahahaaah', 1.0): 1, ('owli', 1.0): 1, ('afternon', 1.0): 1, ('whangarei', 1.0): 1, ('kati', 1.0): 2, ('paulin', 1.0): 1, ('traffick', 1.0): 1, ('wors', 1.0): 3, ('henc', 1.0): 1, ('express', 1.0): 1, ('wot', 1.0): 1, ('hand-lett', 1.0): 1, ('roof', 1.0): 1, ('eas', 1.0): 1, ('2/2', 1.0): 1, ('sour', 1.0): 1, ('dough', 1.0): 1, ('egypt', 1.0): 1, ('hubbi', 1.0): 2, ('sakin', 1.0): 1, ('six', 1.0): 1, ('christma', 1.0): 2, ('avril', 1.0): 1, ('n04j', 1.0): 1, ('25', 1.0): 1, ('prosecco', 1.0): 1, ('pech', 1.0): 1, ('micro', 1.0): 1, ('catspj', 1.0): 1, ('4:15', 1.0): 1, ('lazyweekend', 1.0): 1, ('overdu', 1.0): 1, ('mice', 1.0): 1, ('💃', 1.0): 3, ('jurass', 1.0): 1, ('ding', 1.0): 1, ('nila', 1.0): 1, ('8)', 1.0): 1, ('cooki', 1.0): 1, ('shir', 1.0): 1, ('0', 1.0): 3, ('hale', 1.0): 1, ('cheshir', 1.0): 1, ('decor', 1.0): 1, ('lemm', 1.0): 2, ('rec', 1.0): 1, ('ingat', 1.0): 1, ('din', 1.0): 2, ('mono', 1.0): 1, ('kathryn', 1.0): 1, ('jr', 1.0): 1, ('hsr', 1.0): 1, ('base', 1.0): 3, ('major', 1.0): 1, ('sugarrush', 1.0): 1, ('knit', 1.0): 1, ('partli', 1.0): 1, ('homegirl', 1.0): 1, ('nanci', 1.0): 1, ('fenja', 1.0): 1, ('aapk', 1.0): 1, ('benchmark', 1.0): 1, ('ke', 1.0): 1, ('hisaab', 1.0): 1, ('ho', 1.0): 1, ('gaya', 1.0): 1, ('ofc', 1.0): 1, ('rtss', 1.0): 1, ('hwait', 1.0): 1, ('titanfal', 1.0): 1, ('xbox', 1.0): 2, ('ultim', 1.0): 2, ('gastronomi', 1.0): 1, ('newblogpost', 1.0): 1, ('foodiefriday', 1.0): 1, ('foodi', 1.0): 1, ('yoghurt', 1.0): 1, ('pancak', 1.0): 2, ('sabah', 1.0): 3, ('kapima', 1.0): 1, ('gelen', 1.0): 1, ('guzel', 1.0): 1, ('bir', 1.0): 1, ('hediy', 1.0): 1, ('thanx', 1.0): 1, ('💞', 1.0): 2, ('visa', 1.0): 1, ('parisa', 1.0): 1, ('epiphani', 1.0): 1, ('lit', 1.0): 1, ('em-con', 1.0): 1, ('swore', 1.0): 1, ('0330 333 7234', 1.0): 1, ('kianweareproud', 1.0): 1, ('distract', 1.0): 1, ('dayofarch', 1.0): 1, ('10-20', 1.0): 1, ('bapu', 1.0): 1, ('ivypowel', 1.0): 1, ('newmus', 1.0): 1, ('sexchat', 1.0): 1, ('🍅', 1.0): 1, ('pathway', 1.0): 1, ('balkan', 1.0): 1, ('gypsi', 1.0): 1, ('mayhem', 1.0): 1, ('burek', 1.0): 1, ('meat', 1.0): 1, ('gibanica', 1.0): 1, ('pie', 1.0): 1, ('surrey', 1.0): 1, ('afterward', 1.0): 1, ('10.30', 1.0): 1, ('tempor', 1.0): 1, ('void', 1.0): 1, ('stem', 1.0): 1, ('sf', 1.0): 1, ('ykr', 1.0): 1, ('sparki', 1.0): 1, ('40mm', 1.0): 1, ('3.5', 1.0): 1, ('gr', 1.0): 1, ('rockfish', 1.0): 1, ('topwat', 1.0): 1, ('twitlong', 1.0): 1, ('me.so', 1.0): 1, ('jummah', 1.0): 3, ('durood', 1.0): 1, ('pak', 1.0): 1, ('cjradacomateada', 1.0): 2, ('supris', 1.0): 1, ('debut', 1.0): 1, ('shipper', 1.0): 1, ('asid', 1.0): 1, ('housem', 1.0): 1, ('737bigatingconcert', 1.0): 1, ('jedzjabłka', 1.0): 1, ('pijjabłka', 1.0): 1, ('polish', 1.0): 1, ('cider', 1.0): 1, ('mustread', 1.0): 1, ('cricket', 1.0): 1, ('5pm', 1.0): 1, ('queri', 1.0): 2, ('abbi', 1.0): 1, ('sumedh', 1.0): 1, ('sunnah', 1.0): 2, ('عن', 1.0): 2, ('quad', 1.0): 1, ('bike', 1.0): 1, ('carri', 1.0): 2, ('proprieti', 1.0): 1, ('chronic', 1.0): 1, ('superday', 1.0): 1, ('chocolatey', 1.0): 1, ('yasu', 1.0): 1, ('ooooh', 1.0): 1, ('hallo', 1.0): 2, ('dylan', 1.0): 2, ('laura', 1.0): 1, ('patric', 1.0): 2, ('keepin', 1.0): 1, ('mohr', 1.0): 1, ('guest', 1.0): 1, (\"o'neal\", 1.0): 1, ('tk', 1.0): 1, ('lua', 1.0): 1, ('stone', 1.0): 2, ('quicker', 1.0): 1, ('diet', 1.0): 1, ('sosweet', 1.0): 1, ('nominier', 1.0): 1, ('und', 1.0): 1, ('hardcor', 1.0): 1, ('😌', 1.0): 1, ('ff__special', 1.0): 1, ('acha', 1.0): 2, ('banda', 1.0): 1, ('✌', 1.0): 1, ('bhi', 1.0): 2, ('krta', 1.0): 1, ('beautifully-craft', 1.0): 1, ('mockingbird', 1.0): 1, ('diploma', 1.0): 1, ('blend', 1.0): 3, ('numbero', 1.0): 1, ('lolz', 1.0): 1, ('ambros', 1.0): 1, ('gwinett', 1.0): 1, ('bierc', 1.0): 1, ('ravag', 1.0): 1, ('illadvis', 1.0): 1, ('marriag', 1.0): 1, ('stare', 1.0): 1, ('cynic', 1.0): 2, ('yahuda', 1.0): 1, ('nosmet', 1.0): 1, ('poni', 1.0): 1, ('cuuut', 1.0): 1, (\"f'ing\", 1.0): 1, ('vacant', 1.0): 1, ('hauc', 1.0): 1, ('lovesss', 1.0): 1, ('hiss', 1.0): 1, ('overnight', 1.0): 1, ('cornish', 1.0): 1, ('all-clear', 1.0): 1, ('raincoat', 1.0): 1, ('measur', 1.0): 1, ('wealth', 1.0): 1, ('invest', 1.0): 2, ('garbi', 1.0): 1, ('wash', 1.0): 2, ('refuel', 1.0): 1, ('dunedin', 1.0): 1, ('kall', 1.0): 1, ('rakhi', 1.0): 1, ('12th', 1.0): 2, ('repres', 1.0): 3, ('slovenia', 1.0): 1, ('fridg', 1.0): 2, ('ludlow', 1.0): 1, ('28th', 1.0): 1, ('selway', 1.0): 1, ('submit', 1.0): 1, ('spanish', 1.0): 2, ('90210', 1.0): 1, ('oitnb', 1.0): 1, ('prepar', 1.0): 3, ('condit', 1.0): 1, ('msged', 1.0): 1, ('chiquito', 1.0): 1, ('ohaha', 1.0): 1, ('delhi', 1.0): 1, ('95', 1.0): 1, ('webtogsaward', 1.0): 1, ('grace', 1.0): 2, ('sheffield', 1.0): 1, ('tramlin', 1.0): 1, ('tl', 1.0): 2, ('hack', 1.0): 1, ('lad', 1.0): 1, ('beeepin', 1.0): 1, ('duper', 1.0): 1, ('handl', 1.0): 1, ('critiqu', 1.0): 1, ('contectu', 1.0): 1, ('ultor', 1.0): 2, ('mamaya', 1.0): 1, ('loiyal', 1.0): 1, ('para', 1.0): 1, ('truthfulwordsof', 1.0): 1, ('beanatividad', 1.0): 1, ('nknkkpagpapakumbaba', 1.0): 1, ('birthdaypres', 1.0): 1, ('compliment', 1.0): 1, ('swerv', 1.0): 1, ('goodtim', 1.0): 1, ('sinist', 1.0): 1, ('scare', 1.0): 1, ('tryna', 1.0): 1, ('anonym', 1.0): 1, ('dipsatch', 1.0): 1, ('aunt', 1.0): 1, ('dagga', 1.0): 1, ('burket', 1.0): 1, ('2am', 1.0): 1, ('twine', 1.0): 1, (\"diane'\", 1.0): 1, ('happybirthday', 1.0): 1, ('thanksss', 1.0): 1, ('randomli', 1.0): 1, ('buckinghampalac', 1.0): 1, ('chibi', 1.0): 1, ('maker', 1.0): 1, ('timog', 1.0): 1, ('18th', 1.0): 1, ('otw', 1.0): 1, ('kami', 1.0): 1, ('feelinggood', 1.0): 1, ('demand', 1.0): 2, ('naman', 1.0): 1, ('barkin', 1.0): 1, ('yeap', 1.0): 2, ('onkey', 1.0): 1, ('umma', 1.0): 1, ('pervert', 1.0): 1, ('onyu', 1.0): 1, ('appa', 1.0): 1, ('luci', 1.0): 1, ('horribl', 1.0): 1, ('quantum', 1.0): 1, ('greater', 1.0): 1, ('blockchain', 1.0): 1, ('nowplay', 1.0): 1, ('loftey', 1.0): 1, ('routt', 1.0): 1, ('assia', 1.0): 1, ('.\\n.\\n.', 1.0): 1, ('joint', 1.0): 1, ('futurereleas', 1.0): 1, (\"look'\", 1.0): 1, ('scari', 1.0): 1, ('murder', 1.0): 1, ('mysteri', 1.0): 1, ('comma', 1.0): 1, (\"j'\", 1.0): 1, ('hunni', 1.0): 2, ('diva', 1.0): 1, ('emili', 1.0): 3, ('nathan', 1.0): 1, ('medit', 1.0): 1, ('alumni', 1.0): 1, ('mba', 1.0): 1, ('foto', 1.0): 1, ('what-is-your-fashion', 1.0): 1, ('lorenangel', 1.0): 1, ('kw', 1.0): 2, ('tellanoldjokeday', 1.0): 1, ('reqd', 1.0): 1, ('specul', 1.0): 1, ('consist', 1.0): 4, ('tropic', 1.0): 1, ('startupph', 1.0): 1, ('zodiac', 1.0): 1, ('rapunzel', 1.0): 1, ('therver', 1.0): 1, ('85552', 1.0): 1, ('bestoftheday', 1.0): 1, ('oralsex', 1.0): 1, ('carli', 1.0): 1, ('happili', 1.0): 1, ('contract', 1.0): 1, ('matsu_bouzu', 1.0): 1, ('sonic', 1.0): 2, ('videogam', 1.0): 1, ('harana', 1.0): 1, ('belfast', 1.0): 1, ('danni', 1.0): 1, ('rare', 1.0): 1, ('sponsorship', 1.0): 1, ('aswel', 1.0): 1, ('gigi', 1.0): 1, ('nick', 1.0): 1, ('austin', 1.0): 1, ('youll', 1.0): 1, ('weak', 1.0): 4, ('10,000', 1.0): 1, ('bravo', 1.0): 1, ('iamamonst', 1.0): 1, ('rxthedailysurveyvot', 1.0): 1, ('broke', 1.0): 1, ('ass', 1.0): 1, ('roux', 1.0): 1, ('walkin', 1.0): 1, ('audienc', 1.0): 2, ('pfb', 1.0): 1, ('jute', 1.0): 1, ('walangmakakapigilsakin', 1.0): 1, ('lori', 1.0): 1, ('ehm', 1.0): 1, ('trick', 1.0): 1, ('baekhyun', 1.0): 1, ('eyesmil', 1.0): 1, ('borrow', 1.0): 1, ('knive', 1.0): 1, ('thek', 1.0): 1, ('eventu', 1.0): 1, ('reaapear', 1.0): 1, ('kno', 1.0): 1, ('whet', 1.0): 1, ('gratti', 1.0): 1, ('shorter', 1.0): 1, ('tweetin', 1.0): 1, ('inshallah', 1.0): 1, ('banana', 1.0): 1, ('raspberri', 1.0): 2, ('healthylifestyl', 1.0): 1, ('aint', 1.0): 2, ('skate', 1.0): 1, ('analyz', 1.0): 1, ('varieti', 1.0): 1, ('4:13', 1.0): 1, ('insomnia', 1.0): 1, ('medic', 1.0): 1, ('opposit', 1.0): 1, ('everlast', 1.0): 1, ('yoga', 1.0): 1, ('massag', 1.0): 2, ('osteopath', 1.0): 1, ('trainer', 1.0): 1, ('sharm', 1.0): 1, ('al_master_band', 1.0): 1, ('tbc', 1.0): 1, ('unives', 1.0): 1, ('architectur', 1.0): 1, ('random', 1.0): 1, ('isnt', 1.0): 1, ('typo', 1.0): 1, ('snark', 1.0): 1, ('lession', 1.0): 1, ('drunk', 1.0): 1, ('bruuh', 1.0): 1, ('2week', 1.0): 1, ('50europ', 1.0): 1, ('🇫🇷', 1.0): 4, ('iov', 1.0): 1, ('accord', 1.0): 1, ('mne', 1.0): 1, ('pchelok', 1.0): 1, ('ja', 1.0): 1, ('=:', 1.0): 2, ('sweetest', 1.0): 1, ('comet', 1.0): 1, ('ahah', 1.0): 1, ('candi', 1.0): 2, ('axio', 1.0): 1, ('rabbit', 1.0): 2, ('nutshel', 1.0): 1, ('taken', 1.0): 1, ('letshavecocktailsafternuclai', 1.0): 1, ('malik', 1.0): 1, ('umair', 1.0): 1, ('canon', 1.0): 1, ('gang', 1.0): 1, ('grind', 1.0): 1, ('thoracicbridg', 1.0): 1, ('5minut', 1.0): 1, ('nonscript', 1.0): 1, ('password', 1.0): 1, ('shoshannavassil', 1.0): 1, ('addmeonsnapchat', 1.0): 1, ('dmme', 1.0): 1, ('mpoint', 1.0): 2, ('soph', 1.0): 1, ('anot', 1.0): 1, ('liao', 1.0): 2, ('ord', 1.0): 1, ('lor', 1.0): 1, ('sibei', 1.0): 1, ('xialan', 1.0): 1, ('thnx', 1.0): 1, ('malfunct', 1.0): 1, ('clown', 1.0): 1, ('joker', 1.0): 1, ('\\U000fec00', 1.0): 1, ('nigth', 1.0): 1, ('estoy', 1.0): 1, ('escuchando', 1.0): 1, ('elsewher', 1.0): 1, ('bipolar', 1.0): 1, ('hahahahahahahahahahahahahaha', 1.0): 1, ('yoohoo', 1.0): 1, ('bajrangibhaijaanstorm', 1.0): 1, ('superhappi', 1.0): 1, ('doll', 1.0): 1, ('energi', 1.0): 1, ('f', 1.0): 3, (\"m'dear\", 1.0): 1, ('emma', 1.0): 2, ('alrd', 1.0): 1, ('dhan', 1.0): 2, ('satguru', 1.0): 1, ('tera', 1.0): 1, ('aasra', 1.0): 1, ('pita', 1.0): 1, ('keeo', 1.0): 1, ('darl', 1.0): 2, ('akarshan', 1.0): 1, ('sweetpea', 1.0): 1, ('gluten', 1.0): 1, ('pastri', 1.0): 2, ('highfiv', 1.0): 1, ('artsi', 1.0): 1, ('verbal', 1.0): 1, ('kaaa', 1.0): 1, ('oxford', 1.0): 2, ('wahoo', 1.0): 1, ('anchor', 1.0): 1, ('partnership', 1.0): 1, ('robbenisland', 1.0): 1, ('whale', 1.0): 1, ('aquat', 1.0): 1, ('safari', 1.0): 1, ('garru', 1.0): 1, ('liara', 1.0): 1, ('appoint', 1.0): 1, ('burnley', 1.0): 1, ('453', 1.0): 1, ('110', 1.0): 2, ('49', 1.0): 1, ('footbal', 1.0): 1, ('fm15', 1.0): 1, ('fmfamili', 1.0): 1, ('aamir', 1.0): 1, ('difficult', 1.0): 1, ('medium', 1.0): 1, ('nva', 1.0): 1, ('minuet', 1.0): 1, ('gamec', 1.0): 1, ('headrest', 1.0): 1, ('pit', 1.0): 1, ('spoken', 1.0): 1, ('advis', 1.0): 1, ('paypoint', 1.0): 1, ('deepthroat', 1.0): 1, ('truli', 1.0): 3, ('bee', 1.0): 2, ('upward', 1.0): 1, ('bound', 1.0): 1, ('movingonup', 1.0): 1, ('aitor', 1.0): 1, ('sn', 1.0): 1, ('ps4', 1.0): 2, ('jawad', 1.0): 1, ('presal', 1.0): 1, ('betcha', 1.0): 1, ('dumb', 1.0): 2, ('butt', 1.0): 1, ('qualki', 1.0): 1, ('808', 1.0): 1, ('milf', 1.0): 1, ('4like', 1.0): 1, ('sexysaturday', 1.0): 1, ('vw', 1.0): 1, ('umpfff', 1.0): 1, ('ca', 1.0): 1, ('domg', 1.0): 1, ('nanti', 1.0): 1, ('difollow', 1.0): 1, ('stubborn', 1.0): 1, ('nothavingit', 1.0): 1, ('klee', 1.0): 1, ('hem', 1.0): 1, ('congrad', 1.0): 1, ('accomplish', 1.0): 1, ('kfcroleplay', 1.0): 3, ('tregaron', 1.0): 1, ('boar', 1.0): 1, ('sweati', 1.0): 1, ('glyon', 1.0): 1, ('🚮', 1.0): 1, (\"tee'\", 1.0): 1, ('johnni', 1.0): 1, ('utub', 1.0): 1, (\"video'\", 1.0): 1, ('loss', 1.0): 1, ('combin', 1.0): 2, ('pigeon', 1.0): 1, ('fingerscross', 1.0): 1, ('photobomb', 1.0): 1, ('90', 1.0): 1, ('23', 1.0): 1, ('gimm', 1.0): 1, ('definetli', 1.0): 1, ('exit', 1.0): 1, ('bom-dia', 1.0): 1, ('apod', 1.0): 1, ('ultraviolet', 1.0): 1, ('m31', 1.0): 1, ('jul', 1.0): 1, ('oooh', 1.0): 1, ('yawn', 1.0): 1, ('ftw', 1.0): 1, ('maman', 1.0): 1, ('afterznoon', 1.0): 1, ('tweeep', 1.0): 1, ('abp', 1.0): 2, ('kiya', 1.0): 1, ('van', 1.0): 1, ('olymp', 1.0): 1, ('😷', 1.0): 1, ('classi', 1.0): 1, ('attach', 1.0): 1, ('equip', 1.0): 1, ('bobbl', 1.0): 1, ('anu', 1.0): 1, ('mh3', 1.0): 1, ('patch', 1.0): 1, ('psp', 1.0): 1, ('huffpost', 1.0): 1, ('tribut', 1.0): 1, ('h_eartshapedbox', 1.0): 1, ('magictrikband', 1.0): 1, ('magictrik', 1.0): 2, ('roommat', 1.0): 1, ('tami', 1.0): 1, ('b3dk', 1.0): 1, ('7an', 1.0): 1, ('ank', 1.0): 1, ('purpos', 1.0): 1, ('struggl', 1.0): 1, ('eagl', 1.0): 1, ('oceana', 1.0): 1, ('idk', 1.0): 3, ('med', 1.0): 1, ('fridayfauxpa', 1.0): 1, ('subtl', 1.0): 1, ('hint', 1.0): 1, ('prim', 1.0): 1, ('algorithm', 1.0): 1, ('iii', 1.0): 1, ('rosa', 1.0): 1, ('yvw', 1.0): 1, ('here', 1.0): 1, ('boost', 1.0): 1, ('unforgett', 1.0): 1, ('humor', 1.0): 1, (\"mum'\", 1.0): 1, ('hahahhaah', 1.0): 1, ('sombrero', 1.0): 1, ('lost', 1.0): 2, ('spammer', 1.0): 1, ('proceed', 1.0): 1, ('entertain', 1.0): 1, ('100k', 1.0): 1, ('mileston', 1.0): 1, ('judith', 1.0): 1, ('district', 1.0): 1, ('council', 1.0): 1, ('midar', 1.0): 1, ('gender', 1.0): 1, ('ilysm', 1.0): 1, ('zen', 1.0): 1, ('neat', 1.0): 1, ('rider', 1.0): 1, ('fyi', 1.0): 1, ('dig', 1.0): 2, ('👱🏽', 1.0): 1, ('👽', 1.0): 1, ('🌳', 1.0): 1, ('suspici', 1.0): 1, ('calori', 1.0): 1, ('harder', 1.0): 1, ('jessica', 1.0): 1, ('carina', 1.0): 1, ('francisco', 1.0): 1, ('teret', 1.0): 1, ('potassium', 1.0): 1, ('rehydr', 1.0): 1, ('drinkitallup', 1.0): 1, ('thirstquench', 1.0): 1, ('tapir', 1.0): 1, ('calf', 1.0): 1, ('mealtim', 1.0): 1, ('uhc', 1.0): 1, ('scale', 1.0): 1, ('network', 1.0): 1, ('areal', 1.0): 1, ('extremesport', 1.0): 1, ('quadbik', 1.0): 1, ('bloggersrequir', 1.0): 1, ('bloggersw', 1.0): 1, ('brainer', 1.0): 1, ('mse', 1.0): 1, ('fund', 1.0): 1, ('nooowww', 1.0): 1, ('lile', 1.0): 1, ('tid', 1.0): 1, ('tmi', 1.0): 1, ('deploy', 1.0): 1, ('jule', 1.0): 1, ('betti', 1.0): 1, ('hddc', 1.0): 1, ('salman', 1.0): 1, ('pthht', 1.0): 1, ('lfc', 1.0): 3, ('tope', 1.0): 1, ('xxoo', 1.0): 2, ('russia', 1.0): 2, ('silver-wash', 1.0): 1, ('fritillari', 1.0): 1, ('moon', 1.0): 1, ('ap', 1.0): 2, ('trash', 1.0): 2, ('clever', 1.0): 1, (\"thank'\", 1.0): 1, ('keven', 1.0): 1, ('pastim', 1.0): 1, ('ashramcal', 1.0): 1, ('ontrack', 1.0): 1, ('german', 1.0): 1, ('subtitl', 1.0): 1, ('pinter', 1.0): 1, ('morninggg', 1.0): 1, ('🐶', 1.0): 1, ('pete', 1.0): 1, ('awesome-o', 1.0): 1, ('multipl', 1.0): 1, ('cya', 1.0): 1, ('harrog', 1.0): 1, ('jet', 1.0): 1, ('supplier', 1.0): 1, ('req', 1.0): 1, ('fridayloug', 1.0): 1, ('4thstreetmus', 1.0): 1, ('hawaii', 1.0): 1, ('kick', 1.0): 1, ('deepli', 1.0): 1, ('john@timney.eclipse.co.uk', 1.0): 1, ('thousand', 1.0): 2, ('newspap', 1.0): 1, ('lew', 1.0): 1, ('nah', 1.0): 1, ('fallout', 1.0): 2, ('technic', 1.0): 1, ('gunderson', 1.0): 1, ('europa', 1.0): 1, ('thoroughli', 1.0): 1, ('script', 1.0): 1, ('overtak', 1.0): 1, ('motorway', 1.0): 1, ('thu', 1.0): 1, ('niteflirt', 1.0): 1, ('hbu', 1.0): 2, ('bowl', 1.0): 1, ('chri', 1.0): 2, ('niall', 1.0): 2, ('94', 1.0): 1, ('ik', 1.0): 1, ('stydia', 1.0): 1, ('nawazuddin', 1.0): 1, ('siddiqu', 1.0): 1, ('nomnomnom', 1.0): 1, ('dukefreebiefriday', 1.0): 1, ('z', 1.0): 1, ('insyaallah', 1.0): 1, ('ham', 1.0): 1, ('villa', 1.0): 1, ('brum', 1.0): 1, ('deni', 1.0): 1, ('vagina', 1.0): 1, ('rli', 1.0): 1, ('izzi', 1.0): 1, ('mitch', 1.0): 1, ('minn', 1.0): 1, ('recently.websit', 1.0): 1, ('coolingtow', 1.0): 1, ('soon.thank', 1.0): 1, ('showinginterest', 1.0): 1, ('multicolor', 1.0): 1, ('wid', 1.0): 1, ('wedg', 1.0): 1, ('motiv', 1.0): 1, ('nnnnot', 1.0): 1, (\"gf'\", 1.0): 1, ('bluesidemenxix', 1.0): 1, ('ardent', 1.0): 1, ('mooorn', 1.0): 1, ('wuppert', 1.0): 1, ('fridayfunday', 1.0): 1, ('re-sign', 1.0): 1, ('chalkhil', 1.0): 1, ('midday', 1.0): 1, ('carter', 1.0): 1, ('remedi', 1.0): 1, ('atrack', 1.0): 1, ('christ', 1.0): 1, ('badminton', 1.0): 1, (\"littl'un\", 1.0): 1, ('ikprideofpak', 1.0): 1, ('janjua', 1.0): 1, ('pimpl', 1.0): 1, ('forehead', 1.0): 1, ('volcano', 1.0): 1, ('mag', 1.0): 1, ('miryenda', 1.0): 1, (\"technology'\", 1.0): 1, ('touchétoday', 1.0): 1, ('idownload', 1.0): 1, ('25ish', 1.0): 1, ('snowbal', 1.0): 1, ('nd', 1.0): 1, ('expir', 1.0): 1, ('6gb', 1.0): 1, ('loveu', 1.0): 1, ('morefuninthephilippin', 1.0): 1, ('laho', 1.0): 1, ('caramoan', 1.0): 1, ('kareem', 1.0): 1, ('surah', 1.0): 1, ('kahaf', 1.0): 1, ('melani', 1.0): 1, ('bosch', 1.0): 1, ('machin', 1.0): 1, (\"week'\", 1.0): 1, ('refollow', 1.0): 1, ('😎', 1.0): 1, ('💁🏻', 1.0): 1, ('relaps', 1.0): 1, ('prada', 1.0): 2, ('punjabiswillgetit', 1.0): 1, ('hitter', 1.0): 1, ('mass', 1.0): 2, ('shoud', 1.0): 1, ('1:12', 1.0): 1, ('ughtm', 1.0): 1, ('545', 1.0): 1, ('kissm', 1.0): 1, ('likeforfollow', 1.0): 1, ('overwhelm', 1.0): 1, ('groupmat', 1.0): 1, ('75', 1.0): 2, ('kyunk', 1.0): 1, ('aitchison', 1.0): 1, ('curvi', 1.0): 1, ('mont', 1.0): 1, ('doa', 1.0): 1, ('header', 1.0): 1, ('speaker', 1.0): 3, ('avoid', 1.0): 1, ('laboratori', 1.0): 1, ('idc', 1.0): 1, ('fuckin', 1.0): 2, ('wooo', 1.0): 2, ('neobyt', 1.0): 1, ('pirat', 1.0): 1, ('takedown', 1.0): 1, ('indirag', 1.0): 1, ('judiciari', 1.0): 1, ('commit', 1.0): 4, ('govt', 1.0): 1, ('polici', 1.0): 1, ('rbi', 1.0): 1, ('similar', 1.0): 1, (\"thought'\", 1.0): 1, ('progress', 1.0): 1, ('transfer', 1.0): 1, ('gg', 1.0): 1, ('defenit', 1.0): 1, ('nofx', 1.0): 1, ('friskyfiday', 1.0): 1, ('yipee', 1.0): 1, ('shed', 1.0): 1, ('incent', 1.0): 1, ('vege', 1.0): 1, ('marin', 1.0): 1, ('gz', 1.0): 1, ('rajeev', 1.0): 1, ('hvng', 1.0): 1, ('funfil', 1.0): 1, ('friday.it', 1.0): 1, ('ws', 1.0): 1, ('reali', 1.0): 1, ('diff', 1.0): 1, ('kabir.fel', 1.0): 1, ('dresden', 1.0): 1, ('germani', 1.0): 1, ('plot', 1.0): 1, ('tdf', 1.0): 1, ('🍷', 1.0): 2, ('☀', 1.0): 2, ('🚲', 1.0): 2, ('minion', 1.0): 2, ('slot', 1.0): 1, (\"b'day\", 1.0): 1, ('isabella', 1.0): 1, ('okeyyy', 1.0): 1, ('vddd', 1.0): 1, (');', 1.0): 1, ('selfee', 1.0): 1, ('insta', 1.0): 1, ('🙆', 1.0): 1, ('🙌', 1.0): 1, ('😛', 1.0): 1, ('🐒', 1.0): 1, ('😝', 1.0): 1, ('hhahhaaa', 1.0): 1, ('jeez', 1.0): 1, ('teamcannib', 1.0): 1, ('teamspacewhalingisthebest', 1.0): 1, ('fitfa', 1.0): 1, ('identifi', 1.0): 1, ('pharmaci', 1.0): 1, ('verylaterealis', 1.0): 1, ('iwishiknewbett', 1.0): 1, ('satisfi', 1.0): 1, ('ess-aych-eye-te', 1.0): 1, ('supposedli', 1.0): 1, ('👍', 1.0): 1, ('immedi', 1.0): 1, (\"foxy'\", 1.0): 1, ('instrument', 1.0): 1, ('alon', 1.0): 2, ('goldcoast', 1.0): 1, ('lelomustfal', 1.0): 1, ('meal', 1.0): 1, ('5g', 1.0): 1, ('liker', 1.0): 1, ('newdress', 1.0): 1, ('resist', 1.0): 1, ('fot', 1.0): 1, ('troy', 1.0): 1, ('twitterfollowerswhatsup', 1.0): 1, ('happyfriedday', 1.0): 1, ('keepsafealway', 1.0): 1, ('loveyeah', 1.0): 1, ('emojasp_her', 1.0): 1, ('vanilla', 1.0): 1, ('sidemen', 1.0): 1, ('yaaayyy', 1.0): 1, ('friendaaa', 1.0): 1, ('bulb', 1.0): 5, ('corn', 1.0): 6, ('1tbps4', 1.0): 1, ('divin', 1.0): 1, ('wheeli', 1.0): 1, ('bin', 1.0): 1, ('ubericecream', 1.0): 1, ('messengerforaday', 1.0): 1, ('kyli', 1.0): 1, ('toilet', 1.0): 1, ('ikaw', 1.0): 1, ('musta', 1.0): 1, ('cheatmat', 1.0): 1, ('kyuhyun', 1.0): 1, ('ghanton', 1.0): 1, ('easy.get', 1.0): 1, ('5:30', 1.0): 1, ('therein', 1.0): 1, ('majalah', 1.0): 1, ('dominiqu', 1.0): 1, ('lamp', 1.0): 1, ('a-foot', 1.0): 1, ('revamp', 1.0): 1, ('brainchild', 1.0): 1, ('confid', 1.0): 1, ('confin', 1.0): 1, ('colorado', 1.0): 1, ('goodyear', 1.0): 1, ('upto', 1.0): 1, ('cashback', 1.0): 1, ('yourewelcom', 1.0): 1, ('nightli', 1.0): 1, ('simpin', 1.0): 1, ('sketchbook', 1.0): 1, ('4wild', 1.0): 1, ('colorpencil', 1.0): 1, ('cray', 1.0): 1, ('6:30', 1.0): 1, ('imma', 1.0): 3, ('ob', 1.0): 1, ('11h', 1.0): 1, ('kino', 1.0): 1, ('adult', 1.0): 1, ('kardamena', 1.0): 1, ('samo', 1.0): 1, ('greec', 1.0): 1, ('caesar', 1.0): 1, ('salad', 1.0): 1, ('tad', 1.0): 1, ('bland', 1.0): 1, ('respond', 1.0): 1, ('okk', 1.0): 1, ('den', 1.0): 1, ('allov', 1.0): 1, ('hangout', 1.0): 1, ('whoever', 1.0): 1, ('tourist', 1.0): 1, ('♌', 1.0): 1, ('kutiyapanti', 1.0): 1, ('profession', 1.0): 1, ('boomshot', 1.0): 1, ('fuh', 1.0): 1, ('yeeey', 1.0): 1, ('donot', 1.0): 1, ('expos', 1.0): 1, ('lipstick', 1.0): 1, ('cran', 1.0): 1, ('prayr', 1.0): 1, ('හෙල', 1.0): 1, ('හවුල', 1.0): 1, ('onemochaonelov', 1.0): 1, ('southpaw', 1.0): 1, ('geniu', 1.0): 1, ('stroma', 1.0): 1, ('🔴', 1.0): 1, ('younow', 1.0): 1, ('jonah', 1.0): 1, ('jareddd', 1.0): 1, ('postcod', 1.0): 1, ('talkmobil', 1.0): 1, ('huha', 1.0): 1, ('transform', 1.0): 1, ('sword', 1.0): 3, ('misread', 1.0): 1, ('richard', 1.0): 1, ('ibiza', 1.0): 1, ('birthdaymoneyforjesusjuic', 1.0): 1, ('ytb', 1.0): 1, ('tutori', 1.0): 1, ('construct', 1.0): 2, ('critic', 1.0): 1, ('ganesha', 1.0): 1, ('textur', 1.0): 1, ('photographi', 1.0): 1, ('hinduism', 1.0): 1, ('hindugod', 1.0): 1, ('elephantgod', 1.0): 1, ('selfish', 1.0): 1, ('bboy', 1.0): 1, ('cardgam', 1.0): 1, ('pixelart', 1.0): 1, ('gamedesign', 1.0): 1, ('indiedev', 1.0): 1, ('pixel_daili', 1.0): 1, ('plateau', 1.0): 1, ('laguna', 1.0): 1, ('tha', 1.0): 4, ('bahot', 1.0): 1, ('baje', 1.0): 1, ('raat', 1.0): 1, ('liya', 1.0): 1, ('hath', 1.0): 1, ('ghant', 1.0): 1, ('itna', 1.0): 2, ('bana', 1.0): 1, ('paya', 1.0): 1, ('uta', 1.0): 1, ('manga', 1.0): 1, ('jamuna', 1.0): 1, ('\\\\:', 1.0): 1, ('swiftma', 1.0): 1, ('trion', 1.0): 1, ('forum', 1.0): 1, ('b-day', 1.0): 1, ('disgust', 1.0): 1, ('commodor', 1.0): 1, ('annabel', 1.0): 1, ('bridg', 1.0): 1, ('quest', 1.0): 1, ('borderland', 1.0): 1, ('wanderrook', 1.0): 1, ('gm', 1.0): 1, ('preciou', 1.0): 2, ('mizz', 1.0): 1, ('bleedgreen', 1.0): 1, ('✌🏻', 1.0): 1, ('sophia', 1.0): 1, ('chicago', 1.0): 1, ('honeymoon', 1.0): 1, (\"da'esh\", 1.0): 1, ('co-ord', 1.0): 1, ('fsa', 1.0): 1, ('estat', 1.0): 1, (\"when'\", 1.0): 1, ('dusti', 1.0): 1, ('tunisia', 1.0): 2, (\"class'\", 1.0): 1, ('irrit', 1.0): 1, ('fiverr', 1.0): 1, ('gina', 1.0): 1, ('soproud', 1.0): 1, ('enought', 1.0): 1, ('hole', 1.0): 1, ('melbourneburg', 1.0): 1, ('arianna', 1.0): 1, ('esai', 1.0): 1, ('rotterdam', 1.0): 1, ('jordi', 1.0): 1, ('clasi', 1.0): 1, ('horni', 1.0): 1, ('salon', 1.0): 1, ('bleach', 1.0): 1, ('olaplex', 1.0): 1, ('damag', 1.0): 1, ('teamwork', 1.0): 1, ('zitecofficestori', 1.0): 1, ('다쇼', 1.0): 1, ('colleagu', 1.0): 1, ('eb', 1.0): 1, (\"t'would\", 1.0): 1, ('tweetup', 1.0): 1, ('detect', 1.0): 1, ('jonathancreek', 1.0): 1, ('dvr', 1.0): 1, ('kat', 1.0): 1, ('rarer', 1.0): 1, ('okkk', 1.0): 1, ('frend', 1.0): 1, ('milt', 1.0): 1, ('mario', 1.0): 1, ('rewatch', 1.0): 1, ('1600', 1.0): 1, ('sige', 1.0): 1, ('punta', 1.0): 1, ('kayo', 1.0): 1, ('nooo', 1.0): 1, ('prompt', 1.0): 1, ('t-mobil', 1.0): 1, ('orang', 1.0): 1, ('ee', 1.0): 1, ('teapot', 1.0): 1, ('hotter', 1.0): 1, ('»', 1.0): 1, ('londoutrad', 1.0): 1, ('kal', 1.0): 1, ('wayward', 1.0): 1, ('pine', 1.0): 1, ('muscl', 1.0): 1, ('ilikeit', 1.0): 1, ('belong', 1.0): 1, ('watford', 1.0): 1, ('enterpris', 1.0): 1, ('cube', 1.0): 1, ('particp', 1.0): 1, ('saudi', 1.0): 1, ('arabia', 1.0): 1, ('recogn', 1.0): 1, ('fanbas', 1.0): 3, ('bailona', 1.0): 3, ('responsibilti', 1.0): 1, ('sunlight', 1.0): 1, ('tiger', 1.0): 1, ('elev', 1.0): 1, ('horror', 1.0): 1, ('bitchesss', 1.0): 1, ('shitti', 1.0): 1, ('squash', 1.0): 1, ('becca', 1.0): 1, ('delta', 1.0): 1, ('nut', 1.0): 1, ('yun', 1.0): 1, ('joe', 1.0): 1, ('dirt', 1.0): 1, ('sharon', 1.0): 1, ('medicin', 1.0): 1, ('ttyl', 1.0): 1, ('gav', 1.0): 1, ('linda', 1.0): 1, ('3hr', 1.0): 1, ('tym', 1.0): 2, ('dieback', 1.0): 1, ('endit', 1.0): 1, ('minecon', 1.0): 1, ('sere', 1.0): 1, ('joerin', 1.0): 1, ('joshan', 1.0): 1, ('tandem', 1.0): 1, ('ligao', 1.0): 1, ('albay', 1.0): 1, ('bcyc', 1.0): 1, ('lnh', 1.0): 1, ('sat', 1.0): 1, ('honorari', 1.0): 1, ('alac', 1.0): 1, ('skelo_ghost', 1.0): 1, ('madadagdagan', 1.0): 1, ('bmc', 1.0): 1, ('11:11', 1.0): 2, ('embarrass', 1.0): 1, ('entropi', 1.0): 1, ('evolut', 1.0): 2, ('loop', 1.0): 1, ('eva', 1.0): 1, ('camden', 1.0): 1, ('uhh', 1.0): 1, ('scoup', 1.0): 1, ('jren', 1.0): 1, ('nuest', 1.0): 1, ('lovelayyy', 1.0): 1, ('kidney', 1.0): 1, ('neuer', 1.0): 1, ('spray', 1.0): 1, ('donnae.strydom@westerncape.gov.za', 1.0): 1, ('uni', 1.0): 1, ('uff', 1.0): 1, ('karhi', 1.0): 1, ('thi', 1.0): 1, ('juaquin', 1.0): 1, ('v3nzor99', 1.0): 1, ('shell', 1.0): 1, ('heyi', 1.0): 1, ('flavor', 1.0): 1, ('thakyou', 1.0): 1, ('beatriz', 1.0): 1, ('cancel', 1.0): 1, ('puff', 1.0): 1, ('egg', 1.0): 2, ('tart', 1.0): 1, ('chai', 1.0): 1, ('mtr', 1.0): 1, ('alyssa', 1.0): 1, ('rub', 1.0): 1, ('tummi', 1.0): 1, ('zelda', 1.0): 1, ('ive', 1.0): 1, ('🎂', 1.0): 1, ('jiva', 1.0): 1, ('🍹', 1.0): 1, ('🍻', 1.0): 1, ('mubbarak', 1.0): 1, ('deborah', 1.0): 1, ('coupon', 1.0): 1, ('colourdeb', 1.0): 1, ('purpl', 1.0): 1, (\"chippy'\", 1.0): 1, ('vessel', 1.0): 1, ('ps', 1.0): 2, ('vintag', 1.0): 1, ('✫', 1.0): 4, ('˚', 1.0): 4, ('·', 1.0): 4, ('✵', 1.0): 4, ('⊹', 1.0): 4, ('1710', 1.0): 1, ('gooffeanotter', 1.0): 1, ('kiksex', 1.0): 1, ('mugshot', 1.0): 1, ('token', 1.0): 1, ('maritimen', 1.0): 1, ('rh', 1.0): 1, ('tatton', 1.0): 1, ('jump_julia', 1.0): 1, ('malema', 1.0): 1, ('fren', 1.0): 1, ('nuf', 1.0): 1, ('teas', 1.0): 1, ('alien', 1.0): 2, ('closer', 1.0): 1, ('monitor', 1.0): 1, ('kimmi', 1.0): 1, (\"channel'\", 1.0): 1, ('planetbollywoodnew', 1.0): 1, ('epi', 1.0): 1, ('tricki', 1.0): 1, ('be-shak', 1.0): 1, ('chenoweth', 1.0): 1, ('oodl', 1.0): 1, ('hailey', 1.0): 1, ('craźi', 1.0): 1, ('sęxxxÿ', 1.0): 1, ('cøôl', 1.0): 1, ('runway', 1.0): 1, ('gooodnight', 1.0): 1, ('iv', 1.0): 1, ('ri', 1.0): 1, ('jayci', 1.0): 1, ('karaok', 1.0): 1, ('ltsw', 1.0): 1, ('giant', 1.0): 1, ('1709', 1.0): 1, ('refus', 1.0): 1, ('collagen', 1.0): 1, ('2win', 1.0): 1, ('hopetowin', 1.0): 1, ('inventori', 1.0): 1, ('loveforfood', 1.0): 1, ('foodforthought', 1.0): 1, ('thoughtfortheday', 1.0): 1, ('carp', 1.0): 1, ('diem', 1.0): 1, ('nath', 1.0): 1, ('ning', 1.0): 1, ('although', 1.0): 1, ('harm', 1.0): 1, ('stormi', 1.0): 1, ('sync', 1.0): 1, ('devic', 1.0): 1, ('mess', 1.0): 1, ('nylon', 1.0): 1, ('gvb', 1.0): 1, ('cd', 1.0): 1, ('mountain.titl', 1.0): 1, ('unto', 1.0): 1, ('theworldwouldchang', 1.0): 1, ('categori', 1.0): 1, ('mah', 1.0): 1, ('panel', 1.0): 1, (\"i'am\", 1.0): 1, ('80-1', 1.0): 1, ('1708', 1.0): 1, ('neenkin', 1.0): 1, ('masterpiec', 1.0): 1, ('debit', 1.0): 1, ('beagl', 1.0): 1, ('♫', 1.0): 1, ('feat', 1.0): 1, ('charli', 1.0): 1, ('puth', 1.0): 1, ('wiz', 1.0): 1, ('khalifa', 1.0): 1, ('svu', 1.0): 1, ('darker', 1.0): 1, ('berni', 1.0): 1, ('henri', 1.0): 1, ('trap', 1.0): 1, ('tommi', 1.0): 1, (\"vivian'\", 1.0): 1, ('transpar', 1.0): 1, ('bitcoin', 1.0): 1, ('insight', 1.0): 1, ('ping', 1.0): 1, ('masquerad', 1.0): 1, ('zorroreturm', 1.0): 1, ('1707', 1.0): 1, ('pk', 1.0): 1, ('hay', 1.0): 1, ('jacquelin', 1.0): 1, ('passion', 1.0): 1, ('full-fledg', 1.0): 1, ('workplac', 1.0): 1, ('venu', 1.0): 1, ('lago', 1.0): 1, ('luxord', 1.0): 1, ('potato', 1.0): 1, ('hundr', 1.0): 1, ('cite', 1.0): 1, ('academ', 1.0): 1, ('pokiri', 1.0): 1, ('1nenokkadin', 1.0): 1, ('heritag', 1.0): 1, ('wood', 1.0): 1, ('beleaf', 1.0): 1, ('spnfamili', 1.0): 1, ('spn', 1.0): 1, ('alwayskeepfight', 1.0): 1, ('jaredpadalecki', 1.0): 1, ('jensenackl', 1.0): 1, ('peasant', 1.0): 2, ('ahahha', 1.0): 1, ('distant', 1.0): 1, ('shout-out', 1.0): 1, ('adulthood', 1.0): 1, ('hopeless', 0.0): 2, ('tmr', 0.0): 3, (':(', 0.0): 4571, ('everyth', 0.0): 17, ('kid', 0.0): 20, ('section', 0.0): 3, ('ikea', 0.0): 1, ('cute', 0.0): 43, ('shame', 0.0): 19, (\"i'm\", 0.0): 343, ('nearli', 0.0): 3, ('19', 0.0): 8, ('2', 0.0): 41, ('month', 0.0): 23, ('heart', 0.0): 27, ('slide', 0.0): 1, ('wast', 0.0): 5, ('basket', 0.0): 1, ('“', 0.0): 15, ('hate', 0.0): 57, ('japanes', 0.0): 4, ('call', 0.0): 29, ('bani', 0.0): 2, ('”', 0.0): 11, ('dang', 0.0): 2, ('start', 0.0): 44, ('next', 0.0): 40, ('week', 0.0): 56, ('work', 0.0): 133, ('oh', 0.0): 92, ('god', 0.0): 15, ('babi', 0.0): 47, ('face', 0.0): 20, ('make', 0.0): 102, ('smile', 0.0): 10, ('neighbour', 0.0): 1, ('motor', 0.0): 1, ('ask', 0.0): 29, ('said', 0.0): 33, ('updat', 0.0): 11, ('search', 0.0): 3, ('sialan', 0.0): 1, ('athabasca', 0.0): 2, ('glacier', 0.0): 2, ('1948', 0.0): 1, (':-(', 0.0): 493, ('jasper', 0.0): 1, ('jaspernationalpark', 0.0): 1, ('alberta', 0.0): 1, ('explorealberta', 0.0): 1, ('…', 0.0): 16, ('realli', 0.0): 131, ('good', 0.0): 101, ('g', 0.0): 8, ('idea', 0.0): 10, ('never', 0.0): 57, ('go', 0.0): 224, ('meet', 0.0): 31, ('mare', 0.0): 1, ('ivan', 0.0): 1, ('happi', 0.0): 25, ('trip', 0.0): 11, ('keep', 0.0): 34, ('safe', 0.0): 5, ('see', 0.0): 124, ('soon', 0.0): 45, ('tire', 0.0): 50, ('hahahah', 0.0): 3, ('knee', 0.0): 2, ('replac', 0.0): 4, ('get', 0.0): 232, ('day', 0.0): 149, ('ouch', 0.0): 3, ('relat', 0.0): 2, ('sweet', 0.0): 7, ('n', 0.0): 21, ('sour', 0.0): 2, ('kind', 0.0): 11, ('bi-polar', 0.0): 1, ('peopl', 0.0): 75, ('life', 0.0): 33, ('...', 0.0): 331, ('cuz', 0.0): 4, ('full', 0.0): 16, ('pleass', 0.0): 2, ('im', 0.0): 129, ('sure', 0.0): 31, ('tho', 0.0): 28, ('feel', 0.0): 158, ('stupid', 0.0): 8, (\"can't\", 0.0): 180, ('seem', 0.0): 15, ('grasp', 0.0): 1, ('basic', 0.0): 2, ('digit', 0.0): 8, ('paint', 0.0): 3, ('noth', 0.0): 26, (\"i'v\", 0.0): 77, ('research', 0.0): 1, ('help', 0.0): 54, ('lord', 0.0): 2, ('lone', 0.0): 9, ('someon', 0.0): 57, ('talk', 0.0): 45, ('guy', 0.0): 62, ('girl', 0.0): 28, ('assign', 0.0): 5, ('project', 0.0): 3, ('😩', 0.0): 14, ('want', 0.0): 246, ('play', 0.0): 48, ('video', 0.0): 23, ('game', 0.0): 28, ('watch', 0.0): 77, ('movi', 0.0): 24, ('choreograph', 0.0): 1, ('hard', 0.0): 35, ('email', 0.0): 10, ('link', 0.0): 12, ('still', 0.0): 124, ('say', 0.0): 63, ('longer', 0.0): 12, ('avail', 0.0): 13, ('cri', 0.0): 46, ('bc', 0.0): 50, ('miss', 0.0): 301, ('mingm', 0.0): 1, ('much', 0.0): 139, ('sorri', 0.0): 148, ('mom', 0.0): 13, ('far', 0.0): 18, ('away', 0.0): 28, (\"we'r\", 0.0): 30, ('truli', 0.0): 5, ('flight', 0.0): 6, ('friend', 0.0): 39, ('happen', 0.0): 51, ('sad', 0.0): 123, ('dog', 0.0): 17, ('pee', 0.0): 2, ('’', 0.0): 27, ('bag', 0.0): 8, ('take', 0.0): 49, ('newwin', 0.0): 1, ('15', 0.0): 10, ('doushit', 0.0): 1, ('late', 0.0): 27, ('suck', 0.0): 23, ('sick', 0.0): 43, ('plan', 0.0): 17, ('first', 0.0): 27, ('gundam', 0.0): 1, ('night', 0.0): 46, ('nope', 0.0): 6, ('dollar', 0.0): 1, ('😭', 0.0): 29, ('listen', 0.0): 18, ('back', 0.0): 122, ('old', 0.0): 16, ('show', 0.0): 26, ('know', 0.0): 131, ('weird', 0.0): 10, ('got', 0.0): 104, ('u', 0.0): 193, ('leav', 0.0): 42, ('might', 0.0): 11, ('give', 0.0): 36, ('pale', 0.0): 2, ('imit', 0.0): 1, ('went', 0.0): 32, ('sea', 0.0): 1, ('massiv', 0.0): 4, ('fuck', 0.0): 58, ('rash', 0.0): 1, ('bodi', 0.0): 12, ('pain', 0.0): 21, ('thing', 0.0): 52, ('ever', 0.0): 30, ('home', 0.0): 63, ('hi', 0.0): 34, ('absent', 0.0): 1, ('gran', 0.0): 2, ('knew', 0.0): 6, ('care', 0.0): 20, ('tell', 0.0): 26, ('love', 0.0): 152, ('wish', 0.0): 91, ('would', 0.0): 70, ('sequel', 0.0): 1, ('busi', 0.0): 28, ('sa', 0.0): 15, ('school', 0.0): 32, ('time', 0.0): 166, ('yah', 0.0): 3, ('xx', 0.0): 18, ('ouucchhh', 0.0): 1, ('one', 0.0): 148, ('wisdom', 0.0): 2, ('teeth', 0.0): 6, ('come', 0.0): 91, ('frighten', 0.0): 1, ('case', 0.0): 6, ('pret', 0.0): 1, ('wkwkw', 0.0): 1, ('verfi', 0.0): 1, ('activ', 0.0): 6, ('forget', 0.0): 8, ('follow', 0.0): 262, ('member', 0.0): 6, ('thank', 0.0): 107, ('join', 0.0): 8, ('goodby', 0.0): 14, ('´', 0.0): 4, ('chain', 0.0): 1, ('—', 0.0): 26, ('sentir-s', 0.0): 1, ('incompleta', 0.0): 1, ('okay', 0.0): 38, ('..', 0.0): 108, ('wednesday', 0.0): 5, ('marvel', 0.0): 1, ('thwart', 0.0): 1, ('awh', 0.0): 3, (\"what'\", 0.0): 15, ('chanc', 0.0): 16, ('zant', 0.0): 1, ('need', 0.0): 106, ('someth', 0.0): 28, ('x', 0.0): 39, (\"when'\", 0.0): 1, ('birthday', 0.0): 23, ('worst', 0.0): 14, ('part', 0.0): 11, ('bad', 0.0): 73, ('audraesar', 0.0): 1, ('sushi', 0.0): 3, ('pic', 0.0): 15, ('tl', 0.0): 8, ('drive', 0.0): 16, ('craaazzyy', 0.0): 2, ('pop', 0.0): 3, ('like', 0.0): 228, ('helium', 0.0): 1, ('balloon', 0.0): 1, ('climatechang', 0.0): 5, ('cc', 0.0): 6, (\"california'\", 0.0): 1, ('power', 0.0): 6, ('influenti', 0.0): 1, ('air', 0.0): 3, ('pollut', 0.0): 1, ('watchdog', 0.0): 1, ('califor', 0.0): 1, ('elhaida', 0.0): 1, ('rob', 0.0): 2, ('juri', 0.0): 1, ('came', 0.0): 16, ('10th', 0.0): 1, ('televot', 0.0): 1, ('idaho', 0.0): 2, ('restrict', 0.0): 2, ('fish', 0.0): 2, ('despit', 0.0): 2, ('region', 0.0): 2, ('drought-link', 0.0): 1, ('die-of', 0.0): 1, ('abrupt', 0.0): 1, ('climat', 0.0): 1, ('chang', 0.0): 27, ('may', 0.0): 16, ('doom', 0.0): 2, ('mammoth', 0.0): 1, ('megafauna', 0.0): 1, ('sc', 0.0): 3, (\"australia'\", 0.0): 1, ('dirtiest', 0.0): 2, ('station', 0.0): 3, ('consid', 0.0): 5, ('clean', 0.0): 6, ('energi', 0.0): 3, ('biomass', 0.0): 1, (\"ain't\", 0.0): 5, ('easi', 0.0): 6, ('green', 0.0): 7, ('golf', 0.0): 1, ('cours', 0.0): 7, ('california', 0.0): 1, ('ulti', 0.0): 1, ('well', 0.0): 56, ('mine', 0.0): 12, ('gonna', 0.0): 51, ('sexi', 0.0): 14, ('prexi', 0.0): 1, ('kindergarten', 0.0): 1, ('hungri', 0.0): 19, ('cant', 0.0): 47, ('find', 0.0): 53, ('book', 0.0): 20, ('sane', 0.0): 1, ('liter', 0.0): 15, ('three', 0.0): 7, ('loung', 0.0): 1, ('event', 0.0): 4, ('turn', 0.0): 17, ('boss', 0.0): 5, ('hozier', 0.0): 1, (\"that'\", 0.0): 61, ('true', 0.0): 22, ('soooner', 0.0): 1, ('ahh', 0.0): 7, ('fam', 0.0): 3, ('respectlost', 0.0): 1, ('hypercholesteloremia', 0.0): 1, ('ok', 0.0): 33, ('look', 0.0): 100, ('gift', 0.0): 11, ('calibraska', 0.0): 1, ('actual', 0.0): 24, ('genuin', 0.0): 2, ('contend', 0.0): 1, ('head', 0.0): 23, ('alway', 0.0): 56, ('hurt', 0.0): 41, ('stay', 0.0): 24, ('lmao', 0.0): 13, ('older', 0.0): 5, ('sound', 0.0): 19, ('upset', 0.0): 11, ('infinit', 0.0): 10, ('ao', 0.0): 1, ('stick', 0.0): 1, ('8th', 0.0): 1, ('either', 0.0): 13, ('seriou', 0.0): 8, ('yun', 0.0): 1, ('eh', 0.0): 4, ('room', 0.0): 11, ('way', 0.0): 42, ('hot', 0.0): 15, ('havent', 0.0): 11, ('found', 0.0): 11, ('handsom', 0.0): 2, ('jack', 0.0): 3, ('draw', 0.0): 2, ('shit', 0.0): 36, ('cut', 0.0): 14, ('encor', 0.0): 4, ('4thwin', 0.0): 4, ('baymax', 0.0): 1, ('french', 0.0): 4, ('mixer', 0.0): 1, ('💜', 0.0): 6, ('wft', 0.0): 1, ('awesom', 0.0): 5, ('replay', 0.0): 1, ('parti', 0.0): 15, ('promot', 0.0): 3, ('music', 0.0): 16, ('bank', 0.0): 9, ('short', 0.0): 11, ('boy', 0.0): 18, ('order', 0.0): 16, ('receiv', 0.0): 7, ('hub', 0.0): 1, ('nearest', 0.0): 1, ('deliv', 0.0): 3, ('today', 0.0): 108, ('1/2', 0.0): 3, ('mum', 0.0): 14, ('loud', 0.0): 2, ('final', 0.0): 35, ('parasyt', 0.0): 1, ('alll', 0.0): 1, ('zayniscomingbackonjuli', 0.0): 23, ('26', 0.0): 24, ('bye', 0.0): 8, ('era', 0.0): 1, ('。', 0.0): 3, ('ω', 0.0): 1, ('」', 0.0): 2, ('∠', 0.0): 2, ('):', 0.0): 6, ('nathann', 0.0): 1, ('💕', 0.0): 7, ('hug', 0.0): 29, ('😊', 0.0): 9, ('beauti', 0.0): 11, ('dieididieieiei', 0.0): 1, ('stage', 0.0): 15, ('mean', 0.0): 43, ('hello', 0.0): 13, ('lion', 0.0): 3, ('think', 0.0): 75, ('screw', 0.0): 4, ('netflix', 0.0): 5, ('chill', 0.0): 7, ('di', 0.0): 7, ('ervin', 0.0): 1, ('ohh', 0.0): 8, ('yeah', 0.0): 41, ('hope', 0.0): 102, ('accept', 0.0): 2, ('offer', 0.0): 10, ('desper', 0.0): 2, ('year', 0.0): 46, ('snapchat', 0.0): 79, ('amargolonnard', 0.0): 2, ('kikhorni', 0.0): 13, ('snapm', 0.0): 4, ('tagsforlik', 0.0): 5, ('batalladelosgallo', 0.0): 2, ('webcamsex', 0.0): 4, ('ugh', 0.0): 26, ('stream', 0.0): 24, ('duti', 0.0): 3, (\"u'v\", 0.0): 1, ('gone', 0.0): 24, ('alien', 0.0): 1, ('aww', 0.0): 21, ('wanna', 0.0): 94, ('sorka', 0.0): 1, ('funer', 0.0): 4, ('text', 0.0): 15, ('phone', 0.0): 34, ('sunni', 0.0): 1, ('nonexist', 0.0): 1, ('wowza', 0.0): 1, ('fah', 0.0): 1, ('taylor', 0.0): 3, ('crop', 0.0): 1, ('boo', 0.0): 5, ('count', 0.0): 7, ('new', 0.0): 51, ('guitar', 0.0): 1, ('jonghyun', 0.0): 1, ('hyung', 0.0): 1, ('pleas', 0.0): 275, ('predict', 0.0): 2, ('sj', 0.0): 3, ('nomin', 0.0): 1, ('vs', 0.0): 4, ('pl', 0.0): 45, ('dude', 0.0): 12, ('calm', 0.0): 3, ('brace', 0.0): 5, ('sir', 0.0): 5, ('plu', 0.0): 4, ('4', 0.0): 18, ('shock', 0.0): 3, ('omggg', 0.0): 2, ('yall', 0.0): 4, ('deserv', 0.0): 8, ('whenev', 0.0): 3, ('spend', 0.0): 8, ('smoke', 0.0): 3, ('end', 0.0): 40, ('fall', 0.0): 16, ('asleep', 0.0): 25, ('1', 0.0): 26, ('point', 0.0): 14, ('close', 0.0): 20, ('grand', 0.0): 1, ('whyyi', 0.0): 7, ('long', 0.0): 38, ('must', 0.0): 15, ('annoy', 0.0): 11, ('evan', 0.0): 1, ('option', 0.0): 3, ('opt', 0.0): 1, (\"who'\", 0.0): 7, ('giveaway', 0.0): 3, ('muster', 0.0): 1, ('merch', 0.0): 4, ('ah', 0.0): 18, ('funni', 0.0): 6, ('drink', 0.0): 7, ('savanna', 0.0): 1, ('straw', 0.0): 1, ('ignor', 0.0): 16, ('yester', 0.0): 1, ('afternoon', 0.0): 3, ('sleep', 0.0): 90, ('ye', 0.0): 48, ('sadli', 0.0): 11, ('when', 0.0): 2, ('album', 0.0): 16, ('last', 0.0): 72, ('chocol', 0.0): 8, ('consum', 0.0): 1, ('werk', 0.0): 1, ('morn', 0.0): 31, ('foreal', 0.0): 1, ('wesen', 0.0): 1, ('uwes', 0.0): 1, ('mj', 0.0): 1, ('😂', 0.0): 24, ('catch', 0.0): 9, ('onlin', 0.0): 20, ('enough', 0.0): 24, ('haha', 0.0): 30, (\"he'\", 0.0): 23, ('bosen', 0.0): 1, ('die', 0.0): 21, ('egg', 0.0): 4, ('benni', 0.0): 1, ('sometim', 0.0): 16, ('followback', 0.0): 6, ('huhu', 0.0): 17, ('understand', 0.0): 15, ('badli', 0.0): 12, ('scare', 0.0): 16, ('&gt;:(', 0.0): 47, ('al', 0.0): 4, ('kati', 0.0): 3, ('zaz', 0.0): 1, ('ami', 0.0): 2, ('lot', 0.0): 27, ('diari', 0.0): 1, ('read', 0.0): 20, ('rehash', 0.0): 1, ('websit', 0.0): 7, ('mushroom', 0.0): 1, ('piec', 0.0): 4, ('except', 0.0): 5, ('reach', 0.0): 3, ('anyway', 0.0): 12, ('vicki', 0.0): 1, ('omg', 0.0): 63, ('wtf', 0.0): 13, ('lip', 0.0): 3, ('virgin', 0.0): 2, ('your', 0.0): 8, ('45', 0.0): 1, ('hahah', 0.0): 6, ('ninasti', 0.0): 1, ('tsktsk', 0.0): 1, ('oppa', 0.0): 4, ('wont', 0.0): 9, ('dick', 0.0): 5, ('kawaii', 0.0): 1, ('manli', 0.0): 1, ('xbox', 0.0): 3, ('alreadi', 0.0): 52, ('comfi', 0.0): 1, ('bed', 0.0): 12, ('youu', 0.0): 2, ('sigh', 0.0): 13, ('lol', 0.0): 43, ('potato', 0.0): 1, ('fri', 0.0): 7, ('guess', 0.0): 14, (\"y'all\", 0.0): 2, ('ugli', 0.0): 9, ('asf', 0.0): 1, ('huh', 0.0): 7, ('eish', 0.0): 1, ('ive', 0.0): 11, ('quit', 0.0): 9, ('lost', 0.0): 25, ('twitter', 0.0): 30, ('mojo', 0.0): 1, ('dont', 0.0): 53, ('mara', 0.0): 1, ('neh', 0.0): 2, ('fever', 0.0): 7, ('&lt;3', 0.0): 25, ('poor', 0.0): 35, ('bb', 0.0): 7, ('abl', 0.0): 22, ('associ', 0.0): 1, ('councillor', 0.0): 1, ('confer', 0.0): 2, ('weekend', 0.0): 25, ('skype', 0.0): 6, ('account', 0.0): 20, ('hack', 0.0): 8, ('contact', 0.0): 7, ('creat', 0.0): 2, ('tweet', 0.0): 35, ('spree', 0.0): 4, ('na', 0.0): 29, ('sholong', 0.0): 1, ('reject', 0.0): 7, ('propos', 0.0): 2, ('gee', 0.0): 1, ('fli', 0.0): 10, ('gidi', 0.0): 1, ('pamper', 0.0): 1, ('lago', 0.0): 1, ('ehn', 0.0): 1, ('arrest', 0.0): 1, ('girlfriend', 0.0): 2, ('he', 0.0): 3, ('nice', 0.0): 19, ('person', 0.0): 15, ('idk', 0.0): 26, ('anybodi', 0.0): 7, ('song', 0.0): 27, ('disappear', 0.0): 1, ('itun', 0.0): 3, ('daze', 0.0): 1, ('confus', 0.0): 8, ('surviv', 0.0): 5, ('fragment', 0.0): 1, (\"would'v\", 0.0): 2, ('forc', 0.0): 2, ('horribl', 0.0): 9, ('weather', 0.0): 29, ('us', 0.0): 43, ('could', 0.0): 69, ('walao', 0.0): 1, ('kb', 0.0): 1, ('send', 0.0): 12, ('ill', 0.0): 16, ('djderek', 0.0): 1, ('mani', 0.0): 29, ('fun', 0.0): 32, ('gig', 0.0): 3, ('absolut', 0.0): 6, ('legend', 0.0): 3, ('wait', 0.0): 43, ('till', 0.0): 8, ('saturday', 0.0): 10, ('homework', 0.0): 2, ('pa', 0.0): 8, ('made', 0.0): 23, ('da', 0.0): 5, ('greek', 0.0): 2, ('tragedi', 0.0): 1, ('rain', 0.0): 43, ('gym', 0.0): 6, ('💪🏻', 0.0): 1, ('🐒', 0.0): 1, ('what', 0.0): 8, ('wrong', 0.0): 33, ('struck', 0.0): 1, ('anymor', 0.0): 20, ('belgium', 0.0): 4, ('fabian', 0.0): 2, ('delph', 0.0): 6, ('fallen', 0.0): 3, ('hide', 0.0): 4, ('drake', 0.0): 1, ('silent', 0.0): 1, ('hear', 0.0): 33, ('rest', 0.0): 21, ('peac', 0.0): 5, ('mo', 0.0): 4, ('tonight', 0.0): 24, ('t20blast', 0.0): 1, ('ahhh', 0.0): 5, ('wake', 0.0): 21, ('mumma', 0.0): 2, ('7', 0.0): 16, ('dead', 0.0): 10, ('tomorrow', 0.0): 34, (\"i'll\", 0.0): 41, ('high', 0.0): 8, ('low', 0.0): 8, ('pray', 0.0): 13, ('appropri', 0.0): 1, ('. . .', 0.0): 2, ('awak', 0.0): 10, ('woke', 0.0): 14, ('upp', 0.0): 1, ('dm', 0.0): 23, ('luke', 0.0): 6, ('hey', 0.0): 26, ('babe', 0.0): 19, ('across', 0.0): 4, ('hindi', 0.0): 1, ('reaction', 0.0): 1, ('5s', 0.0): 1, ('run', 0.0): 15, ('space', 0.0): 5, ('tbh', 0.0): 14, ('disabl', 0.0): 2, ('pension', 0.0): 1, ('ptsd', 0.0): 1, ('imposs', 0.0): 4, ('physic', 0.0): 7, ('financi', 0.0): 2, ('nooo', 0.0): 16, ('broke', 0.0): 9, ('soo', 0.0): 3, ('amaz', 0.0): 16, ('toghet', 0.0): 1, ('around', 0.0): 20, ('p', 0.0): 5, ('hold', 0.0): 9, ('anoth', 0.0): 27, ('septemb', 0.0): 2, ('21st', 0.0): 2, ('snsd', 0.0): 2, ('interact', 0.0): 2, ('anna', 0.0): 5, ('akana', 0.0): 1, ('askip', 0.0): 1, (\"t'exist\", 0.0): 1, ('channel', 0.0): 6, ('owner', 0.0): 1, ('decid', 0.0): 10, ('broadcast', 0.0): 6, ('kei', 0.0): 2, ('rate', 0.0): 4, ('se', 0.0): 2, ('notic', 0.0): 26, ('exist', 0.0): 2, ('traffic', 0.0): 5, ('terribl', 0.0): 12, ('eye', 0.0): 12, ('small', 0.0): 9, ('kate', 0.0): 2, ('spade', 0.0): 1, ('pero', 0.0): 3, ('walang', 0.0): 1, ('maganda', 0.0): 1, ('aw', 0.0): 42, ('seen', 0.0): 23, ('agesss', 0.0): 1, ('add', 0.0): 26, ('corinehurleigh', 0.0): 1, ('snapchatm', 0.0): 6, ('instagram', 0.0): 4, ('addmeonsnapchat', 0.0): 2, ('sf', 0.0): 3, ('quot', 0.0): 6, ('kiksext', 0.0): 6, ('bum', 0.0): 2, ('zara', 0.0): 1, ('trouser', 0.0): 1, ('effect', 0.0): 4, ('spanish', 0.0): 1, (\"it'okay\", 0.0): 1, ('health', 0.0): 2, ('luck', 0.0): 6, ('freed', 0.0): 1, ('rock', 0.0): 3, ('orcalov', 0.0): 1, ('tri', 0.0): 65, ('big', 0.0): 21, ('cuddl', 0.0): 8, ('lew', 0.0): 1, ('kiss', 0.0): 4, ('em', 0.0): 1, ('crave', 0.0): 8, ('banana', 0.0): 4, ('crumbl', 0.0): 1, ('mcflurri', 0.0): 1, ('cabl', 0.0): 1, ('car', 0.0): 17, ('brother', 0.0): 10, (\"venus'\", 0.0): 1, ('concept', 0.0): 4, ('rli', 0.0): 5, ('tea', 0.0): 7, ('tagal', 0.0): 2, (\"we'v\", 0.0): 3, ('appoint', 0.0): 1, (\"i'd\", 0.0): 11, ('sinc', 0.0): 35, (\"there'\", 0.0): 18, ('milk', 0.0): 3, ('left', 0.0): 26, ('cereal', 0.0): 2, ('film', 0.0): 6, ('date', 0.0): 7, ('previou', 0.0): 2, ('73', 0.0): 2, ('user', 0.0): 1, ('everywher', 0.0): 6, ('fansign', 0.0): 1, ('photo', 0.0): 15, ('expens', 0.0): 7, ('zzzz', 0.0): 1, ('let', 0.0): 37, ('sun', 0.0): 10, ('yet', 0.0): 33, (\"bff'\", 0.0): 1, ('extrem', 0.0): 3, ('stress', 0.0): 10, ('anyth', 0.0): 19, ('win', 0.0): 27, (\"deosn't\", 0.0): 1, ('liverpool', 0.0): 2, ('pool', 0.0): 3, ('though', 0.0): 57, ('bro', 0.0): 3, ('great', 0.0): 22, ('news', 0.0): 21, ('self', 0.0): 1, ('esteem', 0.0): 1, ('lowest', 0.0): 1, ('better', 0.0): 36, ('tacki', 0.0): 1, ('taken', 0.0): 9, ('man', 0.0): 32, ('lucki', 0.0): 16, ('charm', 0.0): 1, ('haaretz', 0.0): 1, ('israel', 0.0): 1, ('syria', 0.0): 2, ('continu', 0.0): 1, ('develop', 0.0): 5, ('chemic', 0.0): 1, ('weapon', 0.0): 2, ('offici', 0.0): 3, ('wsj', 0.0): 2, ('rep', 0.0): 1, ('bt', 0.0): 4, ('mr', 0.0): 9, ('wong', 0.0): 1, ('confisc', 0.0): 1, ('art', 0.0): 4, ('thought', 0.0): 31, ('icepack', 0.0): 1, ('dose', 0.0): 2, ('killer', 0.0): 2, ('board', 0.0): 1, ('whimper', 0.0): 1, ('fan', 0.0): 17, ('senpai', 0.0): 1, ('buttsex', 0.0): 1, ('joke', 0.0): 8, ('headlin', 0.0): 1, (\"dn't\", 0.0): 1, ('brk', 0.0): 1, (\":'(\", 0.0): 13, ('hit', 0.0): 7, ('voic', 0.0): 9, ('falsetto', 0.0): 1, ('zone', 0.0): 2, ('leannerin', 0.0): 1, ('hornykik', 0.0): 17, ('loveofmylif', 0.0): 2, ('dmme', 0.0): 2, ('pussi', 0.0): 2, ('newmus', 0.0): 3, ('sexo', 0.0): 2, ('s2', 0.0): 1, ('spain', 0.0): 4, ('delay', 0.0): 5, ('kill', 0.0): 22, ('singl', 0.0): 10, ('untruth', 0.0): 1, ('cross', 0.0): 4, ('countri', 0.0): 6, ('ij', 0.0): 1, ('💥', 0.0): 1, ('✨', 0.0): 1, ('💫', 0.0): 1, ('bear', 0.0): 2, ('littl', 0.0): 21, ('apart', 0.0): 7, ('live', 0.0): 37, ('soshi', 0.0): 1, ('didnt', 0.0): 24, ('buttt', 0.0): 2, ('congrat', 0.0): 2, ('sunday', 0.0): 8, ('friday', 0.0): 12, ('shoulda', 0.0): 1, ('move', 0.0): 12, ('w', 0.0): 22, ('caus', 0.0): 16, (\"they'r\", 0.0): 14, ('heyyy', 0.0): 1, ('yeol', 0.0): 2, ('solo', 0.0): 6, ('dancee', 0.0): 1, ('inter', 0.0): 1, ('nemanja', 0.0): 1, ('vidic', 0.0): 1, ('roma', 0.0): 1, (\"mom'\", 0.0): 2, ('linguist', 0.0): 1, (\"dad'\", 0.0): 1, ('comput', 0.0): 6, ('scientist', 0.0): 1, ('dumbest', 0.0): 1, ('famili', 0.0): 9, ('broken', 0.0): 11, ('ice', 0.0): 35, ('cream', 0.0): 32, ('pour', 0.0): 1, ('crash', 0.0): 6, ('scienc', 0.0): 1, ('resourc', 0.0): 1, ('vehicl', 0.0): 5, ('ate', 0.0): 10, ('ayex', 0.0): 1, ('eat', 0.0): 27, ('swear', 0.0): 6, ('lamon', 0.0): 1, ('scroll', 0.0): 1, ('curv', 0.0): 2, ('😉', 0.0): 1, ('cement', 0.0): 1, ('cast', 0.0): 5, ('10.3', 0.0): 1, ('k', 0.0): 9, ('sign', 0.0): 9, ('zayn', 0.0): 8, ('bot', 0.0): 1, ('plz', 0.0): 3, ('mention', 0.0): 9, ('jmu', 0.0): 1, ('camp', 0.0): 7, ('teas', 0.0): 3, ('sweetest', 0.0): 1, ('awuna', 0.0): 1, ('mbulelo', 0.0): 1, ('match', 0.0): 7, ('pig', 0.0): 2, ('although', 0.0): 5, ('crackl', 0.0): 1, ('nois', 0.0): 3, ('plug', 0.0): 2, ('fuse', 0.0): 1, ('dammit', 0.0): 3, ('tip', 0.0): 2, ('carlton', 0.0): 2, ('aflblueshawk', 0.0): 2, (\"alex'\", 0.0): 1, ('hous', 0.0): 16, ('motorsport', 0.0): 1, ('seri', 0.0): 3, ('disc', 0.0): 1, ('right', 0.0): 51, ('cheeki', 0.0): 1, ('j', 0.0): 1, ('instead', 0.0): 4, ('seo', 0.0): 1, ('nl', 0.0): 1, ('bud', 0.0): 1, ('christi', 0.0): 1, ('xo', 0.0): 1, ('niec', 0.0): 1, ('summer', 0.0): 19, ('bloodi', 0.0): 2, ('sandwhich', 0.0): 1, ('buset', 0.0): 1, ('discrimin', 0.0): 4, ('five', 0.0): 5, ('learn', 0.0): 5, ('pregnanc', 0.0): 2, ('foot', 0.0): 5, ('f', 0.0): 4, ('matern', 0.0): 1, ('kick', 0.0): 6, ('domesticviol', 0.0): 1, ('law', 0.0): 4, ('domest', 0.0): 1, ('violenc', 0.0): 2, ('victim', 0.0): 4, ('98fm', 0.0): 1, ('exactli', 0.0): 5, ('unfortun', 0.0): 21, ('yesterday', 0.0): 13, ('uk', 0.0): 9, ('govern', 0.0): 1, ('sapiosexu', 0.0): 1, ('damn', 0.0): 29, ('beta', 0.0): 4, ('12', 0.0): 8, ('hour', 0.0): 35, ('world', 0.0): 17, ('hulk', 0.0): 3, ('hogan', 0.0): 3, ('scrub', 0.0): 1, ('wwe', 0.0): 2, ('histori', 0.0): 2, ('iren', 0.0): 4, ('mistak', 0.0): 6, ('naa', 0.0): 1, ('sold', 0.0): 6, ('h_my_k', 0.0): 1, ('lose', 0.0): 7, ('valentin', 0.0): 2, ('et', 0.0): 3, (\"r'ship\", 0.0): 1, ('btwn', 0.0): 1, ('homo', 0.0): 2, ('biphob', 0.0): 2, ('comment', 0.0): 4, ('certain', 0.0): 6, ('disciplin', 0.0): 2, ('incl', 0.0): 2, ('european', 0.0): 3, ('lang', 0.0): 6, ('lit', 0.0): 2, ('educ', 0.0): 2, ('fresherstofin', 0.0): 1, ('💔', 0.0): 3, ('dream', 0.0): 24, ('gettin', 0.0): 2, ('realist', 0.0): 4, ('thx', 0.0): 1, ('real', 0.0): 21, ('isnt', 0.0): 7, ('prefer', 0.0): 4, ('benzema', 0.0): 2, ('hahahahahaah', 0.0): 1, ('donno', 0.0): 1, ('korean', 0.0): 2, ('languag', 0.0): 5, ('russian', 0.0): 2, ('waaa', 0.0): 1, ('eidwithgrof', 0.0): 1, ('boreddd', 0.0): 1, ('mug', 0.0): 3, ('piss', 0.0): 3, ('tiddler', 0.0): 1, ('silli', 0.0): 2, ('least', 0.0): 15, ('card', 0.0): 7, ('chorong', 0.0): 1, ('leader', 0.0): 1, ('에이핑크', 0.0): 3, ('더쇼', 0.0): 4, ('clan', 0.0): 1, ('slot', 0.0): 2, ('open', 0.0): 16, ('pfff', 0.0): 1, ('privat', 0.0): 2, ('bugbounti', 0.0): 1, ('self-xss', 0.0): 1, ('host', 0.0): 2, ('header', 0.0): 3, ('poison', 0.0): 3, ('code', 0.0): 8, ('execut', 0.0): 1, ('ktksbye', 0.0): 1, ('connect', 0.0): 3, ('compani', 0.0): 3, ('alert', 0.0): 2, ('cancel', 0.0): 10, ('uber', 0.0): 3, ('everyon', 0.0): 26, ('els', 0.0): 4, ('offic', 0.0): 7, ('ahahah', 0.0): 1, ('petit', 0.0): 1, ('relationship', 0.0): 4, ('height', 0.0): 2, ('cost', 0.0): 1, ('600', 0.0): 2, ('£', 0.0): 6, ('secur', 0.0): 4, ('odoo', 0.0): 2, ('8', 0.0): 11, ('partner', 0.0): 2, ('commun', 0.0): 2, ('spirit', 0.0): 3, ('jgh', 0.0): 2, ('effin', 0.0): 1, ('facebook', 0.0): 4, ('anyon', 0.0): 17, (\"else'\", 0.0): 1, ('box', 0.0): 8, ('ap', 0.0): 3, ('stori', 0.0): 13, ('london', 0.0): 12, ('imagin', 0.0): 2, ('elsewher', 0.0): 1, ('someday', 0.0): 1, ('ben', 0.0): 3, ('provid', 0.0): 3, ('name', 0.0): 15, ('branch', 0.0): 1, ('visit', 0.0): 12, ('address', 0.0): 3, ('concern', 0.0): 3, ('welsh', 0.0): 1, ('pod', 0.0): 1, ('juli', 0.0): 12, ('laura', 0.0): 4, ('insid', 0.0): 10, ('train', 0.0): 12, ('d;', 0.0): 1, ('talk-kama', 0.0): 1, ('hawako', 0.0): 1, ('waa', 0.0): 1, ('kimaaani', 0.0): 1, ('prisss', 0.0): 1, ('baggag', 0.0): 2, ('claim', 0.0): 3, ('plane', 0.0): 2, ('niamh', 0.0): 1, ('forev', 0.0): 10, ('hmmm', 0.0): 2, ('sugar', 0.0): 3, ('rare', 0.0): 1, ('paper', 0.0): 16, ('town', 0.0): 14, ('score', 0.0): 3, ('stuck', 0.0): 8, ('agh', 0.0): 2, ('middl', 0.0): 7, ('undercoverboss', 0.0): 1, ('تكفى', 0.0): 1, ('10', 0.0): 8, ('job', 0.0): 13, ('cat', 0.0): 17, ('forgotten', 0.0): 3, ('yep', 0.0): 5, ('stop', 0.0): 43, ('ach', 0.0): 2, ('wrist', 0.0): 1, ('nake', 0.0): 3, ('forgot', 0.0): 14, ('bracelet', 0.0): 3, ('ligo', 0.0): 1, ('dozen', 0.0): 1, ('parent', 0.0): 8, ('children', 0.0): 2, ('shark', 0.0): 2, ('selfi', 0.0): 6, ('heartach', 0.0): 1, ('zayniscomingback', 0.0): 3, ('mix', 0.0): 2, ('sweden', 0.0): 1, ('breath', 0.0): 4, ('moment', 0.0): 14, ('word', 0.0): 16, ('elmhurst', 0.0): 1, ('fc', 0.0): 1, ('etid', 0.0): 1, (\"chillin'with\", 0.0): 1, ('father', 0.0): 2, ('istanya', 0.0): 1, ('2suppli', 0.0): 1, ('extra', 0.0): 3, ('infrastructur', 0.0): 2, ('teacher', 0.0): 2, ('doctor', 0.0): 4, ('nurs', 0.0): 2, ('paramed', 0.0): 1, ('countless', 0.0): 1, ('2cope', 0.0): 1, ('bore', 0.0): 23, ('plea', 0.0): 2, ('arian', 0.0): 1, ('hahahaha', 0.0): 6, ('slr', 0.0): 1, ('kendal', 0.0): 1, ('kyli', 0.0): 3, (\"kylie'\", 0.0): 1, ('manila', 0.0): 3, ('jeebu', 0.0): 1, ('reabsorbt', 0.0): 1, ('tooth', 0.0): 2, ('abscess', 0.0): 1, ('threaten', 0.0): 2, ('affect', 0.0): 1, ('front', 0.0): 6, ('crown', 0.0): 1, ('ooouch', 0.0): 1, ('barney', 0.0): 1, (\"be'\", 0.0): 1, ('yo', 0.0): 4, ('later', 0.0): 14, ('realis', 0.0): 6, ('problemat', 0.0): 1, ('expect', 0.0): 5, ('proud', 0.0): 8, ('mess', 0.0): 7, ('maa', 0.0): 2, ('without', 0.0): 25, ('bangalor', 0.0): 1, ('awww', 0.0): 23, ('lui', 0.0): 1, ('manzano', 0.0): 1, ('shaaa', 0.0): 1, ('super', 0.0): 11, ('7th', 0.0): 1, ('conven', 0.0): 1, ('2:30', 0.0): 2, ('pm', 0.0): 8, ('forward', 0.0): 6, ('delet', 0.0): 5, ('turkey', 0.0): 1, ('bomb', 0.0): 3, ('isi', 0.0): 1, ('allow', 0.0): 9, ('usa', 0.0): 2, ('use', 0.0): 43, ('airfield', 0.0): 1, ('jet', 0.0): 1, (\"jack'\", 0.0): 1, ('spam', 0.0): 6, ('sooo', 0.0): 16, ('☺', 0.0): 3, (\"mommy'\", 0.0): 1, ('reason', 0.0): 8, ('overweight', 0.0): 1, ('sigeg', 0.0): 1, ('habhab', 0.0): 1, ('masud', 0.0): 1, ('kaha', 0.0): 1, ('ko', 0.0): 10, ('akong', 0.0): 1, ('un', 0.0): 1, ('hella', 0.0): 4, ('matter', 0.0): 4, ('pala', 0.0): 1, ('hahaha', 0.0): 11, ('lesson', 0.0): 1, ('dolphin', 0.0): 1, ('xxx', 0.0): 12, ('holi', 0.0): 2, ('anythin', 0.0): 1, ('trend', 0.0): 6, ('radio', 0.0): 4, ('sing', 0.0): 5, ('bewar', 0.0): 1, ('agonis', 0.0): 1, ('experi', 0.0): 2, ('ahead', 0.0): 3, ('modimo', 0.0): 1, ('ho', 0.0): 3, ('tseba', 0.0): 1, ('wena', 0.0): 1, ('fela', 0.0): 1, ('emot', 0.0): 8, ('hubbi', 0.0): 1, ('delight', 0.0): 1, ('return', 0.0): 6, ('bill', 0.0): 6, ('nowt', 0.0): 1, ('wors', 0.0): 8, ('willi', 0.0): 1, ('gon', 0.0): 1, ('vomit', 0.0): 1, ('famou', 0.0): 5, ('bowl', 0.0): 1, ('devast', 0.0): 1, ('titan', 0.0): 1, ('ae', 0.0): 1, ('mark', 0.0): 2, ('hair', 0.0): 21, ('shini', 0.0): 1, ('wavi', 0.0): 1, ('emo', 0.0): 2, ('germani', 0.0): 4, ('load', 0.0): 9, ('shed', 0.0): 2, ('ha', 0.0): 7, ('bheyp', 0.0): 1, ('ayemso', 0.0): 1, ('ear', 0.0): 5, ('swell', 0.0): 2, ('sm', 0.0): 7, ('fb', 0.0): 7, ('remind', 0.0): 3, ('abt', 0.0): 3, ('womad', 0.0): 1, ('wut', 0.0): 1, ('hell', 0.0): 11, ('viciou', 0.0): 1, ('circl', 0.0): 1, ('surpris', 0.0): 5, ('ticket', 0.0): 12, ('codi', 0.0): 1, ('simpson', 0.0): 1, ('concert', 0.0): 11, ('singapor', 0.0): 4, ('august', 0.0): 5, ('pooo', 0.0): 2, ('bh3', 0.0): 1, ('enter', 0.0): 1, ('pitchwar', 0.0): 1, ('chap', 0.0): 1, (\"mine'\", 0.0): 1, ('transcript', 0.0): 1, (\"apma'\", 0.0): 1, ('shoulder', 0.0): 2, ('bitch', 0.0): 11, ('competit', 0.0): 1, (\"it'll\", 0.0): 3, ('fine', 0.0): 6, ('timw', 0.0): 1, ('acc', 0.0): 8, ('rude', 0.0): 11, ('vitamin', 0.0): 1, ('e', 0.0): 9, ('oil', 0.0): 1, ('massag', 0.0): 5, ('everyday', 0.0): 7, ('healthier', 0.0): 1, ('easier', 0.0): 3, ('stretch', 0.0): 1, ('choos', 0.0): 7, ('blockjam', 0.0): 1, (\"schedule'\", 0.0): 1, ('whack', 0.0): 1, ('kik', 0.0): 69, ('thelock', 0.0): 1, ('76', 0.0): 1, ('sex', 0.0): 6, ('omegl', 0.0): 4, ('coupl', 0.0): 2, ('travel', 0.0): 11, ('hotgirl', 0.0): 2, ('2009', 0.0): 1, ('3', 0.0): 32, ('ghantay', 0.0): 1, ('light', 0.0): 8, ('nai', 0.0): 1, ('hay', 0.0): 8, ('deni', 0.0): 1, ('ruin', 0.0): 11, ('laguna', 0.0): 1, ('exit', 0.0): 2, ('gomen', 0.0): 1, ('heck', 0.0): 5, ('fair', 0.0): 12, ('grew', 0.0): 2, ('half', 0.0): 10, ('inch', 0.0): 2, ('two', 0.0): 19, ('problem', 0.0): 7, ('suuuper', 0.0): 1, ('65', 0.0): 1, ('sale', 0.0): 8, ('inact', 0.0): 8, ('orphan', 0.0): 1, ('black', 0.0): 12, ('earlier', 0.0): 9, ('whaaat', 0.0): 5, ('kaya', 0.0): 2, ('naaan', 0.0): 1, ('paus', 0.0): 1, ('randomli', 0.0): 1, ('app', 0.0): 13, ('3:30', 0.0): 1, ('walk', 0.0): 7, ('inglewood', 0.0): 1, ('ummm', 0.0): 4, ('anxieti', 0.0): 3, ('readi', 0.0): 12, ('also', 0.0): 19, ('charcoal', 0.0): 1, ('til', 0.0): 5, ('mid-end', 0.0): 1, ('aug', 0.0): 1, ('noooo', 0.0): 1, ('heard', 0.0): 6, ('rip', 0.0): 12, ('rodfanta', 0.0): 1, ('wasp', 0.0): 2, ('sting', 0.0): 1, ('avert', 0.0): 1, ('bug', 0.0): 3, ('(:', 0.0): 7, ('exo', 0.0): 2, ('seekli', 0.0): 1, ('riptito', 0.0): 1, ('manbearpig', 0.0): 1, ('cannot', 0.0): 7, ('grow', 0.0): 3, ('shorter', 0.0): 1, ('academ', 0.0): 1, ('free', 0.0): 19, ('exclus', 0.0): 2, ('unfair', 0.0): 7, ('esp', 0.0): 4, ('regard', 0.0): 1, ('current', 0.0): 7, ('bleak', 0.0): 1, ('german', 0.0): 1, ('chart', 0.0): 2, ('situat', 0.0): 2, ('entri', 0.0): 4, ('even', 0.0): 70, ('top', 0.0): 6, ('100', 0.0): 8, ('pfft', 0.0): 1, ('place', 0.0): 18, ('white', 0.0): 7, ('wash', 0.0): 1, ('polaroid', 0.0): 1, ('newbethvideo', 0.0): 1, ('greec', 0.0): 2, ('xur', 0.0): 2, ('imi', 0.0): 3, ('fill', 0.0): 1, ('♡', 0.0): 11, ('♥', 0.0): 22, ('xoxoxo', 0.0): 1, ('pictur', 0.0): 17, ('stud', 0.0): 1, ('hund', 0.0): 1, ('6', 0.0): 14, ('kikchat', 0.0): 9, ('amazon', 0.0): 5, ('3.4', 0.0): 1, ('yach', 0.0): 1, ('telat', 0.0): 1, ('huvvft', 0.0): 1, ('zoo', 0.0): 2, ('fieldtrip', 0.0): 1, ('touch', 0.0): 5, ('yan', 0.0): 1, ('posit', 0.0): 2, ('king', 0.0): 1, ('futur', 0.0): 4, ('sizw', 0.0): 1, ('write', 0.0): 13, ('20', 0.0): 9, ('result', 0.0): 3, ('km', 0.0): 2, ('four', 0.0): 4, ('shift', 0.0): 5, ('aaahhh', 0.0): 2, ('boredom', 0.0): 1, ('en', 0.0): 1, ('aint', 0.0): 7, ('who', 0.0): 1, ('sins', 0.0): 1, ('that', 0.0): 13, ('somehow', 0.0): 2, ('tini', 0.0): 4, ('ball', 0.0): 2, ('barbel', 0.0): 1, ('owww', 0.0): 2, ('amsterdam', 0.0): 1, ('luv', 0.0): 2, ('💖', 0.0): 4, ('ps', 0.0): 3, ('looong', 0.0): 1, ('especi', 0.0): 4, (':/', 0.0): 11, ('lap', 0.0): 1, ('litro', 0.0): 1, ('shepherd', 0.0): 2, ('lami', 0.0): 1, ('mayb', 0.0): 27, ('relax', 0.0): 3, ('lungomar', 0.0): 1, ('pesaro', 0.0): 1, ('giachietittiwed', 0.0): 1, ('igersoftheday', 0.0): 1, ('summertim', 0.0): 1, ('nose', 0.0): 7, ('bruis', 0.0): 1, ('lil', 0.0): 8, ('snake', 0.0): 3, ('journey', 0.0): 2, ('scarf', 0.0): 1, ('au', 0.0): 3, ('afford', 0.0): 7, ('fridayfeel', 0.0): 1, ('earli', 0.0): 12, ('money', 0.0): 24, ('chicken', 0.0): 5, ('woe', 0.0): 4, ('nigga', 0.0): 3, ('motn', 0.0): 1, ('make-up', 0.0): 1, ('justic', 0.0): 1, ('import', 0.0): 4, ('sit', 0.0): 5, ('mind', 0.0): 7, ('buy', 0.0): 17, ('limit', 0.0): 4, ('ver', 0.0): 1, ('normal', 0.0): 5, ('edit', 0.0): 7, ('huhuhu', 0.0): 3, ('stack', 0.0): 1, (\"m'ladi\", 0.0): 1, ('j8', 0.0): 1, ('j11', 0.0): 1, ('m20', 0.0): 1, ('jk', 0.0): 5, ('acad', 0.0): 1, ('schedul', 0.0): 9, ('nowww', 0.0): 1, ('cop', 0.0): 1, ('jame', 0.0): 4, ('window', 0.0): 6, ('hugh', 0.0): 2, ('paw', 0.0): 1, ('muddi', 0.0): 1, ('distract', 0.0): 1, ('heyi', 0.0): 1, ('otherwis', 0.0): 3, ('picnic', 0.0): 1, ('24', 0.0): 11, ('cupcak', 0.0): 2, ('talaga', 0.0): 1, ('best', 0.0): 22, ('femal', 0.0): 3, ('poppin', 0.0): 1, ('joc', 0.0): 1, ('playin', 0.0): 1, ('saw', 0.0): 19, ('fix', 0.0): 10, ('coldplay', 0.0): 1, ('media', 0.0): 1, ('player', 0.0): 3, ('fail', 0.0): 10, ('subj', 0.0): 1, ('sobrang', 0.0): 1, ('bv', 0.0): 1, ('zamn', 0.0): 1, ('line', 0.0): 8, ('afropunk', 0.0): 1, ('fest', 0.0): 1, ('brooklyn', 0.0): 2, ('id', 0.0): 5, ('put', 0.0): 14, ('50', 0.0): 5, ('madrid', 0.0): 7, ('shithous', 0.0): 1, ('cutest', 0.0): 2, ('danc', 0.0): 6, ('ur', 0.0): 26, ('arm', 0.0): 3, ('rais', 0.0): 1, ('hand', 0.0): 12, ('ladder', 0.0): 2, ('told', 0.0): 11, ('climb', 0.0): 3, ('success', 0.0): 4, ('nerv', 0.0): 1, ('wrack', 0.0): 1, ('test', 0.0): 8, ('booset', 0.0): 1, ('restart', 0.0): 1, ('assassin', 0.0): 1, ('creed', 0.0): 1, ('ii', 0.0): 1, ('heap', 0.0): 1, ('fell', 0.0): 10, ('daughter', 0.0): 1, ('begin', 0.0): 4, ('ps3', 0.0): 1, ('ankl', 0.0): 4, ('step', 0.0): 5, ('puddl', 0.0): 2, ('wear', 0.0): 5, ('slipper', 0.0): 1, ('eve', 0.0): 1, ('bbi', 0.0): 6, ('sararoc', 0.0): 1, ('angri', 0.0): 5, ('pretti', 0.0): 15, ('fnaf', 0.0): 1, ('holiday', 0.0): 20, ('cheer', 0.0): 6, ('😘', 0.0): 11, ('anywayhedidanicejob', 0.0): 1, ('😞', 0.0): 3, ('3am', 0.0): 2, ('other', 0.0): 7, ('local', 0.0): 3, ('cruis', 0.0): 1, ('done', 0.0): 24, ('doubl', 0.0): 4, ('wail', 0.0): 1, ('manual', 0.0): 2, ('wheelchair', 0.0): 1, ('check', 0.0): 19, ('fit', 0.0): 3, ('nh', 0.0): 3, ('26week', 0.0): 1, ('sbenu', 0.0): 1, ('sasin', 0.0): 1, ('team', 0.0): 14, ('anarchi', 0.0): 1, ('af', 0.0): 14, ('candl', 0.0): 1, ('forehead', 0.0): 4, ('medicin', 0.0): 3, ('welcom', 0.0): 5, ('oop', 0.0): 4, ('hoya', 0.0): 3, ('mah', 0.0): 2, ('a', 0.0): 1, ('nobodi', 0.0): 10, ('awhil', 0.0): 2, ('ago', 0.0): 20, ('b', 0.0): 10, ('hush', 0.0): 2, ('gurli', 0.0): 1, ('bring', 0.0): 9, ('purti', 0.0): 1, ('mouth', 0.0): 5, ('closer', 0.0): 2, ('shiver', 0.0): 1, ('solut', 0.0): 1, ('paid', 0.0): 8, ('properli', 0.0): 2, ('gol', 0.0): 1, ('pea', 0.0): 1, ('english', 0.0): 9, ('mental', 0.0): 4, ('tierd', 0.0): 2, ('third', 0.0): 1, (\"eye'\", 0.0): 1, ('thnkyouuu', 0.0): 1, ('carolin', 0.0): 1, ('neither', 0.0): 6, ('figur', 0.0): 6, ('mirror', 0.0): 1, ('highlight', 0.0): 2, ('pure', 0.0): 3, ('courag', 0.0): 1, ('bit', 0.0): 15, ('fishi', 0.0): 1, ('idek', 0.0): 1, ('apink', 0.0): 5, ('perform', 0.0): 8, ('bulet', 0.0): 1, ('gendut', 0.0): 1, ('noo', 0.0): 5, ('race', 0.0): 3, ('hotwheel', 0.0): 1, ('ms', 0.0): 1, ('patch', 0.0): 1, ('typic', 0.0): 2, ('ahaha', 0.0): 1, ('lay', 0.0): 2, ('wine', 0.0): 1, ('glass', 0.0): 3, (\"where'\", 0.0): 4, ('akon', 0.0): 1, ('somewher', 0.0): 5, ('nightmar', 0.0): 7, ('ya', 0.0): 15, ('mino', 0.0): 2, ('crazyyi', 0.0): 1, ('thooo', 0.0): 1, ('zz', 0.0): 1, ('airport', 0.0): 7, ('straight', 0.0): 4, ('soundcheck', 0.0): 1, ('hmm', 0.0): 4, ('antagonist', 0.0): 1, ('ob', 0.0): 1, ('phantasi', 0.0): 1, ('star', 0.0): 4, ('ip', 0.0): 1, ('issu', 0.0): 11, ('bruce', 0.0): 1, ('sleepdepriv', 0.0): 1, ('tiredashel', 0.0): 1, ('4aspot', 0.0): 1, (\"kinara'\", 0.0): 1, ('awami', 0.0): 1, ('question', 0.0): 9, ('niqqa', 0.0): 1, ('answer', 0.0): 14, ('mockingjay', 0.0): 1, ('slow', 0.0): 9, ('pb.contest', 0.0): 1, ('cycl', 0.0): 2, ('aarww', 0.0): 1, ('lmbo', 0.0): 1, ('dangit', 0.0): 1, ('ohmygod', 0.0): 1, ('scenario', 0.0): 1, ('tooo', 0.0): 2, ('duck', 0.0): 1, ('baechyyi', 0.0): 1, ('okayyy', 0.0): 1, ('noon', 0.0): 3, ('drag', 0.0): 5, ('serious', 0.0): 11, ('misundersrand', 0.0): 1, ('chal', 0.0): 1, ('raha', 0.0): 1, ('hai', 0.0): 11, ('yhm', 0.0): 1, ('edsa', 0.0): 2, ('jasmingarrick', 0.0): 2, ('kikmeguy', 0.0): 5, ('webcam', 0.0): 2, ('milf', 0.0): 1, ('nakamaforev', 0.0): 3, ('kiksex', 0.0): 7, (\"unicef'\", 0.0): 1, ('fu', 0.0): 1, ('alon', 0.0): 16, ('manag', 0.0): 13, ('stephen', 0.0): 1, ('street', 0.0): 2, ('35', 0.0): 1, ('min', 0.0): 7, ('appear', 0.0): 2, ('record', 0.0): 6, ('coz', 0.0): 4, ('frustrat', 0.0): 6, ('sent', 0.0): 9, ('interest', 0.0): 9, ('woza', 0.0): 1, ('promis', 0.0): 4, ('senight', 0.0): 1, ('468', 0.0): 1, ('kikmeboy', 0.0): 9, ('gay', 0.0): 6, ('teen', 0.0): 7, ('amateur', 0.0): 5, ('hotscratch', 0.0): 1, ('sell', 0.0): 8, ('sock', 0.0): 6, ('150-160', 0.0): 1, ('peso', 0.0): 1, ('gotta', 0.0): 8, ('pay', 0.0): 8, ('degrassi', 0.0): 1, ('4-6', 0.0): 1, ('bcz', 0.0): 1, ('kat', 0.0): 3, ('chem', 0.0): 2, ('onscreen', 0.0): 1, ('ofscreen', 0.0): 1, ('kinda', 0.0): 10, ('pak', 0.0): 4, ('class', 0.0): 10, ('monthli', 0.0): 1, ('roll', 0.0): 4, ('band', 0.0): 2, ('throw', 0.0): 2, ('ironi', 0.0): 2, ('rhisfor', 0.0): 1, ('500', 0.0): 2, ('bestoftheday', 0.0): 3, ('chat', 0.0): 9, ('camsex', 0.0): 5, ('unfollow', 0.0): 11, ('particular', 0.0): 1, ('support', 0.0): 26, ('bae', 0.0): 11, ('poopi', 0.0): 1, ('pip', 0.0): 1, ('post', 0.0): 12, ('felt', 0.0): 6, ('uff', 0.0): 1, ('1.300', 0.0): 1, ('credit', 0.0): 3, ('glue', 0.0): 1, ('factori', 0.0): 1, ('kuchar', 0.0): 1, ('fast', 0.0): 7, ('graduat', 0.0): 3, ('up', 0.0): 2, ('definit', 0.0): 3, ('uni', 0.0): 2, ('ee', 0.0): 1, ('tommi', 0.0): 1, ('georgia', 0.0): 2, ('bout', 0.0): 2, ('instant', 0.0): 1, ('transmiss', 0.0): 1, ('malik', 0.0): 1, ('orang', 0.0): 2, ('suma', 0.0): 1, ('shouldeeerr', 0.0): 1, ('outfit', 0.0): 5, ('age', 0.0): 8, ('repack', 0.0): 3, ('group', 0.0): 4, ('charl', 0.0): 1, ('grown', 0.0): 2, ('rememb', 0.0): 17, ('dy', 0.0): 1, ('rihanna', 0.0): 1, ('red', 0.0): 4, ('ging', 0.0): 2, ('boot', 0.0): 4, ('closest', 0.0): 3, ('nike', 0.0): 1, ('adida', 0.0): 1, ('inform', 0.0): 4, ('pro@illamasqua.com', 0.0): 1, ('set', 0.0): 13, ('ifeely', 0.0): 1, ('harder', 0.0): 2, ('usual', 0.0): 7, ('ratbaglat', 0.0): 1, ('second', 0.0): 5, ('semest', 0.0): 2, ('gin', 0.0): 1, ('gut', 0.0): 12, ('reynold', 0.0): 1, ('dessert', 0.0): 2, ('season', 0.0): 9, ('villag', 0.0): 1, ('differ', 0.0): 10, ('citi', 0.0): 11, ('unit', 0.0): 3, ('oppress', 0.0): 1, ('mass', 0.0): 2, ('wat', 0.0): 5, ('afghanistn', 0.0): 1, ('war', 0.0): 2, ('tore', 0.0): 1, ('sunggyu', 0.0): 5, ('injur', 0.0): 7, ('plaster', 0.0): 2, ('rtd', 0.0): 1, ('loui', 0.0): 4, ('harri', 0.0): 10, ('5so', 0.0): 7, ('crowd', 0.0): 1, ('stadium', 0.0): 4, ('welder', 0.0): 1, ('ghost', 0.0): 1, ('hogo', 0.0): 1, ('vishaya', 0.0): 1, ('adu', 0.0): 1, ('bjp', 0.0): 1, ('madatt', 0.0): 1, ('anta', 0.0): 1, ('vishwa', 0.0): 1, ('ne', 0.0): 3, ('illa', 0.0): 1, ('wua', 0.0): 1, ('picki', 0.0): 1, ('finger', 0.0): 8, ('favourit', 0.0): 9, ('mutual', 0.0): 2, ('gn', 0.0): 1, ('along', 0.0): 3, ('ass', 0.0): 9, ('thent', 0.0): 1, ('423', 0.0): 1, ('sabadodeganarseguidor', 0.0): 2, ('sexual', 0.0): 4, ('sync', 0.0): 2, ('plug.dj', 0.0): 1, ('peel', 0.0): 1, ('suspems', 0.0): 1, ('cope', 0.0): 3, ('offroad', 0.0): 1, ('adventur', 0.0): 1, ('there', 0.0): 5, ('harvest', 0.0): 1, ('machineri', 0.0): 1, ('inapropri', 0.0): 1, ('weav', 0.0): 2, ('nowher', 0.0): 3, ('decent', 0.0): 2, ('invest', 0.0): 2, ('scottish', 0.0): 1, ('footbal', 0.0): 3, ('dire', 0.0): 2, ('nomoney', 0.0): 1, ('nawf', 0.0): 1, ('sum', 0.0): 2, ('becho', 0.0): 1, ('danni', 0.0): 3, ('eng', 0.0): 2, (\"let'\", 0.0): 5, ('overli', 0.0): 2, ('lab', 0.0): 1, ('ty', 0.0): 3, ('zap', 0.0): 1, ('distress', 0.0): 1, ('shot', 0.0): 6, ('cinema', 0.0): 4, ('louisianashoot', 0.0): 1, ('laugh', 0.0): 7, ('har', 0.0): 3, (\"how'\", 0.0): 5, ('chum', 0.0): 1, ('ncc', 0.0): 1, ('ph', 0.0): 2, ('balik', 0.0): 1, ('naman', 0.0): 1, ('kayo', 0.0): 1, ('itong', 0.0): 1, ('shirt', 0.0): 3, ('thaaat', 0.0): 1, ('ctto', 0.0): 1, ('expir', 0.0): 3, ('bi', 0.0): 2, ('tough', 0.0): 2, ('11', 0.0): 4, ('3:33', 0.0): 2, ('jfc', 0.0): 1, ('bio', 0.0): 3, ('bodo', 0.0): 1, ('amat', 0.0): 1, ('quick', 0.0): 5, ('yelaaa', 0.0): 1, ('dublin', 0.0): 2, ('potter', 0.0): 1, ('marathon', 0.0): 3, ('balanc', 0.0): 2, ('warm', 0.0): 5, ('comic', 0.0): 5, ('pine', 0.0): 1, ('keybind', 0.0): 1, ('featur', 0.0): 4, ('wild', 0.0): 2, ('warfar', 0.0): 1, ('control', 0.0): 2, ('diagnos', 0.0): 1, ('wiv', 0.0): 1, (\"scheuermann'\", 0.0): 1, ('diseas', 0.0): 3, ('bone', 0.0): 1, ('rlyhurt', 0.0): 1, ('howdo', 0.0): 1, ('georgesampson', 0.0): 1, ('stand', 0.0): 6, ('signal', 0.0): 3, ('reckon', 0.0): 1, ('t20', 0.0): 1, ('action', 0.0): 2, ('taunton', 0.0): 1, ('vacat', 0.0): 3, ('excit', 0.0): 6, ('justiceforsandrabland', 0.0): 2, ('sandrabland', 0.0): 6, ('disturb', 0.0): 1, ('women', 0.0): 5, ('happpi', 0.0): 1, ('justinbieb', 0.0): 4, ('daianerufato', 0.0): 3, ('ilysm', 0.0): 3, ('2015', 0.0): 12, ('07:34', 0.0): 1, ('delphi', 0.0): 2, ('weak', 0.0): 2, ('dom', 0.0): 2, ('techniqu', 0.0): 1, ('minc', 0.0): 2, ('complet', 0.0): 9, ('symphoni', 0.0): 1, ('joe', 0.0): 3, ('co', 0.0): 6, ('wth', 0.0): 2, ('aisyhhh', 0.0): 1, ('bald', 0.0): 1, ('14', 0.0): 3, ('seungchan', 0.0): 1, ('aigooo', 0.0): 1, ('riri', 0.0): 1, ('origin', 0.0): 6, ('depend', 0.0): 2, ('vet', 0.0): 1, ('major', 0.0): 2, ('va', 0.0): 1, ('kept', 0.0): 2, ('lumin', 0.0): 1, ('follback', 0.0): 2, ('treat', 0.0): 5, ('v', 0.0): 6, ('product', 0.0): 4, ('letter', 0.0): 1, ('z', 0.0): 5, ('uniqu', 0.0): 2, ('refresh', 0.0): 1, ('popular', 0.0): 1, ('bebee', 0.0): 2, ('lt', 0.0): 1, ('inaccuraci', 0.0): 1, ('inaccur', 0.0): 1, ('worri', 0.0): 8, ('burn', 0.0): 4, ('rn', 0.0): 17, ('tragic', 0.0): 1, ('joy', 0.0): 2, ('sam', 0.0): 4, ('rush', 0.0): 2, ('toronto', 0.0): 1, ('stuart', 0.0): 1, (\"party'\", 0.0): 2, ('iyalaya', 0.0): 1, ('shade', 0.0): 3, ('round', 0.0): 3, ('clock', 0.0): 2, (';(', 0.0): 6, ('happier', 0.0): 1, ('h', 0.0): 8, ('ubusi', 0.0): 1, ('le', 0.0): 3, ('fifa', 0.0): 1, ('gymnast', 0.0): 1, ('aahhh', 0.0): 1, ('noggin', 0.0): 1, ('bump', 0.0): 1, ('feelslikeanidiot', 0.0): 1, ('pregnant', 0.0): 2, ('woman', 0.0): 5, ('dearli', 0.0): 1, ('sunshin', 0.0): 4, ('suk', 0.0): 2, ('pumpkin', 0.0): 1, ('scone', 0.0): 1, ('outnumb', 0.0): 1, ('vidcon', 0.0): 10, ('eri', 0.0): 1, ('geez', 0.0): 1, ('preciou', 0.0): 4, ('hive', 0.0): 1, ('vote', 0.0): 7, ('vietnam', 0.0): 1, ('decemb', 0.0): 2, ('dunt', 0.0): 1, ('ikr', 0.0): 3, ('sob', 0.0): 3, ('buff', 0.0): 1, ('leg', 0.0): 4, ('toni', 0.0): 1, ('deactiv', 0.0): 6, ('bra', 0.0): 2, (\"shady'\", 0.0): 1, ('isibaya', 0.0): 1, ('special', 0.0): 3, ('❤', 0.0): 21, ('️', 0.0): 19, ('😓', 0.0): 2, ('slept', 0.0): 5, ('colder', 0.0): 1, ('took', 0.0): 9, ('med', 0.0): 1, ('sausag', 0.0): 1, ('adio', 0.0): 1, ('cold', 0.0): 15, ('sore', 0.0): 9, ('ew', 0.0): 3, ('h8', 0.0): 1, ('messeng', 0.0): 2, ('shittier', 0.0): 1, ('leno', 0.0): 1, ('ident', 0.0): 1, ('crisi', 0.0): 2, ('roommat', 0.0): 1, ('knock', 0.0): 3, ('nighter', 0.0): 3, ('bird', 0.0): 2, ('flew', 0.0): 2, ('thru', 0.0): 2, ('derek', 0.0): 3, ('tour', 0.0): 7, ('wetherspoon', 0.0): 1, ('pub', 0.0): 1, ('polic', 0.0): 4, ('frank', 0.0): 2, ('ocean', 0.0): 4, ('releas', 0.0): 8, ('ff', 0.0): 4, ('lisah', 0.0): 2, ('kikm', 0.0): 8, ('eboni', 0.0): 2, ('weloveyounamjoon', 0.0): 1, ('gave', 0.0): 8, ('dress', 0.0): 6, ('polka', 0.0): 1, ('dot', 0.0): 2, ('ndi', 0.0): 1, ('yum', 0.0): 1, ('feed', 0.0): 3, ('leftov', 0.0): 2, ('side', 0.0): 6, ('cs', 0.0): 2, ('own', 0.0): 1, ('walnut', 0.0): 1, ('whip', 0.0): 1, ('wife', 0.0): 6, ('boah', 0.0): 1, ('madi', 0.0): 2, ('def', 0.0): 3, ('manga', 0.0): 1, ('giant', 0.0): 3, ('aminormalyet', 0.0): 1, ('cooki', 0.0): 2, ('breakfast', 0.0): 5, ('clutch', 0.0): 1, ('poorli', 0.0): 6, ('tummi', 0.0): 6, ('pj', 0.0): 1, ('groan', 0.0): 1, ('nou', 0.0): 1, ('adam', 0.0): 2, ('ken', 0.0): 1, ('sara', 0.0): 2, ('sister', 0.0): 4, ('accid', 0.0): 2, ('sort', 0.0): 7, ('mate', 0.0): 2, ('pick', 0.0): 12, ('rang', 0.0): 4, ('fk', 0.0): 2, ('freak', 0.0): 5, ('describ', 0.0): 1, ('eric', 0.0): 2, ('prydz', 0.0): 1, ('sister-in-law', 0.0): 1, ('instal', 0.0): 2, ('seat', 0.0): 4, ('bought', 0.0): 6, ('rear-end', 0.0): 1, (\"everyone'\", 0.0): 4, ('trash', 0.0): 2, ('boob', 0.0): 3, ('whilst', 0.0): 3, ('stair', 0.0): 1, ('childhood', 0.0): 1, ('toothsensit', 0.0): 4, ('size', 0.0): 9, ('ke', 0.0): 3, ('shem', 0.0): 2, ('trust', 0.0): 2, ('awel', 0.0): 1, ('drunk', 0.0): 2, ('weekendofmad', 0.0): 1, ('🍹', 0.0): 3, ('🍸', 0.0): 1, ('cb', 0.0): 1, ('dancer', 0.0): 1, ('choregraph', 0.0): 1, ('626-430-8715', 0.0): 1, ('messag', 0.0): 8, ('repli', 0.0): 14, ('hoe', 0.0): 1, ('xd', 0.0): 7, ('xiu', 0.0): 1, ('nk', 0.0): 1, ('gi', 0.0): 2, ('uss', 0.0): 1, ('eliss', 0.0): 1, ('ksoo', 0.0): 2, ('session', 0.0): 5, ('tat', 0.0): 1, ('bcoz', 0.0): 1, ('bet', 0.0): 10, ('rancho', 0.0): 1, ('imperi', 0.0): 1, ('de', 0.0): 1, ('silang', 0.0): 1, ('subdivis', 0.0): 1, ('center', 0.0): 1, ('39', 0.0): 1, ('cornwal', 0.0): 1, ('verit', 0.0): 1, ('prize', 0.0): 2, ('regular', 0.0): 3, ('workout', 0.0): 1, ('spin', 0.0): 1, ('base', 0.0): 1, ('upon', 0.0): 1, ('penni', 0.0): 1, ('ebook', 0.0): 1, ('фотосет', 0.0): 1, ('addicted-to-analsex', 0.0): 1, ('sweetbj', 0.0): 2, ('blowjob', 0.0): 1, ('mhhh', 0.0): 1, ('sed', 0.0): 1, ('sg', 0.0): 1, ('dinner', 0.0): 4, ('bless', 0.0): 2, ('mee', 0.0): 2, ('enviou', 0.0): 1, ('eonni', 0.0): 1, ('lovey', 0.0): 1, ('dovey', 0.0): 1, ('dongsaeng', 0.0): 1, ('workin', 0.0): 1, ('tuesday', 0.0): 4, ('schade', 0.0): 3, ('belfast', 0.0): 1, ('jealou', 0.0): 9, ('jacob', 0.0): 5, ('isco', 0.0): 4, ('peni', 0.0): 1, ('everi', 0.0): 16, ('convers', 0.0): 6, ('wonder', 0.0): 11, ('soul', 0.0): 5, ('nation', 0.0): 2, ('louisiana', 0.0): 4, ('lafayett', 0.0): 2, ('matteroftheheart', 0.0): 1, ('waduh', 0.0): 1, ('pant', 0.0): 3, ('suspend', 0.0): 2, ('believ', 0.0): 14, ('teenag', 0.0): 2, ('clich', 0.0): 1, ('youuu', 0.0): 5, ('rma', 0.0): 1, ('jersey', 0.0): 2, ('fake', 0.0): 4, ('jaclintil', 0.0): 1, ('model', 0.0): 9, ('likeforlik', 0.0): 7, ('mpoint', 0.0): 4, ('hotfmnoaidilforariana', 0.0): 2, ('ran', 0.0): 5, ('fuckkk', 0.0): 1, ('jump', 0.0): 3, ('justin', 0.0): 3, ('finish', 0.0): 14, ('sanum', 0.0): 1, ('llaollao', 0.0): 1, ('foood', 0.0): 1, ('ubericecream', 0.0): 14, ('glare', 0.0): 1, ('vine', 0.0): 3, ('tweetin', 0.0): 1, ('mood', 0.0): 3, ('elbow', 0.0): 1, ('choreo', 0.0): 1, ('offens', 0.0): 2, ('yeyi', 0.0): 1, ('hd', 0.0): 2, ('brow', 0.0): 1, ('kit', 0.0): 6, ('slightli', 0.0): 2, ('monday', 0.0): 10, ('sux', 0.0): 1, ('enjoy', 0.0): 9, ('nothaveld', 0.0): 1, ('765', 0.0): 1, ('edm', 0.0): 1, ('likeforfollow', 0.0): 3, ('hannib', 0.0): 3, ('mosquito', 0.0): 2, ('bite', 0.0): 5, ('kinki', 0.0): 1, ('hsould', 0.0): 1, ('justget', 0.0): 1, ('marri', 0.0): 2, ('la', 0.0): 11, ('shuffl', 0.0): 4, ('int', 0.0): 1, ('buckl', 0.0): 1, ('spring', 0.0): 1, ('millz', 0.0): 1, ('aski', 0.0): 2, ('awusasho', 0.0): 1, ('unlucki', 0.0): 2, ('driver', 0.0): 7, ('briefli', 0.0): 1, ('spot', 0.0): 4, ('144p', 0.0): 1, ('brook', 0.0): 1, ('crack', 0.0): 2, ('＠', 0.0): 5, ('maverickgam', 0.0): 4, ('07:32', 0.0): 1, ('07:25', 0.0): 1, ('max', 0.0): 3, ('file', 0.0): 2, ('extern', 0.0): 2, ('sd', 0.0): 1, ('via', 0.0): 1, ('airdroid', 0.0): 1, ('android', 0.0): 2, ('4.4+', 0.0): 1, ('googl', 0.0): 5, ('alright', 0.0): 3, ('cramp', 0.0): 2, ('&lt;/3', 0.0): 6, ('unstan', 0.0): 1, ('tay', 0.0): 2, ('ngeze', 0.0): 1, ('cocktaili', 0.0): 1, ('classi', 0.0): 1, ('07:24', 0.0): 1, ('✈', 0.0): 2, ('️2', 0.0): 1, ('raini', 0.0): 2, ('☔', 0.0): 2, ('peter', 0.0): 1, ('pen', 0.0): 1, ('spare', 0.0): 1, ('guest', 0.0): 2, ('barcelona', 0.0): 2, ('bilbao', 0.0): 1, ('booti', 0.0): 2, ('sharyl', 0.0): 1, ('shane', 0.0): 2, ('ta', 0.0): 1, ('giddi', 0.0): 1, ('d1', 0.0): 1, ('zipper', 0.0): 1, ('beyond', 0.0): 1, ('repair', 0.0): 4, ('iphon', 0.0): 5, ('upgrad', 0.0): 1, ('april', 0.0): 1, ('2016', 0.0): 1, ('cont', 0.0): 2, ('england', 0.0): 4, ('wore', 0.0): 2, ('greet', 0.0): 5, ('tempt', 0.0): 2, ('whole', 0.0): 16, ('pack', 0.0): 6, ('oreo', 0.0): 2, ('strength', 0.0): 1, ('wifi', 0.0): 5, ('network', 0.0): 4, ('within', 0.0): 3, ('lolipop', 0.0): 1, ('kebab', 0.0): 1, ('klappertart', 0.0): 1, ('cake', 0.0): 10, ('moodbost', 0.0): 2, ('shoot', 0.0): 6, ('unprepar', 0.0): 1, ('sri', 0.0): 1, ('dresscod', 0.0): 1, ('door', 0.0): 6, ('iam', 0.0): 2, ('dnt', 0.0): 1, ('stab', 0.0): 3, ('meh', 0.0): 3, ('wrocilam', 0.0): 1, ('otp', 0.0): 3, ('5', 0.0): 14, ('looww', 0.0): 1, ('recov', 0.0): 2, ('wayn', 0.0): 2, ('insur', 0.0): 3, ('loss', 0.0): 3, ('stolen', 0.0): 2, ('accident', 0.0): 1, ('damag', 0.0): 5, ('devic', 0.0): 3, ('warranti', 0.0): 1, ('centr', 0.0): 2, ('👌', 0.0): 1, ('lmfaoo', 0.0): 1, ('accur', 0.0): 2, ('fra', 0.0): 4, ('aliv', 0.0): 2, ('steel', 0.0): 2, ('otamendi', 0.0): 1, ('ny', 0.0): 2, ('🚖', 0.0): 1, ('🗽', 0.0): 1, ('🌃', 0.0): 1, ('stealth', 0.0): 2, ('bastard', 0.0): 2, ('inc', 0.0): 3, ('steam', 0.0): 2, ('therapi', 0.0): 1, ('exhaust', 0.0): 3, ('lie', 0.0): 7, ('total', 0.0): 11, ('block', 0.0): 11, ('choic', 0.0): 5, ('switzerland', 0.0): 1, ('kfc', 0.0): 1, ('common', 0.0): 4, ('th', 0.0): 5, ('wolrd', 0.0): 1, ('fyn', 0.0): 1, ('drop', 0.0): 10, ('state', 0.0): 4, ('3g', 0.0): 2, ('christ', 0.0): 1, ('scale', 0.0): 1, ('deck', 0.0): 1, ('chair', 0.0): 4, ('yk', 0.0): 1, ('resi', 0.0): 1, ('memori', 0.0): 5, ('nude', 0.0): 4, ('bruh', 0.0): 3, ('prepar', 0.0): 3, ('lock', 0.0): 2, ('view', 0.0): 7, ('fbc', 0.0): 3, ('mork', 0.0): 1, ('873', 0.0): 1, ('kikgirl', 0.0): 13, ('premiostumundo', 0.0): 2, ('hotspotwithdanri', 0.0): 1, ('hospit', 0.0): 3, ('food', 0.0): 18, ('sone', 0.0): 1, ('produc', 0.0): 1, ('potag', 0.0): 1, ('tomato', 0.0): 1, ('blight', 0.0): 1, ('sheffield', 0.0): 1, ('mych', 0.0): 1, ('shiiit', 0.0): 2, ('screenshot', 0.0): 4, ('prompt', 0.0): 1, ('areadi', 0.0): 1, ('similar', 0.0): 4, ('soulmat', 0.0): 1, ('canon', 0.0): 1, ('zzz', 0.0): 2, ('britain', 0.0): 1, ('😁', 0.0): 3, ('mana', 0.0): 2, ('hw', 0.0): 1, ('jouch', 0.0): 1, ('por', 0.0): 1, ('que', 0.0): 1, ('liceooo', 0.0): 1, ('30', 0.0): 3, ('minut', 0.0): 6, ('pass', 0.0): 13, ('ayala', 0.0): 1, ('tunnel', 0.0): 2, ('thatscold', 0.0): 1, ('80', 0.0): 1, ('snap', 0.0): 3, ('lourd', 0.0): 1, ('bang', 0.0): 3, ('anywher', 0.0): 4, ('water', 0.0): 8, ('road', 0.0): 1, ('showbox', 0.0): 1, ('naruto', 0.0): 1, ('cartoon', 0.0): 1, ('companion', 0.0): 2, ('skinni', 0.0): 3, ('fat', 0.0): 4, ('bare', 0.0): 6, ('dubai', 0.0): 3, ('calum', 0.0): 1, ('ashton', 0.0): 1, ('✧', 0.0): 8, ('｡', 0.0): 8, ('chelni', 0.0): 4, ('disappoint', 0.0): 13, ('everybodi', 0.0): 5, ('due', 0.0): 14, ('laribuggi', 0.0): 1, ('medic', 0.0): 1, ('nutella', 0.0): 1, (\"could'v\", 0.0): 3, ('siriu', 0.0): 1, ('goat', 0.0): 4, ('frudg', 0.0): 1, ('mike', 0.0): 1, ('cloth', 0.0): 6, ('stuff', 0.0): 11, ('sat', 0.0): 3, ('number', 0.0): 6, ('ring', 0.0): 1, ('bbz', 0.0): 1, ('angek', 0.0): 1, ('sbali', 0.0): 1, ('euuuwww', 0.0): 2, ('lunch', 0.0): 10, ('construct', 0.0): 3, ('worker', 0.0): 3, ('1k', 0.0): 3, ('style', 0.0): 4, ('nell', 0.0): 1, ('ik', 0.0): 2, ('death', 0.0): 3, ('jaysu', 0.0): 1, ('toast', 0.0): 1, ('insecur', 0.0): 2, ('buti', 0.0): 1, ('ure', 0.0): 2, ('poop', 0.0): 1, ('gorgeou', 0.0): 2, ('angel', 0.0): 2, ('rome', 0.0): 1, ('throat', 0.0): 10, ('llama', 0.0): 1, ('urself', 0.0): 2, ('getwellsoonamb', 0.0): 1, ('heath', 0.0): 2, ('ledger', 0.0): 1, ('appl', 0.0): 3, ('permiss', 0.0): 2, ('2-0', 0.0): 1, ('lead', 0.0): 3, ('supersport', 0.0): 1, ('milkshak', 0.0): 1, ('witcher', 0.0): 1, ('papertown', 0.0): 1, ('bale', 0.0): 1, ('9', 0.0): 5, ('méxico', 0.0): 1, ('bahay', 0.0): 1, ('bahayan', 0.0): 1, ('magisa', 0.0): 1, ('sadlyf', 0.0): 1, ('bunso', 0.0): 1, ('sleeep', 0.0): 4, ('astonvilla', 0.0): 1, ('berigaud', 0.0): 1, ('bakar', 0.0): 1, ('club', 0.0): 4, ('dear', 0.0): 11, ('allerg', 0.0): 4, ('depress', 0.0): 5, (\"blaine'\", 0.0): 1, ('acoust', 0.0): 2, ('version', 0.0): 5, ('excus', 0.0): 3, ('hernia', 0.0): 3, ('toxin', 0.0): 1, ('freedom', 0.0): 1, ('organ', 0.0): 2, ('ariel', 0.0): 1, ('slap', 0.0): 1, ('slam', 0.0): 1, ('bee', 0.0): 1, ('unknown', 0.0): 2, ('finddjderek', 0.0): 1, ('smell', 0.0): 3, ('uuughhh', 0.0): 1, ('grabe', 0.0): 5, ('ka', 0.0): 5, ('where', 0.0): 1, ('gf', 0.0): 3, ('james_yammouni', 0.0): 1, ('smi', 0.0): 1, ('nemesi', 0.0): 1, ('rule', 0.0): 1, ('doesnt', 0.0): 2, ('appeal', 0.0): 1, ('neeein', 0.0): 1, ('saaad', 0.0): 3, ('less', 0.0): 3, ('hang', 0.0): 7, ('creas', 0.0): 1, ('tan', 0.0): 3, ('dalla', 0.0): 4, ('suppos', 0.0): 7, ('infront', 0.0): 2, ('beato', 0.0): 1, ('tim', 0.0): 2, ('prob', 0.0): 5, ('minha', 0.0): 1, ('deleici', 0.0): 1, ('hr', 0.0): 2, ('pcb', 0.0): 1, ('ep', 0.0): 5, ('peregrin', 0.0): 1, ('8.40', 0.0): 1, ('pigeon', 0.0): 1, ('feet', 0.0): 3, ('tram', 0.0): 1, ('hav', 0.0): 2, ('spent', 0.0): 5, ('outsid', 0.0): 9, ('apt', 0.0): 1, ('build', 0.0): 3, ('key', 0.0): 3, ('bldg', 0.0): 1, ('wrote', 0.0): 3, ('dark', 0.0): 5, ('swan', 0.0): 1, ('fifth', 0.0): 2, ('mmmm', 0.0): 1, ('avi', 0.0): 4, ('nicki', 0.0): 1, ('fucjikg', 0.0): 1, ('disgust', 0.0): 6, ('buynotanapologyonitun', 0.0): 1, ('aval', 0.0): 1, ('denmark', 0.0): 1, ('nw', 0.0): 2, ('sch', 0.0): 2, ('share', 0.0): 11, ('jeslyn', 0.0): 1, ('72', 0.0): 4, ('root', 0.0): 2, ('kuch', 0.0): 1, ('nahi', 0.0): 1, ('hua', 0.0): 2, ('newbi', 0.0): 1, ('crap', 0.0): 3, ('miracl', 0.0): 1, ('4th', 0.0): 1, ('linda', 0.0): 1, ('click', 0.0): 1, ('pin', 0.0): 2, ('wing', 0.0): 3, ('epic', 0.0): 2, ('page', 0.0): 6, ('ang', 0.0): 8, ('ganda', 0.0): 1, ('💗', 0.0): 4, ('nux', 0.0): 1, ('hinanap', 0.0): 1, ('ako', 0.0): 1, ('uy', 0.0): 1, ('sched', 0.0): 1, ('anyar', 0.0): 1, ('entertain', 0.0): 2, ('typa', 0.0): 3, ('buddi', 0.0): 2, ('transpar', 0.0): 1, ('photoshop', 0.0): 2, ('planner', 0.0): 1, ('helppp', 0.0): 2, ('wearig', 0.0): 1, ('dri', 0.0): 2, ('alot', 0.0): 3, ('bu', 0.0): 5, ('prey', 0.0): 1, ('gross', 0.0): 5, ('drain', 0.0): 3, ('ausfailia', 0.0): 1, ('snow', 0.0): 3, ('footi', 0.0): 3, ('2nd', 0.0): 5, ('row', 0.0): 3, (\"m'\", 0.0): 2, ('kitkat', 0.0): 2, ('bday', 0.0): 7, ('😢', 0.0): 8, ('suger', 0.0): 1, ('olivia', 0.0): 2, ('audit', 0.0): 1, ('american', 0.0): 1, ('idol', 0.0): 2, ('injuri', 0.0): 2, ('appendix', 0.0): 1, ('burst', 0.0): 2, ('append', 0.0): 1, ('yeahh', 0.0): 2, ('fack', 0.0): 2, ('nhl', 0.0): 1, ('khami', 0.0): 2, ('favorit', 0.0): 4, ('rise', 0.0): 3, ('reaali', 0.0): 1, ('ja', 0.0): 2, ('naomi', 0.0): 1, ('modern', 0.0): 1, ('contemporari', 0.0): 1, ('slack', 0.0): 1, ('565', 0.0): 1, ('blond', 0.0): 2, ('jahat', 0.0): 3, ('discount', 0.0): 1, ('thorp', 0.0): 2, ('park', 0.0): 7, ('esnho', 0.0): 1, ('node', 0.0): 1, ('advanc', 0.0): 4, ('directx', 0.0): 1, ('workshop', 0.0): 1, ('p2', 0.0): 1, ('upload', 0.0): 2, ('remov', 0.0): 5, ('blackberri', 0.0): 1, ('shitti', 0.0): 1, ('mobil', 0.0): 2, ('povertyyouareevil', 0.0): 1, ('struggl', 0.0): 4, ('math', 0.0): 1, ('emm', 0.0): 1, ('data', 0.0): 6, ('elgin', 0.0): 1, ('vava', 0.0): 1, ('makati', 0.0): 1, ('💛', 0.0): 4, ('baon', 0.0): 1, ('soup', 0.0): 3, ('soak', 0.0): 1, ('bread', 0.0): 2, ('mush', 0.0): 1, (\"they'd\", 0.0): 2, ('matt', 0.0): 2, ('ouat', 0.0): 1, ('beach', 0.0): 5, ('blinkin', 0.0): 1, ('unblock', 0.0): 1, ('headack', 0.0): 1, ('tension', 0.0): 1, ('erit', 0.0): 1, ('perspect', 0.0): 1, ('wed', 0.0): 4, ('playlist', 0.0): 2, ('endlessli', 0.0): 1, ('blush', 0.0): 1, ('bat', 0.0): 1, ('kiddo', 0.0): 1, ('rumbel', 0.0): 1, ('overwhelm', 0.0): 1, ('thrown', 0.0): 2, ('irrespons', 0.0): 1, ('pakighinabi', 0.0): 1, ('pinkfinit', 0.0): 1, ('beb', 0.0): 2, ('migrain', 0.0): 2, ('almost', 0.0): 11, ('coyot', 0.0): 1, ('outta', 0.0): 1, ('mad', 0.0): 11, ('😒', 0.0): 3, ('headach', 0.0): 9, ('인피니트', 0.0): 2, ('save', 0.0): 6, ('baechu', 0.0): 1, ('calibraskaep', 0.0): 3, ('r', 0.0): 19, ('fanci', 0.0): 2, ('yt', 0.0): 3, ('purchas', 0.0): 2, ('elgato', 0.0): 1, ('ant', 0.0): 2, ('unexpect', 0.0): 2, ('bestfriend', 0.0): 9, ('faint', 0.0): 1, ('bp', 0.0): 1, ('appar', 0.0): 5, ('shower', 0.0): 3, ('subway', 0.0): 1, ('cool', 0.0): 5, ('prayer', 0.0): 2, ('fragil', 0.0): 1, ('huge', 0.0): 3, ('gap', 0.0): 1, ('plot', 0.0): 2, ('bungi', 0.0): 1, ('folk', 0.0): 1, ('raspberri', 0.0): 1, ('pi', 0.0): 1, ('shoe', 0.0): 2, ('woohyun', 0.0): 2, ('guilti', 0.0): 1, ('monica', 0.0): 2, ('davao', 0.0): 1, ('luckyyi', 0.0): 1, ('confid', 0.0): 1, ('eunha', 0.0): 1, ('misplac', 0.0): 1, ('den', 0.0): 1, ('dae', 0.0): 1, ('bap', 0.0): 1, ('likewis', 0.0): 1, ('liam', 0.0): 1, ('dylan', 0.0): 3, ('huehu', 0.0): 1, ('rice', 0.0): 1, ('krispi', 0.0): 1, ('marshmallow', 0.0): 2, ('srsli', 0.0): 7, ('birmingham', 0.0): 1, ('m5m6junction', 0.0): 1, ('soulsurvivor', 0.0): 1, ('stafford', 0.0): 1, ('progress', 0.0): 1, ('mixtur', 0.0): 1, (\"they'v\", 0.0): 4, ('practic', 0.0): 1, ('lage', 0.0): 1, ('ramd', 0.0): 1, ('lesbian', 0.0): 3, ('oralsex', 0.0): 4, ('munchkin', 0.0): 1, ('juja', 0.0): 1, ('murugan', 0.0): 1, ('handl', 0.0): 3, ('dia', 0.0): 2, ('bgtau', 0.0): 1, ('harap', 0.0): 1, ('bagi', 0.0): 1, ('aminn', 0.0): 1, ('fraand', 0.0): 1, ('😬', 0.0): 2, ('bigbang', 0.0): 2, ('steak', 0.0): 1, ('younger', 0.0): 2, ('sian', 0.0): 2, ('pizza', 0.0): 7, ('5am', 0.0): 5, ('nicoleapag', 0.0): 1, ('makeup', 0.0): 4, ('hellish', 0.0): 1, ('thirstyyi', 0.0): 1, ('chesti', 0.0): 1, ('dad', 0.0): 9, (\"nando'\", 0.0): 1, ('22', 0.0): 3, ('bow', 0.0): 2, ('queen', 0.0): 3, ('brave', 0.0): 1, ('hen', 0.0): 1, ('leed', 0.0): 9, ('rdd', 0.0): 1, ('dissip', 0.0): 1, ('. .', 0.0): 1, ('pump', 0.0): 2, ('capee', 0.0): 1, ('japan', 0.0): 2, ('random', 0.0): 1, ('young', 0.0): 5, ('outliv', 0.0): 1, ('x-ray', 0.0): 1, ('dental', 0.0): 1, ('spine', 0.0): 1, ('relief', 0.0): 1, ('popol', 0.0): 1, ('stomach', 0.0): 8, ('frog', 0.0): 2, ('brad', 0.0): 1, ('gen.ad', 0.0): 1, ('price', 0.0): 5, ('negoti', 0.0): 3, ('huhuhuhuhu', 0.0): 1, ('bbmadeinmanila', 0.0): 1, ('findavip', 0.0): 1, ('boyirl', 0.0): 1, ('yasss', 0.0): 1, ('6th', 0.0): 1, ('june', 0.0): 3, ('lain', 0.0): 1, ('diffici', 0.0): 1, ('custom', 0.0): 1, ('internet', 0.0): 9, ('near', 0.0): 9, ('speed', 0.0): 2, ('escap', 0.0): 1, ('rapist', 0.0): 1, ('commit', 0.0): 2, ('crime', 0.0): 1, ('bachpan', 0.0): 1, ('ki', 0.0): 2, ('yaadein', 0.0): 1, ('finnair', 0.0): 1, ('heathrow', 0.0): 1, ('norwegian', 0.0): 1, (':\\\\', 0.0): 1, ('batteri', 0.0): 3, ('upvot', 0.0): 4, ('keeno', 0.0): 1, ('whatthefuck', 0.0): 1, ('grotti', 0.0): 1, ('attent', 0.0): 1, ('seeker', 0.0): 1, ('moral', 0.0): 1, ('fern', 0.0): 1, ('mimi', 0.0): 1, ('bali', 0.0): 1, ('she', 0.0): 4, ('pleasee', 0.0): 3, ('brb', 0.0): 1, ('lowbat', 0.0): 1, ('otwolgrandtrail', 0.0): 4, ('funk', 0.0): 1, ('wewanticecream', 0.0): 1, ('sweat', 0.0): 2, ('eugh', 0.0): 1, ('speak', 0.0): 4, ('occasion', 0.0): 1, (\"izzy'\", 0.0): 1, ('dorm', 0.0): 1, ('choppi', 0.0): 1, ('paul', 0.0): 1, ('switch', 0.0): 4, (\"infinite'\", 0.0): 2, ('5:30', 0.0): 2, ('cayton', 0.0): 1, ('bay', 0.0): 2, ('emma', 0.0): 2, ('jen', 0.0): 1, ('darcey', 0.0): 1, ('connor', 0.0): 1, ('spoke', 0.0): 1, ('nail', 0.0): 2, ('biggest', 0.0): 3, ('blue', 0.0): 5, ('bottl', 0.0): 3, ('roommateexperi', 0.0): 1, ('yup', 0.0): 4, ('avoid', 0.0): 2, ('ic', 0.0): 1, ('te', 0.0): 1, ('auto-followback', 0.0): 1, ('asian', 0.0): 2, ('puppi', 0.0): 3, ('ljp', 0.0): 1, ('1/5', 0.0): 1, ('nowday', 0.0): 1, ('attach', 0.0): 2, ('beat', 0.0): 2, ('numb', 0.0): 1, ('dentist', 0.0): 3, ('misss', 0.0): 2, ('muchhh', 0.0): 1, ('youtub', 0.0): 5, ('rid', 0.0): 3, ('tab', 0.0): 2, ('uca', 0.0): 1, ('onto', 0.0): 2, ('track', 0.0): 3, ('bigtim', 0.0): 1, ('rumor', 0.0): 3, ('warmest', 0.0): 1, ('chin', 0.0): 2, ('tickl', 0.0): 1, ('♫', 0.0): 1, ('zikra', 0.0): 1, ('lusi', 0.0): 1, ('hasya', 0.0): 1, ('nugget', 0.0): 3, ('som', 0.0): 1, ('lu', 0.0): 1, ('olymp', 0.0): 1, (\"millie'\", 0.0): 1, ('guinea', 0.0): 1, ('lewi', 0.0): 1, ('748292', 0.0): 1, (\"we'll\", 0.0): 8, ('ano', 0.0): 2, ('22stan', 0.0): 1, ('24/7', 0.0): 2, ('thankyou', 0.0): 2, ('kanina', 0.0): 2, ('breakdown', 0.0): 2, ('mag', 0.0): 2, ('hatee', 0.0): 1, ('leas', 0.0): 1, ('written', 0.0): 2, ('hurri', 0.0): 4, ('attempt', 0.0): 1, ('6g', 0.0): 1, ('unsuccess', 0.0): 1, ('earlob', 0.0): 1, ('sue', 0.0): 1, ('dreari', 0.0): 1, ('denis', 0.0): 1, ('muriel', 0.0): 1, ('ahouré', 0.0): 1, ('pr', 0.0): 1, ('brand', 0.0): 1, ('imag', 0.0): 4, ('opportun', 0.0): 1, ('po', 0.0): 1, ('beg', 0.0): 2, (\"kath'd\", 0.0): 1, ('respond', 0.0): 2, ('chop', 0.0): 1, ('wbu', 0.0): 1, ('yess', 0.0): 2, ('kme', 0.0): 1, ('tom', 0.0): 4, ('cram', 0.0): 1, ('–', 0.0): 1, ('curiou', 0.0): 1, ('on-board', 0.0): 1, ('announc', 0.0): 3, ('trespass', 0.0): 1, ('fr', 0.0): 3, ('clandestin', 0.0): 1, ('muller', 0.0): 1, ('obviou', 0.0): 1, ('mufc', 0.0): 1, ('colour', 0.0): 4, ('stu', 0.0): 2, ('movie', 0.0): 1, ('buddyyi', 0.0): 1, ('feelgoodfriday', 0.0): 1, ('forest', 0.0): 1, ('6:30', 0.0): 1, ('babysit', 0.0): 1, ('opix', 0.0): 1, ('805', 0.0): 1, ('pilllow', 0.0): 1, ('fool', 0.0): 1, ('brag', 0.0): 1, ('skrillah', 0.0): 1, ('drown', 0.0): 2, ('gue', 0.0): 1, ('report', 0.0): 4, ('eventu', 0.0): 1, ('north', 0.0): 1, ('west', 0.0): 2, ('kitti', 0.0): 1, ('sjkao', 0.0): 1, ('mm', 0.0): 2, ('srri', 0.0): 1, ('honma', 0.0): 1, ('yeh', 0.0): 1, ('walay', 0.0): 1, ('bhi', 0.0): 2, ('bohat', 0.0): 1, ('wailay', 0.0): 1, ('hain', 0.0): 2, ('pre-season', 0.0): 1, ('friendli', 0.0): 3, ('pe', 0.0): 3, ('itna', 0.0): 2, ('shor', 0.0): 1, ('machaya', 0.0): 1, ('mein', 0.0): 1, ('samjha', 0.0): 1, ('cup', 0.0): 3, ('note', 0.0): 2, ('😄', 0.0): 1, ('👍', 0.0): 1, ('😔', 0.0): 7, ('sirkay', 0.0): 1, ('wali', 0.0): 1, ('pyaaz', 0.0): 1, ('daal', 0.0): 2, ('onion', 0.0): 1, ('vinegar', 0.0): 1, ('cook', 0.0): 3, ('tutori', 0.0): 1, ('soho', 0.0): 1, ('wobbl', 0.0): 1, ('server', 0.0): 4, ('ciao', 0.0): 1, ('masaan', 0.0): 1, ('muv', 0.0): 1, ('beast', 0.0): 2, ('hayst', 0.0): 1, ('cr', 0.0): 1, ('hnnn', 0.0): 1, ('fluffi', 0.0): 2, ('comeback', 0.0): 3, ('korea', 0.0): 1, ('wow', 0.0): 10, ('act', 0.0): 4, ('optimis', 0.0): 1, ('soniii', 0.0): 1, ('kahaaa', 0.0): 1, ('shave', 0.0): 3, ('tryna', 0.0): 3, ('healthi', 0.0): 2, ('freez', 0.0): 3, ('fml', 0.0): 4, ('jacket', 0.0): 1, ('sleepi', 0.0): 4, ('cyber', 0.0): 1, ('bulli', 0.0): 2, ('racial', 0.0): 2, ('scari', 0.0): 6, ('hall', 0.0): 1, ('stockholm', 0.0): 1, ('loool', 0.0): 3, ('bunch', 0.0): 3, ('among', 0.0): 1, ('__', 0.0): 2, ('busier', 0.0): 1, ('onward', 0.0): 1, ('ol', 0.0): 2, ('coincid', 0.0): 1, ('imac', 0.0): 1, ('launch', 0.0): 2, ('gram', 0.0): 1, ('nearer', 0.0): 1, ('blain', 0.0): 2, ('darren', 0.0): 2, ('layout', 0.0): 3, ('fuuuck', 0.0): 2, ('jesu', 0.0): 1, ('gishwh', 0.0): 1, ('exclud', 0.0): 1, ('unless', 0.0): 4, ('c', 0.0): 7, ('angelica', 0.0): 1, ('pull', 0.0): 5, ('colleg', 0.0): 5, ('movement', 0.0): 1, ('frou', 0.0): 1, ('vaccin', 0.0): 1, ('armor', 0.0): 2, ('legendari', 0.0): 1, ('cash', 0.0): 2, ('effort', 0.0): 2, ('nat', 0.0): 2, ('brake', 0.0): 1, ('grumpi', 0.0): 4, ('wreck', 0.0): 1, ('decis', 0.0): 2, ('gahhh', 0.0): 1, ('teribl', 0.0): 1, ('kilig', 0.0): 1, ('togeth', 0.0): 7, ('weaker', 0.0): 1, ('shravan', 0.0): 1, ('tv', 0.0): 4, ('stooop', 0.0): 1, ('gi-guilti', 0.0): 1, ('akooo', 0.0): 1, ('imveryverysorri', 0.0): 1, ('cd', 0.0): 1, ('grey', 0.0): 3, ('basenam', 0.0): 1, ('path', 0.0): 1, ('theme', 0.0): 2, ('cigar', 0.0): 1, ('speaker', 0.0): 1, ('volum', 0.0): 1, ('promethazin', 0.0): 1, ('zopiclon', 0.0): 1, ('addit', 0.0): 1, ('quetiapin', 0.0): 1, ('modifi', 0.0): 1, ('prescript', 0.0): 1, ('greska', 0.0): 1, ('macedonian', 0.0): 1, ('slovak', 0.0): 1, ('hike', 0.0): 1, ('certainli', 0.0): 2, ('browser', 0.0): 2, ('os', 0.0): 1, ('zokay', 0.0): 1, ('accent', 0.0): 1, ('b-but', 0.0): 1, ('gintama', 0.0): 1, ('shinsengumi', 0.0): 1, ('chapter', 0.0): 1, ('andi', 0.0): 1, ('crappl', 0.0): 1, ('agre', 0.0): 5, ('ftw', 0.0): 2, ('phandroid', 0.0): 1, ('tline', 0.0): 1, ('orchestra', 0.0): 1, ('ppl', 0.0): 5, ('rehears', 0.0): 1, ('bittersweet', 0.0): 1, ('eunji', 0.0): 1, ('bakit', 0.0): 4, ('121st', 0.0): 1, (\"yesterday'\", 0.0): 1, ('rt', 0.0): 8, ('ehdar', 0.0): 1, ('pegea', 0.0): 1, ('panga', 0.0): 1, ('dosto', 0.0): 1, ('nd', 0.0): 1, ('real_liam_payn', 0.0): 1, ('retweet', 0.0): 5, ('3/10', 0.0): 1, ('dmed', 0.0): 1, ('ad', 0.0): 1, ('yay', 0.0): 3, ('23', 0.0): 2, ('alreaddyyi', 0.0): 1, ('luceleva', 0.0): 1, ('21', 0.0): 1, ('porno', 0.0): 3, ('countrymus', 0.0): 4, ('sexysasunday', 0.0): 2, ('naeun', 0.0): 1, ('goal', 0.0): 5, (\"son'\", 0.0): 1, ('kidney', 0.0): 2, ('printer', 0.0): 1, ('ink', 0.0): 2, ('asham', 0.0): 3, ('ihatesomepeopl', 0.0): 1, ('tabl', 0.0): 2, ('0-2', 0.0): 1, ('brain', 0.0): 2, ('hard-wir', 0.0): 1, ('canadian', 0.0): 1, ('acn', 0.0): 2, ('gulo', 0.0): 1, ('kandekj', 0.0): 1, ('rize', 0.0): 1, ('meydan', 0.0): 1, ('experienc', 0.0): 2, ('fcking', 0.0): 1, ('crei', 0.0): 1, ('stabl', 0.0): 1, ('dormmat', 0.0): 1, ('pre', 0.0): 3, ('bo3', 0.0): 1, ('cod', 0.0): 2, ('redeem', 0.0): 1, ('invalid', 0.0): 1, ('wag', 0.0): 1, ('hopia', 0.0): 1, ('campaign', 0.0): 2, ('editor', 0.0): 1, ('reveal', 0.0): 2, ('booo', 0.0): 2, ('extens', 0.0): 1, ('rightnow', 0.0): 1, ('btu', 0.0): 1, ('karaok', 0.0): 1, ('licenc', 0.0): 1, ('apb', 0.0): 2, ('mbf', 0.0): 1, ('kpop', 0.0): 2, ('hahahaokay', 0.0): 1, ('basara', 0.0): 1, ('capcom', 0.0): 3, ('pc', 0.0): 2, ('url', 0.0): 2, ('web', 0.0): 2, ('site', 0.0): 6, ('design', 0.0): 3, ('grumbl', 0.0): 2, ('migrant', 0.0): 1, ('daddi', 0.0): 4, ('legit', 0.0): 1, ('australia', 0.0): 3, ('awsm', 0.0): 1, ('entir', 0.0): 5, ('tmw', 0.0): 1, ('uwu', 0.0): 1, ('jinki', 0.0): 1, ('taem', 0.0): 1, ('gif', 0.0): 2, ('cambridg', 0.0): 1, ('viath', 0.0): 1, ('brilliant', 0.0): 1, ('cypru', 0.0): 1, ('wet', 0.0): 10, ('30th', 0.0): 1, ('zayncomebackto', 0.0): 2, ('1d', 0.0): 6, ('senior', 0.0): 2, ('spazz', 0.0): 1, ('soobin', 0.0): 1, ('27', 0.0): 1, ('unmarri', 0.0): 1, ('float', 0.0): 3, ('pressur', 0.0): 3, ('winter', 0.0): 4, ('lifetim', 0.0): 2, ('hiondsh', 0.0): 1, ('58543', 0.0): 1, ('kikmenow', 0.0): 9, ('sexdat', 0.0): 2, (\"demi'\", 0.0): 1, ('junjou', 0.0): 2, ('romantica', 0.0): 1, ('cruel', 0.0): 1, ('privileg', 0.0): 2, ('mixtap', 0.0): 2, ('convinc', 0.0): 3, ('friex', 0.0): 1, ('taco', 0.0): 2, ('europ', 0.0): 2, ('shaylan', 0.0): 1, ('4:20', 0.0): 1, ('ylona', 0.0): 1, ('nah', 0.0): 4, ('notanapolog', 0.0): 3, ('ouh', 0.0): 1, ('tax', 0.0): 4, ('ohhh', 0.0): 2, ('nm', 0.0): 1, ('term', 0.0): 1, ('apolog', 0.0): 3, ('encanta', 0.0): 1, ('vale', 0.0): 1, ('osea', 0.0): 1, ('bea', 0.0): 1, ('♛', 0.0): 210, ('》', 0.0): 210, ('beli̇ev', 0.0): 35, ('wi̇ll', 0.0): 35, ('justi̇n', 0.0): 35, ('x15', 0.0): 35, ('350', 0.0): 4, ('ｓｅｅ', 0.0): 35, ('ｍｅ', 0.0): 35, ('40', 0.0): 3, ('dj', 0.0): 2, ('net', 0.0): 2, ('349', 0.0): 1, ('baek', 0.0): 1, ('tight', 0.0): 1, ('dunwan', 0.0): 1, ('suan', 0.0): 1, ('ba', 0.0): 3, ('haiz', 0.0): 1, ('otw', 0.0): 1, ('trade', 0.0): 3, ('venic', 0.0): 1, ('348', 0.0): 1, ('strong', 0.0): 6, ('adult', 0.0): 3, ('347', 0.0): 1, ('tree', 0.0): 3, ('hill', 0.0): 1, ('😕', 0.0): 1, ('com', 0.0): 1, ('insonia', 0.0): 1, ('346', 0.0): 1, ('rick', 0.0): 1, ('ross', 0.0): 1, ('wallet', 0.0): 4, ('empti', 0.0): 3, ('heartbreak', 0.0): 2, ('episod', 0.0): 11, ('345', 0.0): 1, ('milli', 0.0): 1, (':)', 0.0): 2, ('diff', 0.0): 1, ('persona', 0.0): 1, ('golden', 0.0): 1, ('scene', 0.0): 1, ('advert', 0.0): 1, ('determin', 0.0): 2, ('roseburi', 0.0): 1, ('familyhom', 0.0): 1, ('daw', 0.0): 2, ('344', 0.0): 1, ('monkey', 0.0): 1, ('yea', 0.0): 2, ('343', 0.0): 1, ('sweeti', 0.0): 2, ('erica', 0.0): 1, ('istg', 0.0): 1, ('lick', 0.0): 1, ('jackson', 0.0): 4, ('nsbzhdnxndamal', 0.0): 1, ('342', 0.0): 1, ('11:15', 0.0): 1, ('2hour', 0.0): 1, ('11:25', 0.0): 1, ('341', 0.0): 1, ('fandom', 0.0): 2, ('mahilig', 0.0): 1, ('mam-bulli', 0.0): 1, ('mtaani', 0.0): 1, ('tunaita', 0.0): 1, ('viazi', 0.0): 1, ('choma', 0.0): 1, ('laid', 0.0): 1, ('celebr', 0.0): 3, ('7am', 0.0): 1, ('jerk', 0.0): 1, ('lah', 0.0): 2, ('magic', 0.0): 1, ('menil', 0.0): 1, ('340', 0.0): 1, (\"kam'\", 0.0): 1, ('meee', 0.0): 1, ('diz', 0.0): 1, ('biooo', 0.0): 1, ('ay', 0.0): 1, ('taray', 0.0): 1, ('yumu-youtub', 0.0): 1, ('339', 0.0): 1, ('parijat', 0.0): 1, ('willmissyouparijat', 0.0): 1, ('abroad', 0.0): 2, ('jolli', 0.0): 1, ('scotland', 0.0): 2, ('338', 0.0): 1, ('mcnugget', 0.0): 1, ('sophi', 0.0): 5, ('feedback', 0.0): 4, ('met', 0.0): 7, ('caramello', 0.0): 2, ('koala', 0.0): 1, ('bar', 0.0): 1, ('suckmejimin', 0.0): 1, ('337', 0.0): 1, ('sucki', 0.0): 2, ('laughter', 0.0): 1, ('pou', 0.0): 1, ('goddamn', 0.0): 1, ('bark', 0.0): 1, ('nje', 0.0): 1, ('blast', 0.0): 1, ('hun', 0.0): 4, ('dbn', 0.0): 2, ('🎀', 0.0): 1, ('336', 0.0): 1, ('hardest', 0.0): 1, ('335', 0.0): 1, ('pledg', 0.0): 1, ('realiz', 0.0): 7, ('viber', 0.0): 1, ('mwah', 0.0): 1, ('estat', 0.0): 1, ('crush', 0.0): 1, ('lansi', 0.0): 1, ('334', 0.0): 1, ('hp', 0.0): 4, ('waah', 0.0): 1, ('miami', 0.0): 1, ('vandag', 0.0): 1, ('kgola', 0.0): 1, ('neng', 0.0): 1, ('eintlik', 0.0): 1, ('porn', 0.0): 2, ('4like', 0.0): 5, ('repost', 0.0): 2, ('333', 0.0): 3, ('magpi', 0.0): 1, ('22.05', 0.0): 1, ('15-24', 0.0): 1, ('05.15', 0.0): 1, ('coach', 0.0): 2, ('ador', 0.0): 1, ('chswiyfxcskcalum', 0.0): 1, ('nvm', 0.0): 2, ('lemm', 0.0): 1, ('quiet', 0.0): 3, ('foof', 0.0): 1, ('332', 0.0): 1, ('casilla', 0.0): 1, ('manchest', 0.0): 3, ('xi', 0.0): 1, ('rmtour', 0.0): 1, ('heavi', 0.0): 3, ('irl', 0.0): 2, ('blooper', 0.0): 2, ('huhuhuhu', 0.0): 1, ('na-tak', 0.0): 1, ('sorta', 0.0): 1, ('unfriend', 0.0): 1, ('greysonch', 0.0): 1, ('sandwich', 0.0): 4, ('bell', 0.0): 1, ('sebastian', 0.0): 1, ('rewatch', 0.0): 1, ('s4', 0.0): 1, ('ser', 0.0): 1, ('past', 0.0): 5, ('heart-break', 0.0): 1, ('outdat', 0.0): 1, ('m4', 0.0): 1, ('abandon', 0.0): 1, ('theater', 0.0): 1, ('smh', 0.0): 6, ('7-3', 0.0): 1, ('7.30-', 0.0): 1, ('ekk', 0.0): 1, ('giriboy', 0.0): 1, ('harriet', 0.0): 1, ('gegu', 0.0): 1, ('gray', 0.0): 1, ('truth', 0.0): 4, ('tbt', 0.0): 1, ('331', 0.0): 1, ('roof', 0.0): 2, ('indian', 0.0): 2, ('polit', 0.0): 3, ('blame', 0.0): 3, ('68', 0.0): 1, ('repres', 0.0): 1, ('corbyn', 0.0): 1, (\"labour'\", 0.0): 1, ('fortun', 0.0): 1, ('icecream', 0.0): 3, ('cuti', 0.0): 2, ('ry', 0.0): 1, ('lfccw', 0.0): 1, ('5ever', 0.0): 1, ('america', 0.0): 3, ('ontheroadagain', 0.0): 1, ('halaaang', 0.0): 1, ('reciev', 0.0): 1, ('flip', 0.0): 4, ('flop', 0.0): 1, ('caesarspalac', 0.0): 1, ('socialreward', 0.0): 1, ('requir', 0.0): 2, ('cali', 0.0): 1, ('fuckboy', 0.0): 1, ('330', 0.0): 1, ('deliveri', 0.0): 3, ('chrompet', 0.0): 1, ('easili', 0.0): 2, ('immun', 0.0): 1, ('system', 0.0): 3, ('lush', 0.0): 1, ('bathtub', 0.0): 1, ('php', 0.0): 1, ('mysql', 0.0): 1, ('libmysqlclient-dev', 0.0): 1, ('dev', 0.0): 2, ('pleasanton', 0.0): 1, ('wala', 0.0): 1, ('329', 0.0): 1, ('quickli', 0.0): 2, ('megan', 0.0): 1, ('heed', 0.0): 2, ('328', 0.0): 1, ('gwss', 0.0): 1, ('thankyouu', 0.0): 1, ('charad', 0.0): 1, ('becom', 0.0): 5, ('piano', 0.0): 2, ('327', 0.0): 1, ('complaint', 0.0): 2, ('yell', 0.0): 2, ('whatsoev', 0.0): 2, ('pete', 0.0): 1, ('wentz', 0.0): 1, ('shogi', 0.0): 1, ('blameshoghicp', 0.0): 1, ('classmat', 0.0): 1, ('troubl', 0.0): 1, ('fixedgearfrenzi', 0.0): 1, ('dispatch', 0.0): 1, ('theyr', 0.0): 2, ('hat', 0.0): 2, (\"shamuon'\", 0.0): 1, ('tokyo', 0.0): 1, ('toe', 0.0): 2, ('horrend', 0.0): 2, (\"someone'\", 0.0): 2, ('326', 0.0): 1, ('hasb', 0.0): 1, ('atti', 0.0): 1, ('muji', 0.0): 1, ('sirf', 0.0): 1, ('sensibl', 0.0): 1, ('etc', 0.0): 2, ('brum', 0.0): 1, ('cyclerevolut', 0.0): 1, ('caaannnttt', 0.0): 1, ('payment', 0.0): 3, ('overdrawn', 0.0): 1, ('tbf', 0.0): 1, ('complain', 0.0): 2, ('perfum', 0.0): 1, ('sampl', 0.0): 1, ('chanel', 0.0): 1, ('burberri', 0.0): 1, ('prada', 0.0): 1, ('325', 0.0): 1, ('noesss', 0.0): 1, ('topgear', 0.0): 1, ('worthi', 0.0): 1, ('bridesmaid', 0.0): 1, (\"tomorrow'\", 0.0): 2, ('gather', 0.0): 1, ('sudden', 0.0): 4, ('324', 0.0): 1, ('randomrestart', 0.0): 1, ('randomreboot', 0.0): 1, ('lumia', 0.0): 1, ('windowsphon', 0.0): 1, (\"microsoft'\", 0.0): 1, ('mañana', 0.0): 1, ('male', 0.0): 1, ('rap', 0.0): 1, ('sponsor', 0.0): 3, ('striker', 0.0): 2, ('lvg', 0.0): 1, ('behind', 0.0): 3, ('refurbish', 0.0): 1, ('cintiq', 0.0): 1, (\"finnick'\", 0.0): 1, ('askfinnick', 0.0): 1, ('contain', 0.0): 1, ('hairi', 0.0): 1, ('323', 0.0): 1, ('buri', 0.0): 1, ('omaygad', 0.0): 1, ('vic', 0.0): 1, ('surgeri', 0.0): 4, ('amber', 0.0): 8, ('tt.tt', 0.0): 1, ('hyper', 0.0): 2, ('vega', 0.0): 2, ('322', 0.0): 1, ('imiss', 0.0): 1, ('321', 0.0): 1, ('320', 0.0): 1, ('know.for', 0.0): 1, ('prepaid', 0.0): 1, ('none', 0.0): 4, ('319', 0.0): 1, ('grandma', 0.0): 1, (\"grandpa'\", 0.0): 1, ('farm', 0.0): 1, ('cow', 0.0): 1, ('sheep', 0.0): 1, ('hors', 0.0): 3, ('fruit', 0.0): 2, ('veget', 0.0): 1, ('puke', 0.0): 2, ('deliri', 0.0): 1, ('motilium', 0.0): 1, ('shite', 0.0): 1, ('318', 0.0): 1, ('schoolwork', 0.0): 1, (\"phoebe'\", 0.0): 1, ('317', 0.0): 1, ('pothol', 0.0): 1, ('316', 0.0): 1, ('notif', 0.0): 3, ('1,300', 0.0): 1, ('robyn', 0.0): 1, ('necklac', 0.0): 1, ('rachel', 0.0): 1, ('bhai', 0.0): 1, ('ramzan', 0.0): 1, ('crosss', 0.0): 1, ('clapham', 0.0): 1, ('investig', 0.0): 2, ('sth', 0.0): 1, ('essenti', 0.0): 1, ('photoshooot', 0.0): 1, ('austin', 0.0): 1, ('mahon', 0.0): 1, ('shut', 0.0): 3, ('andam', 0.0): 1, ('memor', 0.0): 1, ('cotton', 0.0): 1, ('candi', 0.0): 3, ('stock', 0.0): 3, ('swallow', 0.0): 1, ('snot', 0.0): 1, ('choke', 0.0): 1, ('taknottem', 0.0): 1, ('477', 0.0): 1, ('btob', 0.0): 2, ('percentag', 0.0): 1, ('shoshannavassil', 0.0): 1, ('swift', 0.0): 1, ('flat', 0.0): 3, ('a9', 0.0): 2, ('wsalelov', 0.0): 5, ('sexyjan', 0.0): 1, ('horni', 0.0): 2, ('goodmus', 0.0): 4, ('debut', 0.0): 3, ('lart', 0.0): 1, ('sew', 0.0): 1, ('skyfal', 0.0): 1, ('premier', 0.0): 1, ('yummi', 0.0): 2, ('manteca', 0.0): 1, (\"she'd\", 0.0): 2, ('probabl', 0.0): 8, ('shiatsu', 0.0): 1, ('heat', 0.0): 1, ('risk', 0.0): 3, ('edward', 0.0): 1, ('hopper', 0.0): 1, ('eyyah', 0.0): 1, ('utd', 0.0): 2, ('born', 0.0): 1, ('1-0', 0.0): 1, ('cart', 0.0): 1, ('shop', 0.0): 10, ('log', 0.0): 2, ('aaa', 0.0): 2, ('waifu', 0.0): 1, ('break', 0.0): 8, ('breakup', 0.0): 3, ('bother', 0.0): 3, ('bia', 0.0): 1, ('syndrom', 0.0): 1, ('shi', 0.0): 1, ('bias', 0.0): 1, ('pixel', 0.0): 2, ('weh', 0.0): 2, ('area', 0.0): 4, ('maymay', 0.0): 1, ('magpaalam', 0.0): 1, ('tf', 0.0): 3, ('subtitl', 0.0): 1, ('oitnb', 0.0): 1, ('backstori', 0.0): 1, ('jeremi', 0.0): 1, ('kyle', 0.0): 1, ('gimm', 0.0): 2, ('meal', 0.0): 3, ('neat-o', 0.0): 1, ('wru', 0.0): 1, ('scissor', 0.0): 1, ('creation', 0.0): 1, ('public', 0.0): 1, ('amtir', 0.0): 1, ('imysm', 0.0): 2, ('tut', 0.0): 1, ('trop', 0.0): 2, ('tard', 0.0): 1, ('deadlin', 0.0): 1, ('31', 0.0): 2, ('st', 0.0): 3, ('child', 0.0): 4, ('oct', 0.0): 2, ('bush', 0.0): 2, ('premiun', 0.0): 1, ('notcool', 0.0): 1, ('2/3', 0.0): 2, ('lahat', 0.0): 2, ('ng', 0.0): 4, ('araw', 0.0): 1, ('nage', 0.0): 1, ('gyu', 0.0): 4, ('lmfaooo', 0.0): 2, ('download', 0.0): 3, ('leagu', 0.0): 1, ('mashup', 0.0): 1, ('eu', 0.0): 1, ('lc', 0.0): 1, ('typo', 0.0): 2, ('itali', 0.0): 1, ('yass', 0.0): 1, ('christma', 0.0): 2, ('rel', 0.0): 1, ('yr', 0.0): 3, ('sydney', 0.0): 1, ('mb', 0.0): 1, ('perf', 0.0): 2, ('programm', 0.0): 1, ('bff', 0.0): 2, ('hashtag', 0.0): 1, ('omfg', 0.0): 4, ('exercis', 0.0): 2, ('combat', 0.0): 1, ('dosent', 0.0): 1, (\"sod'\", 0.0): 1, ('20min', 0.0): 1, ('request', 0.0): 2, ('yahoo', 0.0): 2, ('yodel', 0.0): 2, ('jokingli', 0.0): 1, ('regret', 0.0): 5, ('starbuck', 0.0): 3, ('lynettelow', 0.0): 1, ('interraci', 0.0): 3, (\"today'\", 0.0): 3, ('tgif', 0.0): 1, ('gahd', 0.0): 1, ('26th', 0.0): 1, ('discov', 0.0): 1, ('12.00', 0.0): 1, ('obyun', 0.0): 1, ('unni', 0.0): 4, ('wayhh', 0.0): 1, ('preval', 0.0): 1, ('controversi', 0.0): 1, ('🍵', 0.0): 2, ('☕', 0.0): 1, ('tube', 0.0): 1, ('strike', 0.0): 3, ('meck', 0.0): 1, ('mcfc', 0.0): 1, ('fresh', 0.0): 1, ('ucan', 0.0): 1, ('anxiou', 0.0): 1, ('poc', 0.0): 1, ('specif', 0.0): 2, ('sinhala', 0.0): 1, ('billionair', 0.0): 1, ('1645', 0.0): 1, ('island', 0.0): 3, ('1190', 0.0): 1, ('maldiv', 0.0): 1, ('dheena', 0.0): 1, ('fasgadah', 0.0): 1, ('alvadhaau', 0.0): 1, ('countdown', 0.0): 1, ('function', 0.0): 3, ('desktop', 0.0): 1, ('evelineconrad', 0.0): 1, ('facetim', 0.0): 4, ('kikmsn', 0.0): 2, ('selfshot', 0.0): 2, ('panda', 0.0): 1, ('backkk', 0.0): 1, ('transfer', 0.0): 3, ('dan', 0.0): 2, ('dull', 0.0): 1, ('overcast', 0.0): 1, ('folder', 0.0): 1, ('truck', 0.0): 2, ('missin', 0.0): 2, ('hangin', 0.0): 1, ('wiff', 0.0): 1, ('dept', 0.0): 1, ('cherri', 0.0): 1, ('bakewel', 0.0): 1, ('collect', 0.0): 3, ('teal', 0.0): 1, ('sect', 0.0): 1, ('tennunb', 0.0): 1, ('rather', 0.0): 4, ('skip', 0.0): 1, ('doomsday', 0.0): 1, ('neglect', 0.0): 1, ('posti', 0.0): 1, ('goodnight', 0.0): 1, ('donat', 0.0): 3, ('ship', 0.0): 6, ('bellami', 0.0): 1, ('raven', 0.0): 2, ('clark', 0.0): 1, ('helmi', 0.0): 1, ('uh', 0.0): 5, ('cnt', 0.0): 1, ('whereisthesun', 0.0): 1, ('summerismiss', 0.0): 1, ('longgg', 0.0): 1, ('ridicul', 0.0): 4, ('stocko', 0.0): 1, ('lucozad', 0.0): 1, ('explos', 0.0): 1, ('beh', 0.0): 2, ('half-rememb', 0.0): 1, (\"melody'\", 0.0): 1, ('recal', 0.0): 2, ('level', 0.0): 3, ('target', 0.0): 1, ('difficult', 0.0): 4, ('mile', 0.0): 1, ('pfb', 0.0): 1, ('nate', 0.0): 2, ('expo', 0.0): 2, ('jisoo', 0.0): 1, ('chloe', 0.0): 2, ('anon', 0.0): 2, ('mager', 0.0): 1, ('wi', 0.0): 1, ('knw', 0.0): 1, ('wht', 0.0): 1, ('distant', 0.0): 1, ('buffer', 0.0): 2, ('insan', 0.0): 1, ('charli', 0.0): 1, ('finland', 0.0): 3, ('gana', 0.0): 1, ('studio', 0.0): 3, ('arch', 0.0): 1, ('lyin', 0.0): 1, ('kian', 0.0): 3, ('supercar', 0.0): 1, ('gurgaon', 0.0): 1, ('locat', 0.0): 7, ('9:15', 0.0): 1, ('satir', 0.0): 1, ('gener', 0.0): 2, ('peanut', 0.0): 3, ('butter', 0.0): 1, ('garden', 0.0): 2, ('beer', 0.0): 1, ('viner', 0.0): 1, ('palembang', 0.0): 1, ('sorrryyi', 0.0): 1, ('fani', 0.0): 1, ('hahahahaha', 0.0): 2, ('boner', 0.0): 1, ('merci', 0.0): 1, ('yuki', 0.0): 1, ('2500k', 0.0): 1, ('mari', 0.0): 1, ('jake', 0.0): 1, ('gyllenha', 0.0): 1, ('impact', 0.0): 1, (\"ledger'\", 0.0): 1, ('btw', 0.0): 5, ('cough', 0.0): 4, ('hunni', 0.0): 1, ('b4', 0.0): 1, ('deplet', 0.0): 1, ('mbasa', 0.0): 1, ('client', 0.0): 3, ('ray', 0.0): 1, ('aah', 0.0): 1, ('type', 0.0): 2, ('suit', 0.0): 5, ('pa-copi', 0.0): 1, ('proper', 0.0): 2, ('biom', 0.0): 1, ('mosqu', 0.0): 1, ('smelli', 0.0): 1, ('taxi', 0.0): 4, ('emptier', 0.0): 1, (\"ciara'\", 0.0): 1, (\"everything'\", 0.0): 1, ('clip', 0.0): 2, ('tall', 0.0): 2, ('gladli', 0.0): 1, ('intent', 0.0): 1, ('amb', 0.0): 1, (\"harry'\", 0.0): 2, ('jean', 0.0): 2, ('mayday', 0.0): 1, ('parad', 0.0): 2, ('lyf', 0.0): 1, ('13th', 0.0): 1, ('anim', 0.0): 4, ('kingdom', 0.0): 1, ('chri', 0.0): 7, ('brown', 0.0): 4, ('riski', 0.0): 1, ('cologn', 0.0): 1, ('duo', 0.0): 3, ('ballad', 0.0): 2, ('bish', 0.0): 2, ('intern', 0.0): 2, ('brought', 0.0): 1, ('yumyum', 0.0): 1, (\"cathy'\", 0.0): 1, ('missyou', 0.0): 1, ('rubi', 0.0): 2, ('rose', 0.0): 2, ('tou', 0.0): 1, ('main', 0.0): 1, ('pora', 0.0): 1, ('stalk', 0.0): 3, ('karlia', 0.0): 1, ('khatam', 0.0): 2, ('bandi', 0.0): 1, ('👑', 0.0): 1, ('pyaari', 0.0): 1, ('gawd', 0.0): 1, ('understood', 0.0): 1, ('review', 0.0): 3, ('massi', 0.0): 1, ('thatselfiethough', 0.0): 1, ('loop', 0.0): 1, ('ofc', 0.0): 1, ('pict', 0.0): 1, ('caught', 0.0): 1, ('aishhh', 0.0): 1, ('viewer', 0.0): 1, ('exam', 0.0): 5, ('sighsss', 0.0): 1, ('burnt', 0.0): 2, ('toffe', 0.0): 2, ('honesti', 0.0): 1, ('cheatday', 0.0): 1, ('protein', 0.0): 1, ('sissi', 0.0): 1, ('tote', 0.0): 1, ('slowli', 0.0): 1, ('church', 0.0): 2, ('pll', 0.0): 1, ('sel', 0.0): 1, ('beth', 0.0): 2, ('serbia', 0.0): 1, ('serbian', 0.0): 1, ('selen', 0.0): 1, ('motav', 0.0): 1, ('💋', 0.0): 2, ('zayyyn', 0.0): 1, ('momma', 0.0): 1, ('happend', 0.0): 1, ('imper', 0.0): 1, ('trmdhesit', 0.0): 1, ('pana', 0.0): 1, ('quickest', 0.0): 2, ('blood', 0.0): 5, ('sake', 0.0): 1, ('hamstr', 0.0): 1, ('rodwel', 0.0): 1, ('trace', 0.0): 1, ('artist', 0.0): 4, ('tp', 0.0): 1, ('powder', 0.0): 1, ('wider', 0.0): 1, ('honestli', 0.0): 4, ('comfort', 0.0): 3, ('bruno', 0.0): 1, ('1.8', 0.0): 1, ('ed', 0.0): 7, ('croke', 0.0): 2, ('deal', 0.0): 6, ('toll', 0.0): 1, ('packag', 0.0): 1, ('shape', 0.0): 1, ('unluckiest', 0.0): 1, ('bettor', 0.0): 1, ('nstp', 0.0): 1, ('sem', 0.0): 2, ('chipotl', 0.0): 1, ('chick-fil-a', 0.0): 1, ('stole', 0.0): 3, ('evet', 0.0): 1, ('ramadhan', 0.0): 1, ('eid', 0.0): 4, ('stexpert', 0.0): 1, ('ripstegi', 0.0): 1, ('nickyyi', 0.0): 1, ('¿', 0.0): 1, ('centralis', 0.0): 1, ('discontinu', 0.0): 1, ('sniff', 0.0): 1, (\"i't\", 0.0): 1, ('glad', 0.0): 2, ('fab', 0.0): 2, ('theres', 0.0): 1, ('cred', 0.0): 1, ('t_t', 0.0): 1, ('elimin', 0.0): 1, ('teamzip', 0.0): 1, ('smtm', 0.0): 1, ('assingn', 0.0): 1, ('editi', 0.0): 1, ('nakaka', 0.0): 1, ('beastmod', 0.0): 1, ('gaaawd', 0.0): 1, ('jane', 0.0): 1, ('mango', 0.0): 1, ('colombia', 0.0): 1, ('yot', 0.0): 1, ('labyo', 0.0): 1, ('pano', 0.0): 1, ('nalamannn', 0.0): 1, ('hardhead', 0.0): 1, ('cell', 0.0): 1, (\"zach'\", 0.0): 1, ('burger', 0.0): 2, ('xpress', 0.0): 1, ('hopkin', 0.0): 1, ('melatonin', 0.0): 1, ('2-4', 0.0): 1, ('nap', 0.0): 2, ('wide', 0.0): 2, ('task', 0.0): 1, ('9pm', 0.0): 1, ('hahaah', 0.0): 1, ('frequent', 0.0): 1, ('jail', 0.0): 2, ('weirddd', 0.0): 1, ('donghyuk', 0.0): 1, ('stan', 0.0): 1, ('bek', 0.0): 1, ('13', 0.0): 4, ('reynoldsgrl', 0.0): 1, ('ole', 0.0): 1, ('beardi', 0.0): 1, ('kaussi', 0.0): 1, ('bummer', 0.0): 3, ('fightingmciren', 0.0): 1, (\"michael'\", 0.0): 1, ('�', 0.0): 21, ('miser', 0.0): 2, ('💦', 0.0): 1, ('yoga', 0.0): 2, ('🌞', 0.0): 1, ('💃🏽', 0.0): 1, ('shouldv', 0.0): 1, ('saffron', 0.0): 1, ('peasant', 0.0): 1, ('wouldv', 0.0): 1, ('nfinit', 0.0): 1, ('admin_myung', 0.0): 1, ('slp', 0.0): 1, ('saddest', 0.0): 2, ('laomma', 0.0): 2, ('kebaya', 0.0): 1, ('bandung', 0.0): 1, ('indonesia', 0.0): 1, ('7df89150', 0.0): 1, ('whatsapp', 0.0): 2, ('62', 0.0): 1, ('08962464174', 0.0): 1, ('laomma_coutur', 0.0): 1, ('haizzz', 0.0): 1, ('urghhh', 0.0): 1, ('working-on-a-tight-schedul', 0.0): 1, ('ganbarimasu', 0.0): 1, ('livid', 0.0): 1, ('whammi', 0.0): 1, ('quuuee', 0.0): 1, ('friooo', 0.0): 1, ('ladi', 0.0): 4, ('stereo', 0.0): 1, ('chwang', 0.0): 1, ('lorm', 0.0): 1, ('823', 0.0): 1, ('rp', 0.0): 1, ('indiemus', 0.0): 10, ('unhappi', 0.0): 2, ('jennyjean', 0.0): 1, ('elfindelmundo', 0.0): 2, ('lolzz', 0.0): 1, ('dat', 0.0): 4, ('corey', 0.0): 1, ('appreci', 0.0): 2, ('weekli', 0.0): 2, ('mahirap', 0.0): 1, ('nash', 0.0): 1, ('gosh', 0.0): 6, ('noodl', 0.0): 1, ('veeerri', 0.0): 1, ('rted', 0.0): 2, ('orig', 0.0): 1, ('starholicxx', 0.0): 1, ('07:17', 0.0): 2, ('@the', 0.0): 1, ('notr', 0.0): 1, ('hwi', 0.0): 1, ('niall', 0.0): 5, ('fraud', 0.0): 1, ('diplomaci', 0.0): 1, ('fittest', 0.0): 1, ('zero', 0.0): 1, ('toler', 0.0): 2, ('gurl', 0.0): 1, ('notion', 0.0): 1, ('pier', 0.0): 1, ('approach', 0.0): 1, ('rattl', 0.0): 1, ('robe', 0.0): 1, ('emphasi', 0.0): 1, ('vocal', 0.0): 1, ('chose', 0.0): 1, ('erm', 0.0): 1, ('abby.can', 0.0): 1, ('persuad', 0.0): 1, ('lyric', 0.0): 1, (\"emily'\", 0.0): 1, ('odd', 0.0): 3, ('possibl', 0.0): 8, ('elect', 0.0): 2, ('kamiss', 0.0): 1, ('mwa', 0.0): 1, ('mommi', 0.0): 3, ('scream', 0.0): 1, ('fight', 0.0): 2, ('cafe', 0.0): 2, ('melbourn', 0.0): 1, ('anyonnee', 0.0): 1, ('loner', 0.0): 1, ('fricken', 0.0): 2, ('rito', 0.0): 1, ('friendzon', 0.0): 1, ('panel', 0.0): 1, ('repeat', 0.0): 2, ('audienc', 0.0): 1, ('hsm', 0.0): 1, ('canario', 0.0): 1, ('hotel', 0.0): 8, ('ukiss', 0.0): 1, ('faith', 0.0): 2, ('kurt', 0.0): 1, (\"fatma'm\", 0.0): 1, ('alex', 0.0): 4, ('swag', 0.0): 1, ('lmfao', 0.0): 2, ('flapjack', 0.0): 1, ('countthecost', 0.0): 1, ('ihop', 0.0): 1, ('infra', 0.0): 1, ('lq', 0.0): 1, ('knive', 0.0): 1, ('sotir', 0.0): 1, ('mybrainneedstoshutoff', 0.0): 1, ('macci', 0.0): 1, ('chees', 0.0): 7, ('25', 0.0): 2, ('tend', 0.0): 1, ('510', 0.0): 1, ('silicon', 0.0): 1, ('cover', 0.0): 2, ('kbye', 0.0): 1, ('ini', 0.0): 1, ('anytim', 0.0): 1, ('citizen', 0.0): 1, ('compar', 0.0): 2, ('rank', 0.0): 1, ('mcountdown', 0.0): 2, ('5h', 0.0): 1, ('thapelo', 0.0): 1, ('op', 0.0): 1, ('civ', 0.0): 1, ('wooden', 0.0): 1, ('mic', 0.0): 1, ('embarrass', 0.0): 2, ('translat', 0.0): 3, ('daili', 0.0): 3, ('mecha-totem', 0.0): 1, ('nak', 0.0): 1, ('tgk', 0.0): 1, ('townsss', 0.0): 1, ('jokid', 0.0): 1, ('rent', 0.0): 2, ('degre', 0.0): 1, ('inconsider', 0.0): 2, ('softbal', 0.0): 1, ('appli', 0.0): 1, ('tomcat', 0.0): 1, ('chel', 0.0): 1, ('jemma', 0.0): 1, ('detail', 0.0): 4, ('list', 0.0): 4, ('matchi', 0.0): 2, ('elsa', 0.0): 1, ('postpon', 0.0): 1, ('karin', 0.0): 1, ('honey', 0.0): 2, ('vist', 0.0): 1, ('unhealthi', 0.0): 1, ('propa', 0.0): 1, ('knockin', 0.0): 1, ('bacon', 0.0): 1, ('market', 0.0): 2, ('pre-holiday', 0.0): 1, ('diet', 0.0): 1, ('meani', 0.0): 1, ('deathbybaconsmel', 0.0): 1, ('init', 0.0): 2, ('destin', 0.0): 1, ('victoria', 0.0): 2, ('luna', 0.0): 1, ('krystal', 0.0): 1, ('sarajevo', 0.0): 1, ('haix', 0.0): 2, ('sp', 0.0): 1, ('student', 0.0): 4, ('wii', 0.0): 2, ('bayonetta', 0.0): 1, ('101', 0.0): 1, ('doabl', 0.0): 1, ('drove', 0.0): 1, ('agenc', 0.0): 1, ('story.miss', 0.0): 1, ('everon', 0.0): 1, ('jp', 0.0): 1, ('mamabear', 0.0): 1, ('imintoh', 0.0): 1, ('underr', 0.0): 1, (\"slovakia'\", 0.0): 1, ('d:', 0.0): 6, ('saklap', 0.0): 1, ('grade', 0.0): 2, ('rizal', 0.0): 1, ('lib', 0.0): 1, ('discuss', 0.0): 1, ('advisori', 0.0): 1, ('period', 0.0): 2, ('dit', 0.0): 1, ('du', 0.0): 1, ('harsh', 0.0): 2, ('ohgod', 0.0): 1, ('abligaverin', 0.0): 2, ('photooftheday', 0.0): 2, ('sexygirlbypreciouslemmi', 0.0): 3, ('ripsandrabland', 0.0): 1, ('edel', 0.0): 1, ('salam', 0.0): 1, ('mubark', 0.0): 1, ('dong', 0.0): 3, ('tammirossm', 0.0): 4, ('speck', 0.0): 1, ('abbymil', 0.0): 2, ('18', 0.0): 8, ('ion', 0.0): 1, ('5min', 0.0): 1, ('hse', 0.0): 1, ('noob', 0.0): 1, ('nxt', 0.0): 1, ('2week', 0.0): 1, ('300', 0.0): 3, ('fck', 0.0): 2, ('nae', 0.0): 2, ('deep', 0.0): 3, ('human', 0.0): 3, ('whit', 0.0): 1, ('van', 0.0): 4, ('bristol', 0.0): 1, ('subserv', 0.0): 1, ('si', 0.0): 4, ('oo', 0.0): 1, ('tub', 0.0): 1, ('penyfan', 0.0): 1, ('forecast', 0.0): 2, ('breconbeacon', 0.0): 1, ('tittheir', 0.0): 1, ('42', 0.0): 1, ('hotti', 0.0): 3, ('uu', 0.0): 2, ('rough', 0.0): 1, ('fuzzi', 0.0): 1, ('san', 0.0): 3, ('antonio', 0.0): 1, ('kang', 0.0): 1, ('junhe', 0.0): 1, ('couldv', 0.0): 1, ('pz', 0.0): 1, ('somerset', 0.0): 1, ('given', 0.0): 2, ('sunburnt', 0.0): 1, ('safer', 0.0): 1, ('k3g', 0.0): 1, ('input', 0.0): 1, ('gamestomp', 0.0): 1, ('desc', 0.0): 1, (\"angelo'\", 0.0): 1, ('yna', 0.0): 1, ('psygustokita', 0.0): 2, ('fiver', 0.0): 1, ('toward', 0.0): 1, ('sakho', 0.0): 1, ('threat', 0.0): 1, ('goalscor', 0.0): 1, ('10:59', 0.0): 1, ('11.00', 0.0): 1, ('sham', 0.0): 1, ('tricki', 0.0): 1, ('baao', 0.0): 1, ('nisrina', 0.0): 1, ('crazi', 0.0): 8, ('ladygaga', 0.0): 1, (\"you'\", 0.0): 2, ('pari', 0.0): 2, ('marrish', 0.0): 1, (\"otp'\", 0.0): 1, ('6:15', 0.0): 1, ('edomnt', 0.0): 1, ('qih', 0.0): 1, ('shxb', 0.0): 1, ('1000', 0.0): 1, ('chilton', 0.0): 1, ('mother', 0.0): 2, ('obsess', 0.0): 1, ('creepi', 0.0): 2, ('josh', 0.0): 1, ('boohoo', 0.0): 1, ('fellow', 0.0): 2, ('tweep', 0.0): 1, ('roar', 0.0): 1, ('victori', 0.0): 1, ('tweepsmatchout', 0.0): 1, ('nein', 0.0): 3, ('404', 0.0): 1, ('midnight', 0.0): 2, ('willlow', 0.0): 1, ('hbd', 0.0): 1, ('sowwi', 0.0): 1, ('3000', 0.0): 1, ('grind', 0.0): 1, ('gear', 0.0): 1, ('0.001', 0.0): 1, ('meant', 0.0): 6, ('portrait', 0.0): 1, ('mode', 0.0): 2, ('fact', 0.0): 4, ('11:11', 0.0): 4, ('shanzay', 0.0): 1, ('salabrati', 0.0): 1, ('journo', 0.0): 1, ('lure', 0.0): 1, ('gang', 0.0): 1, ('twist', 0.0): 1, ('mashaket', 0.0): 1, ('pet', 0.0): 2, ('bapak', 0.0): 1, ('royal', 0.0): 2, ('prima', 0.0): 1, ('mune', 0.0): 1, ('874', 0.0): 1, ('plisss', 0.0): 1, ('elf', 0.0): 1, ('teenchoic', 0.0): 5, ('choiceinternationalartist', 0.0): 5, ('superjunior', 0.0): 5, (\"he'll\", 0.0): 1, ('sunway', 0.0): 1, ('petal', 0.0): 1, ('jaya', 0.0): 1, ('selangor', 0.0): 1, ('glow', 0.0): 1, ('huhuu', 0.0): 1, ('congratul', 0.0): 2, ('margo', 0.0): 1, ('konga', 0.0): 1, ('ni', 0.0): 4, ('wa', 0.0): 2, ('ode', 0.0): 1, ('disvirgin', 0.0): 1, ('bride', 0.0): 3, ('yulin', 0.0): 1, ('meat', 0.0): 1, ('festiv', 0.0): 2, ('imma', 0.0): 2, ('syawal', 0.0): 1, ('lapar', 0.0): 1, ('foundat', 0.0): 1, ('clash', 0.0): 2, ('facil', 0.0): 1, ('dh', 0.0): 2, ('chalet', 0.0): 1, ('suay', 0.0): 1, ('anot', 0.0): 1, ('bugger', 0.0): 1, ('एक', 0.0): 1, ('बार', 0.0): 1, ('फिर', 0.0): 1, ('सेँ', 0.0): 1, ('धोखा', 0.0): 1, ('chandauli', 0.0): 1, ('majhwar', 0.0): 1, ('railway', 0.0): 1, ('tito', 0.0): 2, ('tita', 0.0): 1, ('cousin', 0.0): 3, ('critic', 0.0): 1, ('condit', 0.0): 1, ('steal', 0.0): 1, ('narco', 0.0): 1, ('regen', 0.0): 1, ('unfav', 0.0): 2, ('benadryl', 0.0): 1, ('offlin', 0.0): 1, ('arent', 0.0): 1, ('msg', 0.0): 1, ('yg', 0.0): 1, ('gg', 0.0): 3, ('sxrew', 0.0): 1, ('dissappear', 0.0): 1, ('swap', 0.0): 1, ('bleed', 0.0): 1, ('ishal', 0.0): 1, ('mi', 0.0): 2, ('thaank', 0.0): 1, ('jhezz', 0.0): 1, ('sneak', 0.0): 3, ('soft', 0.0): 1, ('defenc', 0.0): 1, ('defens', 0.0): 1, ('nrltigersroost', 0.0): 1, ('indiana', 0.0): 2, ('hibb', 0.0): 1, ('biblethump', 0.0): 1, ('rlyyi', 0.0): 1, ('septum', 0.0): 1, ('pierc', 0.0): 2, ('goood', 0.0): 1, ('hiya', 0.0): 1, ('fire', 0.0): 1, ('venom', 0.0): 1, ('carriag', 0.0): 1, ('pink', 0.0): 1, ('fur-trim', 0.0): 1, ('stetson', 0.0): 1, ('error', 0.0): 4, ('59', 0.0): 1, ('xue', 0.0): 1, ('midori', 0.0): 1, ('sakit', 0.0): 2, ('mateo', 0.0): 1, ('hawk', 0.0): 2, ('bartend', 0.0): 1, ('surf', 0.0): 1, ('despair', 0.0): 1, ('insta', 0.0): 1, ('promo', 0.0): 1, ('iwantin', 0.0): 1, ('___', 0.0): 2, ('fault', 0.0): 3, ('goodluck', 0.0): 1, ('pocket', 0.0): 1, ('help@veryhq.co.uk', 0.0): 1, ('benedictervent', 0.0): 1, ('content', 0.0): 1, ('221b', 0.0): 1, ('popcorn', 0.0): 3, ('joyc', 0.0): 1, ('ooop', 0.0): 1, ('spotifi', 0.0): 1, ('paalam', 0.0): 1, ('sazbal', 0.0): 1, ('incid', 0.0): 1, ('aaahh', 0.0): 1, ('gooo', 0.0): 1, (\"stomach'\", 0.0): 1, ('growl', 0.0): 1, ('beard', 0.0): 1, ('nooop', 0.0): 1, ('🎉', 0.0): 3, ('ding', 0.0): 3, ('hundr', 0.0): 1, ('meg', 0.0): 1, (\"verity'\", 0.0): 1, ('rupert', 0.0): 1, ('amin', 0.0): 1, ('studi', 0.0): 2, ('pleaaas', 0.0): 1, ('👆🏻', 0.0): 2, ('woaah', 0.0): 1, ('solvo', 0.0): 1, ('twin', 0.0): 2, (\"friday'\", 0.0): 1, ('lego', 0.0): 1, ('barefoot', 0.0): 1, ('twelvyy', 0.0): 1, ('boaz', 0.0): 1, ('myhil', 0.0): 1, ('takeov', 0.0): 1, ('wba', 0.0): 1, (\"taeyeon'\", 0.0): 1, ('derp', 0.0): 1, ('pd', 0.0): 1, ('zoom', 0.0): 2, (\"sunny'\", 0.0): 1, ('besst', 0.0): 1, ('plagu', 0.0): 1, ('pit', 0.0): 1, ('rich', 0.0): 1, ('sight', 0.0): 1, ('frail', 0.0): 1, ('lotteri', 0.0): 1, ('ride', 0.0): 2, ('twurkin', 0.0): 1, ('razzist', 0.0): 1, ('tumblr', 0.0): 1, ('shek', 0.0): 1, ('609', 0.0): 1, ('mugshot', 0.0): 1, ('attend', 0.0): 3, ('plsss', 0.0): 4, ('taissa', 0.0): 1, ('farmiga', 0.0): 1, ('robert', 0.0): 1, ('qualiti', 0.0): 1, ('daniel', 0.0): 1, ('latest', 0.0): 3, ('softwar', 0.0): 1, ('restor', 0.0): 2, ('momo', 0.0): 2, ('pharma', 0.0): 1, ('immov', 0.0): 1, ('messi', 0.0): 1, ('ansh', 0.0): 1, ('f1', 0.0): 1, ('billion', 0.0): 1, ('rand', 0.0): 1, ('bein', 0.0): 1, ('tla', 0.0): 1, ('tweng', 0.0): 1, ('gene', 0.0): 1, ('up.com', 0.0): 1, ('counti', 0.0): 2, ('cooler', 0.0): 1, ('minhyuk', 0.0): 1, ('gold', 0.0): 2, ('1900', 0.0): 1, ('😪', 0.0): 3, ('yu', 0.0): 1, ('hz', 0.0): 2, ('selena', 0.0): 2, ('emta', 0.0): 1, ('hatigii', 0.0): 1, ('b2aa', 0.0): 1, ('yayyy', 0.0): 1, ('anesthesia', 0.0): 1, ('penrith', 0.0): 1, ('emu', 0.0): 1, ('plain', 0.0): 1, ('staff', 0.0): 3, ('untouch', 0.0): 1, ('brienn', 0.0): 1, ('lsh', 0.0): 1, ('gunna', 0.0): 1, ('former', 0.0): 1, ('darn', 0.0): 1, ('allah', 0.0): 4, ('pakistan', 0.0): 2, ('juudiciari', 0.0): 1, (\"horton'\", 0.0): 1, ('dunkin', 0.0): 1, ('socialis', 0.0): 1, ('cara', 0.0): 1, (\"delevingne'\", 0.0): 1, ('fear', 0.0): 1, ('drug', 0.0): 1, ('lace', 0.0): 1, ('fank', 0.0): 1, ('takfaham', 0.0): 1, ('ufff', 0.0): 1, ('sr', 0.0): 2, ('dard', 0.0): 1, ('katekyn', 0.0): 1, ('ehh', 0.0): 1, ('yeahhh', 0.0): 2, ('hacharatt', 0.0): 1, ('niwll', 0.0): 1, ('defin', 0.0): 1, ('wit', 0.0): 2, ('goa', 0.0): 1, ('lini', 0.0): 1, ('kasi', 0.0): 3, ('rhd', 0.0): 1, ('1st', 0.0): 3, ('wae', 0.0): 1, ('subsid', 0.0): 1, ('20th', 0.0): 1, ('anniversari', 0.0): 1, ('youngja', 0.0): 1, ('harumph', 0.0): 1, ('soggi', 0.0): 1, ('weed', 0.0): 1, ('ireland', 0.0): 3, ('sakura', 0.0): 1, ('flavour', 0.0): 1, ('chokki', 0.0): 1, ('🌸', 0.0): 1, ('unavail', 0.0): 2, ('richard', 0.0): 2, ('laptop', 0.0): 2, ('satya', 0.0): 1, ('aditya', 0.0): 1, ('🍜', 0.0): 3, ('vibrat', 0.0): 1, ('an', 0.0): 2, ('cu', 0.0): 1, ('dhaka', 0.0): 1, ('jam', 0.0): 1, ('shall', 0.0): 2, ('cornetto', 0.0): 3, ('noseble', 0.0): 1, ('nintendo', 0.0): 3, ('wew', 0.0): 1, ('ramo', 0.0): 1, ('ground', 0.0): 2, ('shawn', 0.0): 1, ('mend', 0.0): 1, ('l', 0.0): 2, ('dinghi', 0.0): 1, ('skye', 0.0): 1, ('store', 0.0): 3, ('descript', 0.0): 2, ('colleagu', 0.0): 2, ('gagal', 0.0): 2, ('txt', 0.0): 1, ('sim', 0.0): 1, ('nooot', 0.0): 1, ('notch', 0.0): 1, ('tht', 0.0): 2, ('starv', 0.0): 4, ('\\U000fe196', 0.0): 1, ('pyjama', 0.0): 1, ('swifti', 0.0): 1, ('sorna', 0.0): 1, ('lurgi', 0.0): 1, ('jim', 0.0): 2, ('6gb', 0.0): 1, ('fenestoscop', 0.0): 1, ('etienn', 0.0): 1, ('bandana', 0.0): 3, ('bigger', 0.0): 2, ('vagina', 0.0): 1, ('suriya', 0.0): 1, ('dangl', 0.0): 1, ('mjhe', 0.0): 2, ('aaj', 0.0): 1, ('tak', 0.0): 3, ('kisi', 0.0): 1, ('kiya', 0.0): 1, ('eyesight', 0.0): 1, ('25x30', 0.0): 1, ('aftenoon', 0.0): 1, ('booor', 0.0): 1, ('uuu', 0.0): 1, ('boyfriend', 0.0): 8, ('freebiefriday', 0.0): 1, ('garag', 0.0): 1, ('michael', 0.0): 1, ('obvious', 0.0): 1, ('denim', 0.0): 1, ('somebodi', 0.0): 1, ('ce', 0.0): 1, ('gw', 0.0): 1, ('anatomi', 0.0): 1, ('no1', 0.0): 1, (\"morisette'\", 0.0): 1, ('flash', 0.0): 1, ('non-trial', 0.0): 1, ('sayhernam', 0.0): 1, ('lootcrat', 0.0): 1, ('item', 0.0): 1, ('inca', 0.0): 1, ('trail', 0.0): 1, ('sandboard', 0.0): 1, ('derbi', 0.0): 1, ('coffe', 0.0): 1, ('unabl', 0.0): 3, ('signatur', 0.0): 1, ('dish', 0.0): 1, ('unfamiliar', 0.0): 1, ('kitchen', 0.0): 3, ('coldest', 0.0): 1, (\"old'\", 0.0): 1, ('14518344', 0.0): 1, ('61', 0.0): 1, ('thirdwheel', 0.0): 1, ('lovebird', 0.0): 1, ('nth', 0.0): 1, ('imo', 0.0): 1, ('familiar', 0.0): 1, ('@juliettemaughan', 0.0): 1, ('copi', 0.0): 1, ('sensiesha', 0.0): 1, ('eldest', 0.0): 1, ('netbal', 0.0): 1, ('😟', 0.0): 1, ('keedz', 0.0): 1, ('taybigail', 0.0): 1, ('jordan', 0.0): 1, ('tournament', 0.0): 1, ('goin', 0.0): 1, ('ps4', 0.0): 3, ('kink', 0.0): 1, ('charger', 0.0): 1, ('streak', 0.0): 1, ('scorch', 0.0): 1, ('srski', 0.0): 1, ('tdc', 0.0): 1, ('egypt', 0.0): 1, ('in-sensit', 0.0): 1, ('cooper', 0.0): 3, ('invit', 0.0): 1, ('donna', 0.0): 1, ('thurston', 0.0): 1, ('collin', 0.0): 1, ('quietli', 0.0): 2, ('kennel', 0.0): 1, ('911', 0.0): 1, ('pluckersss', 0.0): 1, ('gion', 0.0): 1, ('886', 0.0): 1, ('nsfw', 0.0): 1, ('kidschoiceaward', 0.0): 1, ('ming', 0.0): 1, ('pbr', 0.0): 1, ('shoutout', 0.0): 1, ('periscop', 0.0): 1, ('ut', 0.0): 1, ('shawti', 0.0): 1, ('naw', 0.0): 4, (\"sterling'\", 0.0): 1, ('9muse', 0.0): 1, ('hrryok', 0.0): 2, ('asap', 0.0): 2, ('wnt', 0.0): 1, ('9:30', 0.0): 1, ('9:48', 0.0): 1, ('9/11', 0.0): 1, ('bueno', 0.0): 1, ('receptionist', 0.0): 1, ('ella', 0.0): 2, ('goe', 0.0): 4, ('ketchup', 0.0): 1, ('tasteless', 0.0): 1, ('deantd', 0.0): 1, ('justgotkanekifi', 0.0): 1, ('notgonnabeactivefor', 0.0): 1, ('2weeksdontmissittoomuch', 0.0): 1, ('2013', 0.0): 1, ('disney', 0.0): 2, ('vlog', 0.0): 1, ('swim', 0.0): 1, ('turtl', 0.0): 2, ('cnn', 0.0): 2, ('straplin', 0.0): 1, ('theatr', 0.0): 1, ('guncontrol', 0.0): 1, ('stung', 0.0): 2, ('tweak', 0.0): 1, (\"thát'\", 0.0): 1, ('powerpoint', 0.0): 1, ('present', 0.0): 5, ('diner', 0.0): 1, ('no-no', 0.0): 1, ('hind', 0.0): 1, ('circuit', 0.0): 1, ('secondari', 0.0): 1, ('sodder', 0.0): 1, ('perhap', 0.0): 2, ('mobitel', 0.0): 1, ('colin', 0.0): 1, ('playstat', 0.0): 2, ('charg', 0.0): 4, ('exp', 0.0): 1, ('misspelt', 0.0): 1, ('wan', 0.0): 1, ('hyungwon', 0.0): 2, ('alarm', 0.0): 1, ('needicecreamnow', 0.0): 1, ('shake', 0.0): 1, ('repeatedli', 0.0): 1, ('nu-uh', 0.0): 1, ('jace', 0.0): 1, ('mostest', 0.0): 1, ('vip', 0.0): 1, ('urgh', 0.0): 1, ('consol', 0.0): 1, (\"grigson'\", 0.0): 1, ('carrot', 0.0): 1, ('&gt;:-(', 0.0): 4, ('sunburn', 0.0): 1, ('ughh', 0.0): 2, ('enabl', 0.0): 1, ('otter', 0.0): 1, ('protect', 0.0): 1, ('argh', 0.0): 1, ('pon', 0.0): 1, ('otl', 0.0): 2, ('sleepov', 0.0): 2, ('jess', 0.0): 2, ('bebe', 0.0): 1, ('fabina', 0.0): 1, (\"barrista'\", 0.0): 1, ('plant', 0.0): 3, ('pup', 0.0): 2, ('brolli', 0.0): 1, ('mere', 0.0): 2, ('nhi', 0.0): 1, ('dey', 0.0): 2, ('serv', 0.0): 1, ('kepo', 0.0): 1, ('bitin', 0.0): 1, ('pretzel', 0.0): 1, ('bb17', 0.0): 1, ('bblf', 0.0): 1, ('fuckin', 0.0): 1, ('vanilla', 0.0): 1, ('latt', 0.0): 1, ('skulker', 0.0): 1, ('thread', 0.0): 1, ('hungrrryyi', 0.0): 1, ('icloud', 0.0): 1, ('ipod', 0.0): 3, ('hallyu', 0.0): 1, ('buuut', 0.0): 1, ('über', 0.0): 1, ('oki', 0.0): 2, ('8p', 0.0): 1, ('champagn', 0.0): 1, ('harlo', 0.0): 1, ('torrentialrain', 0.0): 1, ('lloyd', 0.0): 1, ('asshol', 0.0): 1, ('clearli', 0.0): 2, ('knowww', 0.0): 2, ('runni', 0.0): 1, ('sehun', 0.0): 1, ('sweater', 0.0): 1, ('intoler', 0.0): 2, ('xenophob', 0.0): 1, ('wtfff', 0.0): 1, ('tone', 0.0): 1, ('wasnt', 0.0): 1, ('1pm', 0.0): 2, ('fantasi', 0.0): 1, ('newer', 0.0): 1, ('pish', 0.0): 1, ('comparison', 0.0): 1, ('remast', 0.0): 1, ('fe14', 0.0): 1, ('icon', 0.0): 2, ('strawberri', 0.0): 1, ('loos', 0.0): 1, ('kapatidkongpogi', 0.0): 1, ('steph', 0.0): 1, ('mel', 0.0): 1, ('longest', 0.0): 1, ('carmen', 0.0): 1, ('login', 0.0): 1, ('respons', 0.0): 3, ('00128835', 0.0): 1, ('wingstop', 0.0): 1, ('budg', 0.0): 1, ('fuq', 0.0): 1, ('ilhoon', 0.0): 1, ('ganteng', 0.0): 1, ('simpl', 0.0): 1, ('getthescoop', 0.0): 1, ('hearess', 0.0): 1, ('677', 0.0): 1, ('txt_shot', 0.0): 1, ('standbi', 0.0): 1, ('inatal', 0.0): 1, ('zenmat', 0.0): 1, ('namecheck', 0.0): 1, ('whistl', 0.0): 1, ('junmyeon', 0.0): 1, ('ddi', 0.0): 1, ('arini', 0.0): 1, ('je', 0.0): 1, ('bright', 0.0): 2, ('igbo', 0.0): 1, ('blamehoney', 0.0): 1, ('whhr', 0.0): 1, ('juan', 0.0): 1, ('snuggl', 0.0): 1, ('internship', 0.0): 1, ('usag', 0.0): 1, ('warn', 0.0): 1, ('vertigo', 0.0): 1, ('panic', 0.0): 1, ('attack', 0.0): 4, ('dual', 0.0): 1, ('carriageway', 0.0): 1, ('aragalang', 0.0): 1, ('08', 0.0): 1, ('tam', 0.0): 1, ('bose', 0.0): 1, ('theo', 0.0): 1, ('anymoree', 0.0): 1, ('rubbish', 0.0): 1, ('cactu', 0.0): 1, ('sorrri', 0.0): 1, ('bowel', 0.0): 1, ('nasti', 0.0): 2, ('tumour', 0.0): 1, ('faster', 0.0): 1, ('puffi', 0.0): 1, ('eyelid', 0.0): 1, ('musica', 0.0): 1, ('dota', 0.0): 1, ('4am', 0.0): 1, ('campsit', 0.0): 1, ('miah', 0.0): 1, ('hahay', 0.0): 1, ('churro', 0.0): 1, ('montana', 0.0): 2, ('reign', 0.0): 1, ('exampl', 0.0): 1, ('inflat', 0.0): 1, ('sic', 0.0): 1, ('reset', 0.0): 1, ('entlerbountli', 0.0): 1, ('tinder', 0.0): 3, ('dirtykik', 0.0): 2, ('sexcam', 0.0): 3, ('spray', 0.0): 1, ('industri', 0.0): 1, ('swollen', 0.0): 1, ('distanc', 0.0): 2, ('jojo', 0.0): 1, ('postcod', 0.0): 1, ('kafi', 0.0): 1, ('din', 0.0): 1, ('mene', 0.0): 1, ('aj', 0.0): 1, ('koi', 0.0): 1, ('rewert', 0.0): 1, ('bunta', 0.0): 1, ('warnaaa', 0.0): 1, ('tortur', 0.0): 2, ('field', 0.0): 1, ('wall', 0.0): 2, ('iran', 0.0): 1, ('irand', 0.0): 1, ('us-iran', 0.0): 1, ('nuclear', 0.0): 1, (\"mit'\", 0.0): 1, ('expert', 0.0): 1, ('sever', 0.0): 3, ('li', 0.0): 1, ('s2e12', 0.0): 1, ('rumpi', 0.0): 1, ('gallon', 0.0): 1, ('ryan', 0.0): 1, ('secret', 0.0): 2, ('dandia', 0.0): 1, ('rbi', 0.0): 1, ('cage', 0.0): 2, ('parrot', 0.0): 1, ('1li', 0.0): 1, ('commiss', 0.0): 1, ('cag', 0.0): 1, ('stripe', 0.0): 2, ('gujarat', 0.0): 1, ('tear', 0.0): 3, ('ily.melani', 0.0): 1, ('unlik', 0.0): 2, ('talent', 0.0): 2, ('deepxcap', 0.0): 1, ('doin', 0.0): 3, ('5:08', 0.0): 1, ('thesi', 0.0): 11, ('belieb', 0.0): 2, ('gtg', 0.0): 1, ('compet', 0.0): 1, ('vv', 0.0): 1, ('respect', 0.0): 5, ('opt-out', 0.0): 1, ('vam', 0.0): 1, ('spece', 0.0): 1, ('ell', 0.0): 1, ('articl', 0.0): 1, ('sexyameli', 0.0): 1, ('fineandyu', 0.0): 1, ('gd', 0.0): 1, ('flesh', 0.0): 1, ('daft', 0.0): 1, ('imsorri', 0.0): 1, ('aku', 0.0): 1, ('chelsea', 0.0): 2, ('koe', 0.0): 1, ('emyu', 0.0): 1, ('confetti', 0.0): 1, ('bf', 0.0): 2, ('sini', 0.0): 1, ('dipoppo', 0.0): 1, ('hop', 0.0): 2, ('bestweekend', 0.0): 1, ('okay-ish', 0.0): 1, ('html', 0.0): 1, ('geneva', 0.0): 1, ('patml', 0.0): 1, ('482', 0.0): 1, ('orgasm', 0.0): 3, ('abouti', 0.0): 1, ('797', 0.0): 1, ('reaalli', 0.0): 1, ('aldub', 0.0): 1, ('nila', 0.0): 1, ('smart', 0.0): 1, ('meter', 0.0): 1, ('display', 0.0): 1, ('unansw', 0.0): 1, ('bri', 0.0): 1, ('magcon', 0.0): 1, ('sinuend', 0.0): 1, ('kak', 0.0): 1, ('laper', 0.0): 2, ('rage', 0.0): 1, ('loser', 0.0): 1, ('brendon', 0.0): 1, (\"urie'\", 0.0): 1, ('sumer', 0.0): 1, ('repackag', 0.0): 1, (\":'d\", 0.0): 1, ('matthew', 0.0): 1, ('yongb', 0.0): 1, ('sued', 0.0): 1, ('suprem', 0.0): 1, ('warm-up', 0.0): 1, ('arriv', 0.0): 4, ('brill', 0.0): 1, ('120', 0.0): 1, ('rub', 0.0): 1, ('belli', 0.0): 1, ('jannatul', 0.0): 1, ('ferdou', 0.0): 1, ('ekta', 0.0): 1, ('kharap', 0.0): 1, ('manush', 0.0): 1, ('mart', 0.0): 2, ('gua', 0.0): 1, ('can', 0.0): 1, (\"khloe'\", 0.0): 1, ('nhe', 0.0): 1, ('yar', 0.0): 1, ('minkyuk', 0.0): 1, ('hol', 0.0): 1, ('isol', 0.0): 1, ('hk', 0.0): 1, ('sensor', 0.0): 1, ('broker', 0.0): 1, ('wna', 0.0): 1, ('flaviana', 0.0): 1, ('chickmt', 0.0): 1, ('123', 0.0): 1, ('letsfootbal', 0.0): 2, ('atk', 0.0): 2, ('greymind', 0.0): 2, ('43', 0.0): 2, ('gayl', 0.0): 2, ('cricket', 0.0): 3, ('2-3', 0.0): 2, ('mood-dump', 0.0): 1, ('livestream', 0.0): 1, ('gotten', 0.0): 1, ('felton', 0.0): 1, ('veriti', 0.0): 1, (\"standen'\", 0.0): 1, ('shortli', 0.0): 1, ('😆', 0.0): 2, ('takoyaki', 0.0): 1, ('piti', 0.0): 1, ('aisyah', 0.0): 1, ('ffvi', 0.0): 1, ('youtu.be/2_gpctsojkw', 0.0): 1, ('donutsss', 0.0): 1, ('50p', 0.0): 1, ('grate', 0.0): 1, ('spars', 0.0): 1, ('dd', 0.0): 1, ('lagi', 0.0): 1, ('rider', 0.0): 1, ('pride', 0.0): 1, ('hueee', 0.0): 1, ('password', 0.0): 1, ('thingi', 0.0): 1, ('georg', 0.0): 1, ('afraid', 0.0): 2, ('chew', 0.0): 2, ('toy', 0.0): 1, ('stella', 0.0): 1, ('threw', 0.0): 2, ('theaccidentalcoupl', 0.0): 1, ('smooth', 0.0): 1, ('handov', 0.0): 1, ('spick', 0.0): 1, ('bebii', 0.0): 1, ('happenend', 0.0): 1, ('dr', 0.0): 1, ('balm', 0.0): 1, ('hmph', 0.0): 1, ('bubba', 0.0): 2, ('floor', 0.0): 3, ('georgi', 0.0): 1, ('oi', 0.0): 1, ('bengali', 0.0): 1, ('masterchef', 0.0): 1, ('whatchya', 0.0): 1, ('petrol', 0.0): 1, ('diesel', 0.0): 1, ('wardrob', 0.0): 1, ('awe', 0.0): 1, ('cock', 0.0): 1, ('nyquil', 0.0): 1, ('poootek', 0.0): 1, ('1,500', 0.0): 1, ('bobbl', 0.0): 1, ('leak', 0.0): 1, ('thermo', 0.0): 1, ('classic', 0.0): 1, ('ti5', 0.0): 1, ('12th', 0.0): 1, ('skate', 0.0): 1, ('tae', 0.0): 1, ('kita', 0.0): 4, ('ia', 0.0): 1, ('pkwalasawa', 0.0): 1, ('india', 0.0): 1, ('corrupt', 0.0): 2, ('access', 0.0): 2, ('anything.sur', 0.0): 1, ('info', 0.0): 6, ('octob', 0.0): 1, ('mubank', 0.0): 2, ('ene', 0.0): 2, ('3k', 0.0): 1, ('zehr', 0.0): 1, ('khani', 0.0): 1, ('groceri', 0.0): 1, ('hubba', 0.0): 1, ('bubbl', 0.0): 1, ('gum', 0.0): 2, ('closet', 0.0): 1, ('jhalak', 0.0): 1, ('. ..', 0.0): 2, ('bakwa', 0.0): 1, ('. ...', 0.0): 1, ('seehiah', 0.0): 1, ('goy', 0.0): 1, ('nacho', 0.0): 1, ('braid', 0.0): 2, ('initi', 0.0): 1, ('ruth', 0.0): 1, ('boong', 0.0): 1, ('recommend', 0.0): 3, ('gta', 0.0): 1, ('cwnt', 0.0): 1, ('trivia', 0.0): 1, ('belat', 0.0): 1, ('rohingya', 0.0): 1, ('muslim', 0.0): 2, ('indict', 0.0): 1, ('traffick', 0.0): 1, ('thailand', 0.0): 1, ('asia', 0.0): 1, ('rumbl', 0.0): 1, ('kumbl', 0.0): 1, ('scold', 0.0): 1, ('phrase', 0.0): 1, ('includ', 0.0): 1, ('tag', 0.0): 2, ('melt', 0.0): 1, ('tfw', 0.0): 1, ('jest', 0.0): 1, ('offend', 0.0): 2, ('sleepingwithsiren', 0.0): 1, ('17th', 0.0): 1, ('bringmethehorizon', 0.0): 1, ('18th', 0.0): 2, ('carva', 0.0): 1, ('regularli', 0.0): 2, ('sympathi', 0.0): 1, ('revamp', 0.0): 1, ('headphon', 0.0): 1, ('cunt', 0.0): 1, ('wacha', 0.0): 1, ('niend', 0.0): 1, ('bravo', 0.0): 1, ('2hr', 0.0): 1, ('13m', 0.0): 1, ('kk', 0.0): 2, ('calibraksaep', 0.0): 2, ('darlin', 0.0): 1, ('stun', 0.0): 1, (\"doedn't\", 0.0): 1, ('meaning', 0.0): 1, ('horrif', 0.0): 2, ('scoup', 0.0): 2, ('paypal', 0.0): 3, ('sweedi', 0.0): 1, ('nam', 0.0): 1, (\"sacconejoly'\", 0.0): 1, ('bethesda', 0.0): 1, ('fallout', 0.0): 1, ('minecon', 0.0): 1, ('perfect', 0.0): 2, ('katee', 0.0): 1, ('iloveyouu', 0.0): 1, ('linux', 0.0): 1, ('nawww', 0.0): 1, ('chikka', 0.0): 1, ('ug', 0.0): 1, ('rata', 0.0): 1, ('soonest', 0.0): 1, ('mwamwa', 0.0): 1, ('faggot', 0.0): 1, ('doubt', 0.0): 2, ('fyi', 0.0): 1, ('profil', 0.0): 1, ('nicest', 0.0): 1, ('mehendi', 0.0): 1, ('dash', 0.0): 1, ('bookmark', 0.0): 1, ('whay', 0.0): 1, ('shaa', 0.0): 1, ('prami', 0.0): 1, ('😚', 0.0): 4, ('ngee', 0.0): 1, ('ann', 0.0): 1, ('crikey', 0.0): 2, ('snit', 0.0): 1, ('nathanielhinanakit', 0.0): 1, ('naya', 0.0): 1, ('spinni', 0.0): 1, ('wheel', 0.0): 2, ('albeit', 0.0): 1, ('athlet', 0.0): 1, ('gfriend', 0.0): 2, ('yung', 0.0): 2, ('fugli', 0.0): 1, ('💞', 0.0): 4, ('jongda', 0.0): 1, ('hardli', 0.0): 2, ('tlist', 0.0): 1, ('budget', 0.0): 1, ('pabebegirl', 0.0): 1, ('pabeb', 0.0): 2, ('alter', 0.0): 1, ('sandra', 0.0): 2, ('bland', 0.0): 2, ('storifi', 0.0): 1, ('abbi', 0.0): 2, ('mtvhottest', 0.0): 1, ('gaga', 0.0): 1, ('rib', 0.0): 1, ('😵', 0.0): 1, ('hulkamania', 0.0): 1, ('unlov', 0.0): 1, ('lazi', 0.0): 3, ('ihhh', 0.0): 1, ('stackar', 0.0): 1, ('basil', 0.0): 1, ('remedi', 0.0): 1, ('ov', 0.0): 2, ('raiz', 0.0): 1, ('nvr', 0.0): 1, ('gv', 0.0): 1, ('up.wt', 0.0): 1, ('wt', 0.0): 1, ('imran', 0.0): 2, ('achiev', 0.0): 1, ('thr', 0.0): 1, ('soln', 0.0): 1, (\"sister'\", 0.0): 1, ('hong', 0.0): 1, ('kong', 0.0): 1, ('31st', 0.0): 1, ('pipe', 0.0): 1, ('sept', 0.0): 2, ('lawn', 0.0): 1, (\"cupid'\", 0.0): 1, ('torn', 0.0): 1, ('retain', 0.0): 1, ('clown', 0.0): 2, ('lipstick', 0.0): 1, ('haiss', 0.0): 1, ('todayi', 0.0): 1, ('thoo', 0.0): 1, ('everday', 0.0): 1, ('hangout', 0.0): 2, ('steven', 0.0): 2, ('william', 0.0): 1, ('umboh', 0.0): 1, ('goodafternoon', 0.0): 1, ('jadin', 0.0): 1, ('thiz', 0.0): 1, ('iz', 0.0): 1, ('emeg', 0.0): 1, ('kennat', 0.0): 1, ('reunit', 0.0): 1, ('abi', 0.0): 1, ('arctic', 0.0): 1, ('chicsirif', 0.0): 1, ('structur', 0.0): 1, ('cumbia', 0.0): 1, ('correct', 0.0): 1, ('badlif', 0.0): 1, ('4-5', 0.0): 2, ('kaslkdja', 0.0): 1, ('3wk', 0.0): 1, ('flower', 0.0): 1, ('feverfew', 0.0): 1, ('weddingflow', 0.0): 1, ('diyflow', 0.0): 1, ('fitn', 0.0): 1, ('worth', 0.0): 4, ('wolverin', 0.0): 1, ('khan', 0.0): 1, ('innoc', 0.0): 1, ('🙏🏻', 0.0): 1, ('🎂', 0.0): 2, ('memem', 0.0): 2, ('krystoria', 0.0): 1, ('snob', 0.0): 1, ('zumba', 0.0): 1, ('greekcrisi', 0.0): 1, ('remain', 0.0): 1, ('dutch', 0.0): 1, ('legibl', 0.0): 2, ('isra', 0.0): 1, ('passport', 0.0): 1, ('froze', 0.0): 1, ('theori', 0.0): 1, ('23rd', 0.0): 1, ('24th', 0.0): 1, ('stomachach', 0.0): 1, ('slice', 0.0): 1, ('ཀ', 0.0): 1, ('again', 0.0): 1, ('otani', 0.0): 1, ('3-0', 0.0): 1, ('3rd', 0.0): 3, ('bottom', 0.0): 2, ('niaaa', 0.0): 1, ('2/4', 0.0): 1, ('scheme', 0.0): 2, ('fckin', 0.0): 1, ('hii', 0.0): 1, ('vin', 0.0): 1, ('plss', 0.0): 1, ('rpli', 0.0): 1, ('rat', 0.0): 3, ('bollywood', 0.0): 1, ('mac', 0.0): 1, ('backup', 0.0): 2, ('lune', 0.0): 1, ('robinhood', 0.0): 1, ('robinhoodi', 0.0): 1, ('🚙', 0.0): 1, ('💚', 0.0): 1, ('docopenhagen', 0.0): 1, ('setter', 0.0): 1, ('swipe', 0.0): 1, ('bbygurl', 0.0): 1, ('neil', 0.0): 1, ('caribbean', 0.0): 1, ('6yr', 0.0): 1, ('jabongatpumaurbanstamped', 0.0): 2, ('takraw', 0.0): 1, ('fersure', 0.0): 1, ('angi', 0.0): 1, ('sheriff', 0.0): 1, ('aaag', 0.0): 1, (\"i'mo\", 0.0): 1, ('sulk', 0.0): 1, ('selfish', 0.0): 1, ('trick', 0.0): 2, ('nonc', 0.0): 1, ('pad', 0.0): 1, ('bison', 0.0): 1, ('motiv', 0.0): 2, (\"q'don\", 0.0): 1, ('cheat', 0.0): 2, ('stomp', 0.0): 1, ('aaaaaaaaah', 0.0): 1, ('kany', 0.0): 1, ('mama', 0.0): 1, ('jdjdjdjd', 0.0): 1, (\"jimin'\", 0.0): 1, ('fancaf', 0.0): 1, ('waffl', 0.0): 1, ('87.7', 0.0): 1, ('2fm', 0.0): 1, ('himseek', 0.0): 1, ('kissm', 0.0): 1, ('akua', 0.0): 1, ('glo', 0.0): 1, ('cori', 0.0): 1, ('monteith', 0.0): 1, ('often', 0.0): 1, ('hashbrown', 0.0): 1, ('💘', 0.0): 2, ('pg', 0.0): 1, ('msc', 0.0): 1, ('hierro', 0.0): 1, ('shirleycam', 0.0): 1, ('phonesex', 0.0): 2, ('pal', 0.0): 1, ('111', 0.0): 1, ('gilet', 0.0): 1, ('cheek', 0.0): 1, ('squishi', 0.0): 1, ('lahhh', 0.0): 1, ('eon', 0.0): 1, ('sunris', 0.0): 1, ('beeti', 0.0): 1, ('697', 0.0): 1, ('kikkomansabor', 0.0): 1, ('getaway', 0.0): 1, ('crimin', 0.0): 1, ('amiibo', 0.0): 1, ('batman', 0.0): 1, ('habe', 0.0): 1, ('siannn', 0.0): 1, ('march', 0.0): 1, ('2017', 0.0): 1, ('chuckin', 0.0): 1, ('ampsha', 0.0): 1, ('nia', 0.0): 1, ('strap', 0.0): 1, ('dz9055', 0.0): 1, ('entlead', 0.0): 1, ('590', 0.0): 1, ('twice', 0.0): 5, ('07:02', 0.0): 1, ('ifsc', 0.0): 1, ('mayor', 0.0): 1, ('biodivers', 0.0): 1, ('taxonom', 0.0): 1, ('collabor', 0.0): 1, ('speci', 0.0): 1, ('discoveri', 0.0): 1, ('collar', 0.0): 1, ('3:03', 0.0): 1, ('belt', 0.0): 1, ('smith', 0.0): 2, ('eyelin', 0.0): 1, ('therefor', 0.0): 1, ('netherland', 0.0): 1, ('el', 0.0): 1, ('jeb', 0.0): 1, ('blacklivesmatt', 0.0): 1, ('slogan', 0.0): 1, ('msnbc', 0.0): 1, ('jebbush', 0.0): 1, ('famish', 0.0): 1, ('marino', 0.0): 1, ('qualifi', 0.0): 2, ('suzi', 0.0): 1, ('skirt', 0.0): 1, ('tama', 0.0): 1, ('warrior', 0.0): 2, ('wound', 0.0): 1, ('iraq', 0.0): 1, ('be', 0.0): 2, ('camara', 0.0): 1, ('coveral', 0.0): 1, ('happili', 0.0): 1, ('sneezi', 0.0): 1, ('rogerwatch', 0.0): 1, ('stalker', 0.0): 1, ('velvet', 0.0): 1, ('tradit', 0.0): 1, (\"people'\", 0.0): 1, ('beheaviour', 0.0): 1, (\"robert'\", 0.0): 1, ('.\\n.', 0.0): 2, ('aaron', 0.0): 1, ('jelous', 0.0): 1, ('mtg', 0.0): 1, ('thoughtseiz', 0.0): 1, ('playabl', 0.0): 1, ('oldi', 0.0): 1, ('goodi', 0.0): 1, ('mcg', 0.0): 1, ('inspirit', 0.0): 1, ('shine', 0.0): 1, ('ise', 0.0): 1, ('assum', 0.0): 2, ('waist', 0.0): 2, ('guin', 0.0): 1, ('venu', 0.0): 1, ('evil', 0.0): 1, ('pepper', 0.0): 1, ('thessidew', 0.0): 1, ('877', 0.0): 1, ('genesi', 0.0): 1, ('mexico', 0.0): 2, ('novemb', 0.0): 1, ('mash', 0.0): 1, ('whattsap', 0.0): 1, ('inuyasha', 0.0): 2, ('outfwith', 0.0): 1, ('myungsoo', 0.0): 1, ('organis', 0.0): 1, ('satisfi', 0.0): 1, ('wah', 0.0): 1, ('challo', 0.0): 1, ('pliss', 0.0): 1, ('juliana', 0.0): 1, ('enrol', 0.0): 1, ('darlen', 0.0): 1, ('emoji', 0.0): 2, ('brisban', 0.0): 1, ('merlin', 0.0): 1, ('nawwwe', 0.0): 1, ('hyperbulli', 0.0): 1, ('tong', 0.0): 1, ('nga', 0.0): 1, ('seatmat', 0.0): 1, ('rajud', 0.0): 1, ('barkada', 0.0): 1, ('ore', 0.0): 1, ('kayla', 0.0): 1, ('ericavan', 0.0): 1, ('jong', 0.0): 1, ('dongwoo', 0.0): 1, ('photocard', 0.0): 1, ('wh', 0.0): 1, ('dw', 0.0): 1, ('tumor', 0.0): 1, ('vivian', 0.0): 1, ('mmsmalubhangsakit', 0.0): 1, ('jillcruz', 0.0): 2, ('lgbt', 0.0): 3, ('qt', 0.0): 1, ('19th', 0.0): 1, ('toss', 0.0): 1, ('co-work', 0.0): 1, ('mia', 0.0): 1, ('push', 0.0): 4, ('dare', 0.0): 2, ('unsettl', 0.0): 1, ('gh', 0.0): 1, ('18c', 0.0): 1, ('rlli', 0.0): 2, ('hamster', 0.0): 2, ('sheeran', 0.0): 2, ('preform', 0.0): 2, ('monash', 0.0): 1, ('hitmark', 0.0): 1, ('glitch', 0.0): 1, ('safaa', 0.0): 1, (\"selena'\", 0.0): 1, ('galat', 0.0): 1, ('tum', 0.0): 1, ('ab', 0.0): 5, ('non', 0.0): 1, ('lrka', 0.0): 1, ('bna', 0.0): 1, ('kia', 0.0): 1, ('bhook', 0.0): 1, ('jai', 0.0): 1, ('social', 0.0): 2, ('afterschool', 0.0): 1, ('bilal', 0.0): 1, ('ashraf', 0.0): 1, ('icu', 0.0): 1, ('thanksss', 0.0): 1, ('annnd', 0.0): 1, ('winchest', 0.0): 1, ('{:', 0.0): 1, ('grepe', 0.0): 1, ('grepein', 0.0): 1, ('panem', 0.0): 1, ('lover', 0.0): 1, ('sulli', 0.0): 1, ('cpm', 0.0): 1, ('condemn', 0.0): 1, ('✔', 0.0): 1, ('occur', 0.0): 1, ('unagi', 0.0): 1, ('7elw', 0.0): 1, ('mesh', 0.0): 1, ('beyt', 0.0): 1, ('3a2ad', 0.0): 1, ('fluent', 0.0): 1, ('varsiti', 0.0): 1, ('sengenza', 0.0): 1, ('context', 0.0): 1, ('movnat', 0.0): 1, ('yield', 0.0): 1, ('nbhero', 0.0): 1, (\"it'd\", 0.0): 1, ('background', 0.0): 1, ('agov', 0.0): 1, ('brasileirao', 0.0): 2, ('abus', 0.0): 1, ('unpar', 0.0): 1, ('bianca', 0.0): 1, ('bun', 0.0): 1, ('dislik', 0.0): 1, ('burdensom', 0.0): 1, ('clear', 0.0): 2, ('amelia', 0.0): 1, ('melon', 0.0): 2, ('useless', 0.0): 1, ('soccer', 0.0): 2, ('interview', 0.0): 2, ('thursday', 0.0): 1, ('nevermind', 0.0): 1, ('jeon', 0.0): 1, ('claw', 0.0): 1, ('thigh', 0.0): 2, ('traction', 0.0): 1, ('damnit', 0.0): 1, ('pri', 0.0): 1, ('pv', 0.0): 2, ('reliv', 0.0): 1, ('nyc', 0.0): 2, ('klm', 0.0): 1, ('11am', 0.0): 1, (\"mcd'\", 0.0): 1, ('hung', 0.0): 1, ('bam', 0.0): 1, ('seventh', 0.0): 1, ('splendour', 0.0): 1, ('swedish', 0.0): 1, ('metal', 0.0): 1, ('häirførc', 0.0): 1, ('givecodpieceach', 0.0): 1, ('alic', 0.0): 3, ('stile', 0.0): 1, ('explain', 0.0): 3, ('ili', 0.0): 1, ('pragu', 0.0): 1, ('sadi', 0.0): 1, ('charact', 0.0): 1, ('915', 0.0): 1, ('hayee', 0.0): 2, ('patwari', 0.0): 1, ('mam', 0.0): 1, (\"ik'\", 0.0): 1, ('vision', 0.0): 2, ('ga', 0.0): 1, ('awhhh', 0.0): 1, ('nalang', 0.0): 1, ('hehe', 0.0): 1, ('albanian', 0.0): 1, ('curs', 0.0): 2, ('tava', 0.0): 1, ('chara', 0.0): 1, ('teteh', 0.0): 1, ('verri', 0.0): 1, ('shatter', 0.0): 2, ('sb', 0.0): 1, ('nawe', 0.0): 1, ('bulldog', 0.0): 1, ('macho', 0.0): 1, ('puriti', 0.0): 1, ('kwento', 0.0): 1, ('nakakapikon', 0.0): 1, ('nagbabasa', 0.0): 1, ('blog', 0.0): 2, ('cancer', 0.0): 1, (':-\\\\', 0.0): 1, ('jonatha', 0.0): 4, ('beti', 0.0): 4, ('sogok', 0.0): 1, ('premium', 0.0): 2, ('instrument', 0.0): 1, ('howev', 0.0): 1, ('dastardli', 0.0): 1, ('swine', 0.0): 1, ('envelop', 0.0): 1, ('pipol', 0.0): 1, ('tad', 0.0): 1, ('wiper', 0.0): 2, ('supposedli', 0.0): 1, ('kernel', 0.0): 1, ('intel', 0.0): 1, ('mega', 0.0): 1, ('bent', 0.0): 1, ('socket', 0.0): 1, ('pcgame', 0.0): 1, ('pcupgrad', 0.0): 1, ('brainwash', 0.0): 2, ('smosh', 0.0): 1, ('plawnew', 0.0): 1, ('837', 0.0): 1, ('aswel', 0.0): 1, ('litter', 0.0): 1, ('mensch', 0.0): 1, ('sepanx', 0.0): 1, ('pci', 0.0): 1, ('caerphilli', 0.0): 1, ('omw', 0.0): 1, ('😍', 0.0): 1, ('hahdhdhshh', 0.0): 1, ('growinguppoor', 0.0): 1, ('🇺🇸', 0.0): 2, (\"bangtan'\", 0.0): 1, ('taimoor', 0.0): 1, ('meray', 0.0): 1, ('dost', 0.0): 1, ('tya', 0.0): 1, ('refollow', 0.0): 1, ('dumb', 0.0): 2, ('butt', 0.0): 1, ('pissbabi', 0.0): 1, ('plank', 0.0): 1, ('inconsist', 0.0): 1, ('moor', 0.0): 1, ('bin', 0.0): 1, ('osx', 0.0): 1, ('chrome', 0.0): 1, ('voiceov', 0.0): 1, ('devo', 0.0): 1, ('hulkhogan', 0.0): 1, ('unpleas', 0.0): 1, ('daaamn', 0.0): 1, ('dada', 0.0): 1, ('fulli', 0.0): 1, ('spike', 0.0): 1, (\"panic'\", 0.0): 1, ('22nd', 0.0): 1, ('south', 0.0): 2, ('africa', 0.0): 2, ('190', 0.0): 2, ('lizardz', 0.0): 1, ('deepli', 0.0): 1, ('emerg', 0.0): 1, ('engin', 0.0): 1, ('dormtel', 0.0): 1, ('scho', 0.0): 1, ('siya', 0.0): 1, ('onee', 0.0): 1, ('carri', 0.0): 1, ('7pm', 0.0): 1, ('feta', 0.0): 1, ('blaaaz', 0.0): 1, ('nausea', 0.0): 1, ('awar', 0.0): 1, ('top-up', 0.0): 1, ('sharknado', 0.0): 1, ('erni', 0.0): 1, ('ezoo', 0.0): 1, ('lilybutl', 0.0): 1, ('seduc', 0.0): 2, ('powai', 0.0): 1, ('neighbor', 0.0): 1, ('delhi', 0.0): 1, ('unsaf', 0.0): 1, ('halo', 0.0): 1, ('fred', 0.0): 1, ('gaon', 0.0): 1, ('infnt', 0.0): 1, ('elig', 0.0): 1, ('acub', 0.0): 1, (\"why'd\", 0.0): 1, ('bullshit', 0.0): 2, ('hanaaa', 0.0): 1, ('jn', 0.0): 1, ('tau', 0.0): 1, ('basta', 0.0): 1, ('sext', 0.0): 1, ('addm', 0.0): 1, ('hotmusicdeloco', 0.0): 2, ('dhi', 0.0): 1, ('👉', 0.0): 1, ('8ball', 0.0): 1, ('fakmarey', 0.0): 1, ('doo', 0.0): 2, ('six', 0.0): 3, ('flag', 0.0): 1, ('fulltim', 0.0): 1, ('awkward', 0.0): 1, ('beet', 0.0): 1, ('juic', 0.0): 1, ('dci', 0.0): 1, ('granddad', 0.0): 1, ('minion', 0.0): 3, ('bucket', 0.0): 1, ('kapan', 0.0): 1, ('udah', 0.0): 1, ('dihapu', 0.0): 1, ('hilang', 0.0): 1, ('dari', 0.0): 1, ('muka', 0.0): 1, ('bumi', 0.0): 1, ('narrow', 0.0): 1, ('gona', 0.0): 2, ('chello', 0.0): 1, ('gate', 0.0): 1, ('guard', 0.0): 1, ('crepe', 0.0): 1, ('forsaken', 0.0): 1, ('kanin', 0.0): 1, ('hypixel', 0.0): 1, ('grrr', 0.0): 1, ('thestruggleisr', 0.0): 1, ('geek', 0.0): 1, ('gamer', 0.0): 2, ('afterbirth', 0.0): 1, (\"apink'\", 0.0): 1, ('overperhatian', 0.0): 1, ('son', 0.0): 1, ('pox', 0.0): 1, ('ahm', 0.0): 1, ('karli', 0.0): 1, ('kloss', 0.0): 1, ('goofi', 0.0): 1, ('pcd', 0.0): 1, ('antagonis', 0.0): 1, ('writer', 0.0): 1, ('nudg', 0.0): 1, ('delv', 0.0): 1, ('grandad', 0.0): 1, (\"gray'\", 0.0): 1, ('followk', 0.0): 1, ('suggest', 0.0): 2, ('pace', 0.0): 1, ('maker', 0.0): 1, ('molli', 0.0): 1, ('higher', 0.0): 1, ('ceremoni', 0.0): 1, ('christin', 0.0): 1, ('moodi', 0.0): 1, ('throwback', 0.0): 1, ('fav', 0.0): 3, ('barb', 0.0): 1, ('creasi', 0.0): 1, ('deputi', 0.0): 1, ('tast', 0.0): 1, (\"banana'\", 0.0): 1, ('saludo', 0.0): 1, ('dissapoint', 0.0): 1, ('😫', 0.0): 1, ('&lt;--', 0.0): 1, (\"bae'\", 0.0): 1, ('pimpl', 0.0): 2, ('amount', 0.0): 2, ('tdi', 0.0): 1, ('pamela', 0.0): 1, ('mini', 0.0): 1, ('mast', 0.0): 1, ('intermitt', 0.0): 1, ('servic', 0.0): 3, ('janniecam', 0.0): 1, ('musicbiz', 0.0): 1, ('braxton', 0.0): 1, ('pro', 0.0): 2, ('urban', 0.0): 1, ('unpreced', 0.0): 1, ('tebow', 0.0): 1, ('okaaay', 0.0): 1, ('sayanggg', 0.0): 1, ('housework', 0.0): 1, ('bust', 0.0): 2, ('disneyland', 0.0): 1, ('thoma', 0.0): 1, ('tommyy', 0.0): 1, ('billi', 0.0): 1, ('kevin', 0.0): 1, ('clifton', 0.0): 1, ('strictli', 0.0): 1, ('nsc', 0.0): 1, ('mat', 0.0): 1, ('0', 0.0): 1, ('awhh', 0.0): 1, ('ram', 0.0): 2, ('voucher', 0.0): 1, ('smadvow', 0.0): 1, ('544', 0.0): 1, ('acdc', 0.0): 1, ('aker', 0.0): 1, ('gmail', 0.0): 1, ('sprevelink', 0.0): 1, ('633', 0.0): 1, ('lana', 0.0): 2, ('loveyoutilltheendcart', 0.0): 1, ('sfv', 0.0): 1, ('6/7', 0.0): 1, ('winner', 0.0): 1, ('20/1', 0.0): 1, ('david', 0.0): 1, ('rosi', 0.0): 1, ('hayoung', 0.0): 1, ('nlb', 0.0): 1, ('@_', 0.0): 1, ('tayo', 0.0): 1, ('forth', 0.0): 1, ('suspect', 0.0): 1, ('mening', 0.0): 1, ('viral', 0.0): 1, ('tonsil', 0.0): 1, ('😷', 0.0): 1, ('😝', 0.0): 1, ('babyy', 0.0): 2, ('cushion', 0.0): 1, ('😿', 0.0): 1, ('💓', 0.0): 2, ('weigh', 0.0): 1, ('keen', 0.0): 1, ('petrofac', 0.0): 1, (';-)', 0.0): 1, ('wig', 0.0): 1, (\"mark'\", 0.0): 1, ('pathet', 0.0): 1, ('burden.say', 0.0): 1, ('itchi', 0.0): 1, ('cheaper', 0.0): 1, ('malaysia', 0.0): 1, ('130', 0.0): 1, ('snapchattimg', 0.0): 1, ('😏', 0.0): 4, ('sin', 0.0): 1, ('lor', 0.0): 1, ('dedic', 0.0): 1, ('worriedli', 0.0): 1, ('stare', 0.0): 1, ('toneadi', 0.0): 1, ('46532', 0.0): 1, ('snapdirti', 0.0): 1, ('sheskindahot', 0.0): 1, ('corps', 0.0): 1, ('taeni', 0.0): 1, ('fyeah', 0.0): 1, ('andromeda', 0.0): 1, ('yunni', 0.0): 1, ('whdjwksja', 0.0): 1, ('ziam', 0.0): 1, ('100k', 0.0): 1, ('spoil', 0.0): 1, ('curtain', 0.0): 1, ('watchabl', 0.0): 1, ('migrin', 0.0): 1, ('gdce', 0.0): 1, ('gamescom', 0.0): 1, (\"do't\", 0.0): 1, ('parcel', 0.0): 1, ('num', 0.0): 1, ('oooouch', 0.0): 1, ('pinki', 0.0): 1, ('👣', 0.0): 1, ('podiatrist', 0.0): 1, ('gusto', 0.0): 1, (\"rodic'\", 0.0): 1, (\"one'\", 0.0): 1, ('adoohh', 0.0): 1, ('b-butt', 0.0): 1, ('tigermilk', 0.0): 1, ('east', 0.0): 1, ('dulwich', 0.0): 1, ('intens', 0.0): 1, ('kagami', 0.0): 1, ('kuroko', 0.0): 1, ('sana', 0.0): 2, ('makita', 0.0): 1, ('spooki', 0.0): 1, ('smol', 0.0): 1, ('bean', 0.0): 1, ('fagan', 0.0): 1, ('meadowhal', 0.0): 1, ('lola', 0.0): 1, ('nadalaw', 0.0): 1, ('labyu', 0.0): 1, ('jot', 0.0): 1, ('ivypowel', 0.0): 1, ('homeslic', 0.0): 1, ('33', 0.0): 2, ('emoticon', 0.0): 2, ('eyebrow', 0.0): 1, ('prettylook', 0.0): 1, ('whitney', 0.0): 1, ('houston', 0.0): 1, ('aur', 0.0): 1, ('shamil', 0.0): 1, ('tonn', 0.0): 1, ('statu', 0.0): 1, ('→', 0.0): 1, ('suddenli', 0.0): 2, ('alli', 0.0): 2, ('wrap', 0.0): 1, ('neck', 0.0): 1, ('heartbroken', 0.0): 1, ('chover', 0.0): 1, ('cebu', 0.0): 1, ('lechon', 0.0): 1, ('kitten', 0.0): 2, ('jannygreen', 0.0): 2, ('suicid', 0.0): 2, ('forgiv', 0.0): 1, ('conno', 0.0): 1, ('brooo', 0.0): 1, ('rout', 0.0): 1, ('lovebox', 0.0): 1, ('prod', 0.0): 1, ('osad', 0.0): 1, ('scam', 0.0): 1, ('itb', 0.0): 1, ('omigod', 0.0): 1, ('ehem', 0.0): 1, ('ala', 0.0): 1, ('yeke', 0.0): 1, ('jumpa', 0.0): 1, ('😋', 0.0): 1, ('ape', 0.0): 1, ('1.2', 0.0): 1, ('map', 0.0): 1, ('namin', 0.0): 1, ('govt', 0.0): 1, ('e-petit', 0.0): 1, ('pretend', 0.0): 1, ('irk', 0.0): 1, ('ruess', 0.0): 1, ('program', 0.0): 1, ('aigoo', 0.0): 1, ('doujin', 0.0): 1, ('killua', 0.0): 1, ('ginggon', 0.0): 1, ('guys.al', 0.0): 1, ('ytd', 0.0): 1, ('pdapaghimok', 0.0): 1, ('flexibl', 0.0): 1, ('sheet', 0.0): 1, ('nanaman', 0.0): 1, ('pinay', 0.0): 1, ('pie', 0.0): 1, ('jadi', 0.0): 1, ('langsung', 0.0): 1, ('flasback', 0.0): 1, ('franc', 0.0): 1, (':|', 0.0): 1, ('lo', 0.0): 1, ('nicknam', 0.0): 1, ('involv', 0.0): 1, ('scrape', 0.0): 1, ('pile', 0.0): 1, ('sare', 0.0): 1, ('bandar', 0.0): 1, ('varg', 0.0): 1, ('hammer', 0.0): 1, ('lolo', 0.0): 1, ('xbsbabnb', 0.0): 1, ('stilll', 0.0): 1, ('apma', 0.0): 2, ('leadership', 0.0): 1, ('wakeupgop', 0.0): 1, ('mv', 0.0): 1, ('bull', 0.0): 1, ('trafficcc', 0.0): 1, ('oscar', 0.0): 1, ('pornographi', 0.0): 1, ('slutsham', 0.0): 1, ('ect', 0.0): 1, ('poland', 0.0): 1, ('faraway', 0.0): 1, ('700', 0.0): 1, ('800', 0.0): 1, ('cgi', 0.0): 1, ('pun', 0.0): 1, (\"x'\", 0.0): 1, ('osaka', 0.0): 1, ('junior', 0.0): 1, ('aytona', 0.0): 1, ('hala', 0.0): 1, ('mathird', 0.0): 1, ('jkjk', 0.0): 1, ('backtrack', 0.0): 1, ('util', 0.0): 1, ('pat', 0.0): 1, ('jay', 0.0): 2, ('broh', 0.0): 1, ('calll', 0.0): 1, ('icaru', 0.0): 1, ('awn', 0.0): 1, ('bach', 0.0): 1, ('court', 0.0): 1, ('landlord', 0.0): 1, (\"mp'\", 0.0): 1, ('dame', 0.0): 1, ('gossip', 0.0): 1, ('purpl', 0.0): 2, ('tie', 0.0): 1, ('ishii', 0.0): 1, ('clara', 0.0): 1, ('yile', 0.0): 1, ('whatev', 0.0): 1, ('stil', 0.0): 1, ('sidharth', 0.0): 1, ('ndabenhl', 0.0): 1, ('doggi', 0.0): 1, ('antag', 0.0): 1, ('41', 0.0): 1, ('thu', 0.0): 1, ('jenner', 0.0): 1, ('troubleshoot', 0.0): 1, (\"convo'\", 0.0): 1, ('dem', 0.0): 1, ('tix', 0.0): 2, ('automat', 0.0): 1, ('redirect', 0.0): 1, ('gigi', 0.0): 1, ('carter', 0.0): 1, ('corn', 0.0): 2, ('chip', 0.0): 2, ('nnnooo', 0.0): 1, ('cz', 0.0): 1, ('gorilla', 0.0): 1, ('hbm', 0.0): 1, ('humid', 0.0): 1, ('admir', 0.0): 1, ('consist', 0.0): 1, ('jason', 0.0): 1, (\"shackell'\", 0.0): 1, ('podcast', 0.0): 1, ('envi', 0.0): 1, ('twer', 0.0): 1, ('782', 0.0): 1, ('hahaahahahaha', 0.0): 1, ('sm1', 0.0): 1, ('mutil', 0.0): 1, ('robot', 0.0): 1, ('destroy', 0.0): 1, ('freakin', 0.0): 1, ('haestarr', 0.0): 1, ('😀', 0.0): 3, ('audio', 0.0): 1, ('snippet', 0.0): 1, ('brotherhood', 0.0): 1, ('mefd', 0.0): 1, ('diana', 0.0): 1, ('master', 0.0): 1, ('led', 0.0): 1, ('award', 0.0): 1, ('meowkd', 0.0): 1, ('complic', 0.0): 1, (\"c'mon\", 0.0): 1, (\"swimmer'\", 0.0): 1, ('leh', 0.0): 1, ('corner', 0.0): 1, ('didnot', 0.0): 1, ('usanel', 0.0): 2, ('nathan', 0.0): 1, ('micha', 0.0): 1, ('fave', 0.0): 2, ('creep', 0.0): 1, ('throughout', 0.0): 1, ('whose', 0.0): 1, ('ave', 0.0): 1, ('tripl', 0.0): 1, ('lectur', 0.0): 1, ('2-5', 0.0): 1, ('jaw', 0.0): 1, ('quarter', 0.0): 1, ('soni', 0.0): 1, ('followmeaaron', 0.0): 1, ('tzelumxoxo', 0.0): 1, ('drank', 0.0): 1, ('mew', 0.0): 1, ('indic', 0.0): 1, ('ouliv', 0.0): 1, ('70748', 0.0): 1, ('viernesderolenahot', 0.0): 1, ('longmorn', 0.0): 1, ('tobermori', 0.0): 1, ('32', 0.0): 1, ('tail', 0.0): 1, ('recuerda', 0.0): 1, ('tanto', 0.0): 1, ('bath', 0.0): 1, ('muna', 0.0): 1, ('await', 0.0): 1, ('urslef', 0.0): 1, ('lime', 0.0): 1, ('truckload', 0.0): 1, ('favour', 0.0): 2, ('spectat', 0.0): 1, ('sail', 0.0): 1, (\"w'end\", 0.0): 1, ('bbc', 0.0): 1, ('‘', 0.0): 1, ('foil', 0.0): 1, ('ac45', 0.0): 1, ('catamaran', 0.0): 1, ('peli', 0.0): 1, ('829', 0.0): 1, ('sextaatequemfimseguesdvcomvalentino', 0.0): 1, ('befor', 0.0): 1, ('valu', 0.0): 1, ('cinnamon', 0.0): 1, ('mtap', 0.0): 1, ('peng', 0.0): 1, ('frozen', 0.0): 1, ('bagu', 0.0): 1, ('emang', 0.0): 1, ('engg', 0.0): 1, ('cmc', 0.0): 1, ('mage', 0.0): 1, ('statement', 0.0): 1, ('moodsw', 0.0): 1, ('termin', 0.0): 1, ('men', 0.0): 1, ('peep', 0.0): 1, ('multipl', 0.0): 1, ('mef', 0.0): 1, ('rebound', 0.0): 1, ('pooor', 0.0): 1, ('2am', 0.0): 1, ('perpetu', 0.0): 1, ('bitchfac', 0.0): 1, ('clever', 0.0): 1, ('iceland', 0.0): 1, ('zayn_come_back_we_miss_y', 0.0): 1, ('pmsl', 0.0): 1, ('mianh', 0.0): 1, ('milkeu', 0.0): 1, ('lrt', 0.0): 1, ('bambam', 0.0): 1, ('soda', 0.0): 1, ('payback', 0.0): 1, ('87000', 0.0): 1, ('jobe', 0.0): 1, ('muchi', 0.0): 1, ('🎈', 0.0): 1, ('bathroom', 0.0): 1, ('lagg', 0.0): 1, ('banget', 0.0): 1, ('novel', 0.0): 1, (\"there'd\", 0.0): 1, ('invis', 0.0): 1, ('scuttl', 0.0): 1, ('worm', 0.0): 1, ('bauuukkk', 0.0): 1, ('jessica', 0.0): 1, ('5:15', 0.0): 1, ('argument', 0.0): 1, ('couldnt', 0.0): 2, ('yepp', 0.0): 1, ('😺', 0.0): 1, ('💒', 0.0): 1, ('💎', 0.0): 1, ('feelin', 0.0): 1, ('biscuit', 0.0): 1, ('slather', 0.0): 1, ('jsut', 0.0): 1, ('belov', 0.0): 1, ('grandmoth', 0.0): 1, ('princess', 0.0): 2, ('babee', 0.0): 1, ('demn', 0.0): 1, ('hotaisndonwyvauwjoqhsjsnaihsuswtf', 0.0): 1, ('sia', 0.0): 1, ('niram', 0.0): 1, ('geng', 0.0): 1, ('fikri', 0.0): 1, ('tirtagangga', 0.0): 1, ('char', 0.0): 1, ('font', 0.0): 2, ('riprishikeshwari', 0.0): 1, ('creamist', 0.0): 1, ('challeng', 0.0): 1, ('substitut', 0.0): 1, ('skin', 0.0): 1, ('cplt', 0.0): 1, ('cp', 0.0): 1, ('hannah', 0.0): 1, ('💙', 0.0): 1, ('💪', 0.0): 1, ('opu', 0.0): 1, ('inner', 0.0): 1, ('pleasur', 0.0): 1, ('bbq', 0.0): 1, ('lolliv', 0.0): 1, ('split', 0.0): 3, ('collat', 0.0): 2, ('spilt', 0.0): 2, ('quitkarwaoyaaro', 0.0): 1, ('deacti̇v', 0.0): 1, ('2.5', 0.0): 1, ('g2a', 0.0): 1, ('sherep', 0.0): 1, ('nemen', 0.0): 1, ('behey', 0.0): 1, ('motherfuck', 0.0): 1, ('tattoo', 0.0): 1, ('reec', 0.0): 1, ('vm', 0.0): 1, ('deth', 0.0): 2, ('lest', 0.0): 1, ('gp', 0.0): 1, ('departur', 0.0): 1, ('wipe', 0.0): 1, ('yuck', 0.0): 1, ('ystrday', 0.0): 1, ('seolhyun', 0.0): 1, ('drama', 0.0): 1, ('spici', 0.0): 1, ('owl', 0.0): 1, ('mumbai', 0.0): 1, (\"pj'\", 0.0): 1, ('wallpap', 0.0): 1, ('cba', 0.0): 1, ('hotter', 0.0): 1, ('rec', 0.0): 1, ('gotdamn', 0.0): 1, ('baaack', 0.0): 1, ('honest', 0.0): 1, ('srw', 0.0): 1, ('mobag', 0.0): 1, ('dunno', 0.0): 1, ('stroke', 0.0): 1, ('gnr', 0.0): 1, ('backstag', 0.0): 1, ('slash', 0.0): 1, ('prolli', 0.0): 1, ('bunni', 0.0): 1, ('sooner', 0.0): 1, ('analyst', 0.0): 1, ('expedia', 0.0): 1, ('bellevu', 0.0): 1, ('prison', 0.0): 1, ('alcohol', 0.0): 1, ('huhuh', 0.0): 1, ('heartburn', 0.0): 1, ('awalmu', 0.0): 1, ('njareeem', 0.0): 1, ('maggi', 0.0): 1, ('psycho', 0.0): 1, ('wahhh', 0.0): 1, ('abudhabi', 0.0): 1, ('hiby', 0.0): 1, ('shareyoursumm', 0.0): 1, ('b8', 0.0): 1, ('must.b', 0.0): 1, ('dairi', 0.0): 1, ('produxt', 0.0): 1, ('lactos', 0.0): 2, ('midland', 0.0): 1, ('knacker', 0.0): 1, ('footag', 0.0): 1, ('lifeless', 0.0): 1, ('shell', 0.0): 1, ('44', 0.0): 1, ('7782', 0.0): 1, ('pengen', 0.0): 1, ('girlll', 0.0): 1, ('tsunami', 0.0): 1, ('indi', 0.0): 1, ('nick', 0.0): 1, ('tirad', 0.0): 1, ('stoop', 0.0): 1, ('lower', 0.0): 1, ('role', 0.0): 1, ('thunder', 0.0): 1, ('paradis', 0.0): 1, ('habit', 0.0): 1, ('facad', 0.0): 1, ('democraci', 0.0): 1, ('brat', 0.0): 1, ('tb', 0.0): 1, (\"o'\", 0.0): 1, ('bade', 0.0): 1, ('fursat', 0.0): 1, ('usey', 0.0): 2, ('banaya', 0.0): 1, ('uppar', 0.0): 1, ('waal', 0.0): 1, ('ney', 0.0): 1, ('afso', 0.0): 1, ('hums', 0.0): 1, ('dur', 0.0): 1, ('wo', 0.0): 1, (\"who'd\", 0.0): 1, ('naruhina', 0.0): 1, ('namee', 0.0): 1, ('haiqal', 0.0): 1, ('360hr', 0.0): 1, ('picc', 0.0): 1, ('instor', 0.0): 1, ('pre-vot', 0.0): 1, ('5th', 0.0): 1, ('usernam', 0.0): 1, ('minho', 0.0): 1, ('durian', 0.0): 1, ('strudel', 0.0): 1, ('tsk', 0.0): 1, ('marin', 0.0): 1, ('kailan', 0.0): 1, ('separ', 0.0): 1, ('payday', 0.0): 1, ('payhour', 0.0): 1, ('immedi', 0.0): 1, ('natur', 0.0): 1, ('pre-ord', 0.0): 1, ('fwm', 0.0): 1, ('guppi', 0.0): 1, ('poorkid', 0.0): 1, ('lack', 0.0): 1, ('misunderstood', 0.0): 1, ('cuddli', 0.0): 1, ('scratch', 0.0): 1, ('thumb', 0.0): 1, ('compens', 0.0): 1, ('kirkiri', 0.0): 1, ('phase', 0.0): 1, ('wonho', 0.0): 1, ('visual', 0.0): 1, (\"='(\", 0.0): 1, ('mission', 0.0): 1, ('pap', 0.0): 1, ('danzel', 0.0): 1, ('craft', 0.0): 1, ('devil', 0.0): 1, ('phil', 0.0): 1, ('sheff', 0.0): 1, ('york', 0.0): 1, ('visa', 0.0): 1, ('gim', 0.0): 1, ('bench', 0.0): 1, ('harm', 0.0): 1, ('yolo', 0.0): 1, ('bloat', 0.0): 1, ('olli', 0.0): 1, ('alterni', 0.0): 1, ('earth', 0.0): 1, ('influenc', 0.0): 1, ('overal', 0.0): 1, ('continent', 0.0): 1, ('🔫', 0.0): 1, ('tank', 0.0): 1, ('thirsti', 0.0): 1, ('konami', 0.0): 1, ('polici', 0.0): 1, ('ranti', 0.0): 1, ('atm', 0.0): 1, ('pervers', 0.0): 1, ('bylfnnz', 0.0): 1, ('ban', 0.0): 1, ('failsatlif', 0.0): 1, ('press', 0.0): 1, ('duper', 0.0): 1, ('waaah', 0.0): 1, ('jaebum', 0.0): 1, ('ahmad', 0.0): 1, ('maslan', 0.0): 1, ('hull', 0.0): 1, ('misser', 0.0): 1}\n\n\nUnfortunately, this does not help much to understand the data. It would be better to visualize this output to gain better insights.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "L2 - Frequencies"
    ]
  },
  {
    "objectID": "notes/c1w1/lab02.html#table-of-word-counts",
    "href": "notes/c1w1/lab02.html#table-of-word-counts",
    "title": "Lab: Building and Visualizing word frequencies",
    "section": "Table of word counts",
    "text": "Table of word counts\nWe will select a set of words that we would like to visualize. It is better to store this temporary information in a table that is very easy to use later.\n\n# select some words to appear in the report. we will assume that each word is unique (i.e. no duplicates)\nkeys = ['happi', 'merri', 'nice', 'good', 'bad', 'sad', 'mad', 'best', 'pretti',\n        '❤', ':)', ':(', '😒', '😬', '😄', '😍', '♛',\n        'song', 'idea', 'power', 'play', 'magnific']\n\n# list representing our table of word counts.\n# each element consist of a sublist with this pattern: [&lt;word&gt;, &lt;positive_count&gt;, &lt;negative_count&gt;]\ndata = []\n\n# loop through our selected words\nfor word in keys:\n    \n    # initialize positive and negative counts\n    pos = 0\n    neg = 0\n    \n    # retrieve number of positive counts\n    if (word, 1) in freqs:\n        pos = freqs[(word, 1)]\n        \n    # retrieve number of negative counts\n    if (word, 0) in freqs:\n        neg = freqs[(word, 0)]\n        \n    # append the word counts to the table\n    data.append([word, pos, neg])\n    \ndata\n\n[['happi', 211, 25],\n ['merri', 1, 0],\n ['nice', 98, 19],\n ['good', 238, 101],\n ['bad', 18, 73],\n ['sad', 5, 123],\n ['mad', 4, 11],\n ['best', 65, 22],\n ['pretti', 20, 15],\n ['❤', 29, 21],\n [':)', 3568, 2],\n [':(', 1, 4571],\n ['😒', 1, 3],\n ['😬', 0, 2],\n ['😄', 5, 1],\n ['😍', 2, 1],\n ['♛', 0, 210],\n ['song', 22, 27],\n ['idea', 26, 10],\n ['power', 7, 6],\n ['play', 46, 48],\n ['magnific', 2, 0]]\n\n\nWe can then use a scatter plot to inspect this table visually. Instead of plotting the raw counts, we will plot it in the logarithmic scale to take into account the wide discrepancies between the raw counts (e.g. :) has 3568 counts in the positive while only 2 in the negative). The red line marks the boundary between positive and negative areas. Words close to the red line can be classified as neutral.\n\nfig, ax = plt.subplots(figsize = (8, 8))\n\n# convert positive raw counts to logarithmic scale. we add 1 to avoid log(0)\nx = np.log([x[1] + 1 for x in data])  \n\n# do the same for the negative counts\ny = np.log([x[2] + 1 for x in data]) \n\n# Plot a dot for each pair of words\nax.scatter(x, y)  \n\n# assign axis labels\nplt.xlabel(\"Log Positive count\")\nplt.ylabel(\"Log Negative count\")\n\n# Add the word as the label at the same position as we added the points just before\nfor i in range(0, len(data)):\n    ax.annotate(data[i][0], (x[i], y[i]), fontsize=12)\n\nax.plot([0, 9], [0, 9], color = 'red') # Plot the red line that divides the 2 areas.\nplt.show()\n\nText(0.5, 0, 'Log Positive count')\n\n\nText(0, 0.5, 'Log Negative count')\n\n\nText(5.356586274672012, 3.258096538021482, 'happi')\n\n\nText(0.6931471805599453, 0.0, 'merri')\n\n\nText(4.59511985013459, 2.995732273553991, 'nice')\n\n\nText(5.476463551931511, 4.624972813284271, 'good')\n\n\nText(2.9444389791664403, 4.30406509320417, 'bad')\n\n\nText(1.791759469228055, 4.820281565605037, 'sad')\n\n\nText(1.6094379124341003, 2.4849066497880004, 'mad')\n\n\nText(4.189654742026425, 3.1354942159291497, 'best')\n\n\nText(3.044522437723423, 2.772588722239781, 'pretti')\n\n\nText(3.4011973816621555, 3.091042453358316, '❤')\n\n\nText(8.18004072349016, 1.0986122886681098, ':)')\n\n\nText(0.6931471805599453, 8.427706024914702, ':(')\n\n\nText(0.6931471805599453, 1.3862943611198906, '😒')\n\n\nText(0.0, 1.0986122886681098, '😬')\n\n\nText(1.791759469228055, 0.6931471805599453, '😄')\n\n\nText(1.0986122886681098, 0.6931471805599453, '😍')\n\n\nText(0.0, 5.351858133476067, '♛')\n\n\nText(3.1354942159291497, 3.332204510175204, 'song')\n\n\nText(3.295836866004329, 2.3978952727983707, 'idea')\n\n\nText(2.0794415416798357, 1.9459101490553132, 'power')\n\n\nText(3.8501476017100584, 3.8918202981106265, 'play')\n\n\nText(1.0986122886681098, 0.0, 'magnific')\n\n\n/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 128556 (\\N{GRIMACING FACE}) missing from font(s) DejaVu Sans.\n  fig.canvas.print_figure(bytes_io, **kw)\n\n\n\n\n\n\n\n\n\nThis chart is straightforward to interpret. It shows that emoticons :) and :( are very important for sentiment analysis. Thus, we should not let preprocessing steps get rid of these symbols!\nFurthermore, what is the meaning of the crown symbol? It seems to be very negative!\n\nConclusion\nThat’s all for this lab!\nWe’ve seen how to build a word frequency dictionary and this will come in handy when extracting the features of a list of tweets. Next up, we will be reviewing Logistic Regression. Keep it up!",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "L2 - Frequencies"
    ]
  },
  {
    "objectID": "notes/c1w1/lab03.html",
    "href": "notes/c1w1/lab03.html",
    "title": "Lab: Visualizing tweets & the Logistic Regression model",
    "section": "",
    "text": "Figure 1: course banner\nObjectives: Visualize and interpret the logistic regression model\nSteps:",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "L3 - Visualizing tweets"
    ]
  },
  {
    "objectID": "notes/c1w1/lab03.html#import-the-required-libraries",
    "href": "notes/c1w1/lab03.html#import-the-required-libraries",
    "title": "Lab: Visualizing tweets & the Logistic Regression model",
    "section": "Import the required libraries",
    "text": "Import the required libraries\nWe will be using NLTK, an opensource NLP library, for collecting, handling, and processing Twitter data. In this lab, we will use the example dataset that comes alongside with NLTK. This dataset has been manually annotated and serves to establish baselines for models quickly.\nSo, to start, let’s import the required libraries.\n\nimport nltk                         # NLP toolbox\nfrom os import getcwd\nimport pandas as pd                 # Library for Dataframes \nfrom nltk.corpus import twitter_samples \nimport matplotlib.pyplot as plt     # Library for visualization\nimport numpy as np                  # Library for math functions\n\nfrom utils import process_tweet, build_freqs # Our functions for NLP",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "L3 - Visualizing tweets"
    ]
  },
  {
    "objectID": "notes/c1w1/lab03.html#load-the-nltk-sample-dataset",
    "href": "notes/c1w1/lab03.html#load-the-nltk-sample-dataset",
    "title": "Lab: Visualizing tweets & the Logistic Regression model",
    "section": "Load the NLTK sample dataset",
    "text": "Load the NLTK sample dataset\nTo complete this lab, we need the sample dataset of the previous lab. Here, we assume the files are already available, and we only need to load into Python lists.\n\n# select the set of positive and negative tweets\nall_positive_tweets = twitter_samples.strings('positive_tweets.json')\nall_negative_tweets = twitter_samples.strings('negative_tweets.json')\n\ntweets = all_positive_tweets + all_negative_tweets ## Concatenate the lists. \nlabels = np.append(np.ones((len(all_positive_tweets),1)), np.zeros((len(all_negative_tweets),1)), axis = 0)\n\n# split the data into two pieces, one for training and one for testing (validation set) \ntrain_pos  = all_positive_tweets[:4000]\ntrain_neg  = all_negative_tweets[:4000]\n\ntrain_x = train_pos + train_neg \n\nprint(\"Number of tweets: \", len(train_x))\n\nNumber of tweets:  8000",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "L3 - Visualizing tweets"
    ]
  },
  {
    "objectID": "notes/c1w1/lab03.html#load-the-extracted-features",
    "href": "notes/c1w1/lab03.html#load-the-extracted-features",
    "title": "Lab: Visualizing tweets & the Logistic Regression model",
    "section": "Load the extracted features",
    "text": "Load the extracted features\nPart of this week’s assignment is the creation of the numerical features needed for the Logistic regression model. In order not to interfere with it, we have previously calculated and stored these features in a CSV file for the entire training set.\nSo, please load these features created for the tweets sample.\n\ndata = pd.read_csv('logistic_features.csv'); # Load a 3 columns csv file using pandas function\ndata.head(10) # Print the first 10 data entries\n\n\n\n\n\n\n\n\nbias\npositive\nnegative\nsentiment\n\n\n\n\n0\n1.0\n3020.0\n61.0\n1.0\n\n\n1\n1.0\n3573.0\n444.0\n1.0\n\n\n2\n1.0\n3005.0\n115.0\n1.0\n\n\n3\n1.0\n2862.0\n4.0\n1.0\n\n\n4\n1.0\n3119.0\n225.0\n1.0\n\n\n5\n1.0\n2955.0\n119.0\n1.0\n\n\n6\n1.0\n3934.0\n538.0\n1.0\n\n\n7\n1.0\n3162.0\n276.0\n1.0\n\n\n8\n1.0\n628.0\n189.0\n1.0\n\n\n9\n1.0\n264.0\n112.0\n1.0\n\n\n\n\n\n\n\nNow let us get rid of the data frame to keep only Numpy arrays.\n\n# Each feature is labeled as bias, positive and negative\nX = data[['bias', 'positive', 'negative']].values # Get only the numerical values of the dataframe\nY = data['sentiment'].values; # Put in Y the corresponding labels or sentiments\n\nprint(X.shape) # Print the shape of the X part\nprint(X) # Print some rows of X\n\n(8000, 3)\n[[1.000e+00 3.020e+03 6.100e+01]\n [1.000e+00 3.573e+03 4.440e+02]\n [1.000e+00 3.005e+03 1.150e+02]\n ...\n [1.000e+00 1.440e+02 7.830e+02]\n [1.000e+00 2.050e+02 3.890e+03]\n [1.000e+00 1.890e+02 3.974e+03]]",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "L3 - Visualizing tweets"
    ]
  },
  {
    "objectID": "notes/c1w1/lab03.html#load-a-pretrained-logistic-regression-model",
    "href": "notes/c1w1/lab03.html#load-a-pretrained-logistic-regression-model",
    "title": "Lab: Visualizing tweets & the Logistic Regression model",
    "section": "Load a pretrained Logistic Regression model",
    "text": "Load a pretrained Logistic Regression model\nIn the same way, as part of this week’s assignment, a Logistic regression model must be trained. The next cell contains the resulting model from such training. Notice that a list of 3 numeric values represents the whole model, that we have called theta \\theta.\n\ntheta = [7e-08, 0.0005239, -0.00055517]",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "L3 - Visualizing tweets"
    ]
  },
  {
    "objectID": "notes/c1w1/lab03.html#plot-the-samples-in-a-scatter-plot",
    "href": "notes/c1w1/lab03.html#plot-the-samples-in-a-scatter-plot",
    "title": "Lab: Visualizing tweets & the Logistic Regression model",
    "section": "Plot the samples in a scatter plot",
    "text": "Plot the samples in a scatter plot\nThe vector theta represents a plane that split our feature space into two parts. Samples located over that plane are considered positive, and samples located under that plane are considered negative. Remember that we have a 3D feature space, i.e., each tweet is represented as a vector comprised of three values: [bias, positive_sum, negative_sum], always having bias = 1.\nIf we ignore the bias term, we can plot each tweet in a cartesian plane, using positive_sum and negative_sum. In the cell below, we do precisely this. Additionally, we color each tweet, depending on its class. Positive tweets will be green and negative tweets will be red.\n\n# Plot the samples using columns 1 and 2 of the matrix\nfig, ax = plt.subplots(figsize = (8, 8))\n\ncolors = ['red', 'green']\n\n# Color based on the sentiment Y\nax.scatter(X[:,1], X[:,2], c=[colors[int(k)] for k in Y], s = 0.1)  # Plot a dot for each pair of words\nplt.xlabel(\"Positive\")\nplt.ylabel(\"Negative\")\n\nText(0.5, 0, 'Positive')\n\n\nText(0, 0.5, 'Negative')\n\n\n\n\n\n\n\n\n\nFrom the plot, it is evident that the features that we have chosen to represent tweets as numerical vectors allow an almost perfect separation between positive and negative tweets. So we can expect a very high accuracy for this model!",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "L3 - Visualizing tweets"
    ]
  },
  {
    "objectID": "notes/c1w1/lab03.html#plot-the-model-alongside-the-data",
    "href": "notes/c1w1/lab03.html#plot-the-model-alongside-the-data",
    "title": "Lab: Visualizing tweets & the Logistic Regression model",
    "section": "Plot the model alongside the data",
    "text": "Plot the model alongside the data\nWe will draw a gray line to show the cutoff between the positive and negative regions. In other words, the gray line marks the line where \nz = \\theta * x = 0.\n\nTo draw this line, we have to solve the above equation in terms of one of the independent variables.\n\nz = \\theta * x = 0\n\n\nx = [1, pos, neg]\n\n\nz(\\theta, x) = \\theta_0+ \\theta_1 * pos + \\theta_2 * neg = 0\n\n\nneg = (-\\theta_0 - \\theta_1 * pos) / \\theta_2\n\nThe red and green lines that point in the direction of the corresponding sentiment are calculated using a perpendicular line to the separation line calculated in the previous equations(neg function). It must point in the same direction as the derivative of the Logit function, but the magnitude may differ. It is only for a visual representation of the model.\n\ndirection = pos * \\theta_2 / \\theta_1\n\n\n# Equation for the separation plane\n# It give a value in the negative axe as a function of a positive value\n# f(pos, neg, W) = w0 + w1 * pos + w2 * neg = 0\n# s(pos, W) = (w0 - w1 * pos) / w2\ndef neg(theta, pos):\n    return (-theta[0] - pos * theta[1]) / theta[2]\n\n# Equation for the direction of the sentiments change\n# We don't care about the magnitude of the change. We are only interested \n# in the direction. So this direction is just a perpendicular function to the \n# separation plane\n# df(pos, W) = pos * w2 / w1\ndef direction(theta, pos):\n    return    pos * theta[2] / theta[1]\n\nThe green line in the chart points in the direction where z &gt; 0 and the red line points in the direction where z &lt; 0. The direction of these lines are given by the weights \\theta_1 and \\theta_2\n\n# Plot the samples using columns 1 and 2 of the matrix\nfig, ax = plt.subplots(figsize = (8, 8))\n\ncolors = ['red', 'green']\n\n# Color base on the sentiment Y\nax.scatter(X[:,1], X[:,2], c=[colors[int(k)] for k in Y], s = 0.1)  # Plot a dot for each pair of words\nplt.xlabel(\"Positive\")\nplt.ylabel(\"Negative\")\n\n# Now lets represent the logistic regression model in this chart. \nmaxpos = np.max(X[:,1])\n\noffset = 5000 # The pos value for the direction vectors origin\n\n# Plot a gray line that divides the 2 areas.\nax.plot([0,  maxpos], [neg(theta, 0),   neg(theta, maxpos)], color = 'gray') \n\n# Plot a green line pointing to the positive direction\nax.arrow(offset, neg(theta, offset), offset, direction(theta, offset), head_width=500, head_length=500, fc='g', ec='g')\n# Plot a red line pointing to the negative direction\nax.arrow(offset, neg(theta, offset), -offset, -direction(theta, offset), head_width=500, head_length=500, fc='r', ec='r')\n\nplt.show()\n\nText(0.5, 0, 'Positive')\n\n\nText(0, 0.5, 'Negative')\n\n\n\n\n\n\n\n\n\nNote that more critical than the Logistic regression itself, are the features extracted from tweets that allow getting the right results in this exercise.\nThat is all, folks.\nHopefully, now we understand better what the Logistic regression model represents, and why it works that well for this specific problem.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "L3 - Visualizing tweets"
    ]
  },
  {
    "objectID": "notes/c2w2/index.html",
    "href": "notes/c2w2/index.html",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "",
    "text": "course banner\n\n\n\n\nFigure 1\n\n\n\n\n\n\n\n\nFigure 2",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w2/index.html#part-of-speech-tagging",
    "href": "notes/c2w2/index.html#part-of-speech-tagging",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "Part of Speech Tagging",
    "text": "Part of Speech Tagging\nPart of Speech Tagging (POS) is the process of assigning a part of speech to a word. By doing so, we will learn the following:\n\nMarkov Chains\nHidden Markov Models\nViterbi algorithm\n\nHere is a concrete example:\n\n\n\n\n\n\n\nautocorrect\n\n\n\n\nFigure 3: Learning Objectives\n\n\nWe can use part of speech tagging for:\n\nIdentifying named entities\nSpeech recognition\nCoreference Resolution\n\nWe can use the probabilities of POS tags happening near one another to come up with the most reasonable output",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w2/index.html#lab1-working-with-text-data",
    "href": "notes/c2w2/index.html#lab1-working-with-text-data",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "Lab1: Working with text data",
    "text": "Lab1: Working with text data\nWorking with text data",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w2/index.html#markov-chains",
    "href": "notes/c2w2/index.html#markov-chains",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "Markov Chains",
    "text": "Markov Chains\n\n\n\n\n\n\n\nPOS Tagging\n\n\n\n\nFigure 4: POS Tagging\n\n\n\n\n\n\n\n\nFSM\n\n\n\n\nFigure 5: FSM representation for POS tagging\n\n\n\nThe circles of the graph represent the states of your model. A state refers to a certain condition of the present moment. We can think of these as the POS tags of the current word.\n\nQ={q_1, q_2, q_3} \\qquad \\text{ is the set of all states in your model. }",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w2/index.html#markov-chains-and-pos-tags",
    "href": "notes/c2w2/index.html#markov-chains-and-pos-tags",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "Markov Chains and POS Tags",
    "text": "Markov Chains and POS Tags\nTo help identify the parts of speech for every word, we need to build a transition matrix that gives we the probabilities from one state to another.\n\n\n\n\n\n\n\nFSM\n\n\n\n\nFigure 6: Learning Objectives\n\n\nIn the diagram above, the blue circles correspond to the part of speech tags, and the arrows correspond to the transition probabilities from one part of speech to another. We can populate the table on the right from the diagram on the left. The first row in your A matrix corresponds to the initial distribution among all the states. According to the table, the sentence has a 40% chance to start as a noun, 10% chance to start with a verb, and a 50% chance to start with another part of speech tag.\nIn more general notation, we can write the transition matrix A, given some states Q, as follows:\n\n\n\n\n\n\n\nFSM\n\n\n\n\nFigure 7: Learning Objectives",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w2/index.html#hidden-markov-models",
    "href": "notes/c2w2/index.html#hidden-markov-models",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "Hidden Markov Models",
    "text": "Hidden Markov Models\nIn the previous video, I showed we an example with a simple markov model. The transition probabilities allowed we to identify the transition probability from one POS to another. We will now explore hidden markov models. In hidden markov models we make use of emission probabilities that give we the probability to go from one state (POS tag) to a specific word.\n\n\n\n\n\n\n\nState Transition Graph\n\n\n\n\nFigure 8: Emission probabilities\n\n\nFor example, given that we are in a verb state, we can go to other words with certain probabilities. This emission matrix B, will be used with your transition matrix A, to help we identify the part of speech of a word in a sentence. To populate your matrix B, we can just have a labelled dataset and compute the probabilities of going from a POS to each word in your vocabulary. Here is a recap of what we have seen so far:\n\n\n\n\n\n\n\nHMM\n\n\n\n\nFigure 9: HMM Summary\n\n\nNote that the sum of each row in your A and B matrix has to be 1. Next, I will show we how we can calculate the probabilities inside these matrices.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w2/index.html#calculating-probabilities",
    "href": "notes/c2w2/index.html#calculating-probabilities",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "Calculating Probabilities",
    "text": "Calculating Probabilities\nHere is a visual representation on how to calculate the probabilities:\n\n\n\n\n\n\n\nTransition Probabilities\n\n\n\n\nFigure 10: Transition Probabilities\n\n\nThe number of times that blue is followed by purple is 2 out of 3. We will use the same logic to populate our transition and emission matrices. In the transition matrix we will count the number of times tag t_{(i−1)},t{(i)} show up near each other and divide by the total number of times t_{(i−1)} shows up. (which is the same as the number of times it shows up followed by anything else).\n\n\n\n\n\n\n\nTransition Probabilities\n\n\n\n\nFigure 11: Transition Probabilities\n\n\n\ncalculate co-occurrence of tag pairs \nC(t_{(i-1)},t_{(i)})\n\\tag{1}\ncalculate the probabilities using the counts \nP(t_{(i)}|t_{(i-1)}) = \\frac{C(t_{(i)}),t_{(i-1)},}{\\sum_{i=1}^{N} C(t_{(i-1)})}\n\\tag{2}\n\nWhere\nC(t_{(i−1)} ,t_{(i)}) is the count of times tag t_{(i-1)} shows up before tag i.\nFrom this we can compute the probability that a tag shows up after another tag.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w2/index.html#populating-the-transition-matrix",
    "href": "notes/c2w2/index.html#populating-the-transition-matrix",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "Populating the Transition Matrix",
    "text": "Populating the Transition Matrix\nTo populate the transition matrix we have to keep track of the number of times each tag shows up before another tag.\n\n\n\n\n\n\n\nTransition Probabilities\n\n\n\n\nFigure 12: Transition Probabilities\n\n\nIn the table above, we can see that green corresponds to nouns (NN), purple corresponds to verbs (VB), and blue corresponds to other (O). Orange (π) corresponds to the initial state. The numbers inside the matrix correspond to the number of times a part of speech tag shows up right after another one.\nTo go from O to NN or in other words to calculate P(O∣NN) we have to compute the following:\n\n\n\n\n\n\n\nTransition Probabilities\n\n\n\n\nFigure 13: Transition Probabilities\n\n\nTo generalize:\n\nP(t_{(i)} \\mid t_{(i-1)}) = \\frac{C(t_{(i)},t_{(i-1)})}{\\sum_{j=1}^{N} C(t_{(i-1)},t_{(j)})}\n\\tag{3}\nWhere:\n\nC(t_{(i)},t_{(i-1)}) is the count of times tag t_{(i-1)} shows up before tag i.\n\nUnfortunately, sometimes we might not see two POS tags in front each other. This will give we a probability of 0. To solve this issue, we will “smooth” it as follows:\n\n\n\n\n\n\n\nTransition Probabilities\n\n\n\n\nFigure 14: Transition Probabilities\n\n\nThe \\epsilon allows we to not have any two sequences showing up with 0 probability. Why is this important?",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w2/index.html#populating-the-emission-matrix",
    "href": "notes/c2w2/index.html#populating-the-emission-matrix",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "Populating the Emission Matrix",
    "text": "Populating the Emission Matrix\nTo populate the emission matrix, we have to keep track of the words associated with their parts of speech tags.\n\n\n\n\n\n\n\nThe Emission Matrix\n\n\n\n\nFigure 15: The Emission Matrix\n\n\nTo populate the matrix, we will also use smoothing as we have previously used:\n\nP(w_i \\mid t_i) = \\frac{C(t_i,w_i)+\\epsilon}{\\sum_{j=1}^{N} C(t_i,w_j)+ N \\times \\epsilon} = \\frac{C(t_i,w_i)+\\epsilon}{\\sum_{j=1}^{N} C(t_i)+N\\times \\epsilon}\n\nWhere C(t_i,w_i) is the count associated with how many times the tag t_i is associated with the word w_i. The epsilon above is the smoothing parameter. In the next video, we will talk about the Viterbi algorithm and discuss how we can use the transition and emission matrix to come up with probabilities.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w2/index.html#lab2-working-with-text-data",
    "href": "notes/c2w2/index.html#lab2-working-with-text-data",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "Lab2: Working with text data",
    "text": "Lab2: Working with text data\nWorking with text data",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w2/index.html#the-viterbi-algorithm",
    "href": "notes/c2w2/index.html#the-viterbi-algorithm",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "The Viterbi Algorithm",
    "text": "The Viterbi Algorithm\nThe Viterbi algorithm makes use of the transition probabilities and the emission probabilities as follows.\n\n\n\n\n\n\n\nThe Emission Matrix\n\n\n\n\nFigure 16: The Viterbi Algorithm\n\n\nTo go from π to O we need to multiply the corresponding transition probability (0.3) and the corresponding emission probability (0.5), which gives we 0.15. We keep doing that for all the words, until we get the probability of an entire sequence.\n\n\n\n\n\n\n\nThe Emission Matrix\n\n\n\n\nFigure 17: The Viterbi Algorithm\n\n\nWe can then see how we will just pick the sequence with the highest probability. We will show we a systematic way to accomplish this (Viterbi!).",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w2/index.html#viterbi-initialization",
    "href": "notes/c2w2/index.html#viterbi-initialization",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "Viterbi Initialization",
    "text": "Viterbi Initialization\nWe will now populate a matrix C of dimension (num_tags, num_words). This matrix will have the probabilities that will tell we what part of speech each word belongs to.\n\n\n\n\n\n\n\nThe Emission Matrix\n\n\n\n\nFigure 18: The Viterbi Initialization\n\n\nNow to populate the first column, we just multiply the initial π distribution, for each tag, times b_{i,cindex(w_1)}, which is the emission probability of the word 1 given the tag i. Where the i, corresponds to the tag of the initial distribution and the cindex(w_1), is the index of word 1 in the emission matrix. And that’s it, we are done with populating the first column of your new C matrix. We will now need to keep track what part of speech we are coming from. Hence we introduce a matrix D, which allows we to store the labels that represent the different states we are going through when finding the most likely sequence of POS tags for the given sequence of words w_2 ,…,w_k. At first we set the first column to 0, because we are not coming from any POS tag.\n\n\n\n\n\n\n\nThe Emission Matrix\n\n\n\n\nFigure 19: The Viterbi Initialization\n\n\nThese two matrices will make more sense in the next videos.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w2/index.html#viterbi-forward-pass",
    "href": "notes/c2w2/index.html#viterbi-forward-pass",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "Viterbi: Forward Pass",
    "text": "Viterbi: Forward Pass\nThis will be best illustrated with an example:\n\n\n\n\n\n\n\nViterbi: Forward Pass\n\n\n\n\nFigure 20: Viterbi: Forward Pass\n\n\nSo to populate a cell (i.e. 1,2) in the image above, we have to take the max of [kth cells in the previous column, times the corresponding transition probability of the kth POS to the first POS times the emission probability of the first POS and the current word we are looking at]. We do that for all the cells. Take a paper and a pencil, and make sure we understand how it is done.\nThe general rule is c_{ij}= max_k c_{k,j-1} \\times a_{k,i} \\times b_{i,cindex(w_j)}\nNow to populate the D matrix, we will keep track of the argmax of where we came from as follows:\nNote that the only difference between c_{ij} and d_{ij}, is that in the former we compute the probability and in the latter we keep track of the index of the row where that probability came from. So we keep track of which k was used to get that max probability.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w2/index.html#viterbi-backward-pass",
    "href": "notes/c2w2/index.html#viterbi-backward-pass",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "Viterbi: Backward Pass",
    "text": "Viterbi: Backward Pass\nGreat, now that we know how to compute A, B, C, and D, we will put it all together and show we how to construct the path that will give we the part of speech tags for your sentence.\n\n\n\n\n\n\n\nViterbi: Forward Pass\n\n\n\n\nFigure 21: Viterbi: Forward Pass\n\n\nThe equation above just gives we the index of the highest row in the last column of C. Once we have that, we can go ahead and start using your D matrix as follows:\n\n\n\n\n\n\n\nViterbi: Forward Pass\n\n\n\n\nFigure 22: Viterbi: Forward Pass\n\n\nNote that since we started at index one, hence the last word (w5) is t_1. Then we go to the first row of D and what ever that number is, it indicated the row of the next part of speech tag. Then next part of speech tag indicates the row of the next and so forth. This allows we to reconstruct the POS tags for your sentence.\nWe will be implementing this in this week’s programming assignment.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w2/index.html#assignment",
    "href": "notes/c2w2/index.html#assignment",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "Assignment",
    "text": "Assignment\nPart-of-speech (POS) tagging",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w2/lab01.html",
    "href": "notes/c2w2/lab01.html",
    "title": "Parts-of-Speech Tagging - First Steps: Working with text files, Creating a Vocabulary and Handling Unknown Words",
    "section": "",
    "text": "course banner\n\n\n\n\nFigure 1\nIn this lecture notebook we will create a vocabulary from a tagged dataset and learn how to deal with words that are not present in this vocabulary when working with other text sources.\nAside from this we will also learn how to:\nimport string\nfrom collections import defaultdict",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "L1 - Vocabulary with unknowns"
    ]
  },
  {
    "objectID": "notes/c2w2/lab01.html#processing-new-text-sources",
    "href": "notes/c2w2/lab01.html#processing-new-text-sources",
    "title": "Parts-of-Speech Tagging - First Steps: Working with text files, Creating a Vocabulary and Handling Unknown Words",
    "section": "Processing new text sources",
    "text": "Processing new text sources\n\nDealing with unknown words\nNow that we have a vocabulary, we will use it when processing new text sources. A new text will have words that do not appear in the current vocabulary. To tackle this, we can simply classify each new word as an unknown one, but we can do better by creating a function that tries to classify the type of each unknown word and assign it a corresponding unknown token.\nThis function will do the following checks and return an appropriate token:\n\nCheck if the unknown word contains any character that is a digit\n\nreturn --unk_digit--\n\nCheck if the unknown word contains any punctuation character\n\nreturn --unk_punct--\n\nCheck if the unknown word contains any upper-case character\n\nreturn --unk_upper--\n\nCheck if the unknown word ends with a suffix that could indicate it is a noun, verb, adjective or adverb\n\nreturn --unk_noun--, --unk_verb--, --unk_adj--, --unk_adv-- respectively\n\n\nIf a word fails to fall under any condition then its token will be a plain --unk--. The conditions will be evaluated in the same order as listed here. So if a word contains a punctuation character but does not contain digits, it will fall under the second condition. To achieve this behaviour some if/elif statements can be used along with early returns.\nThis function is implemented next. Notice that the any() function is being heavily used. It returns True if at least one of the cases it evaluates is True.\n\ndef assign_unk(word):\n    \"\"\"\n    Assign tokens to unknown words\n    \"\"\"\n    \n    # Punctuation characters\n    # Try printing them out in a new cell!\n    punct = set(string.punctuation)\n    \n    # Suffixes\n    noun_suffix = [\"action\", \"age\", \"ance\", \"cy\", \"dom\", \"ee\", \"ence\", \"er\", \"hood\", \"ion\", \"ism\", \"ist\", \"ity\", \"ling\", \"ment\", \"ness\", \"or\", \"ry\", \"scape\", \"ship\", \"ty\"]\n    verb_suffix = [\"ate\", \"ify\", \"ise\", \"ize\"]\n    adj_suffix = [\"able\", \"ese\", \"ful\", \"i\", \"ian\", \"ible\", \"ic\", \"ish\", \"ive\", \"less\", \"ly\", \"ous\"]\n    adv_suffix = [\"ward\", \"wards\", \"wise\"]\n\n    # Loop the characters in the word, check if any is a digit\n    if any(char.isdigit() for char in word):\n        return \"--unk_digit--\"\n\n    # Loop the characters in the word, check if any is a punctuation character\n    elif any(char in punct for char in word):\n        return \"--unk_punct--\"\n\n    # Loop the characters in the word, check if any is an upper case character\n    elif any(char.isupper() for char in word):\n        return \"--unk_upper--\"\n\n    # Check if word ends with any noun suffix\n    elif any(word.endswith(suffix) for suffix in noun_suffix):\n        return \"--unk_noun--\"\n\n    # Check if word ends with any verb suffix\n    elif any(word.endswith(suffix) for suffix in verb_suffix):\n        return \"--unk_verb--\"\n\n    # Check if word ends with any adjective suffix\n    elif any(word.endswith(suffix) for suffix in adj_suffix):\n        return \"--unk_adj--\"\n\n    # Check if word ends with any adverb suffix\n    elif any(word.endswith(suffix) for suffix in adv_suffix):\n        return \"--unk_adv--\"\n    \n    # If none of the previous criteria is met, return plain unknown\n    return \"--unk--\"\n\nA POS tagger will always encounter words that are not within the vocabulary that is being used. By augmenting the dataset to include these unknown word tokens we are helping the tagger to have a better idea of the appropriate tag for these words.\n\n\nGetting the correct tag for a word\nAll that is left is to implement a function that will get the correct tag for a particular word taking special considerations for unknown words. Since the dataset provides each word and tag within the same line and a word being known depends on the vocabulary used, these two elements should be arguments to this function.\nThis function should check if a line is empty and if so, it should return a placeholder word and tag, --n-- and --s-- respectively.\nIf not, it should process the line to return the correct word and tag pair, considering if a word is unknown in which scenario the function assign_unk() should be used.\nThe function is implemented next. Notice That the split() method can be used without specifying the delimiter, in which case it will default to any whitespace.\n\ndef get_word_tag(line, vocab):\n    # If line is empty return placeholders for word and tag\n    if not line.split():\n        word = \"--n--\"\n        tag = \"--s--\"\n    else:\n        # Split line to separate word and tag\n        word, tag = line.split()\n        # Check if word is not in vocabulary\n        if word not in vocab: \n            # Handle unknown word\n            word = assign_unk(word)\n    return word, tag\n\nNow we can try this function with some examples to test that it is working as intended:\n\nget_word_tag('\\n', vocab)\n\n('--n--', '--s--')\n\n\nSince this line only includes a newline character it returns a placeholder word and tag.\n\nget_word_tag('In\\tIN\\n', vocab)\n\n('In', 'IN')\n\n\nThis one is a valid line and the function does a fair job at returning the correct (word, tag) pair.\n\nget_word_tag('tardigrade\\tNN\\n', vocab)\n\n('--unk--', 'NN')\n\n\nThis line includes a noun that is not present in the vocabulary.\nThe assign_unk function fails to detect that it is a noun so it returns an unknown token.\n\nget_word_tag('scrutinize\\tVB\\n', vocab)\n\n('--unk_verb--', 'VB')\n\n\nThis line includes a verb that is not present in the vocabulary.\nIn this case the assign_unk is able to detect that it is a verb so it returns an unknown verb token.\nCongratulations on finishing this lecture notebook! Now we should be more familiar with working with text data and have a better understanding of how a basic POS tagger works.\nKeep it up!",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "L1 - Vocabulary with unknowns"
    ]
  },
  {
    "objectID": "notes/c1w3/index.html",
    "href": "notes/c1w3/index.html",
    "title": "Vector Space Models",
    "section": "",
    "text": "Figure 1: course banner\n\n\n\n\n\n\n\n\nFigure 2",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w3/index.html#lab-linear-algebra-in-python-with-numpy",
    "href": "notes/c1w3/index.html#lab-linear-algebra-in-python-with-numpy",
    "title": "Vector Space Models",
    "section": "LAB: Linear algebra in Python with numpy",
    "text": "LAB: Linear algebra in Python with numpy\nThe Numpy lab\n\nCosine Similarity Intuition\nOne of the issues with euclidean distance is that it is not always accurate and sometimes we are not looking for that type of similarity metric. For example, when comparing large documents to smaller ones with euclidean distance one could get an inaccurate result.\nLook at the diagram:\n\n\n\n\n\n\n\nCosine Similarity: Intuition\n\n\n\n\nFigure 11: The cosine similarity is the cosine of the angle between two vectors.\n\n\nNormally the food corpus and the agriculture corpus are more similar because they have the same proportion of words. However the food corpus is much smaller than the agriculture corpus. To further clarify, although the history corpus and the agriculture corpus are different, they have a smaller euclidean distance. Hence d_2&lt;d_1.\nTo solve this problem, we look at the cosine between the vectors. This allows us to compare B and α.\n\n\nBackground\nBefore getting into the cosine similarity function remember that the norm of a vector is defined as:\n\n\nNorm of a Vector\n\n||\\vec{A}|| = \\sqrt{\\sum_{i=1}^{n} a_i^2}\n\\tag{2}\n\n\nDot-product of Two Vectors\nThe dot product is then defined as:\n\n\\vec{A} \\cdot \\vec{B} = \\sum_{i=1}^{n} a_i \\cdot b_i\n\\tag{3}\n\n\n\n\n\n\n\nCosine Similarity\n\n\n\n\nFigure 12: The cosine similarity is the cosine of the angle between two vectors.\n\n\nThe following cosine similarity equation makes sense:\n\n\\cos(\\theta) = \\frac{\\vec{v} \\cdot \\vec{w}}{||\\vec{v}|| \\cdot ||\\vec{w}||}\n\\tag{4}",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w3/index.html#implementation",
    "href": "notes/c1w3/index.html#implementation",
    "title": "Vector Space Models",
    "section": "Implementation",
    "text": "Implementation\nWhen \\vec{v} and \\vec{u} are parallel the numerator is equal to the denominator so cos(\\beta)=1 thus \\angle \\beta=0.\nOn the other hand, the dot product of two orthogonal (perpendicular) vectors is 0. That takes place when \\angle \\beta=90.\n\n\n\n\n\n\n\nCosine Similarity Examples\n\n\n\n\nFigure 13: Examples of cosine similarity between similar and dissimilar vectors.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w3/index.html#lab-on-manipulating-word-vectors",
    "href": "notes/c1w3/index.html#lab-on-manipulating-word-vectors",
    "title": "Vector Space Models",
    "section": "Lab on Manipulating Word Vectors",
    "text": "Lab on Manipulating Word Vectors\nThe lab",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w3/index.html#visualization-of-word-vectors",
    "href": "notes/c1w3/index.html#visualization-of-word-vectors",
    "title": "Vector Space Models",
    "section": "Visualization of Word Vectors",
    "text": "Visualization of Word Vectors\nPrincipal component analysis is an unsupervised learning algorithm which can be used to reduce the dimension of your data. As a result, it allows we to visualize your data. It tries to combine variances across features. Here is a concrete example of PCA:\n\n\n\n\n\n\n\nWord analogy\n\n\n\n\nFigure 16: Some word analogies\n\n\n\n\n\n\n\n\nWord analogy\n\n\n\n\nFigure 17: Some word analogies\n\n\n\nThose are the results of plotting a couple of vectors in two dimensions. Note that words with similar part of speech (POS) tags are next to one another. This is because many of the training algorithms learn words by identifying the neighboring words. Thus, words with similar POS tags tend to be found in similar locations. An interesting insight is that synonyms and antonyms tend to be found next to each other in the plot. Why is that the case?",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w3/index.html#implementation-1",
    "href": "notes/c1w3/index.html#implementation-1",
    "title": "Vector Space Models",
    "section": "Implementation",
    "text": "Implementation",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w3/index.html#pca-algorithm",
    "href": "notes/c1w3/index.html#pca-algorithm",
    "title": "Vector Space Models",
    "section": "PCA algorithm",
    "text": "PCA algorithm\nPCA is commonly used to reduce the dimension of your data. Intuitively the model collapses the data across principal components. We can think of the first principal component (in a 2D dataset) as the line where there is the most amount of variance. We can then collapse the data points on that line. Hence we went from 2D to 1D. We can generalize this intuition to several dimensions.\n\n\n\n\n\n\n\nPCA\n\n\n\n\nFigure 18: Some word analogies\n\n\n\nEigenvector\n\nthe resulting vectors, also known as the uncorrelated features of your data\n\nEigenvalue\n\nthe amount of information retained by each new feature. We can think of it as the variance in the eigenvector.\n\n\nAlso each eigenvalue has a corresponding eigenvector. The eigenvalue tells we how much variance there is in the eigenvector. Here are the steps required to compute PCA:\n\n\n\n\n\n\n\nPCA\n\n\n\n\nFigure 19: Some word analogies\n\n\n\nSteps to Compute PCA:\n\nMean normalize your data\nCompute the covariance matrix\nCompute SVD on your covariance matrix. This returns [USV]=svd(Σ) . The three matrices U, S, V are drawn above. U is labelled with eigenvectors, and S is labelled with eigenvalues.\nWe can then use the first n columns of vector U, to get your new data by multiplying XU[:,0:n].",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w3/index.html#putting-it-together-with-code",
    "href": "notes/c1w3/index.html#putting-it-together-with-code",
    "title": "Vector Space Models",
    "section": "Putting It Together with Code",
    "text": "Putting It Together with Code\n\nimport numpy as np \n\ndef PCA(X , num_components):\n  # center data around the mean\n  X_meaned = X - np.mean(X , axis = 0) \n  # calculate the covariance matrix   \n  cov_mat = np.cov(X_meaned , rowvar = False) \n  # compute an uncorrelated feature basis (eigen vectors) \n  eigen_values , eigen_vectors = np.linalg.eigh(cov_mat)\n  # sort the new basis by decreasing eigen values (variance) \n  sorted_index = np.argsort(eigen_values)[::-1] \n  sorted_eigenvalue = eigen_values[sorted_index] \n  sorted_eigenvectors = eigen_vectors[:,sorted_index] \n  # by subseting the most leading features  \n  eigenvector_subset = sorted_eigenvectors[:,0:num_components] \n  #Step-6 \n  X_reduced = np.dot(eigenvector_subset.transpose() ,     \n                   X_meaned.transpose() ).transpose() \n  return X_reduced",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w3/index.html#lab-on-pca",
    "href": "notes/c1w3/index.html#lab-on-pca",
    "title": "Vector Space Models",
    "section": "Lab on PCA",
    "text": "Lab on PCA\nPCA lab",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w3/index.html#resources",
    "href": "notes/c1w3/index.html#resources",
    "title": "Vector Space Models",
    "section": "Resources",
    "text": "Resources\n\nAlex Williams - Everything we did and didn’t know about PCA\nUdell et al. (2015). Generalized Low-Rank Models  arxiv preprint\nTipping & Bishop (1999). Probabilistic principal component analysis Journal of the Royal Statistical Society: Series B\nIlin & Raiko (2010) Practical Approaches to Principal Component Analysis in the Presence of Missing Values Journal of Machine Learning Research\nGordon (2002). Generalized2 Linear2 Models NIPS\nCunningham & Ghahramani (2015)  Linear dimensionality reduction: survey, insights, and generalizations Journal of Machine Learning Research\nBurges (2009). Dimension Reduction: A Guided Tour Foundations varia Trends in Machine Learning\nM. Gavish and D. L. Donoho, The Optimal Hard Threshold for Singular Values is \\frac{4}{\\sqrt{3}} in IEEE Transactions on Information Theory, vol. 60, no. 8, pp. 5040-5053, Aug. 2014, doi: 10.1109/TIT.2014.2323359.\nThomas P. Minka Automatic choice of dimensionality for PCA Dec. 2000",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w3/lab01.html",
    "href": "notes/c1w3/lab01.html",
    "title": "Linear algebra in Python with NumPy",
    "section": "",
    "text": "Figure 1: course banner\nIn this lab, we will have the opportunity to remember some basic concepts about linear algebra and how to use them in Python.\nNumpy is one of the most used libraries in Python for arrays manipulation. It adds to Python a set of functions that allows us to operate on large multidimensional arrays with just a few lines. So forget about writing nested loops for adding matrices! With NumPy, this is as simple as adding numbers.\nLet us import the numpy library and assign the alias np for it. We will follow this convention in almost every notebook in this course, and you’ll see this in many resources outside this course as well.\nimport numpy as np  # The swiss knife of the data scientist.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "L1 - Linear algebra with NumPy"
    ]
  },
  {
    "objectID": "notes/c1w3/lab01.html#defining-lists-and-numpy-arrays",
    "href": "notes/c1w3/lab01.html#defining-lists-and-numpy-arrays",
    "title": "Linear algebra in Python with NumPy",
    "section": "Defining lists and numpy arrays",
    "text": "Defining lists and numpy arrays\n\nalist = [1, 2, 3, 4, 5]   # Define a python list. It looks like an np array\nnarray = np.array([1, 2, 3, 4]) # Define a numpy array\n\nNote the difference between a Python list and a NumPy array.\n\nprint(alist)\nprint(narray)\n\nprint(type(alist))\nprint(type(narray))\n\n[1, 2, 3, 4, 5]\n[1 2 3 4]\n&lt;class 'list'&gt;\n&lt;class 'numpy.ndarray'&gt;",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "L1 - Linear algebra with NumPy"
    ]
  },
  {
    "objectID": "notes/c1w3/lab01.html#algebraic-operators-on-numpy-arrays-vs.-python-lists",
    "href": "notes/c1w3/lab01.html#algebraic-operators-on-numpy-arrays-vs.-python-lists",
    "title": "Linear algebra in Python with NumPy",
    "section": "Algebraic operators on NumPy arrays vs. Python lists",
    "text": "Algebraic operators on NumPy arrays vs. Python lists\nOne of the common beginner mistakes is to mix up the concepts of NumPy arrays and Python lists. Just observe the next example, where we add two objects of the two mentioned types. Note that the ‘+’ operator on NumPy arrays perform an element-wise addition, while the same operation on Python lists results in a list concatenation. Be careful while coding. Knowing this can save many headaches.\n\nprint(narray + narray)\nprint(alist + alist)\n\n[2 4 6 8]\n[1, 2, 3, 4, 5, 1, 2, 3, 4, 5]\n\n\nIt is the same as with the product operator, *. In the first case, we scale the vector, while in the second case, we concatenate three times the same list.\n\nprint(narray * 3)\nprint(alist * 3)\n\n[ 3  6  9 12]\n[1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5]\n\n\nBe aware of the difference because, within the same function, both types of arrays can appear. Numpy arrays are designed for numerical and matrix operations, while lists are for more general purposes.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "L1 - Linear algebra with NumPy"
    ]
  },
  {
    "objectID": "notes/c1w3/lab01.html#matrix-or-array-of-arrays",
    "href": "notes/c1w3/lab01.html#matrix-or-array-of-arrays",
    "title": "Linear algebra in Python with NumPy",
    "section": "Matrix or Array of Arrays",
    "text": "Matrix or Array of Arrays\nIn linear algebra, a matrix is a structure composed of n rows by m columns. That means each row must have the same number of columns. With NumPy, we have two ways to create a matrix: * Creating an array of arrays using np.array (recommended). * Creating a matrix using np.matrix (still available but might be removed soon).\nNumPy arrays or lists can be used to initialize a matrix, but the resulting matrix will be composed of NumPy arrays only.\n\nnpmatrix1 = np.array([narray, narray, narray]) # Matrix initialized with NumPy arrays\nnpmatrix2 = np.array([alist, alist, alist]) # Matrix initialized with lists\nnpmatrix3 = np.array([narray, [1, 1, 1, 1], narray]) # Matrix initialized with both types\n\nprint(npmatrix1)\nprint(npmatrix2)\nprint(npmatrix3)\n\n[[1 2 3 4]\n [1 2 3 4]\n [1 2 3 4]]\n[[1 2 3 4 5]\n [1 2 3 4 5]\n [1 2 3 4 5]]\n[[1 2 3 4]\n [1 1 1 1]\n [1 2 3 4]]\n\n\nHowever, when defining a matrix, be sure that all the rows contain the same number of elements. Otherwise, the linear algebra operations could lead to unexpected results.\nAnalyze the following two examples:\n\n# Example 1:\n\nokmatrix = np.array([[1, 2], [3, 4]]) # Define a 2x2 matrix\nprint(okmatrix) # Print okmatrix\nprint(okmatrix * 2) # Print a scaled version of okmatrix\n\n[[1 2]\n [3 4]]\n[[2 4]\n [6 8]]\n\n\n\n# Example 2:\n\nbadmatrix = np.array([[1, 2], [3, 4], [5, 6, 7]]) # Define a matrix. Note the third row contains 3 elements\nprint(badmatrix) # Print the malformed matrix\nprint(badmatrix * 2) # It is supposed to scale the whole matrix\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[8], line 3\n      1 # Example 2:\n----&gt; 3 badmatrix = np.array([[1, 2], [3, 4], [5, 6, 7]]) # Define a matrix. Note the third row contains 3 elements\n      4 print(badmatrix) # Print the malformed matrix\n      5 print(badmatrix * 2) # It is supposed to scale the whole matrix\n\nValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "L1 - Linear algebra with NumPy"
    ]
  },
  {
    "objectID": "notes/c1w3/lab01.html#scaling-and-translating-matrices",
    "href": "notes/c1w3/lab01.html#scaling-and-translating-matrices",
    "title": "Linear algebra in Python with NumPy",
    "section": "Scaling and translating matrices",
    "text": "Scaling and translating matrices\nNow that we know how to build correct NumPy arrays and matrices, let us see how easy it is to operate with them in Python using the regular algebraic operators like + and -.\nOperations can be performed between arrays and arrays or between arrays and scalars.\n\n# Scale by 2 and translate 1 unit the matrix\nresult = okmatrix * 2 + 1 # For each element in the matrix, multiply by 2 and add 1\nprint(result)\n\n[[3 5]\n [7 9]]\n\n\n\n# Add two compatible matrices\nresult1 = okmatrix + okmatrix\nprint(result1)\n\n# Subtract two compatible matrices. This is called the difference vector\nresult2 = okmatrix - okmatrix\nprint(result2)\n\n[[2 4]\n [6 8]]\n[[0 0]\n [0 0]]\n\n\nThe product operator * when used on arrays or matrices indicates element-wise multiplications. Do not confuse it with the dot product.\n\nresult = okmatrix * okmatrix # Multiply each element by itself\nprint(result)\n\n[[ 1  4]\n [ 9 16]]",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "L1 - Linear algebra with NumPy"
    ]
  },
  {
    "objectID": "notes/c1w3/lab01.html#transpose-a-matrix",
    "href": "notes/c1w3/lab01.html#transpose-a-matrix",
    "title": "Linear algebra in Python with NumPy",
    "section": "Transpose a matrix",
    "text": "Transpose a matrix\nIn linear algebra, the transpose of a matrix is an operator that flips a matrix over its diagonal, i.e., the transpose operator switches the row and column indices of the matrix producing another matrix. If the original matrix dimension is n by m, the resulting transposed matrix will be m by n.\nT denotes the transpose operations with NumPy matrices.\n\nmatrix3x2 = np.array([[1, 2], [3, 4], [5, 6]]) # Define a 3x2 matrix\nprint('Original matrix 3 x 2')\nprint(matrix3x2)\nprint('Transposed matrix 2 x 3')\nprint(matrix3x2.T)\n\nOriginal matrix 3 x 2\n[[1 2]\n [3 4]\n [5 6]]\nTransposed matrix 2 x 3\n[[1 3 5]\n [2 4 6]]\n\n\nHowever, note that the transpose operation does not affect 1D arrays.\n\nnparray = np.array([1, 2, 3, 4]) # Define an array\nprint('Original array')\nprint(nparray)\nprint('Transposed array')\nprint(nparray.T)\n\nOriginal array\n[1 2 3 4]\nTransposed array\n[1 2 3 4]\n\n\nperhaps in this case we wanted to do:\n\nnparray = np.array([[1, 2, 3, 4]]) # Define a 1 x 4 matrix. Note the 2 level of square brackets\nprint('Original array')\nprint(nparray)\nprint('Transposed array')\nprint(nparray.T)\n\nOriginal array\n[[1 2 3 4]]\nTransposed array\n[[1]\n [2]\n [3]\n [4]]",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "L1 - Linear algebra with NumPy"
    ]
  },
  {
    "objectID": "notes/c1w3/lab01.html#get-the-norm-of-a-nparray-or-matrix",
    "href": "notes/c1w3/lab01.html#get-the-norm-of-a-nparray-or-matrix",
    "title": "Linear algebra in Python with NumPy",
    "section": "Get the norm of a nparray or matrix",
    "text": "Get the norm of a nparray or matrix\nIn linear algebra, the norm of an n-dimensional vector \\vec a is defined as:\n norm(\\vec a) = ||\\vec a|| = \\sqrt {\\sum_{i=1}^{n} a_i ^ 2}\nCalculating the norm of vector or even of a matrix is a general operation when dealing with data. Numpy has a set of functions for linear algebra in the subpackage linalg, including the norm function. Let us see how to get the norm a given array or matrix:\n\nnparray1 = np.array([1, 2, 3, 4]) # Define an array\nnorm1 = np.linalg.norm(nparray1)\n\nnparray2 = np.array([[1, 2], [3, 4]]) # Define a 2 x 2 matrix. Note the 2 level of square brackets\nnorm2 = np.linalg.norm(nparray2) \n\nprint(norm1)\nprint(norm2)\n\n5.477225575051661\n5.477225575051661\n\n\nNote that without any other parameter, the norm function treats the matrix as being just an array of numbers. However, it is possible to get the norm by rows or by columns. The axis parameter controls the form of the operation: * axis=0 means get the norm of each column * axis=1 means get the norm of each row.\n\nnparray2 = np.array([[1, 1], [2, 2], [3, 3]]) # Define a 3 x 2 matrix. \n\nnormByCols = np.linalg.norm(nparray2, axis=0) # Get the norm for each column. Returns 2 elements\nnormByRows = np.linalg.norm(nparray2, axis=1) # get the norm for each row. Returns 3 elements\n\nprint(normByCols)\nprint(normByRows)\n\n[3.74165739 3.74165739]\n[1.41421356 2.82842712 4.24264069]\n\n\nHowever, there are more ways to get the norm of a matrix in Python. For that, let us see all the different ways of defining the dot product between 2 arrays.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "L1 - Linear algebra with NumPy"
    ]
  },
  {
    "objectID": "notes/c1w3/lab01.html#the-dot-product-between-arrays-all-the-flavors",
    "href": "notes/c1w3/lab01.html#the-dot-product-between-arrays-all-the-flavors",
    "title": "Linear algebra in Python with NumPy",
    "section": "The dot product between arrays: All the flavors",
    "text": "The dot product between arrays: All the flavors\nThe dot product or scalar product or inner product between two vectors \\vec a and \\vec b of the same size is defined as: \\vec a \\cdot \\vec b = \\sum_{i=1}^{n} a_i b_i\nThe dot product takes two vectors and returns a single number.\n\nnparray1 = np.array([0, 1, 2, 3]) # Define an array\nnparray2 = np.array([4, 5, 6, 7]) # Define an array\n\nflavor1 = np.dot(nparray1, nparray2) # Recommended way\nprint(flavor1)\n\nflavor2 = np.sum(nparray1 * nparray2) # Ok way\nprint(flavor2)\n\nflavor3 = nparray1 @ nparray2         # Geeks way\nprint(flavor3)\n\n# As we never should do:             # Noobs way\nflavor4 = 0\nfor a, b in zip(nparray1, nparray2):\n    flavor4 += a * b\n    \nprint(flavor4)\n\n38\n38\n38\n38\n\n\nWe strongly recommend using np.dot, since it is the only method that accepts arrays and lists without problems\n\nnorm1 = np.dot(np.array([1, 2]), np.array([3, 4])) # Dot product on nparrays\nnorm2 = np.dot([1, 2], [3, 4]) # Dot product on python lists\n\nprint(norm1, '=', norm2 )\n\n11 = 11\n\n\nFinally, note that the norm is the square root of the dot product of the vector with itself. That gives many options to write that function:\n norm(\\vec a) = ||\\vec a|| = \\sqrt {\\sum_{i=1}^{n} a_i ^ 2} = \\sqrt {a \\cdot a}",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "L1 - Linear algebra with NumPy"
    ]
  },
  {
    "objectID": "notes/c1w3/lab01.html#sums-by-rows-or-columns",
    "href": "notes/c1w3/lab01.html#sums-by-rows-or-columns",
    "title": "Linear algebra in Python with NumPy",
    "section": "Sums by rows or columns",
    "text": "Sums by rows or columns\nAnother general operation performed on matrices is the sum by rows or columns. Just as we did for the function norm, the axis parameter controls the form of the operation: * axis=0 means to sum the elements of each column together. * axis=1 means to sum the elements of each row together.\n\nnparray2 = np.array([[1, -1], [2, -2], [3, -3]]) # Define a 3 x 2 matrix. \n\nsumByCols = np.sum(nparray2, axis=0) # Get the sum for each column. Returns 2 elements\nsumByRows = np.sum(nparray2, axis=1) # get the sum for each row. Returns 3 elements\n\nprint('Sum by columns: ')\nprint(sumByCols)\nprint('Sum by rows:')\nprint(sumByRows)\n\nSum by columns: \n[ 6 -6]\nSum by rows:\n[0 0 0]",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "L1 - Linear algebra with NumPy"
    ]
  },
  {
    "objectID": "notes/c1w3/lab01.html#get-the-mean-by-rows-or-columns",
    "href": "notes/c1w3/lab01.html#get-the-mean-by-rows-or-columns",
    "title": "Linear algebra in Python with NumPy",
    "section": "Get the mean by rows or columns",
    "text": "Get the mean by rows or columns\nAs with the sums, one can get the mean by rows or columns using the axis parameter. Just remember that the mean is the sum of the elements divided by the length of the vector  mean(\\vec a) = \\frac {{\\sum_{i=1}^{n} a_i }}{n}\n\nnparray2 = np.array([[1, -1], [2, -2], [3, -3]]) # Define a 3 x 2 matrix. Chosen to be a matrix with 0 mean\n\nmean = np.mean(nparray2) # Get the mean for the whole matrix\nmeanByCols = np.mean(nparray2, axis=0) # Get the mean for each column. Returns 2 elements\nmeanByRows = np.mean(nparray2, axis=1) # get the mean for each row. Returns 3 elements\n\nprint('Matrix mean: ')\nprint(mean)\nprint('Mean by columns: ')\nprint(meanByCols)\nprint('Mean by rows:')\nprint(meanByRows)\n\nMatrix mean: \n0.0\nMean by columns: \n[ 2. -2.]\nMean by rows:\n[0. 0. 0.]",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "L1 - Linear algebra with NumPy"
    ]
  },
  {
    "objectID": "notes/c1w3/lab01.html#center-the-columns-of-a-matrix",
    "href": "notes/c1w3/lab01.html#center-the-columns-of-a-matrix",
    "title": "Linear algebra in Python with NumPy",
    "section": "Center the columns of a matrix",
    "text": "Center the columns of a matrix\nCentering the attributes of a data matrix is another essential preprocessing step. Centering a matrix means to remove the column mean to each element inside the column. The mean by columns of a centered matrix is always 0.\nWith NumPy, this process is as simple as this:\n\nnparray2 = np.array([[1, 1], [2, 2], [3, 3]]) # Define a 3 x 2 matrix. \n\nnparrayCentered = nparray2 - np.mean(nparray2, axis=0) # Remove the mean for each column\n\nprint('Original matrix')\nprint(nparray2)\nprint('Centered by columns matrix')\nprint(nparrayCentered)\n\nprint('New mean by column')\nprint(nparrayCentered.mean(axis=0))\n\nOriginal matrix\n[[1 1]\n [2 2]\n [3 3]]\nCentered by columns matrix\n[[-1. -1.]\n [ 0.  0.]\n [ 1.  1.]]\nNew mean by column\n[0. 0.]\n\n\nWarning: This process does not apply for row centering. In such cases, consider transposing the matrix, centering by columns, and then transpose back the result.\nSee the example below:\n\nnparray2 = np.array([[1, 3], [2, 4], [3, 5]]) # Define a 3 x 2 matrix. \n\nnparrayCentered = nparray2.T - np.mean(nparray2, axis=1) # Remove the mean for each row\nnparrayCentered = nparrayCentered.T # Transpose back the result\n\nprint('Original matrix')\nprint(nparray2)\nprint('Centered by rows matrix')\nprint(nparrayCentered)\n\nprint('New mean by rows')\nprint(nparrayCentered.mean(axis=1))\n\nOriginal matrix\n[[1 3]\n [2 4]\n [3 5]]\nCentered by rows matrix\n[[-1.  1.]\n [-1.  1.]\n [-1.  1.]]\nNew mean by rows\n[0. 0. 0.]\n\n\nNote that some operations can be performed using static functions like np.sum() or np.mean(), or by using the inner functions of the array\n\nnparray2 = np.array([[1, 3], [2, 4], [3, 5]]) # Define a 3 x 2 matrix. \n\nmean1 = np.mean(nparray2) # Static way\nmean2 = nparray2.mean()   # Dinamic way\n\nprint(mean1, ' == ', mean2)\n\n3.0  ==  3.0\n\n\nEven if they are equivalent, we recommend the use of the static way always.\nCongratulations! We have successfully reviewed vector and matrix operations with Numpy!",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "L1 - Linear algebra with NumPy"
    ]
  },
  {
    "objectID": "notes/cs11-737-w03-typology/index.html",
    "href": "notes/cs11-737-w03-typology/index.html",
    "title": "Typology: The Space of Languages",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\nVideo 1: Lesson Video\n\n\n\n\n\n\n\n\nThis week’s slides\n\n\n\n\nSupplementary Figure 1"
  },
  {
    "objectID": "notes/cs11-737-w03-typology/index.html#multilingual-aspects",
    "href": "notes/cs11-737-w03-typology/index.html#multilingual-aspects",
    "title": "Typology: The Space of Languages",
    "section": "Multilingual aspects",
    "text": "Multilingual aspects\nout there if you want to learn about morphology or phonology you can look at multilingual aspects in the in the computational linguistics and conferences there’s lots of geographical groups that are looking at say specifically looking at indian languages african languages there are lots of things that are looking at low resource languages there are lots of ones looking at interesting morphology languages and so often it’s worth looking at and though everybody who doesn’t know a language thinks i’ll just train from an infinite amount of data the answer is well you won’t have an infinite amount of data and sometimes it’s quite hard to find data and if you discover that morphology is rich in the particular language it might be worth doing morphological segmentation and there may already be an existing morphological analyzer that’s there or at least help to be able to find that okay so that’s a very quick view of typology on how we actually structure languages and it’s becoming more and more important in the computational form than what it was been before 10 15 years ago you’d see less papers about it but now people are really caring about it because we are doing much more multilingual work you were asked to read this particular paper which was a survey paper on looking at aspects of typology across different people trying to do predictions and how well they were actually doing and the issues that are involved in this and what we’re going to do now is we’re going to split you off into groups and in those groups you’ll have a ta or an instructor will be there and what we want you to do is we want you each of you to identify things which are unique or very rare compared to other languages that are important over the languages you know in distinct from things which are not very interesting maybe whole classes of languages that are unrelated are all using a romanized form and to write them but they’re not related but things that are going to be unique from that point of view now a has someone set up the groups graham have you set up the groups have maybe we could take some questions if people had questions yes sure yes thank you 22 indian languages yeah yeah so what language you write things in is quite interesting and especially once you’re in a code switching space in india almost everybody when they’re code switching will write in a romanized form they’ll often call it english but it’s not english it’s the romanized form so they’re writing both hindi in a romanized form and english in romanesque form while when you look at singapore for example where people can be as fluent in chinese and english they actually use hansi for writing chinese and english for writing english words most of that’s got to do with input method actually it’s like how easy is it to type these things on a computer and for historical reasons actually partly because there was less Chinese speakers who spoke english the Chinese input systems became better while in india for the past 200 years the educated elite were all English-speaking and therefore they were used to reading and writing in using romanized m form probably that’s got something to do with it but definitely information is lost when you may be using a non-native script and for example spelling goes on completely out of the way in English when you’re doing it but remember most written most scripts are not appropriate for the language we are using a latin script for a germanic english in english. We use Kanji in Japanese for writing lots of things and yeah there’s other scripts in Japanese for dealing with more native things. Hangul is native in Korea but there’s still lots of Chinese borrowed words, especially scientific words, that come from Chinese. So often the writing system even for the native speakers is not very appropriate. Often it’s just convention. It’s like this is the way we write it and we’ve always written it. And it was only since last year that people live but they think it’s facts that we’ve done it forever. You know basically we’ve always sent dances to tiktok as a way to communicate and I can’t remember when when we ever did anything else"
  },
  {
    "objectID": "notes/cs11-737-w01-inro/index.html",
    "href": "notes/cs11-737-w01-inro/index.html",
    "title": "Text Classification",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\nVideo 1: Lesson Video\n\n\n\n\n\n\n\n\nThis week’s slides\n\n\n\n\nSupplementary Figure 1: Slides for this week\n\n\n\n\n\n\n\n\nThis week’s slides\n\n\n\n\nSupplementary Figure 2: Speech Slides\n\n\n\n\n\n\n\n\nThis week’s slides\n\n\n\n\nSupplementary Figure 3: Hindi in 10 minutes!"
  },
  {
    "objectID": "notes/cs11-737-w01-inro/index.html#discussion-question",
    "href": "notes/cs11-737-w01-inro/index.html#discussion-question",
    "title": "Text Classification",
    "section": "Discussion Question:",
    "text": "Discussion Question:\n\nWhat are some unique typological features of a language that you know, regarding phonology, morphology, syntax, semantics, pragmatics?\n\nHebrew has unique typological features:\n\nEtymology:\n\nMost of the Hebrew words are are derived from the bible.\nHebrew is a semitic language, and has a large number of cognates with other semitic languages like arabic and aramaic.\nHebrew has a large number of loan words from other languages, including English, Arabic, and Yiddish.\nHebrew is an old language, and has a large number of archaic words and constructions that are not used in modern Hebrew.\nWords that were introduced into Hebrew during different areas may have some differences in stress patterns, etc.\nHebrew has died out as a spoken language and been revived a number of times being preserved in the intrim as a liturgical language. During the time of the new testament, Hebrew had been replaced by Aramaic as the spoken language, but was revived after the inserection of Bar Kochba. Historical israel was depopulated from hebrew speakers by the arabs in the 4th century A.D. The modern revival of Hebrew was started by Eliezer Ben Yehuda in the 19th century.\n\nOrthography:\n\nHebrew has a unique script that is written from right to left.\nHebrew orthography where the vowels are not written and final forms of the letters are used at the end of the word.\n\nPhonology:\n\nMany sounds in hebrew have effectively merged when compared to neighboring semitic languages.\nHebrew also has a script for indicating vowels, but these are omitted in most written texts. Correctly annotating the full range vowels required a sophisticated understanding of the language and is not in the skill set of most writers.\n\nMorphology: Hebrew has\n\nA templatic morphology, where roots are three or more consonants and template are vowels and affixes. The root are inserted into the template to form words.\nMost foriegn words do not follow this pattern, but undergo a process of nativization that allow them to take some suffixes.\nHebrew has a construct state AKA smichut, where two nouns are combined to form a new noun and these follow a number of rules.\nHebrew can combines one or more small words as prefixes to the verb.\nHebrew morphology has a number of additional mechanism for word formation described in “The final word” by Uzzi Ornan.\n\nSemantics\n\nWritten hebrew is highly ambiguous, and the meaning of a word is frequently determined by the context. It takes about a year for readers to learn to read hebrew fluently and even professional1 news casters make mistakes when the contexts the determine the meaning of a word appears later in the sentence.\n\nSyntax\n\nHebrew has a subject verb object word order, but this is not strict and can be changed for emphasis.\nHebrew has a number of particles that are used to indicate the relationship between words in a sentence.\n\n\n1 today most news are not read by professionals."
  },
  {
    "objectID": "notes/nmt-01/index.html",
    "href": "notes/nmt-01/index.html",
    "title": "Machine Translation Course 2020 - Lecture 1 - Introduction",
    "section": "",
    "text": "Video 1\n\n\n\n\n\n\n\n\nThis week’s slides\n\n\n\n\nFigure 1\n\n\n\nIn 2020 Roee Aharoni gave a course on Neural Machine Translation at Bar Ilan University.\nThis are some notes on the first lecture in the series\nLets imagine we have come into contact with good and bad aliens. We have a small parallel corpus of good alien’s languages AND the bad alien’s languages.\nWe start by asking if we had a small parallel corpus of good aliens languages how would we learn to translate between them?\n\nWe have S^g_i \\to S^b_j a translation mapping.\n\nwe want to extend this mapping\n\n\n\nCitationBibTeX citation:@online{bochman2021,\n  author = {Bochman, Oren},\n  title = {Machine {Translation} {Course} 2020 - {Lecture} 1 -\n    {Introduction}},\n  date = {2021-03-01},\n  url = {https://orenbochman.github.io/notes-nlp/notes/nmt-01/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2021. “Machine Translation Course 2020 - Lecture 1\n- Introduction.” March 1, 2021. https://orenbochman.github.io/notes-nlp/notes/nmt-01/."
  },
  {
    "objectID": "notes/c1w2/code.html",
    "href": "notes/c1w2/code.html",
    "title": "Probability and Bayes Rule",
    "section": "",
    "text": "course banner\n\n\nimport pandas as pd\nimport string \nraw_tweets=[\n  \"I am happy because I am learning NLP\",\n  \"I am sad, I am not learning NLP\",\n  \"I am happy, not sad\",\n  \"I am sad, not happy\",\n]\ndef clean(tweet:str):\n  return  tweet.translate(str.maketrans('', '', string.punctuation)).lower()\ntweets = [clean(tweet) for tweet in raw_tweets]\nlabels=['+','-','+','-']\ndf = pd.DataFrame({'tweets': tweets, 'labels': labels})\ndf\n\n\n\n\n\n\n\n\ntweets\nlabels\n\n\n\n\n0\ni am happy because i am learning nlp\n+\n\n\n1\ni am sad i am not learning nlp\n-\n\n\n2\ni am happy not sad\n+\n\n\n3\ni am sad not happy\n-\n\n\n\n\n\n\n\n\nimport numpy as np\nfrom collections import Counter\np_freq,n_freq = Counter(), Counter()\n#print( df[df.labels == '+']['tweets'].to_list())\n[p_freq.update(tweet.split()) for tweet in df[df.labels == '+']['tweets'].to_list()]\n[n_freq.update(tweet.split()) for tweet in df[df.labels == '-']['tweets'].to_list()]\nprint(p_freq)\nprint(n_freq)\nvocab = list(set(p_freq.keys()).union(set(n_freq.keys())))\npos_freq = [p_freq[word] for word in vocab ]\nneg_freq = [n_freq[word] for word in vocab ]\nvocab_df=pd.DataFrame({'vocab':vocab,'pos_freq':pos_freq,'neg_freq':neg_freq})\nvocab_df['p_pos']=vocab_df.pos_freq/vocab_df.pos_freq.sum()\nvocab_df['p_neg']=vocab_df.neg_freq/vocab_df.neg_freq.sum()\nvocab_df['p_pos_sm']=(vocab_df.pos_freq+1)/(vocab_df.pos_freq.sum()+vocab_df.shape[1])\nvocab_df['p_neg_sm']=(vocab_df.neg_freq+1)/(vocab_df.neg_freq.sum()+vocab_df.shape[1])\nvocab_df['ratio']= vocab_df.p_pos_sm/vocab_df.p_neg_sm\nvocab_df['lambda']= np.log(vocab_df.p_pos_sm/vocab_df.p_neg_sm)\npd.set_option('display.float_format', '{:.2f}'.format)\nvocab_df\nprint(vocab_df.shape)\n\n[None, None]\n\n\n[None, None]\n\n\nCounter({'i': 3, 'am': 3, 'happy': 2, 'because': 1, 'learning': 1, 'nlp': 1, 'not': 1, 'sad': 1})\nCounter({'i': 3, 'am': 3, 'sad': 2, 'not': 2, 'learning': 1, 'nlp': 1, 'happy': 1})\n\n\n\n\n\n\n\n\n\nvocab\npos_freq\nneg_freq\np_pos\np_neg\np_pos_sm\np_neg_sm\nratio\nlambda\n\n\n\n\n0\nam\n3\n3\n0.23\n0.23\n0.22\n0.21\n1.06\n0.05\n\n\n1\nnlp\n1\n1\n0.08\n0.08\n0.11\n0.11\n1.06\n0.05\n\n\n2\nbecause\n1\n0\n0.08\n0.00\n0.11\n0.05\n2.11\n0.75\n\n\n3\nlearning\n1\n1\n0.08\n0.08\n0.11\n0.11\n1.06\n0.05\n\n\n4\nhappy\n2\n1\n0.15\n0.08\n0.17\n0.11\n1.58\n0.46\n\n\n5\nsad\n1\n2\n0.08\n0.15\n0.11\n0.16\n0.70\n-0.35\n\n\n6\ni\n3\n3\n0.23\n0.23\n0.22\n0.21\n1.06\n0.05\n\n\n7\nnot\n1\n2\n0.08\n0.15\n0.11\n0.16\n0.70\n-0.35\n\n\n\n\n\n\n\n(8, 9)\n\n\n\nfrom IPython.display import Markdown\nfrom tabulate import tabulate\ntable = [[\"Sun\",696000,1989100000],\n         [\"Earth\",6371,5973.6],\n         [\"Moon\",1737,73.5],\n         [\"Mars\",3390,641.85]]\nMarkdown(tabulate(\n  table, \n  headers=[\"Planet\",\"R (km)\", \"mass (x 10^29 kg)\"]\n))\n\n\n\nTable 1: Planets\n\n\n\n\n\n\nPlanet\nR (km)\nmass (x 10^29 kg)\n\n\n\n\nSun\n696000\n1.9891e+09\n\n\nEarth\n6371\n5973.6\n\n\nMoon\n1737\n73.5\n\n\nMars\n3390\n641.85\n\n\n\n\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{bochman2020,\n  author = {Bochman, Oren},\n  title = {Probability and {Bayes} {Rule}},\n  date = {2020-10-06},\n  url = {https://orenbochman.github.io/notes-nlp/notes/c1w2/code.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2020. “Probability and Bayes Rule.” October\n6, 2020. https://orenbochman.github.io/notes-nlp/notes/c1w2/code.html."
  },
  {
    "objectID": "notes/c2w4/lab05.html",
    "href": "notes/c2w4/lab05.html",
    "title": "Word Embeddings: Hands On",
    "section": "",
    "text": "course banner\nIn previous lecture notebooks we saw all the steps needed to train the CBOW model. This notebook will walk we through how to extract the word embedding vectors from a model.\nLet’s dive into it!\nimport numpy as np\nfrom utils2 import get_dict\nBefore moving on, we will be provided with some variables needed for further procedures, which should be familiar by now. Also a trained CBOW model will be simulated, the corresponding weights and biases are provided:\n# Define the tokenized version of the corpus\nwords = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n\n# Define V. Remember this is the size of the vocabulary\nV = 5\n\n# Get 'word2Ind' and 'Ind2word' dictionaries for the tokenized corpus\nword2Ind, Ind2word = get_dict(words)\n\n\n# Define first matrix of weights\nW1 = np.array([[ 0.41687358,  0.08854191, -0.23495225,  0.28320538,  0.41800106],\n               [ 0.32735501,  0.22795148, -0.23951958,  0.4117634 , -0.23924344],\n               [ 0.26637602, -0.23846886, -0.37770863, -0.11399446,  0.34008124]])\n\n# Define second matrix of weights\nW2 = np.array([[-0.22182064, -0.43008631,  0.13310965],\n               [ 0.08476603,  0.08123194,  0.1772054 ],\n               [ 0.1871551 , -0.06107263, -0.1790735 ],\n               [ 0.07055222, -0.02015138,  0.36107434],\n               [ 0.33480474, -0.39423389, -0.43959196]])\n\n# Define first vector of biases\nb1 = np.array([[ 0.09688219],\n               [ 0.29239497],\n               [-0.27364426]])\n\n# Define second vector of biases\nb2 = np.array([[ 0.0352008 ],\n               [-0.36393384],\n               [-0.12775555],\n               [-0.34802326],\n               [-0.07017815]])"
  },
  {
    "objectID": "notes/c2w4/lab05.html#extracting-word-embedding-vectors",
    "href": "notes/c2w4/lab05.html#extracting-word-embedding-vectors",
    "title": "Word Embeddings: Hands On",
    "section": "Extracting word embedding vectors",
    "text": "Extracting word embedding vectors\nOnce we have finished training the neural network, we have three options to get word embedding vectors for the words of your vocabulary, based on the weight matrices \\mathbf{W_1} and/or \\mathbf{W_2}.\n\nOption 1: extract embedding vectors from \\mathbf{W_1}\nThe first option is to take the columns of \\mathbf{W_1} as the embedding vectors of the words of the vocabulary, using the same order of the words as for the input and output vectors.\n\nNote: in this practice notebooks the values of the word embedding vectors are meaningless after a single iteration with just one training example, but here’s how we would proceed after the training process is complete.\n\nFor example \\mathbf{W_1} is this matrix:\n\n# Print W1\nW1\n\narray([[ 0.41687358,  0.08854191, -0.23495225,  0.28320538,  0.41800106],\n       [ 0.32735501,  0.22795148, -0.23951958,  0.4117634 , -0.23924344],\n       [ 0.26637602, -0.23846886, -0.37770863, -0.11399446,  0.34008124]])\n\n\nThe first column, which is a 3-element vector, is the embedding vector of the first word of your vocabulary. The second column is the word embedding vector for the second word, and so on.\nThe first, second, etc. words are ordered as follows.\n\n# Print corresponding word for each index within vocabulary's range\nfor i in range(V):\n    print(Ind2word[i])\n\nam\nbecause\nhappy\ni\nlearning\n\n\nSo the word embedding vectors corresponding to each word are:\n\n# Loop through each word of the vocabulary\nfor word in word2Ind:\n    # Extract the column corresponding to the index of the word in the vocabulary\n    word_embedding_vector = W1[:, word2Ind[word]]\n    # Print word alongside word embedding vector\n    print(f'{word}: {word_embedding_vector}')\n\nam: [0.41687358 0.32735501 0.26637602]\nbecause: [ 0.08854191  0.22795148 -0.23846886]\nhappy: [-0.23495225 -0.23951958 -0.37770863]\ni: [ 0.28320538  0.4117634  -0.11399446]\nlearning: [ 0.41800106 -0.23924344  0.34008124]\n\n\n\n\nOption 2: extract embedding vectors from \\mathbf{W_2}\nThe second option is to take \\mathbf{W_2} transposed, and take its columns as the word embedding vectors just like we did for \\mathbf{W_1}.\n\n# Print transposed W2\nW2.T\n\narray([[-0.22182064,  0.08476603,  0.1871551 ,  0.07055222,  0.33480474],\n       [-0.43008631,  0.08123194, -0.06107263, -0.02015138, -0.39423389],\n       [ 0.13310965,  0.1772054 , -0.1790735 ,  0.36107434, -0.43959196]])\n\n\n\n# Loop through each word of the vocabulary\nfor word in word2Ind:\n    # Extract the column corresponding to the index of the word in the vocabulary\n    word_embedding_vector = W2.T[:, word2Ind[word]]\n    # Print word alongside word embedding vector\n    print(f'{word}: {word_embedding_vector}')\n\nam: [-0.22182064 -0.43008631  0.13310965]\nbecause: [0.08476603 0.08123194 0.1772054 ]\nhappy: [ 0.1871551  -0.06107263 -0.1790735 ]\ni: [ 0.07055222 -0.02015138  0.36107434]\nlearning: [ 0.33480474 -0.39423389 -0.43959196]\n\n\n\n\nOption 3: extract embedding vectors from \\mathbf{W_1} and \\mathbf{W_2}\nThe third option, which is the one we will use in this week’s assignment, uses the average of \\mathbf{W_1} and \\mathbf{W_2^\\top}.\nCalculate the average of \\mathbf{W_1} and \\mathbf{W_2^\\top}, and store the result in W3.\n\n# Compute W3 as the average of W1 and W2 transposed\nW3 = (W1+W2.T)/2\n\n# Print W3\nW3\n\narray([[ 0.09752647,  0.08665397, -0.02389858,  0.1768788 ,  0.3764029 ],\n       [-0.05136565,  0.15459171, -0.15029611,  0.19580601, -0.31673866],\n       [ 0.19974284, -0.03063173, -0.27839106,  0.12353994, -0.04975536]])\n\n\nExpected output:\narray([[ 0.09752647,  0.08665397, -0.02389858,  0.1768788 ,  0.3764029 ],\n       [-0.05136565,  0.15459171, -0.15029611,  0.19580601, -0.31673866],\n       [ 0.19974284, -0.03063173, -0.27839106,  0.12353994, -0.04975536]])\nExtracting the word embedding vectors works just like the two previous options, by taking the columns of the matrix you’ve just created.\n\n# Loop through each word of the vocabulary\nfor word in word2Ind:\n    # Extract the column corresponding to the index of the word in the vocabulary\n    word_embedding_vector = W3[:, word2Ind[word]]\n    # Print word alongside word embedding vector\n    print(f'{word}: {word_embedding_vector}')\n\nam: [ 0.09752647 -0.05136565  0.19974284]\nbecause: [ 0.08665397  0.15459171 -0.03063173]\nhappy: [-0.02389858 -0.15029611 -0.27839106]\ni: [0.1768788  0.19580601 0.12353994]\nlearning: [ 0.3764029  -0.31673866 -0.04975536]\n\n\nNow we know 3 different options to get the word embedding vectors from a model!\n\n\nHow this practice relates to and differs from the upcoming graded assignment\n\nAfter extracting the word embedding vectors, we will use principal component analysis (PCA) to visualize the vectors, which will enable we to perform an intrinsic evaluation of the quality of the vectors, as explained in the lecture.\n\nCongratulations on finishing all lecture notebooks for this week!\nYou’re now ready to take on this week’s assignment!\nKeep it up!"
  },
  {
    "objectID": "notes/c2w4/lab02.html",
    "href": "notes/c2w4/lab02.html",
    "title": "Word Embeddings: Intro to CBOW model, activation functions and working with Numpy",
    "section": "",
    "text": "course banner\nIn this lecture notebook we will be given an introduction to the continuous bag-of-words model, its activation functions and some considerations when working with Numpy.\nLet’s dive into it!\nimport numpy as np",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "L2 - Intro to CBOW"
    ]
  },
  {
    "objectID": "notes/c2w4/lab02.html#activation-functions",
    "href": "notes/c2w4/lab02.html#activation-functions",
    "title": "Word Embeddings: Intro to CBOW model, activation functions and working with Numpy",
    "section": "Activation functions",
    "text": "Activation functions\nLet’s start by implementing the activation functions, ReLU and softmax.\n\nReLU\nReLU is used to calculate the values of the hidden layer, in the following formulas:\n\\begin{align}\n\\mathbf{z_1} &= \\mathbf{W_1}\\mathbf{x} + \\mathbf{b_1}  \\tag{1} \\\\\n\\mathbf{h} &= \\mathrm{ReLU}(\\mathbf{z_1})  \\tag{2} \\\\\n\\end{align}\nLet’s fix a value for \\mathbf{z_1} as a working example.\n\n# Define a random seed so all random outcomes can be reproduced\nnp.random.seed(10)\n\n# Define a 5X1 column vector using numpy\nz_1 = 10*np.random.rand(5, 1)-5\n\n# Print the vector\nz_1\n\narray([[ 2.71320643],\n       [-4.79248051],\n       [ 1.33648235],\n       [ 2.48803883],\n       [-0.01492988]])\n\n\nNotice that using numpy’s random.rand function returns a numpy array filled with values taken from a uniform distribution over [0, 1). Numpy allows vectorization so each value is multiplied by 10 and then substracted 5.\nTo get the ReLU of this vector, we want all the negative values to become zeros.\nFirst create a copy of this vector.\n\n# Create copy of vector and save it in the 'h' variable\nh = z_1.copy()\n\nNow determine which of its values are negative.\n\n# Determine which values met the criteria (this is possible because of vectorization)\nh &lt; 0\n\narray([[False],\n       [ True],\n       [False],\n       [False],\n       [ True]])\n\n\nWe can now simply set all of the values which are negative to 0.\n\n# Slice the array or vector. This is the same as applying ReLU to it\nh[h &lt; 0] = 0\n\nAnd that’s it: we have the ReLU of \\mathbf{z_1}!\n\n# Print the vector after ReLU\nh\n\narray([[2.71320643],\n       [0.        ],\n       [1.33648235],\n       [2.48803883],\n       [0.        ]])\n\n\nNow implement ReLU as a function.\n\n# Define the 'relu' function that will include the steps previously seen\ndef relu(z):\n    result = z.copy()\n    result[result &lt; 0] = 0\n    return result\n\nAnd check that it’s working.\n\n# Define a new vector and save it in the 'z' variable\nz = np.array([[-1.25459881], [ 4.50714306], [ 2.31993942], [ 0.98658484], [-3.4398136 ]])\n\n# Apply ReLU to it\nrelu(z)\n\narray([[0.        ],\n       [4.50714306],\n       [2.31993942],\n       [0.98658484],\n       [0.        ]])\n\n\nExpected output:\narray([[0.        ],\n       [4.50714306],\n       [2.31993942],\n       [0.98658484],\n       [0.        ]])\n\n\nSoftmax\nThe second activation function that we need is softmax. This function is used to calculate the values of the output layer of the neural network, using the following formulas:\n\\begin{align}\n\\mathbf{z_2} &= \\mathbf{W_2}\\mathbf{h} + \\mathbf{b_2}   \\tag{3} \\\\\n\\mathbf{\\hat y} &= \\mathrm{softmax}(\\mathbf{z_2})   \\tag{4} \\\\\n\\end{align}\nTo calculate softmax of a vector \\mathbf{z}, the i-th component of the resulting vector is given by:\n \\textrm{softmax}(\\textbf{z})_i = \\frac{e^{z_i} }{\\sum\\limits_{j=1}^{V} e^{z_j} }  \\tag{5} \nLet’s work through an example.\n\n# Define a new vector and save it in the 'z' variable\nz = np.array([9, 8, 11, 10, 8.5])\n\n# Print the vector\nz\n\narray([ 9. ,  8. , 11. , 10. ,  8.5])\n\n\nYou’ll need to calculate the exponentials of each element, both for the numerator and for the denominator.\n\n# Save exponentials of the values in a new vector\ne_z = np.exp(z)\n\n# Print the vector with the exponential values\ne_z\n\narray([ 8103.08392758,  2980.95798704, 59874.1417152 , 22026.46579481,\n        4914.7688403 ])\n\n\nThe denominator is equal to the sum of these exponentials.\n\n# Save the sum of the exponentials\nsum_e_z = np.sum(e_z)\n\n# Print sum of exponentials\nsum_e_z\n\nnp.float64(97899.41826492078)\n\n\nAnd the value of the first element of \\textrm{softmax}(\\textbf{z}) is given by:\n\n# Print softmax value of the first element in the original vector\ne_z[0]/sum_e_z\n\nnp.float64(0.08276947985173956)\n\n\nThis is for one element. We can use numpy’s vectorized operations to calculate the values of all the elements of the \\textrm{softmax}(\\textbf{z}) vector in one go.\nImplement the softmax function.\n\n# Define the 'softmax' function that will include the steps previously seen\ndef softmax(z):\n    e_z = np.exp(z)\n    sum_e_z = np.sum(e_z)\n    return e_z / sum_e_z\n\nNow check that it works.\n\n# Print softmax values for original vector\nsoftmax([9, 8, 11, 10, 8.5])\n\narray([0.08276948, 0.03044919, 0.61158833, 0.22499077, 0.05020223])\n\n\nExpected output:\narray([0.08276948, 0.03044919, 0.61158833, 0.22499077, 0.05020223])\nNotice that the sum of all these values is equal to 1.\n\n# Assert that the sum of the softmax values is equal to 1\nnp.sum(softmax([9, 8, 11, 10, 8.5])) == 1\n\nnp.True_",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "L2 - Intro to CBOW"
    ]
  },
  {
    "objectID": "notes/c2w4/lab02.html#dimensions-1-d-arrays-vs-2-d-column-vectors",
    "href": "notes/c2w4/lab02.html#dimensions-1-d-arrays-vs-2-d-column-vectors",
    "title": "Word Embeddings: Intro to CBOW model, activation functions and working with Numpy",
    "section": "Dimensions: 1-D arrays vs 2-D column vectors",
    "text": "Dimensions: 1-D arrays vs 2-D column vectors\nBefore moving on to implement forward propagation, backpropagation, and gradient descent in the next lecture notebook, let’s have a look at the dimensions of the vectors you’ve been handling until now.\nCreate a vector of length V filled with zeros.\n\n# Define V. Remember this was the size of the vocabulary in the previous lecture notebook\nV = 5\n\n# Define vector of length V filled with zeros\nx_array = np.zeros(V)\n\n# Print vector\nx_array\n\narray([0., 0., 0., 0., 0.])\n\n\nThis is a 1-dimensional array, as revealed by the .shape property of the array.\n\n# Print vector's shape\nx_array.shape\n\n(5,)\n\n\nTo perform matrix multiplication in the next steps, we actually need your column vectors to be represented as a matrix with one column. In numpy, this matrix is represented as a 2-dimensional array.\nThe easiest way to convert a 1D vector to a 2D column matrix is to set its .shape property to the number of rows and one column, as shown in the next cell.\n\n# Copy vector\nx_column_vector = x_array.copy()\n\n# Reshape copy of vector\nx_column_vector.shape = (V, 1)  # alternatively ... = (x_array.shape[0], 1)\n\n# Print vector\nx_column_vector\n\narray([[0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.]])\n\n\nThe shape of the resulting “vector” is:\n\n# Print vector's shape\nx_column_vector.shape\n\n(5, 1)\n\n\nSo we now have a 5x1 matrix that we can use to perform standard matrix multiplication.\nCongratulations on finishing this lecture notebook! Hopefully we now have a better understanding of the activation functions used in the continuous bag-of-words model, as well as a clearer idea of how to leverage Numpy’s power for these types of mathematical computations.\nIn the next lecture notebook we will get a comprehensive dive into:\n\nForward propagation.\nCross-entropy loss.\nBackpropagation.\nGradient descent.\n\nSee we next time!",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "L2 - Intro to CBOW"
    ]
  },
  {
    "objectID": "notes/c2w4/lab03.html",
    "href": "notes/c2w4/lab03.html",
    "title": "Word Embeddings: Training the CBOW model",
    "section": "",
    "text": "course banner\nIn previous lecture notebooks we saw\nThis notebook will walk we through:\nWhich are concepts necessary to understand how the training of the model works.\nLet’s dive into it!\nimport numpy as np\nfrom utils2 import get_dict",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "L3 - Training the CBOW"
    ]
  },
  {
    "objectID": "notes/c2w4/lab03.html#forward-propagation",
    "href": "notes/c2w4/lab03.html#forward-propagation",
    "title": "Word Embeddings: Training the CBOW model",
    "section": "Forward propagation",
    "text": "Forward propagation\nLet’s dive into the neural network itself, which is shown below with all the dimensions and formulas you’ll need.\n\n\n\n\n\n\n\nFigure 1\n\n\nSet N equal to 3. Remember that N is a hyperparameter of the CBOW model that represents the size of the word embedding vectors, as well as the size of the hidden layer.\nAlso set V equal to 5, which is the size of the vocabulary we have used so far.\n\n# Define the size of the word embedding vectors and save it in the variable 'N'\nN = 3\n\n# Define V. Remember this was the size of the vocabulary in the previous lecture notebooks\nV = 5\n\n\nInitialization of the weights and biases\nBefore we start training the neural network, we need to initialize the weight matrices and bias vectors with random values.\nIn the assignment we will implement a function to do this yourself using numpy.random.rand. In this notebook, we’ve pre-populated these matrices and vectors for you.\n\n# Define first matrix of weights\nW1 = np.array([[ 0.41687358,  0.08854191, -0.23495225,  0.28320538,  0.41800106],\n               [ 0.32735501,  0.22795148, -0.23951958,  0.4117634 , -0.23924344],\n               [ 0.26637602, -0.23846886, -0.37770863, -0.11399446,  0.34008124]])\n\n# Define second matrix of weights\nW2 = np.array([[-0.22182064, -0.43008631,  0.13310965],\n               [ 0.08476603,  0.08123194,  0.1772054 ],\n               [ 0.1871551 , -0.06107263, -0.1790735 ],\n               [ 0.07055222, -0.02015138,  0.36107434],\n               [ 0.33480474, -0.39423389, -0.43959196]])\n\n# Define first vector of biases\nb1 = np.array([[ 0.09688219],\n               [ 0.29239497],\n               [-0.27364426]])\n\n# Define second vector of biases\nb2 = np.array([[ 0.0352008 ],\n               [-0.36393384],\n               [-0.12775555],\n               [-0.34802326],\n               [-0.07017815]])\n\nCheck that the dimensions of these matrices match those shown in the figure above.\n\nprint(f'V (vocabulary size): {V}')\nprint(f'N (embedding size / size of the hidden layer): {N}')\nprint(f'size of W1: {W1.shape} (NxV)')\nprint(f'size of b1: {b1.shape} (Nx1)')\nprint(f'size of W2: {W2.shape} (VxN)')\nprint(f'size of b2: {b2.shape} (Vx1)')\n\nV (vocabulary size): 5\nN (embedding size / size of the hidden layer): 3\nsize of W1: (3, 5) (NxV)\nsize of b1: (3, 1) (Nx1)\nsize of W2: (5, 3) (VxN)\nsize of b2: (5, 1) (Vx1)\n\n\nBefore moving forward, we will need some functions and variables defined in previous notebooks. They can be found next. Be sure we understand everything that is going on in the next cell, if not consider doing a refresh of the first lecture notebook.\n\n# Define the tokenized version of the corpus\nwords = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n\n# Get 'word2Ind' and 'Ind2word' dictionaries for the tokenized corpus\nword2Ind, Ind2word = get_dict(words)\n\n# Define the 'get_windows' function as seen in a previous notebook\ndef get_windows(words, C):\n    i = C\n    while i &lt; len(words) - C:\n        center_word = words[i]\n        context_words = words[(i - C):i] + words[(i+1):(i+C+1)]\n        yield context_words, center_word\n        i += 1\n\n# Define the 'word_to_one_hot_vector' function as seen in a previous notebook\ndef word_to_one_hot_vector(word, word2Ind, V):\n    one_hot_vector = np.zeros(V)\n    one_hot_vector[word2Ind[word]] = 1\n    return one_hot_vector\n\n# Define the 'context_words_to_vector' function as seen in a previous notebook\ndef context_words_to_vector(context_words, word2Ind, V):\n    context_words_vectors = [word_to_one_hot_vector(w, word2Ind, V) for w in context_words]\n    context_words_vectors = np.mean(context_words_vectors, axis=0)\n    return context_words_vectors\n\n# Define the generator function 'get_training_example' as seen in a previous notebook\ndef get_training_example(words, C, word2Ind, V):\n    for context_words, center_word in get_windows(words, C):\n        yield context_words_to_vector(context_words, word2Ind, V), word_to_one_hot_vector(center_word, word2Ind, V)\n\n\n\nTraining example\nRun the next cells to get the first training example, made of the vector representing the context words “i am because i”, and the target which is the one-hot vector representing the center word “happy”.\n\nWe don’t need to worry about the Python syntax, but there are some explanations below if we want to know what’s happening behind the scenes.\n\n\n# Save generator object in the 'training_examples' variable with the desired arguments\ntraining_examples = get_training_example(words, 2, word2Ind, V)\n\n\nget_training_examples, which uses the yield keyword, is known as a generator. When run, it builds an iterator, which is a special type of object that… we can iterate on (using a for loop for instance), to retrieve the successive values that the function generates.\nIn this case get_training_examples yields training examples, and iterating on training_examples will return the successive training examples.\n\n\n# Get first values from generator\nx_array, y_array = next(training_examples)\n\n\nnext is another special keyword, which gets the next available value from an iterator. Here, you’ll get the very first value, which is the first training example. If we run this cell again, you’ll get the next value, and so on until the iterator runs out of values to return.\nIn this notebook next is used because we will only be performing one iteration of training. In this week’s assignment with the full training over several iterations you’ll use regular for loops with the iterator that supplies the training examples.\n\nThe vector representing the context words, which will be fed into the neural network, is:\n\n# Print context words vector\nx_array\n\narray([0.25, 0.25, 0.  , 0.5 , 0.  ])\n\n\nThe one-hot vector representing the center word to be predicted is:\n\n# Print one hot vector of center word\ny_array\n\narray([0., 0., 1., 0., 0.])\n\n\nNow convert these vectors into matrices (or 2D arrays) to be able to perform matrix multiplication on the right types of objects, as explained in a previous notebook.\n\n# Copy vector\nx = x_array.copy()\n\n# Reshape it\nx.shape = (V, 1)\n\n# Print it\nprint(f'x:\\n{x}\\n')\n\n# Copy vector\ny = y_array.copy()\n\n# Reshape it\ny.shape = (V, 1)\n\n# Print it\nprint(f'y:\\n{y}')\n\nx:\n[[0.25]\n [0.25]\n [0.  ]\n [0.5 ]\n [0.  ]]\n\ny:\n[[0.]\n [0.]\n [1.]\n [0.]\n [0.]]\n\n\nNow we will need the activation functions seen before. Again, if this feel unfamiliar consider checking the previous lecture notebook.\n\n# Define the 'relu' function as seen in the previous lecture notebook\ndef relu(z):\n    result = z.copy()\n    result[result &lt; 0] = 0\n    return result\n\n# Define the 'softmax' function as seen in the previous lecture notebook\ndef softmax(z):\n    e_z = np.exp(z)\n    sum_e_z = np.sum(e_z)\n    return e_z / sum_e_z\n\n\n\nValues of the hidden layer\nNow that we have initialized all the variables that we need for forward propagation, we can calculate the values of the hidden layer using the following formulas:\n\n\\begin{align}\n\\mathbf{z_1} = \\mathbf{W_1}\\mathbf{x} + \\mathbf{b_1}  \\tag{1} \\\\\n\\mathbf{h} = \\mathrm{ReLU}(\\mathbf{z_1})  \\tag{2} \\\\\n\\end{align}\n\nFirst, we can calculate the value of \\mathbf{z_1}.\n\n# Compute z1 (values of first hidden layer before applying the ReLU function)\nz1 = np.dot(W1, x) + b1\n\n\n np.dot is numpy’s function for matrix multiplication.\n\nAs expected we get an N by 1 matrix, or column vector with N elements, where N is equal to the embedding size, which is 3 in this example.\n\n# Print z1\nz1\n\narray([[ 0.36483875],\n       [ 0.63710329],\n       [-0.3236647 ]])\n\n\nWe can now take the ReLU of \\mathbf{z_1} to get \\mathbf{h}, the vector with the values of the hidden layer.\n\n# Compute h (z1 after applying ReLU function)\nh = relu(z1)\n\n# Print h\nh\n\narray([[0.36483875],\n       [0.63710329],\n       [0.        ]])\n\n\nApplying ReLU means that the negative element of \\mathbf{z_1} has been replaced with a zero.\n\n\nValues of the output layer\nHere are the formulas we need to calculate the values of the output layer, represented by the vector \\mathbf{\\hat y}:\n\n\\begin{align}\n\\mathbf{z_2} &= \\mathbf{W_2}\\mathbf{h} + \\mathbf{b_2}   \\tag{3} \\\\\n\\mathbf{\\hat y} &= \\mathrm{softmax}(\\mathbf{z_2})   \\tag{4} \\\\\n\\end{align}\n\nFirst, calculate \\mathbf{z_2}.\n\n# Compute z2 (values of the output layer before applying the softmax function)\nz2 = np.dot(W2, h) + b2\n\n# Print z2\nz2\n\narray([[-0.31973737],\n       [-0.28125477],\n       [-0.09838369],\n       [-0.33512159],\n       [-0.19919612]])\n\n\nExpected output:\narray([[-0.31973737],\n       [-0.28125477],\n       [-0.09838369],\n       [-0.33512159],\n       [-0.19919612]])\nThis is a V by 1 matrix, where V is the size of the vocabulary, which is 5 in this example.\nNow calculate the value of \\mathbf{\\hat y}.\n\n# Compute y_hat (z2 after applying softmax function)\ny_hat = softmax(z2)\n\n# Print y_hat\ny_hat\n\narray([[0.18519074],\n       [0.19245626],\n       [0.23107446],\n       [0.18236353],\n       [0.20891502]])\n\n\nExpected output:\narray([[0.18519074],\n       [0.19245626],\n       [0.23107446],\n       [0.18236353],\n       [0.20891502]])\nAs you’ve performed the calculations with random matrices and vectors (apart from the input vector), the output of the neural network is essentially random at this point. The learning process will adjust the weights and biases to match the actual targets better.\nThat being said, what word did the neural network predict?\n\n\nSolution\n\n\nThe neural network predicted the word “happy”: the largest element of \\mathbf{\\hat y} is the third one, and the third word of the vocabulary is “happy”.\n\n\nHere’s how we could implement this in Python:\n\n\nprint(Ind2word[np.argmax(y_hat)])\n\n\nWell done, you’ve completed the forward propagation phase!",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "L3 - Training the CBOW"
    ]
  },
  {
    "objectID": "notes/c2w4/lab03.html#cross-entropy-loss",
    "href": "notes/c2w4/lab03.html#cross-entropy-loss",
    "title": "Word Embeddings: Training the CBOW model",
    "section": "Cross-entropy loss",
    "text": "Cross-entropy loss\nNow that we have the network’s prediction, we can calculate the cross-entropy loss to determine how accurate the prediction was compared to the actual target.\n\nRemember that we are working on a single training example, not on a batch of examples, which is why we are using loss and not cost, which is the generalized form of loss.\n\nFirst let’s recall what the prediction was.\n\n# Print prediction\ny_hat\n\narray([[0.18519074],\n       [0.19245626],\n       [0.23107446],\n       [0.18236353],\n       [0.20891502]])\n\n\nAnd the actual target value is:\n\n# Print target value\ny\n\narray([[0.],\n       [0.],\n       [1.],\n       [0.],\n       [0.]])\n\n\nThe formula for cross-entropy loss is:\n J=-\\sum\\limits_{k=1}^{V}y_k\\log{\\hat{y}_k} \\tag{6}\nTry implementing the cross-entropy loss function so we get more familiar working with numpy\nHere are a some hints if you’re stuck.\n\ndef cross_entropy_loss(y_predicted, y_actual):\n    # Fill the loss variable with your code\n    loss = np.sum(-np.log(y_predicted)*y_actual)\n    return loss\n\n\n\nHint 1\n\n&lt;p&gt;To multiply two numpy matrices (such as &lt;code&gt;y&lt;/code&gt; and &lt;code&gt;y_hat&lt;/code&gt;) element-wise, we can simply use the &lt;code&gt;*&lt;/code&gt; operator.&lt;/p&gt;\n\n\n\nHint 2\n\n\nOnce we have a vector equal to the element-wise multiplication of y and y_hat, we can use np.sum to calculate the sum of the elements of this vector.\n\n\n\n\nSolution\n\n\nloss = np.sum(-np.log(y_hat)*y)\n\n\nDon’t forget to run the cell containing the cross_entropy_loss function once it is solved.\nNow use this function to calculate the loss with the actual values of \\mathbf{y} and \\mathbf{\\hat y}.\n\n# Print value of cross entropy loss for prediction and target value\ncross_entropy_loss(y_hat, y)\n\nnp.float64(1.4650152923611108)\n\n\nExpected output:\n1.4650152923611106\nThis value is neither good nor bad, which is expected as the neural network hasn’t learned anything yet.\nThe actual learning will start during the next phase: backpropagation.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "L3 - Training the CBOW"
    ]
  },
  {
    "objectID": "notes/c2w4/lab03.html#backpropagation",
    "href": "notes/c2w4/lab03.html#backpropagation",
    "title": "Word Embeddings: Training the CBOW model",
    "section": "Backpropagation",
    "text": "Backpropagation\nThe formulas that we will implement for backpropagation are the following.\n\n\\begin{align}\n\\frac{\\partial J}{\\partial \\mathbf{W_1}} &= \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right )\\mathbf{x}^\\top \\tag{7}\\\\\n\\frac{\\partial J}{\\partial \\mathbf{W_2}} &= (\\mathbf{\\hat{y}} - \\mathbf{y})\\mathbf{h^\\top} \\tag{8}\\\\\n\\frac{\\partial J}{\\partial \\mathbf{b_1}} &= \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right ) \\tag{9}\\\\\n\\frac{\\partial J}{\\partial \\mathbf{b_2}} &= \\mathbf{\\hat{y}} - \\mathbf{y} \\tag{10}\n\\end{align}\n\n\nNote: these formulas are slightly simplified compared to the ones in the lecture as you’re working on a single training example, whereas the lecture provided the formulas for a batch of examples. In the assignment you’ll be implementing the latter.\n\nLet’s start with an easy one.\nCalculate the partial derivative of the loss function with respect to \\mathbf{b_2}, and store the result in grad_b2.\n\n\\frac{\\partial J}{\\partial \\mathbf{b_2}} = \\mathbf{\\hat{y}} - \\mathbf{y} \\tag{10}\n\n\n# Compute vector with partial derivatives of loss function with respect to b2\ngrad_b2 = y_hat - y\n\n# Print this vector\ngrad_b2\n\narray([[ 0.18519074],\n       [ 0.19245626],\n       [-0.76892554],\n       [ 0.18236353],\n       [ 0.20891502]])\n\n\nExpected output:\narray([[ 0.18519074],\n       [ 0.19245626],\n       [-0.76892554],\n       [ 0.18236353],\n       [ 0.20891502]])\nNext, calculate the partial derivative of the loss function with respect to \\mathbf{W_2}, and store the result in grad_W2.\n\n\\frac{\\partial J}{\\partial \\mathbf{W_2}} = (\\mathbf{\\hat{y}} - \\mathbf{y})\\mathbf{h^\\top} \\tag{8}\n\n\nHint: use .T to get a transposed matrix, e.g. h.T returns \\mathbf{h^\\top}.\n\n\n# Compute matrix with partial derivatives of loss function with respect to W2\ngrad_W2 = np.dot(y_hat - y, h.T)\n\n# Print matrix\ngrad_W2\n\narray([[ 0.06756476,  0.11798563,  0.        ],\n       [ 0.0702155 ,  0.12261452,  0.        ],\n       [-0.28053384, -0.48988499,  0.        ],\n       [ 0.06653328,  0.1161844 ,  0.        ],\n       [ 0.07622029,  0.13310045,  0.        ]])\n\n\nExpected output:\narray([[ 0.06756476,  0.11798563,  0.        ],\n       [ 0.0702155 ,  0.12261452,  0.        ],\n       [-0.28053384, -0.48988499,  0.        ],\n       [ 0.06653328,  0.1161844 ,  0.        ],\n       [ 0.07622029,  0.13310045,  0.        ]])\nNow calculate the partial derivative with respect to \\mathbf{b_1} and store the result in grad_b1.\n\\frac{\\partial J}{\\partial \\mathbf{b_1}} = \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right ) \\tag{9}\n\n# Compute vector with partial derivatives of loss function with respect to b1\ngrad_b1 = relu(np.dot(W2.T, y_hat - y))\n\n# Print vector\ngrad_b1\n\narray([[0.        ],\n       [0.        ],\n       [0.17045858]])\n\n\nExpected output:\narray([[0.        ],\n       [0.        ],\n       [0.17045858]])\nFinally, calculate the partial derivative of the loss with respect to \\mathbf{W_1}, and store it in grad_W1.\n\n\\frac{\\partial J}{\\partial \\mathbf{W_1}} = \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right )\\mathbf{x}^\\top \\tag{7}\n\n\n# Compute matrix with partial derivatives of loss function with respect to W1\ngrad_W1 = np.dot(relu(np.dot(W2.T, y_hat - y)), x.T)\n\n# Print matrix\ngrad_W1\n\narray([[0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.04261464, 0.04261464, 0.        , 0.08522929, 0.        ]])\n\n\nExpected output:\narray([[0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.04261464, 0.04261464, 0.        , 0.08522929, 0.        ]])\nBefore moving on to gradient descent, double-check that all the matrices have the expected dimensions.\n\nprint(f'V (vocabulary size): {V}')\nprint(f'N (embedding size / size of the hidden layer): {N}')\nprint(f'size of grad_W1: {grad_W1.shape} (NxV)')\nprint(f'size of grad_b1: {grad_b1.shape} (Nx1)')\nprint(f'size of grad_W2: {grad_W2.shape} (VxN)')\nprint(f'size of grad_b2: {grad_b2.shape} (Vx1)')\n\nV (vocabulary size): 5\nN (embedding size / size of the hidden layer): 3\nsize of grad_W1: (3, 5) (NxV)\nsize of grad_b1: (3, 1) (Nx1)\nsize of grad_W2: (5, 3) (VxN)\nsize of grad_b2: (5, 1) (Vx1)",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "L3 - Training the CBOW"
    ]
  },
  {
    "objectID": "notes/c2w4/lab03.html#gradient-descent",
    "href": "notes/c2w4/lab03.html#gradient-descent",
    "title": "Word Embeddings: Training the CBOW model",
    "section": "Gradient descent",
    "text": "Gradient descent\nDuring the gradient descent phase, we will update the weights and biases by subtracting \\alpha times the gradient from the original matrices and vectors, using the following formulas.\n\n\\begin{align}\n\\mathbf{W_1} &:= \\mathbf{W_1} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{W_1}} \\tag{11}\\\\\n\\mathbf{W_2} &:= \\mathbf{W_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{W_2}} \\tag{12}\\\\\n\\mathbf{b_1} &:= \\mathbf{b_1} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_1}} \\tag{13}\\\\\n\\mathbf{b_2} &:= \\mathbf{b_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_2}} \\tag{14}\\\\\n\\end{align}\n\nFirst, let set a value for \\alpha.\n\n# Define alpha\nalpha = 0.03\n\nThe updated weight matrix \\mathbf{W_1} will be:\n\n# Compute updated W1\nW1_new = W1 - alpha * grad_W1\n\nLet’s compare the previous and new values of \\mathbf{W_1}:\n\nprint('old value of W1:')\nprint(W1)\nprint()\nprint('new value of W1:')\nprint(W1_new)\n\nold value of W1:\n[[ 0.41687358  0.08854191 -0.23495225  0.28320538  0.41800106]\n [ 0.32735501  0.22795148 -0.23951958  0.4117634  -0.23924344]\n [ 0.26637602 -0.23846886 -0.37770863 -0.11399446  0.34008124]]\n\nnew value of W1:\n[[ 0.41687358  0.08854191 -0.23495225  0.28320538  0.41800106]\n [ 0.32735501  0.22795148 -0.23951958  0.4117634  -0.23924344]\n [ 0.26509758 -0.2397473  -0.37770863 -0.11655134  0.34008124]]\n\n\nThe difference is very subtle (hint: take a closer look at the last row), which is why it takes a fair amount of iterations to train the neural network until it reaches optimal weights and biases starting from random values.\nNow calculate the new values of \\mathbf{W_2} (to be stored in W2_new), \\mathbf{b_1} (in b1_new), and \\mathbf{b_2} (in b2_new).\n\\begin{align}\n\\mathbf{W_2} &:= \\mathbf{W_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{W_2}} \\tag{12}\\\\\n\\mathbf{b_1} &:= \\mathbf{b_1} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_1}} \\tag{13}\\\\\n\\mathbf{b_2} &:= \\mathbf{b_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_2}} \\tag{14}\\\\\n\\end{align}\n\n# Compute updated W2\nW2_new = W2 - alpha * grad_W2\n\n# Compute updated b1\nb1_new = b1 - alpha * grad_b1\n\n# Compute updated b2\nb2_new = b2 - alpha * grad_b2\n\n\nprint('W2_new')\nprint(W2_new)\nprint()\nprint('b1_new')\nprint(b1_new)\nprint()\nprint('b2_new')\nprint(b2_new)\n\nW2_new\n[[-0.22384758 -0.43362588  0.13310965]\n [ 0.08265956  0.0775535   0.1772054 ]\n [ 0.19557112 -0.04637608 -0.1790735 ]\n [ 0.06855622 -0.02363691  0.36107434]\n [ 0.33251813 -0.3982269  -0.43959196]]\n\nb1_new\n[[ 0.09688219]\n [ 0.29239497]\n [-0.27875802]]\n\nb2_new\n[[ 0.02964508]\n [-0.36970753]\n [-0.10468778]\n [-0.35349417]\n [-0.0764456 ]]\n\n\nExpected output:\nW2_new\n[[-0.22384758 -0.43362588  0.13310965]\n [ 0.08265956  0.0775535   0.1772054 ]\n [ 0.19557112 -0.04637608 -0.1790735 ]\n [ 0.06855622 -0.02363691  0.36107434]\n [ 0.33251813 -0.3982269  -0.43959196]]\n\nb1_new\n[[ 0.09688219]\n [ 0.29239497]\n [-0.27875802]]\n\nb2_new\n[[ 0.02964508]\n [-0.36970753]\n [-0.10468778]\n [-0.35349417]\n [-0.0764456 ]]\nCongratulations, we have completed one iteration of training using one training example!\nYou’ll need many more iterations to fully train the neural network, and we can optimize the learning process by training on batches of examples, as described in the lecture. We will get to do this during this week’s assignment.\n\nHow this practice relates to and differs from the upcoming graded assignment\n\nIn the assignment, for each iteration of training we will use batches of examples instead of a single example. The formulas for forward propagation and backpropagation will be modified accordingly, and we will use cross-entropy cost instead of cross-entropy loss.\nWe will also complete several iterations of training, until we reach an acceptably low cross-entropy cost, at which point we can extract good word embeddings from the weight matrices.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "L3 - Training the CBOW"
    ]
  },
  {
    "objectID": "notes/c4w1/lab02.html",
    "href": "notes/c4w1/lab02.html",
    "title": "Calculating the Bilingual Evaluation Understudy (BLEU) score: Ungraded Lab",
    "section": "",
    "text": "In this ungraded lab, we will implement a popular metric for evaluating the quality of machine-translated text: the BLEU score proposed by Kishore Papineni, et al. In their 2002 paper “BLEU: a Method for Automatic Evaluation of Machine Translation”, the BLEU score works by comparing “candidate” text to one or more “reference” translations. The result is better the closer the score is to 1. Let’s see how to get this value in the following sections.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation",
      "L2 - BLEU Score"
    ]
  },
  {
    "objectID": "notes/c4w1/lab02.html#importing-the-libraries",
    "href": "notes/c4w1/lab02.html#importing-the-libraries",
    "title": "Calculating the Bilingual Evaluation Understudy (BLEU) score: Ungraded Lab",
    "section": "1.1 Importing the Libraries",
    "text": "1.1 Importing the Libraries\nWe will first start by importing the Python libraries we will use in the first part of this lab. For learning, we will implement our own version of the BLEU Score using Numpy. To verify that our implementation is correct, we will compare our results with those generated by the SacreBLEU library. This package provides hassle-free computation of shareable, comparable, and reproducible BLEU scores. It also knows all the standard test sets and handles downloading, processing, and tokenization.\n\nimport numpy as np                  # import numpy to make numerical computations.\nimport nltk                         # import NLTK to handle simple NL tasks like tokenization.\nnltk.download(\"punkt\")\nfrom nltk.util import ngrams\nfrom collections import Counter     # import the Counter module.\n!pip3 install 'sacrebleu'           # install the sacrebleu package.\nimport sacrebleu                    # import sacrebleu in order compute the BLEU score.\nimport matplotlib.pyplot as plt     # import pyplot in order to make some illustrations.\n\n[nltk_data] Downloading package punkt to /home/oren/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n\n\nTrue\n\n\nRequirement already satisfied: sacrebleu in /home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages (2.5.1)\nRequirement already satisfied: numpy&gt;=1.17 in /home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages (from sacrebleu) (2.0.2)\nRequirement already satisfied: portalocker in /home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages (from sacrebleu) (3.1.1)\nRequirement already satisfied: tabulate&gt;=0.8.9 in /home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: lxml in /home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages (from sacrebleu) (5.3.0)\nRequirement already satisfied: regex in /home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages (from sacrebleu) (2024.11.6)\nRequirement already satisfied: colorama in /home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages (from sacrebleu) (0.4.6)",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation",
      "L2 - BLEU Score"
    ]
  },
  {
    "objectID": "notes/c4w1/lab02.html#defining-the-bleu-score",
    "href": "notes/c4w1/lab02.html#defining-the-bleu-score",
    "title": "Calculating the Bilingual Evaluation Understudy (BLEU) score: Ungraded Lab",
    "section": "1.2 Defining the BLEU Score",
    "text": "1.2 Defining the BLEU Score\nYou have seen the formula for calculating the BLEU score in this week’s lectures. More formally, we can express the BLEU score as:\n\nBLEU = BP\\Bigl(\\prod_{i=1}^{4}precision_i\\Bigr)^{(1/4)}\n\nwith the Brevity Penalty and precision defined as:\n\nBP = min\\Bigl(1, e^{(1-({ref}/{cand}))}\\Bigr)\n\n\nprecision_i = \\frac {\\sum_{snt \\in{cand}}\\sum_{i\\in{snt}}min\\Bigl(m^{i}_{cand}, m^{i}_{ref}\\Bigr)}{w^{i}_{t}}\n\nwhere:\n\nm^{i}_{cand}, is the count of i-gram in candidate matching the reference translation.\nm^{i}_{ref}, is the count of i-gram in the reference translation.\nw^{i}_{t}, is the total number of i-grams in candidate translation.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation",
      "L2 - BLEU Score"
    ]
  },
  {
    "objectID": "notes/c4w1/lab02.html#explaining-the-bleu-score",
    "href": "notes/c4w1/lab02.html#explaining-the-bleu-score",
    "title": "Calculating the Bilingual Evaluation Understudy (BLEU) score: Ungraded Lab",
    "section": "1.3 Explaining the BLEU score",
    "text": "1.3 Explaining the BLEU score\n\nBrevity Penalty (example):\n\nref_length = np.ones(100)\ncan_length = np.linspace(1.5, 0.5, 100)\nx = ref_length / can_length\ny = 1 - x\ny = np.exp(y)\ny = np.minimum(np.ones(y.shape), y)\n\n# Code for in order to make the plot\nfig, ax = plt.subplots(1)\nlines = ax.plot(x, y)\nax.set(\n    xlabel=\"Ratio of the length of the reference to the candidate text\",\n    ylabel=\"Brevity Penalty\",\n)\nplt.show()\n\n[Text(0.5, 0, 'Ratio of the length of the reference to the candidate text'),\n Text(0, 0.5, 'Brevity Penalty')]\n\n\n\n\n\n\n\n\n\nThe brevity penalty penalizes generated translations that are too short compared to the closest reference length with an exponential decay. The brevity penalty compensates for the fact that the BLEU score has no recall term.\n\n\nN-Gram Precision (example):\n\ndata = {\"1-gram\": 0.8, \"2-gram\": 0.7, \"3-gram\": 0.6, \"4-gram\": 0.5}\nnames = list(data.keys())\nvalues = list(data.values())\n\nfig, ax = plt.subplots(1)\nbars = ax.bar(names, values)\nax.set(ylabel=\"N-gram precision\")\n\nplt.show()\n\n\n\n\n\n\n\n\nThe n-gram precision counts how many unigrams, bigrams, trigrams, and four-grams (i=1,…,4) match their n-gram counterpart in the reference translations. This term acts as a precision metric. Unigrams account for adequacy while longer n-grams account for fluency of the translation. To avoid overcounting, the n-gram counts are clipped to the maximal n-gram count occurring in the reference (m_{n}^{ref}). Typically precision shows exponential decay with the with the degree of the n-gram.\n\n\nN-gram BLEU score (example):\n\ndata = {\"1-gram\": 0.8, \"2-gram\": 0.77, \"3-gram\": 0.74, \"4-gram\": 0.71}\nnames = list(data.keys())\nvalues = list(data.values())\n\nfig, ax = plt.subplots(1)\nbars = ax.bar(names, values)\nax.set(ylabel=\"Modified N-gram precision\")\n\nplt.show()\n\n\n\n\n\n\n\n\nWhen the n-gram precision is multiplied by the BP, then the exponential decay of n-grams is almost fully compensated. The BLEU score corresponds to a geometric average of this modified n-gram precision.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation",
      "L2 - BLEU Score"
    ]
  },
  {
    "objectID": "notes/c4w1/lab02.html#example-calculations-of-the-bleu-score",
    "href": "notes/c4w1/lab02.html#example-calculations-of-the-bleu-score",
    "title": "Calculating the Bilingual Evaluation Understudy (BLEU) score: Ungraded Lab",
    "section": "1.4 Example Calculations of the BLEU score",
    "text": "1.4 Example Calculations of the BLEU score\nIn this example we will have a reference translation and 2 candidates translations. We will tokenize all sentences using the NLTK package introduced in Course 2 of this NLP specialization.\n\nreference = \"The NASA Opportunity rover is battling a massive dust storm on planet Mars.\"\ncandidate_1 = \"The Opportunity rover is combating a big sandstorm on planet Mars.\"\ncandidate_2 = \"A NASA rover is fighting a massive storm on planet Mars.\"\n\ntokenized_ref = nltk.word_tokenize(reference.lower())\ntokenized_cand_1 = nltk.word_tokenize(candidate_1.lower())\ntokenized_cand_2 = nltk.word_tokenize(candidate_2.lower())\n\nprint(f\"{reference} -&gt; {tokenized_ref}\")\nprint(\"\\n\")\nprint(f\"{candidate_1} -&gt; {tokenized_cand_1}\")\nprint(\"\\n\")\nprint(f\"{candidate_2} -&gt; {tokenized_cand_2}\")\n\nThe NASA Opportunity rover is battling a massive dust storm on planet Mars. -&gt; ['the', 'nasa', 'opportunity', 'rover', 'is', 'battling', 'a', 'massive', 'dust', 'storm', 'on', 'planet', 'mars', '.']\n\n\nThe Opportunity rover is combating a big sandstorm on planet Mars. -&gt; ['the', 'opportunity', 'rover', 'is', 'combating', 'a', 'big', 'sandstorm', 'on', 'planet', 'mars', '.']\n\n\nA NASA rover is fighting a massive storm on planet Mars. -&gt; ['a', 'nasa', 'rover', 'is', 'fighting', 'a', 'massive', 'storm', 'on', 'planet', 'mars', '.']\n\n\n\nSTEP 1: Computing the Brevity Penalty\n\ndef brevity_penalty(reference, candidate):\n    ref_length = len(reference)\n    can_length = len(candidate)\n\n    # Brevity Penalty\n    if ref_length &gt; can_length:\n        BP = 1\n    else:\n        penalty = 1 - (ref_length / can_length)\n        BP = np.exp(penalty)\n\n    return BP\n\n\n\nSTEP 2: Computing the Precision\n\ndef clipped_precision(reference, candidate):\n    \"\"\"\n    Bleu score function given a original and a machine translated sentences\n    \"\"\"\n\n    clipped_precision_score = []\n\n    for i in range(1, 5):\n        candidate_n_gram = Counter(\n            ngrams(candidate, i)\n        )  # counts of n-gram n=1...4 tokens for the candidate\n        reference_n_gram = Counter(\n            ngrams(reference, i)\n        )  # counts of n-gram n=1...4 tokens for the reference\n\n        c = sum(\n            reference_n_gram.values()\n        )  # sum of the values of the reference the denominator in the precision formula\n\n        for j in reference_n_gram:  # for every n_gram token in the reference\n            if j in candidate_n_gram:  # check if it is in the candidate n-gram\n\n                if (\n                    reference_n_gram[j] &gt; candidate_n_gram[j]\n                ):  # if the count of the reference n-gram is bigger\n                    # than the corresponding count in the candidate n-gram\n                    reference_n_gram[j] = candidate_n_gram[\n                        j\n                    ]  # then set the count of the reference n-gram to be equal\n                    # to the count of the candidate n-gram\n            else:\n\n                reference_n_gram[j] = 0  # else reference n-gram = 0\n\n        clipped_precision_score.append(sum(reference_n_gram.values()) / c)\n\n    weights = [0.25] * 4\n\n    s = (w_i * np.log(p_i) for w_i, p_i in zip(weights, clipped_precision_score))\n    s = np.exp(np.sum(s))\n    return s\n\n\n\nSTEP 3: Computing the BLEU score\n\ndef bleu_score(reference, candidate):\n    BP = brevity_penalty(reference, candidate)\n    precision = clipped_precision(reference, candidate)\n    return BP * precision\n\n\n\nSTEP 4: Testing with our Example Reference and Candidates Sentences\n\nprint(\n    \"Results reference versus candidate 1 our own code BLEU: \",\n    round(bleu_score(tokenized_ref, tokenized_cand_1) * 100, 1),\n)\nprint(\n    \"Results reference versus candidate 2 our own code BLEU: \",\n    round(bleu_score(tokenized_ref, tokenized_cand_2) * 100, 1),\n)\n\nResults reference versus candidate 1 our own code BLEU:  27.4\nResults reference versus candidate 2 our own code BLEU:  35.0\n\n\n/tmp/ipykernel_127084/273199063.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n  s = np.exp(np.sum(s))\n\n\n\n\nSTEP 5: Comparing the Results from our Code with the SacreBLEU Library\n\nprint(\n    \"Results reference versus candidate 1 sacrebleu library sentence BLEU: \",\n    round(sacrebleu.corpus_bleu(reference, candidate_1).score, 1),\n)\nprint(\n    \"Results reference versus candidate 2 sacrebleu library sentence BLEU: \",\n    round(sacrebleu.corpus_bleu(reference, candidate_2).score, 1),\n)\n\nResults reference versus candidate 1 sacrebleu library sentence BLEU:  0.0\nResults reference versus candidate 2 sacrebleu library sentence BLEU:  0.0",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation",
      "L2 - BLEU Score"
    ]
  },
  {
    "objectID": "notes/c4w1/lab02.html#loading-data-sets-for-evaluation-using-the-bleu-score",
    "href": "notes/c4w1/lab02.html#loading-data-sets-for-evaluation-using-the-bleu-score",
    "title": "Calculating the Bilingual Evaluation Understudy (BLEU) score: Ungraded Lab",
    "section": "Loading Data Sets for Evaluation Using the BLEU Score",
    "text": "Loading Data Sets for Evaluation Using the BLEU Score\nIn this section, we will show a simple pipeline for evaluating machine translated text. Due to storage and speed constraints, we will not be using our own model in this lab (you’ll get to do that in the assignment!). Instead, we will be using Google Translate to generate English to German translations and we will evaluate it against a known evaluation set. There are three files we will need:\n\nA source text in English. In this lab, we will use the first 1671 words of the wmt19 evaluation dataset downloaded via SacreBLEU. We just grabbed a subset because of limitations in the number of words that can be translated using Google Translate.\nA reference translation to German of the corresponding first 1671 words from the original English text. This is also provided by SacreBLEU.\nA candidate machine translation to German from the same 1671 words. This is generated by feeding the source text to a machine translation model. As mentioned above, we will use Google Translate to generate the translations in this file.\n\nWith that, we can now compare the reference an candidate translation to get the BLEU Score.\n\n# Loading the raw data\nwmt19news_src = open(\"wmt19_src.txt\", \"rU\")\nwmt19news_src_1 = wmt19news_src.read()\nwmt19news_src.close()\nwmt19news_ref = open(\"wmt19_ref.txt\", \"rU\")\nwmt19news_ref_1 = wmt19news_ref.read()\nwmt19news_ref.close()\nwmt19news_can = open(\"wmt19_can.txt\", \"rU\")\nwmt19news_can_1 = wmt19news_can.read()\nwmt19news_can.close()\n\n# Tokenizing the raw data\ntokenized_corpus_src = nltk.word_tokenize(wmt19news_src_1.lower())\ntokenized_corpus_ref = nltk.word_tokenize(wmt19news_ref_1.lower())\ntokenized_corpus_cand = nltk.word_tokenize(wmt19news_can_1.lower())\n\nInspecting the first sentence of the data.\n\nprint(\"English source text:\")\nprint(\"\\n\")\nprint(f\"{wmt19news_src_1[0:170]} -&gt; {tokenized_corpus_src[0:30]}\")\nprint(\"\\n\")\nprint(\"German reference translation:\")\nprint(\"\\n\")\nprint(f\"{wmt19news_ref_1[0:219]} -&gt; {tokenized_corpus_ref[0:35]}\")\nprint(\"\\n\")\nprint(\"German machine translation:\")\nprint(\"\\n\")\nprint(f\"{wmt19news_can_1[0:199]} -&gt; {tokenized_corpus_cand[0:29]}\")\n\nEnglish source text:\n\n\n﻿Welsh AMs worried about 'looking like muppets'\nThere is consternation among some AMs at a suggestion their title should change to MWPs (Member of the Welsh Parliament).\n -&gt; ['\\ufeffwelsh', 'ams', 'worried', 'about', \"'looking\", 'like', 'muppets', \"'\", 'there', 'is', 'consternation', 'among', 'some', 'ams', 'at', 'a', 'suggestion', 'their', 'title', 'should', 'change', 'to', 'mwps', '(', 'member', 'of', 'the', 'welsh', 'parliament', ')']\n\n\nGerman reference translation:\n\n\n﻿Walisische Ageordnete sorgen sich \"wie Dödel auszusehen\"\nEs herrscht Bestürzung unter einigen Mitgliedern der Versammlung über einen Vorschlag, der ihren Titel zu MWPs (Mitglied der walisischen Parlament) ändern soll.\n -&gt; ['\\ufeffwalisische', 'ageordnete', 'sorgen', 'sich', '``', 'wie', 'dödel', 'auszusehen', \"''\", 'es', 'herrscht', 'bestürzung', 'unter', 'einigen', 'mitgliedern', 'der', 'versammlung', 'über', 'einen', 'vorschlag', ',', 'der', 'ihren', 'titel', 'zu', 'mwps', '(', 'mitglied', 'der', 'walisischen', 'parlament', ')', 'ändern', 'soll', '.']\n\n\nGerman machine translation:\n\n\n﻿Walisische AMs machten sich Sorgen, dass sie wie Muppets aussehen könnten\nEinige AMs sind bestürzt über den Vorschlag, ihren Titel in MWPs (Mitglied des walisischen Parlaments) zu ändern.\nEs ist auf -&gt; ['\\ufeffwalisische', 'ams', 'machten', 'sich', 'sorgen', ',', 'dass', 'sie', 'wie', 'muppets', 'aussehen', 'könnten', 'einige', 'ams', 'sind', 'bestürzt', 'über', 'den', 'vorschlag', ',', 'ihren', 'titel', 'in', 'mwps', '(', 'mitglied', 'des', 'walisischen', 'parlaments']\n\n\n\nprint(\n    \"Results reference versus candidate 1 our own BLEU implementation: \",\n    round(bleu_score(tokenized_corpus_ref, tokenized_corpus_cand) * 100, 1),\n)\n\nResults reference versus candidate 1 our own BLEU implementation:  23.6\n\n\n/tmp/ipykernel_127084/273199063.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n  s = np.exp(np.sum(s))\n\n\n\nprint(\n    \"Results reference versus candidate 1 sacrebleu library BLEU: \",\n    round(sacrebleu.corpus_bleu(wmt19news_ref_1, wmt19news_can_1).score, 1),\n)\n\nResults reference versus candidate 1 sacrebleu library BLEU:  0.0\n\n\nBLEU Score Interpretation on a Corpus\n\n\n\n\n\n\n\nScore\nInterpretation\n\n\n\n\n&lt; 10\nAlmost useless\n\n\n10 - 19\nHard to get the gist\n\n\n20 - 29\nThe gist is clear, but has significant grammatical errors\n\n\n30 - 40\nUnderstandable to good translations\n\n\n40 - 50\nHigh quality translations\n\n\n50 - 60\nVery high quality, adequate, and fluent translations\n\n\n&gt; 60\nQuality often better than human\n\n\n\nFrom the table above (taken here), we can see the gist of the translation is clear but has significant grammatical errors. Nonetheless, the results of our coded BLEU score are almost identical to those of the SacreBLEU package.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation",
      "L2 - BLEU Score"
    ]
  },
  {
    "objectID": "notes/c3w2/index.html",
    "href": "notes/c3w2/index.html",
    "title": "Recurrent Neural Networks for Language Modeling",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\n\nThis week’s slides\n\n\n\n\nFigure 1\nMy notes for Week 2 of the Natural Language Processing with Sequence Models Course in the Natural Language Processing Specialization Offered by DeepLearning.AI on Coursera",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w2/index.html#traditional-language-models",
    "href": "notes/c3w2/index.html#traditional-language-models",
    "title": "Recurrent Neural Networks for Language Modeling",
    "section": "Traditional Language models",
    "text": "Traditional Language models\n\nIn the example above, the second sentence is the one that is most likely to take place as it has the highest probability of happening. To compute the probabilities, we can do the following:\n\nLarge N-grams capture dependencies between distant words and need a lot of space and RAM. Hence, we resort to using different types of alternatives.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w2/index.html#recurrent-neural-networks",
    "href": "notes/c3w2/index.html#recurrent-neural-networks",
    "title": "Recurrent Neural Networks for Language Modeling",
    "section": "Recurrent Neural Networks",
    "text": "Recurrent Neural Networks\nPreviously, we tried using traditional language models, but it turns out they took a lot of space and RAM. For example, in the sentence below:\n\nAn N-gram (trigram) would only look at “did not” and would try to complete the sentence from there. As a result, the model will not be able to see the beginning of the sentence “I called her but she”. Probably the most likely word is have after “did not”. RNNs help us solve this problem by being able to track dependencies that are much further apart from each other. As the RNN makes its way through a text corpus, it picks up some information as follows:\n\nNote that as we feed in more information into the model, the previous word’s retention gets weaker, but it is still there. Look at the orange rectangle above and see how it becomes smaller as we make your way through the text. This shows that your model is capable of capturing dependencies and remembers a previous word although it is at the beginning of a sentence or paragraph. Another advantage of RNNs is that a lot of the computation shares parameters.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w2/index.html#application-of-rnns",
    "href": "notes/c3w2/index.html#application-of-rnns",
    "title": "Recurrent Neural Networks for Language Modeling",
    "section": "Application of RNNs",
    "text": "Application of RNNs\nRNNs could be used in a variety of tasks ranging from machine translation to caption generation. There are many ways to implement an RNN model:\n\nOne to One: given some scores of a championship, we can predict the winner.\nOne to Many: given an image, we can predict what the caption is going to be.\nMany to One: given a tweet, we can predict the sentiment of that tweet.\nMany to Many: given an english sentence, we can translate it to its German equivalent.\n\nIn the next video, we will see the math in simple RNNs.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w2/index.html#math-in-simple-rnns",
    "href": "notes/c3w2/index.html#math-in-simple-rnns",
    "title": "Recurrent Neural Networks for Language Modeling",
    "section": "Math in Simple RNNs",
    "text": "Math in Simple RNNs\nIt is best to explain the math behind a simple RNN with a diagram:\n\nNote that:\n\nh^{&lt;t&gt;} = g(W_{h}[h^{&lt;t-1&gt;}, x^{&lt;t&gt;}] + b_h)\n\nIs the same as multiplying W_{hh} by h and W_{hx} by x. In other words, we can concatenate it as follows:\n\nh^{&lt;t&gt;} = g(W_{hh}[h^{&lt;t-1&gt;} \\oplus W_{hx}x^{&lt;t&gt;}] + b_h)\n\nFor the prediction at each time step, we can use the following:\n\n\\hat{y}^{&lt;t&gt;} = g(W_{yh}h^{&lt;t&gt;} + b_y)\n\nNote that we end up training W_{hh}, W_{hx}, W_{yh}, b_h, and b_y. Here is a visualization of the model.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w2/index.html#lab-lecture-notebook-hidden-state-activation",
    "href": "notes/c3w2/index.html#lab-lecture-notebook-hidden-state-activation",
    "title": "Recurrent Neural Networks for Language Modeling",
    "section": "Lab: Lecture Notebook: Hidden State Activation",
    "text": "Lab: Lecture Notebook: Hidden State Activation\nHidden State Activation",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w2/index.html#cost-function-for-rnns",
    "href": "notes/c3w2/index.html#cost-function-for-rnns",
    "title": "Recurrent Neural Networks for Language Modeling",
    "section": "Cost Function for RNNs",
    "text": "Cost Function for RNNs\nThe cost function used in an RNN is the cross entropy loss. If we were to visualize it\n\nwe are basically summing over the all the classes and then multiplying y_j times log(\\hat{y}_j). If we were to compute the loss over several time steps, use the following formula:\n\nJ = -\\frac{1}{T} \\sum_{t=1}^{T} \\sum_{j=1}^{K} y_j^{&lt;t&gt;} log \\hat{y}^{&lt;t&gt;}\n\nwhere T is the number of time steps and K is the number of classes.\nNote that we are simply summing over all the time steps and dividing by T, to get the average cost in each time step. Hence, we are just taking an average through time.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w2/index.html#implementation-note",
    "href": "notes/c3w2/index.html#implementation-note",
    "title": "Recurrent Neural Networks for Language Modeling",
    "section": "Implementation Note",
    "text": "Implementation Note\nThe scan function is built as follows:\n\nNote, that is basically what an RNN is doing. It takes the initializer, and returns a list of outputs (ys), and uses the current value, to get the next y and the next current value. These type of abstractions allow for much faster computation.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w2/index.html#lab-working-with-jax-numpy-and-calculating-perplexity",
    "href": "notes/c3w2/index.html#lab-working-with-jax-numpy-and-calculating-perplexity",
    "title": "Recurrent Neural Networks for Language Modeling",
    "section": "Lab: Working with JAX NumPy and Calculating Perplexity",
    "text": "Lab: Working with JAX NumPy and Calculating Perplexity\nWorking with JAX NumPy and Calculating Perplexity",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w2/index.html#gated-recurrent-units",
    "href": "notes/c3w2/index.html#gated-recurrent-units",
    "title": "Recurrent Neural Networks for Language Modeling",
    "section": "Gated Recurrent Units",
    "text": "Gated Recurrent Units\nGated recurrent units are very similar to vanilla RNNs, except that they have a “relevance” and “update” gate that allow the model to update and get relevant information. I personally find it easier to understand by looking at the formulas:\n\nTo the left, we have the diagram and equations for a simple RNN. To the right, we explain the GRU. Note that we add 3 layers before computing h and y.\n\n\\begin{align*}\n\\Gamma_u &= \\sigma(W_u[h^{&lt;t-1&gt;}, x^{&lt;t&gt;}]+b_u) \\\\\n\\Gamma_r &= \\sigma(W_r[h^{&lt;t-1&gt;}, x^{&lt;t&gt;}]+b_r) \\\\\nh'^{&lt;t&gt;} &= \\tanh(W_h[\\Gamma_r*h^{&lt;t-1&gt;}, x^{&lt;t&gt;}]+b_h) \\\\\nh^{&lt;t&gt;} &= \\Gamma_u*h^{&lt;t-1&gt;} + (1-\\Gamma_u)*h'^{&lt;t&gt;}\n\\end{align*}\n\nThe first gate Γ_u allows we to decide how much we want to update the weights by. The second gate Γ_r, helps we find a relevance score. We can compute the new h by using the relevance gate. Finally we can compute h, using the update gate. GRUs “decide” how to update the hidden state. GRUs help preserve important information.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w2/index.html#lab-vanilla-rnns-grus-and-the-scan-function",
    "href": "notes/c3w2/index.html#lab-vanilla-rnns-grus-and-the-scan-function",
    "title": "Recurrent Neural Networks for Language Modeling",
    "section": "Lab: Vanilla RNNs, GRUs and the scan function",
    "text": "Lab: Vanilla RNNs, GRUs and the scan function\nVanilla RNNs, GRUs and the scan function",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w2/index.html#lab-creating-a-gru-model-using-trax",
    "href": "notes/c3w2/index.html#lab-creating-a-gru-model-using-trax",
    "title": "Recurrent Neural Networks for Language Modeling",
    "section": "Lab: Creating a GRU model using Trax",
    "text": "Lab: Creating a GRU model using Trax\nCreating a GRU model using Trax",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w2/index.html#deep-and-bi-directional-rnns",
    "href": "notes/c3w2/index.html#deep-and-bi-directional-rnns",
    "title": "Recurrent Neural Networks for Language Modeling",
    "section": "Deep and Bi-directional RNNs",
    "text": "Deep and Bi-directional RNNs\nBi-directional RNNs are important, because knowing what is next in the sentence could give we more context about the sentence itself.\n\nSo we can see, in order to make a prediction \\hat{y}, we will use the hidden states from both directions and combine them to make one hidden state, we can then proceed as we would with a simple vanilla RNN. When implementing Deep RNNs, we would compute the following.\n\nNote that at layer l, we are using the input from the bottom a^{[;-1]} and the hidden state h^{&lt;l&gt;} That allows we to get your new h, and then to get your new a, we will train another weight matrix W_{a}, which we will multiply by the corresponding h add the bias and then run it through an activation layer.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w2/index.html#resources",
    "href": "notes/c3w2/index.html#resources",
    "title": "Recurrent Neural Networks for Language Modeling",
    "section": "Resources:",
    "text": "Resources:\n\n(Chadha 2020) Aman Chadha’s Notes\nIbrahim Jelliti’s Notes",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w2/lab01.html",
    "href": "notes/c3w2/lab01.html",
    "title": "Hidden State Activation : Ungraded Lecture Notebook",
    "section": "",
    "text": "course banner\nIn this notebook you’ll take another look at the hidden state activation function. It can be written in two different ways.\nI’ll show you, step by step, how to implement each of them and then how to verify whether the results produced by each of them are same or not.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "L1 - Hidden State Activation"
    ]
  },
  {
    "objectID": "notes/c3w2/lab01.html#background",
    "href": "notes/c3w2/lab01.html#background",
    "title": "Hidden State Activation : Ungraded Lecture Notebook",
    "section": "Background",
    "text": "Background\n\n\n\nvanilla rnn\n\n\nThis is the hidden state activation function for a vanilla RNN.\nh^{&lt;t&gt;}=g(W_{h}[h^{&lt;t-1&gt;},x^{&lt;t&gt;}] + b_h)\nWhich is another way of writing this:\nh^{&lt;t&gt;}=g(W_{hh}h^{&lt;t-1&gt;} \\oplus W_{hx}x^{&lt;t&gt;} + b_h)\nWhere\n\nW_{h} in the first formula is denotes the horizontal concatenation of W_{hh} and W_{hx} from the second formula.\nW_{h} in the first formula is then multiplied by [h^{&lt;t-1&gt;},x^{&lt;t&gt;}], another concatenation of parameters from the second formula but this time in a different direction, i.e vertical!\n\nLet us see what this means computationally.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "L1 - Hidden State Activation"
    ]
  },
  {
    "objectID": "notes/c3w2/lab01.html#imports",
    "href": "notes/c3w2/lab01.html#imports",
    "title": "Hidden State Activation : Ungraded Lecture Notebook",
    "section": "Imports",
    "text": "Imports\n\nimport numpy as np",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "L1 - Hidden State Activation"
    ]
  },
  {
    "objectID": "notes/c3w2/lab01.html#joining-concatenation",
    "href": "notes/c3w2/lab01.html#joining-concatenation",
    "title": "Hidden State Activation : Ungraded Lecture Notebook",
    "section": "Joining (Concatenation)",
    "text": "Joining (Concatenation)\n\nWeights\nA join along the vertical boundary is called a horizontal concatenation or horizontal stack.\nVisually, it looks like this:- W_h = \\left [ W_{hh} \\ | \\ W_{hx} \\right ]\nI’ll show you two different ways to achieve this using numpy.\nNote: The values used to populate the arrays, below, have been chosen to aid in visual illustration only. They are NOT what you’d expect to use building a model, which would typically be random variables instead.\n\nTry using random initializations for the weight arrays.\n\n\n# Create some dummy data\n\nw_hh = np.full((3, 2), 1)  # illustration purposes only, returns an array of size 3x2 filled with all 1s\nw_hx = np.full((3, 3), 9)  # illustration purposes only, returns an array of size 3x3 filled with all 9s\n\n\n### START CODE HERE ###\n# Try using some random initializations, though it will obfuscate the join. eg: uncomment these lines\n# w_hh = np.random.standard_normal((3,2))\n# w_hx = np.random.standard_normal((3,3))\n### END CODE HERE ###\n\nprint(\"-- Data --\\n\")\nprint(\"w_hh :\")\nprint(w_hh)\nprint(\"w_hh shape :\", w_hh.shape, \"\\n\")\nprint(\"w_hx :\")\nprint(w_hx)\nprint(\"w_hx shape :\", w_hx.shape, \"\\n\")\n\n# Joining the arrays\nprint(\"-- Joining --\\n\")\n# Option 1: concatenate - horizontal\nw_h1 = np.concatenate((w_hh, w_hx), axis=1)\nprint(\"option 1 : concatenate\\n\")\nprint(\"w_h :\")\nprint(w_h1)\nprint(\"w_h shape :\", w_h1.shape, \"\\n\")\n\n# Option 2: hstack\nw_h2 = np.hstack((w_hh, w_hx))\nprint(\"option 2 : hstack\\n\")\nprint(\"w_h :\")\nprint(w_h2)\nprint(\"w_h shape :\", w_h2.shape)\n\n-- Data --\n\nw_hh :\n[[1 1]\n [1 1]\n [1 1]]\nw_hh shape : (3, 2) \n\nw_hx :\n[[9 9 9]\n [9 9 9]\n [9 9 9]]\nw_hx shape : (3, 3) \n\n-- Joining --\n\noption 1 : concatenate\n\nw_h :\n[[1 1 9 9 9]\n [1 1 9 9 9]\n [1 1 9 9 9]]\nw_h shape : (3, 5) \n\noption 2 : hstack\n\nw_h :\n[[1 1 9 9 9]\n [1 1 9 9 9]\n [1 1 9 9 9]]\nw_h shape : (3, 5)\n\n\n\n\nHidden State & Inputs\nJoining along a horizontal boundary is called a vertical concatenation or vertical stack. Visually it looks like this:\n[h^{&lt;t-1&gt;},x^{&lt;t&gt;}] = \\left[ \\frac{h^{&lt;t-1&gt;}}{x^{&lt;t&gt;}} \\right]\nI’ll show you two different ways to achieve this using numpy.\nTry using random initializations for the hiddent state and input matrices.\n\n# Create some more dummy data\nh_t_prev = np.full((2, 1), 1)  # illustration purposes only, returns an array of size 2x1 filled with all 1s\nx_t = np.full((3, 1), 9)       # illustration purposes only, returns an array of size 3x1 filled with all 9s\n\n# Try using some random initializations, though it will obfuscate the join. eg: uncomment these lines\n\n### START CODE HERE ###\n# h_t_prev = np.random.standard_normal((2,1))\n# x_t = np.random.standard_normal((3,1))\n### END CODE HERE ###\n\nprint(\"-- Data --\\n\")\nprint(\"h_t_prev :\")\nprint(h_t_prev)\nprint(\"h_t_prev shape :\", h_t_prev.shape, \"\\n\")\nprint(\"x_t :\")\nprint(x_t)\nprint(\"x_t shape :\", x_t.shape, \"\\n\")\n\n# Joining the arrays\nprint(\"-- Joining --\\n\")\n\n# Option 1: concatenate - vertical\nax_1 = np.concatenate(\n    (h_t_prev, x_t), axis=0\n)  # note the difference in axis parameter vs earlier\nprint(\"option 1 : concatenate\\n\")\nprint(\"ax_1 :\")\nprint(ax_1)\nprint(\"ax_1 shape :\", ax_1.shape, \"\\n\")\n\n# Option 2: vstack\nax_2 = np.vstack((h_t_prev, x_t))\nprint(\"option 2 : vstack\\n\")\nprint(\"ax_2 :\")\nprint(ax_2)\nprint(\"ax_2 shape :\", ax_2.shape)\n\n-- Data --\n\nh_t_prev :\n[[1]\n [1]]\nh_t_prev shape : (2, 1) \n\nx_t :\n[[9]\n [9]\n [9]]\nx_t shape : (3, 1) \n\n-- Joining --\n\noption 1 : concatenate\n\nax_1 :\n[[1]\n [1]\n [9]\n [9]\n [9]]\nax_1 shape : (5, 1) \n\noption 2 : vstack\n\nax_2 :\n[[1]\n [1]\n [9]\n [9]\n [9]]\nax_2 shape : (5, 1)",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "L1 - Hidden State Activation"
    ]
  },
  {
    "objectID": "notes/c3w2/lab01.html#verify-formulas",
    "href": "notes/c3w2/lab01.html#verify-formulas",
    "title": "Hidden State Activation : Ungraded Lecture Notebook",
    "section": "Verify Formulas",
    "text": "Verify Formulas\nNow you know how to do the concatenations, horizontal and vertical, lets verify if the two formulas produce the same result.\nFormula 1: h^{&lt;t&gt;}=g(W_{h}[h^{&lt;t-1&gt;},x^{&lt;t&gt;}] + b_h)\nFormula 2: h^{&lt;t&gt;}=g(W_{hh}h^{&lt;t-1&gt;} \\oplus W_{hx}x^{&lt;t&gt;} + b_h)\nTo prove:- Formula 1 \\Leftrightarrow Formula 2\nWe will ignore the bias term b_h and the activation function g(\\ ) because the transformation will be identical for each formula. So what we really want to compare is the result of the following parameters inside each formula:\n$W_{h}[h{},x{}] W_{hh}h^{} W_{hx}x^{} $\nWe’ll see how to do this using matrix multiplication combined with the data and techniques (stacking/concatenating) from above.\n\nTry adding a sigmoid activation function and bias term to the checks for completeness.\n\n\n# Data\n\nw_hh = np.full((3, 2), 1)  # returns an array of size 3x2 filled with all 1s\nw_hx = np.full((3, 3), 9)  # returns an array of size 3x3 filled with all 9s\nh_t_prev = np.full((2, 1), 1)  # returns an array of size 2x1 filled with all 1s\nx_t = np.full((3, 1), 9)       # returns an array of size 3x1 filled with all 9s\n\n\n# If you want to randomize the values, uncomment the next 4 lines\n\n# w_hh = np.random.standard_normal((3,2))\n# w_hx = np.random.standard_normal((3,3))\n# h_t_prev = np.random.standard_normal((2,1))\n# x_t = np.random.standard_normal((3,1))\n\n# Results\nprint(\"-- Results --\")\n# Formula 1\nstack_1 = np.hstack((w_hh, w_hx))\nstack_2 = np.vstack((h_t_prev, x_t))\n\nprint(\"\\nFormula 1\")\nprint(\"Term1:\\n\",stack_1)\nprint(\"Term2:\\n\",stack_2)\nformula_1 = np.matmul(np.hstack((w_hh, w_hx)), np.vstack((h_t_prev, x_t)))\nprint(\"Output:\")\nprint(formula_1)\n\n# Formula 2\nmul_1 = np.matmul(w_hh, h_t_prev)\nmul_2 = np.matmul(w_hx, x_t)\nprint(\"\\nFormula 2\")\nprint(\"Term1:\\n\",mul_1)\nprint(\"Term2:\\n\",mul_2)\n\nformula_2 = np.matmul(w_hh, h_t_prev) + np.matmul(w_hx, x_t)\nprint(\"\\nOutput:\")\nprint(formula_2, \"\\n\")\n\n# Verification \n# np.allclose - to check if two arrays are elementwise equal upto certain tolerance, here  \n# https://numpy.org/doc/stable/reference/generated/numpy.allclose.html\n\nprint(\"-- Verify --\")\nprint(\"Results are the same :\", np.allclose(formula_1, formula_2))\n\n### START CODE HERE ###\n# # Try adding a sigmoid activation function and bias term as a final check\n# # Activation\n# def sigmoid(x):\n#     return 1 / (1 + np.exp(-x))\n\n# # Bias and check\n# b = np.random.standard_normal((formula_1.shape[0],1))\n# print(\"Formula 1 Output:\\n\",sigmoid(formula_1+b))\n# print(\"Formula 2 Output:\\n\",sigmoid(formula_2+b))\n\n# all_close = np.allclose(sigmoid(formula_1+b), sigmoid(formula_2+b))\n# print(\"Results after activation are the same :\",all_close)\n### END CODE HERE ###\n\n-- Results --\n\nFormula 1\nTerm1:\n [[1 1 9 9 9]\n [1 1 9 9 9]\n [1 1 9 9 9]]\nTerm2:\n [[1]\n [1]\n [9]\n [9]\n [9]]\nOutput:\n[[245]\n [245]\n [245]]\n\nFormula 2\nTerm1:\n [[2]\n [2]\n [2]]\nTerm2:\n [[243]\n [243]\n [243]]\n\nOutput:\n[[245]\n [245]\n [245]] \n\n-- Verify --\nResults are the same : True",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "L1 - Hidden State Activation"
    ]
  },
  {
    "objectID": "notes/c3w2/lab01.html#summary",
    "href": "notes/c3w2/lab01.html#summary",
    "title": "Hidden State Activation : Ungraded Lecture Notebook",
    "section": "Summary",
    "text": "Summary\nThat’s it! We’ve verified that the two formulas produce the same results, and seen how to combine matrices vertically and horizontally to make that happen. We now have all the intuition needed to understand the math notation of RNNs.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "L1 - Hidden State Activation"
    ]
  },
  {
    "objectID": "notes/c3w2/lab04.html",
    "href": "notes/c3w2/lab04.html",
    "title": "Creating a GRU model using Trax: Ungraded Lecture Notebook",
    "section": "",
    "text": "course banner\nFor this lecture notebook you will be using Trax’s layers. These are the building blocks for creating neural networks with Trax.\nimport trax\nfrom trax import layers as tl\n\n2025-02-10 16:57:42.066058: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1739199462.079538  125276 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1739199462.084360  125276 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nTrax allows to define neural network architectures by stacking layers (similarly to other libraries such as Keras). For this the Serial() is often used as it is a combinator that allows to stack layers serially using function composition.\nNext you can see a simple vanilla NN architecture containing 1 hidden(dense) layer with 128 cells and output (dense) layer with 10 cells on which we apply the final layer of logsoftmax.\nmlp = tl.Serial(\n  tl.Dense(128),\n  tl.Relu(),\n  tl.Dense(10),\n  tl.LogSoftmax()\n)\nEach of the layers within the Serial combinator layer is considered a sublayer. Notice that unlike similar libraries, in Trax the activation functions are considered layers. To know more about the Serial layer check the docs here.\nYou can try printing this object:\nprint(mlp)\n\nSerial[\n  Dense_128\n  Serial[\n    Relu\n  ]\n  Dense_10\n  LogSoftmax\n]\nPrinting the model gives you the exact same information as the model’s definition itself.\nBy just looking at the definition you can clearly see what is going on inside the neural network. Trax is very straightforward in the way a network is defined, that is one of the things that makes it awesome!",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "L4 - Creating a GRU model using Trax"
    ]
  },
  {
    "objectID": "notes/c3w2/lab04.html#gru-model",
    "href": "notes/c3w2/lab04.html#gru-model",
    "title": "Creating a GRU model using Trax: Ungraded Lecture Notebook",
    "section": "GRU MODEL",
    "text": "GRU MODEL\nTo create a GRU model you will need to be familiar with the following layers (Documentation link attached with each layer name): - ShiftRight Shifts the tensor to the right by padding on axis 1. The mode should be specified and it refers to the context in which the model is being used. Possible values are: ‘train’, ‘eval’ or ‘predict’, predict mode is for fast inference. Defaults to “train”.\n\nEmbedding Maps discrete tokens to vectors. It will have shape (vocabulary length X dimension of output vectors). The dimension of output vectors (also called d_feature) is the number of elements in the word embedding.\nGRU The GRU layer. It leverages another Trax layer called GRUCell. The number of GRU units should be specified and should match the number of elements in the word embedding. If you want to stack two consecutive GRU layers, it can be done by using python’s list comprehension.\nDense Vanilla Dense layer.\nLogSoftMax Log Softmax function.\n\nPutting everything together the GRU model will look like this:\n\nmode = 'train'\nvocab_size = 256\nmodel_dimension = 512\nn_layers = 2\n\nGRU = tl.Serial(\n      tl.ShiftRight(mode=mode), # Do remember to pass the mode parameter if you are using it for interence/test as default is train \n      tl.Embedding(vocab_size=vocab_size, d_feature=model_dimension),\n      [tl.GRU(n_units=model_dimension) for _ in range(n_layers)], # You can play around n_layers if you want to stack more GRU layers together\n      tl.Dense(n_units=vocab_size),\n      tl.LogSoftmax()\n    )\n\nNext is a helper function that prints information for every layer (sublayer within Serial):\nTry changing the parameters defined before the GRU model and see how it changes!\n\ndef show_layers(model, layer_prefix=\"Serial.sublayers\"):\n    print(f\"Total layers: {len(model.sublayers)}\\n\")\n    for i in range(len(model.sublayers)):\n        print('========')\n        print(f'{layer_prefix}_{i}: {model.sublayers[i]}\\n')\n        \nshow_layers(GRU)\n\nTotal layers: 6\n\n========\nSerial.sublayers_0: Serial[\n  ShiftRight(1)\n]\n\n========\nSerial.sublayers_1: Embedding_256_512\n\n========\nSerial.sublayers_2: GRU_512\n\n========\nSerial.sublayers_3: GRU_512\n\n========\nSerial.sublayers_4: Dense_256\n\n========\nSerial.sublayers_5: LogSoftmax\n\n\n\nHope you are now more familiarized with creating GRU models using Trax.\nYou will train this model in this week’s assignment and see it in action.\nGRU and the trax minions will return, in this week’s endgame.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "L4 - Creating a GRU model using Trax"
    ]
  },
  {
    "objectID": "notes/cs11-737-w07/index.html",
    "href": "notes/cs11-737-w07/index.html",
    "title": "Data-driven Strategies for NMT",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\nVideo 1: Lesson Video\n\n\n\n\n\n\n\n\nThis week’s slides\n\n\n\n\nSupplementary Figure 1\n\n\n\n\n\n\n\n\n\n\nTranscript\n\n\n\nIntroduction\n\nThis time i’ll be talking about uh machine translation uh one more time uh i didn’t get a chance to talk about evaluation of translation two times ago so i’d like to start out with that because that’s pretty important and um then we can talk about uh the data augmentation strategies is this a little bit bright like and hard to see for people yeah okay did anything change okay cool um great so let me skip to the evaluation part since uh we’re going to be catching up on that so Machine Translation Evaluation so machine translation evaluation is a very important and difficult topic in doing machine translation research and in fact i think we’ve gotten to the point where almost evaluating how well we’re doing is maybe as difficult as like actually doing the translation itself and the reason for this is um if we output a translation there are many different correct translations right so if i say um like we could have paraphrases where the output is this is a dog i see a dog um there is a dog here other things like this and all of those would be appropriate for uh you know an equivalent sentence in another language Manual Evaluation so um the basic evaluation paradigm uh in machine translation there’s two different types um there’s human evaluation or manual evaluation in automatic evaluation and in the this is the basic evaluation paradigm for uh automatic evaluation where basically what we do is we have a parallel test set where we have an input and an output in the language fair that we’re interested in we use a system to generate translations and we compare the target translations with references um before i talk a little bit more detail about that i’d like to talk about kind of uh manual evaluation and this is the gold standard of doing evaluation of translation systems where basically we ask human evaluators to go in and check whether uh the answer is correct or not and to give an example um we’ve taken a source side sentence we generate some outputs and you can either evaluate by having a human evaluator look at the source in the output or look at a reference and um in the output and the reference would be like the so-called correct translation looking at the source and the output only works if you’re bilingual in both languages and it’s somewhat difficult to get bilingual speakers or at least more expensive however given the quality of machine translation systems nowadays very often you don’t know if the output of a human translator like your reference is better than the machine translation system uh so like very often if you hire a person to do evaluation you know they might not try super hard um and uh like i mentioned before so kind of the gold standard is to get somebody who knows the source and the target to do the evaluation um there’s a number of different axes along which you can do evaluation i just listed um a couple of them here one is adequacy and adequacy is basically whether the meaning of the translation is conveyed properly and the uh in this case this is the correct answer here you would know this if you knew japanese um which you know most people don’t but um if you knew japanese you would know the first one is correct so this is perfectly uh adequate it conveys the target message the middle one is um conveys the target message but is this fluent so it would score high on an adequacy scale but on a fluency scale it would score low the one over here is fluent um but not adequate so basically it switched the subject to that object for uh order so it would be wrong um and uh one notable thing about fluency is you don’t need to know the source language to evaluate fluency all you need to know is the target language because it only has to do with whether the output is fluent or not you can also do pairwise evaluation which just says which one of these is better um one of the good things about pairwise evaluation is it’s very simple uh because you just ask question which do you like better which do you think is a better translation the problem with it is it doesn’t give you an absolute idea of how well you’re doing so if you have two really bad systems and say which is better one might be better but they’re both really bad if you have two really good systems and say which is better one might be better but they’re both really good so um kind of absolute scales have that advantage um another thing is uh just like you might get bad translators you might get lazy evaluators uh you know if you hire people on mechanical turks uh and they’re not very motivated for example so um you need to be careful about quality control as well Human Evaluation Shared Tasks there’s human evaluation shared tasks so the most famous one is the conference on machine translation shared tasks so i can show you a little bit what this looks like they basically have um a whole bunch whoops they have a whole bunch of tasks uh that you can participate in most of these are tasks for actual uh translation but they also have evaluation tasks on metrics and quality estimation so basically what you try to do here is you try to create a metric that has the highest um correlation with uh with human evaluation and um for quality estimation what this is is this is essentially evaluation without a gold standard reference so you’re just given the input and the output and you want to guess how good the system output is and this is harder obviously because you don’t have an example of what a good translation looks like but it’s also very useful in practical situations where like let’s say you’re a machine translation company and you want to decide whether um you need to get a human translator to go in and check the output and correct it or something like that so if that’s the case if you can estimate very accurately whether the input and output are correct or not then that would save you money save you time uh give you confidence in the results Blue Scores there’s also other leaderboards and stuff for other sequence sequence models but that’s a little bit less important for uh this multilingual class um so there are other metrics uh like blue scores so blue score is very famous you know if you’ve done any research on machine translation or even heard of it you probably encountered blue the exact details of how blue is calculated are um what you do is you take the precision of engrams output by the system so for example if you look at the unigram precision that’s out of all the unigrams that the system output how many of them exist in a reference or one of the references that you’re provided um if in the case that you’re using multiple references and this gives you a position of like three out of five and then you take the geometric mean of uh engrams usually one to four um and then you also have a penalty for outputting two short sentences because one way to improve your precision is to not output very many things um so this is basically to prevent you from gaming system but the important thing is now the details of how blue is calculated probably the important thing is that this is a lexical metric which means that you’re just doing exact match with the references um and this has a few uh issues um one of the issues is essentially that um you so there’s there’s two major issues um that cause blue to either underestimate how good a translation is or overestimate how good a translation is blue tends to underestimate how good translations are when the translations are uh paraphrases of the true reference so um if you uh for example had um i have uh like i went uh i went to the store and bought a book yesterday and you compare that with uh yesterday i bought a book at the store those are almost identical in meaning but they would get a low blue square because i’ve just rearranged the phrases a little bit um so that’s when blue tends to underestimate scores blue tends to overestimate scores when you get very critical words in the sentence wrong but get everything else right so for example um could you please send this big package to uh philadelphia turns into could you please send this big package to japan um that would get a very good blue score because most of the words are the same but your package would go to japan and that’s probably not what you intended by that otherwise right so um Shortness Penalty uh that’s uh the the downside of blue it’s basically not smart enough with respect to these so recently um yeah so the brevity the brevity penalty basically gives you a penalty if your output is shorter than the intended output so if the reference is like 20 words then you’re um and your sentence is uh 15 words you would get a penalty of about 0.75 it’s not exactly like just the ratio it actually drops off faster and stuff like that but that’s a basic idea that’s a really good question so you pay a penalty in your precision your precision goes down by a lot because there’s no way to get good precision if you output too many things um one thing you should know about blue if you’re using it in your research which you might is that it’s very very sensitive to the length so if your length is a little bit too short or a little bit too long it hurts your blue really badly so that’s another problem with blue essentially is that it’s not sensitive to like paraphrases that are too long or too short as well um so recently in the past three years or Bert Score so there’s been a huge um improvement in embedding based metrics which are basically metrics that you know take advantage of um recent nlp techniques and one of the first ones was bert score and these can be separated into unsupervised metrics and supervised metrics unsupervised metrics require no annotated data of um whether a [Music] translation is a good translation or a bad translation supervised metrics are trained to basically regress to an estimation of how good a transplantation is or not so a birth score is an unsupervised metric that’s based essentially on the similarity between burton betting so it has this matching algorithm where you basically uh for each word in the output you try to find um you know how good uh it matches with one of the words in the input and this is good because it can do things like handle paraphrases as long as the paraphrases have similar bert embeddings um another famous one is bluert BlueRT and what they did essentially was they trained bert to predict uh human evaluation scores um so they essentially solve a regression problem from the uh sentences uh like the reference in this uh system output to an evaluation score so they’re just going in directly predicting uh evaluation and they have a bunch of other tricks like unsupervised training where they try to predict blue or rouge or other lexical metrics beforehand and that makes it more robust um the uh favorite one that uh we use in our Comet research on mt now is a comet and comet is also similarly it trains the model to predict human evaluation uh but in addition to using the uh just the system output in reference it also uses a source sentence which means that essentially uh i talked about human evaluation right uh where you can either ask a human evaluator to look only at the the reference or also look at the source and um for a similar reason to why we would like a speaker human speaker to do that um it’s also useful to have the model do that because the model can look at the source and see if the information is reflected in the target Bart Score and the final one um uh prism’s another one based on paraphrasing a final one is a bart score environment score is one that i’m a co-author on this is a unsupervised metric uh that is based on basically a generative model that tries to generate the uh the system output using the reference or the reference using the system output or the source using the system output et cetera et cetera and um bart score i think is good because it’s unsupervised like birth score but it’s essentially more accurate and more controllable so you can do things like calculate recall calculate a precision and other things like that so if that sounds interesting you can take a look at the paper as well but basically if you’re doing empty i would suggest using comment now because uh it’s well supported it has a nice uh package it’s pretty widely tested and follow-up reports have suggested that it has very good correlation with human evaluation um so that’s my suggestion uh you can Meta Evaluation trust me but you don’t also you don’t don’t necessarily need to trust me so like as i mentioned um meta evaluation basically what it does is it runs human evaluation and automatic evaluation on the same outputs and calculates the correlation this is what they do in the wmt shared tasks like the wmt metrics task for mt other things for summarization etc and one interesting thing is that evaluation as i mentioned at the beginning is pretty hard especially with good systems so most metrics actually had no correlation with human eval uh over a subset of the best systems at some of the wmt 2019 tasks which means that basically all of the evaluation metrics we had were kind of broken on uh like evaluating really good mt systems so fortunately now we have comment we have other things like this that actually seemed to be a lot more robust but um it was a major problem so you basically calculate the correlation you calculate pearson’s correlation there’s experiments correlation and um the way you do that is you human you do human eval of a whole bunch of sentences or humans develop a whole bunch of systems and you try to find the metric that has the highest correlation between the evaluation scores of the systems and the evaluations given by the humans yep all right they often don’t support that many languages well so that’s a good that’s a really good question um i many of them do uh use something like embert or xlmr which support a lot of languages xlmr actually envert’s a little bit more biased xlmr has pretty good coverage of the most common languages in the world but of course as you go down to less well resource languages that’s going to continue to be a problem um there you might be stuck with blue for now but honestly if you have really bad systems really bad empty systems i still think blue is probably good enough in many cases oh another option is carefu chrf and that’s a character-based evaluation metric for mt that’s particularly good for languages with like rich morphology or something like that so i think when you’re working with low resource languages your mnt systems are also going to be really bad so any metric you have is still going to be like reasonably good at measuring progress so um cool yeah so um the next thing i’d like to Database Strategies talk about is the database strategies uh to low resource mt um there’s not a whole lot of content here so i’ll try to go through it rather quickly to leave time for the discussion um but basically we have data challenges in the resource mt um so mt of high resource languages with large parallel corpora gives us you know very good uh translations but low resource languages with small parallel corporate you just train there you can end up with nonsense so this is an example of a system trained on 5 000 languages and the most frequent failure state is basically that a neural nt system will just spit out something that has nothing to do with the original inputs there’s a famous example of this um so uh that says why is google translate uh spitting out sinister religious prophecies um and basically if you put in a dog dog dog dog dog in maori it outputs doomsday clock is three minutes at 12 we are experiencing characters in a dramatic development uh in which jesus returns um [Music] can you guess why this happened exactly they use bible data in training their system and when you use bible data and training your system and your system doesn’t know what to do because it has so few resources or it sees something it doesn’t know it just reverts to using the language model and basically outputs whatever the language model thinks and uh thinks it looks likely and so you know if your system is trained on bible data that looks like the bible if your system’s trained on something else it looks like something else High and Low Resource Languages um so you know that’s basically what happened here as well um so some ways to fix this um we can transfer from high resource languages to low resource languages so basically what you do is you train on a high resource language or multiple high resource languages and then you adapt to the low resource language one the simplest way to do that is just to continue fine tuning on the low resource language um you can also do joint training with the low resource language in the high resource language so just concatenate all the data together um and uh in training so um this is okay but there are some problems with this as well one problem is a sub-optimal lexical or syntactic sharing and another problem is it’s not possible to leverage monolingual data because you still require a parallel data here and um i’m going to be talking more about like lexical overlap and loanwords and stuff in uh in the next class so i’ll cover that more there but basically suffice to say the high resource language and the lower resource language are different so training on different data is sub-optimal for uh information sure Data Augmentation so if we think about data augmentation data augmentation is basically generating other data that looks like the data that you want to have the very convenient thing about this is um generating more training data and feeding it into your existing system is uh easy but effective in uh in improving mt performance so it’s actually a pretty widely used technique now so if we look at the available resources um we might have a low resource language parallel data a high resource language parallel data and also for example target data which is monolingual hence the m here and uh what we do is we would like to create augmented data where we have target data and like pseudo low resource language data and uh train our model on this with the idea being that if we can create this uh this will be closer to our final evaluation scenario where we uh where we want to generate the target given a low resource language Back Translation so the first example is a back translation and the way back translation works is basically we train a target two low resource language system and we take our monolingual target data and we generate uh fake low resource language data by translating the target data into the low resource language data so um this is how it works uh we take our target to low resource language system um we back translate using the system and then we train a low resource language to target language system using the concatenation of this augmented data in the original data and the key point here is that when we are um when we’re training like a sequence sequence model or a machine translation model um we’re we’re training it to do two things we’re training it to do language modeling on the target side only um and we’re trying to do mapping between the source side and the target side and in order to do language modeling we only really need good target site data so even if there’s some degree of error in this uh like low resource language here we’ll still be able to learn uh target site data and we’ll be able to learn a the language model from target side data and we’ll be able to learn a mapping you know even if it’s imperfect from the low resource language to the target language Training Schedule so there’s a couple ways to generate translations uh when doing back translation um the first one is using beam search oh sorry yeah uh yeah that’s a really that’s a really good question so the question was is there any sort of training schedule that you use when you do this um so the the kind of quote-unquote obvious training schedule that you might do is you might train jointly on both of them at first and then fine-tune on this data over here um and uh that would make sense because you know this is good data this is like actually translated data however there’s another issue um which actually is not super obvious at first but it’s maybe obvious in hindsight which is that if this data is all from the bible and then you want to translate news then actually fine tuning on bible data will be really out of domain and cause issues for you um so in fact in the original black translation paper they threw away this data and only trained on this because it was more in domain and that ended up giving better results but that was predicated on the fact that they have a good you know batch translation system in the first place so um it’s not necessarily clear what the ideal schedule is but you would almost certainly benefit from some sort of schedule or balancing or something um but that’s a complicated hyper parameter so because it’s a complicated hyper parameter it’s also very common to just concatenate the two and these are good details to know for assignment too by the way because they might make a difference in your final scores Generating Translations um how to generate translations so beam search is one way uh and basically what this is doing is selecting the highest scoring output uh this was done in the original paper um this has the advantage of having higher quality but also lower diversity in the outputs and the potential for bias um so you might uh like for example uh one result is beam search tends to mostly output um pronouns from the majority gender because they’re over represented so you might get only get mail inflections uh if you do beam search so that’s the type of data bias that could result from here and so the other option is sampling and what you do is you randomly sample from the back translation model which gives a lower overall quality but higher diversity and most reports say this works better at the moment simply and uh also we had a recent paper uh which i’m going to introduce in a second but this has kind of a theoretical explanation for why we think sampling should be better which is that it’s a better model of the underlying data distribution that we’re trying to model so um i i think i’m pretty firmly a believer that sampling is the way to go there’s also a method of iterative back In iterative back translation translation this is particularly useful when you have a large monolingual data in both languages and again the idea is simple you train a low resource language to target system first so this is going in the direction you originally want to translate you generate pseudo data um with the target language you use that to train your target to low resource language system you back translate and then you use this to train your final system so this is uh now you have three systems your forward translation data augmentation system your back translation data augmentation system and your forward translation final system um you can do this as many times as you want obviously um you could also do it on the fly in the process of training the system so uh this can become arbitrarily complicated if you want Metaback translation um just one example of this um this is a paper that i just talked about but we have a paper called meta back translation which i think is kind of a an interesting idea um so normally uh when we’re training this system uh to train the uh the low resource language to uh the target language system we’re back propping the gradient uh from the slow resource language data um but uh we can also uh do a back propagation step where we uh um basically train oh sorry that arrow is thrown i apologize so the arrow actually should be going from here around this to here so the basic idea i’ll fix this later in the slides but the basic idea is we use the signal that we get from um from training the final system that we want to train to update the parameters of the back translation system so we’re essentially training the ideal back translation system to train a good forward translation system so um this is a uh i like the idea behind here uh which is basically the final goal of the back translation system is to improve the forward translation system so we can directly optimize it to do this Metaback translation issues um so there are a couple issues here um the first issue is that back translation often fails in low resource languages or domains and um as a solution uh one thing that we can do is we can use other high resource languages or we can combine them with monolingual data maybe with denoising objectives which we’re covering in a following class and we can perform other uh varieties of rule-based uh augmentation so i’m gonna go through these uh in a little bit uh in maybe uh in a few minutes so um also actually we’ll have discussion about uh about these two so maybe i’ll just briefly explain the idea and we can discuss more in the discussion for people who read those papers so um using uh high resource languages High resource languages augmentation and augmentation um the problem is uh target to low resource language back translation might be very low quality so the idea is we can also use a high resource language that’s similar to the low resource language and basically for example if we have um something like azerbaijani in turkish azerbaijani and turkish are very highly related so maybe we could uh use information from azerbaijani to english translation back translate into az into turkish which is certainly going to give us higher quality data and use that to augment our data for azerbaijani english system and then we can just throw away this azerbaijani data that we know is not going to be very useful um so that would High resource languages pivoting give us additional high resource language to target language data and another thing we can do is we can augment via pivoting and so basically what that does is that gives us data where we take the high resource language data and we translate that into the low resource language and presumably translation from the high resource language to low resource language is easier because these languages are more related so basically what this does is that gives us a better uh like low resource language pseudo data here and um we can also do a similar thing where we generate more high resource language data and uh this basically gives us um three different ways to create this pseudo-parallel data between the low resource language and target language another simple trick uh this is kind of Monolingual data copying like frustratingly effective at improving your models is monolingual data copying and um the issue is that um back translation uh may help with structure but one of the issues with the resource language systems is that they tend to fail really badly on um unusual vocabulary so like for example proper names or something like this so you might get a back translation system that’s very good at getting the structure right but get it gets you know all of your proper names and entities incorrect so uh basically one thing that you can do here is you just copy the target data into the source data and then you’re done um and uh this uh kind of guarantees to maintain the entities so um or the the rare words so uh that will help uh mitigate these issues of like vocabulary being dropped yeah something to point out with copying is that even in languages with different scripts it seems to work really well maybe because of auto and clutter objective stuff yeah even in languages with different scripts and there’s actually a nice paper by the same authors who wrote this paper um Transfer learning where they examine this and basically the um the idea here is they are trying to figure out like what transfer learning why does transfer learning have this um uh positive effect and basically one of the things that they show is that even just you know making sure that the length is the same is approximately the same or making sure that the words are output in approximately the same order as the input is uh is effective for improving translation accuracy so if you have a low resource language um the translation system might drop half the content or it might uh like totally mess up the order or something like this so this uh this paper is demonstrating that kind of just like a monotonic bias and a bias towards outputting approximately the same number of words gets you a long way in improving the results um which of course monolingual data copying would also do um and so for the um Dictionarybased augmentation for the final things uh which we’re also reading so we can talk more about them in the discussion um we had dictionary based augmentation and dictionary based augmentation um basically finds rare words in the source sentences um it could also be in the target sentence and uh tries to replace uh the words with other words that are kind of in the same semantic class so it replaces car with motorbike um and then using a lexicon it uh replaces the words in the targeted sentence as well so it’s basically creating more uh sentences to augment uh augmented data uh with uh like words that are less frequent uh in the original purpose and in order to do this they need to use a tool called word alignment and what Word alignment word alignment does is it essentially takes in two parallel corpora and um the parallel corpora you want to find which words align to each other in the uh source and target sentences and this is useful for a number of reasons it’s useful for analysis it’s useful for cross-lingual transfer learning i talked about supervised alignment as a training method uh last time i believe um and there’s a couple uh methods to do so um there’s again traditional symbolic methods uh which like blue are based on exact lexical match or um you know some variety of clustering uh giza plus plus has some clustering involved in it but recently uh neural methods have been largely um uh outperforming these and i can recommend a highly our aligner called awesome lion i i didn’t name it um i’m far too humble to name my alignment or awesome line but um uh but it’s pretty awesome i have to say um and basically it uses multilingual perks and it tries to find things that are similar but it’s also fine-tuned multilingually on supervised data so basically there’s some supervision that goes into it to try to uh inform the aligner about the outputs and it works on any language that’s included in mvert again uh like the question before uh it won’t work on very low resource languages of course so then you might be stuck with keystone plus plus and faster um Word by word data augmentation so you can also do things like word by word data augmentation where you simply translate sentences word by word into the target sentence using a dictionary uh this is another frustratingly you know effective method like monolingual data copying um however there are problems like word order and syntactic divergence so if you get like i the new car bought number one the order is strange number two uh these words don’t actually align with each other so that’s a problem uh so um Reordering other things you can do or you can try to decrease this divergence with uh reordering or rules so this was also another paper in the potential reading and basically what the idea is that you a priori do some reordering from one language uh from english into like reordered english and then do data augmentation on top of that and the good thing about this is like english has a lot of analysis tools you could like do syntactic parsing of english get the syntactic structure build reordering rules on top of that and then uh just apply dictionary-based translation and then the hope would be that you would get something that looks a lot more like japanese than if you just translated english word by word and one interesting thing we showed here was we demonstrated that this was useful for japanese translation but then we applied the exact same reordering rules and also applied it to wigger which is another language that’s completely different different language family but it’s um it has a very similar syntax to japanese so because of that the exact same reordering rules for english were still effective in improving the results for weaker english translations so um because of that you know it’s not language dependent it’s rather syntax dependent and because there’s syntactic similarities between the language it helps so um yeah given that we now have um the Assignment assignment uh actually this is this slide is missing one of the uh one of the papers that was a potential paper to read um so first before we go to the discussion are there any questions so i kind of breezed through it the last part quickly but hopefully we can also talk about them in the discussion um okay if not this time we’re going to try a new experiment we’re going to try to make six groups um so the groups are going to be half the size and they’re going to be front middle front right front left back middle back left back middle in that right so we’re gonna ask everybody to talk a little bit more quietly uh but also you’ll be in a smaller circle so hopefully that’ll be easier um and yeah let’s go ahead and actually guys uh since we’re running a little bit late i think maybe we’ll skip the reporting part this time is that okay and we’ll just you know be within our groups and if there’s anything really interesting we can share on piazza or something okay\n\n\n\n\n\n\nCitationBibTeX citation:@online{bochman2022,\n  author = {Bochman, Oren},\n  title = {Data-Driven {Strategies} for {NMT}},\n  date = {2022-02-03},\n  url = {https://orenbochman.github.io/notes-nlp/notes/cs11-737-w07/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2022. “Data-Driven Strategies for NMT.”\nFebruary 3, 2022. https://orenbochman.github.io/notes-nlp/notes/cs11-737-w07/."
  },
  {
    "objectID": "notes/c3w4/lab02.html",
    "href": "notes/c3w4/lab02.html",
    "title": "Modified Triplet Loss",
    "section": "",
    "text": "course banner\nIn this notebook you’ll see how to calculate the full triplet loss, step by step, including the mean negative and the closest negative. You’ll also calculate the matrix of similarity scores.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Siamese Networks",
      "L2 - Modified Triplet Loss"
    ]
  },
  {
    "objectID": "notes/c3w4/lab02.html#background",
    "href": "notes/c3w4/lab02.html#background",
    "title": "Modified Triplet Loss",
    "section": "Background",
    "text": "Background\nThis is the original triplet loss function:\n\n\\mathcal{L_\\mathrm{Original}} = \\max{(\\mathrm{s}(A,N) -\\mathrm{s}(A,P) +\\alpha, 0)}\n\nIt can be improved by including the mean negative and the closest negative, to create a new full loss function. The inputs are the Anchor \\mathrm{A}, Positive \\mathrm{P} and Negative \\mathrm{N}.\n\n\\mathcal{L_\\mathrm{1}} = \\max{(mean\\_neg -\\mathrm{s}(A,P)  +\\alpha, 0)}\n\n\n\\mathcal{L_\\mathrm{2}} = \\max{(closest\\_neg -\\mathrm{s}(A,P)  +\\alpha, 0)}\n\n\n\\mathcal{L_\\mathrm{Full}} = \\mathcal{L_\\mathrm{1}} + \\mathcal{L_\\mathrm{2}}\n\nLet me show you what that means exactly, and how to calculate each step.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Siamese Networks",
      "L2 - Modified Triplet Loss"
    ]
  },
  {
    "objectID": "notes/c3w4/lab02.html#imports",
    "href": "notes/c3w4/lab02.html#imports",
    "title": "Modified Triplet Loss",
    "section": "Imports",
    "text": "Imports\n\nimport numpy as np",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Siamese Networks",
      "L2 - Modified Triplet Loss"
    ]
  },
  {
    "objectID": "notes/c3w4/lab02.html#similarity-scores",
    "href": "notes/c3w4/lab02.html#similarity-scores",
    "title": "Modified Triplet Loss",
    "section": "Similarity Scores",
    "text": "Similarity Scores\nThe first step is to calculate the matrix of similarity scores using cosine similarity so that you can look up \\mathrm{s}(A,P), \\mathrm{s}(A,N) as needed for the loss formulas.\n\nTwo Vectors\nFirst I’ll show you how to calculate the similarity score, using cosine similarity, for 2 vectors.\n\\mathrm{s}(v_1,v_2) = \\mathrm{cosine \\ similarity}(v_1,v_2) = \\frac{v_1 \\cdot v_2}{||v_1||~||v_2||} * Try changing the values in the second vector to see how it changes the cosine similarity.\n\n# Two vector example\n# Input data\nprint(\"-- Inputs --\")\nv1 = np.array([1, 2, 3], dtype=float)\nv2 = np.array([1, 2, 3.5])  # notice the 3rd element is offset by 0.5\n### START CODE HERE ###\n# Try modifying the vector v2 to see how it impacts the cosine similarity\n# v2 = v1                   # identical vector\n# v2 = v1 * -1              # opposite vector\n# v2 = np.array([0,-42,1])  # random example\n### END CODE HERE ###\nprint(\"v1 :\", v1)\nprint(\"v2 :\", v2, \"\\n\")\n\n# Similarity score\ndef cosine_similarity(v1, v2):\n    numerator = np.dot(v1, v2)\n    denominator = np.sqrt(np.dot(v1, v1)) * np.sqrt(np.dot(v2, v2))\n    return numerator / denominator\n\nprint(\"-- Outputs --\")\nprint(\"cosine similarity :\", cosine_similarity(v1, v2))\n\n-- Inputs --\nv1 : [1. 2. 3.]\nv2 : [1.  2.  3.5] \n\n-- Outputs --\ncosine similarity : 0.9974086507360697\n\n\n\n\nTwo Batches of Vectors\nNow i’ll show you how to calculate the similarity scores, using cosine similarity, for 2 batches of vectors. These are rows of individual vectors, just like in the example above, but stacked vertically into a matrix. They would look like the image below for a batch size (row count) of 4 and embedding size (column count) of 5.\nThe data is setup so that v_{1\\_1} and v_{2\\_1} represent duplicate inputs, but they are not duplicates with any other rows in the batch. This means v_{1\\_1} and v_{2\\_1} (green and green) have more similar vectors than say v_{1\\_1} and v_{2\\_2} (green and magenta).\nI’ll show you two different methods for calculating the matrix of similarities from 2 batches of vectors.\n\n\n# Two batches of vectors example\n# Input data\nprint(\"-- Inputs --\")\nv1_1 = np.array([1, 2, 3])\nv1_2 = np.array([9, 8, 7])\nv1_3 = np.array([-1, -4, -2])\nv1_4 = np.array([1, -7, 2])\nv1 = np.vstack([v1_1, v1_2, v1_3, v1_4])\nprint(\"v1 :\")\nprint(v1, \"\\n\")\nv2_1 = v1_1 + np.random.normal(0, 2, 3)  # add some noise to create approximate duplicate\nv2_2 = v1_2 + np.random.normal(0, 2, 3)\nv2_3 = v1_3 + np.random.normal(0, 2, 3)\nv2_4 = v1_4 + np.random.normal(0, 2, 3)\nv2 = np.vstack([v2_1, v2_2, v2_3, v2_4])\nprint(\"v2 :\")\nprint(v2, \"\\n\")\n\n# Batch sizes must match\nb = len(v1)\nprint(\"batch sizes match :\", b == len(v2), \"\\n\")\n\n# Similarity scores\nprint(\"-- Outputs --\")\n# Option 1 : nested loops and the cosine similarity function\nsim_1 = np.zeros([b, b])  # empty array to take similarity scores\n# Loop\nfor row in range(0, sim_1.shape[0]):\n    for col in range(0, sim_1.shape[1]):\n        sim_1[row, col] = cosine_similarity(v1[row], v2[col])\n\nprint(\"option 1 : loop\")\nprint(sim_1, \"\\n\")\n\n# Option 2 : vector normalization and dot product\ndef norm(x):\n    return x / np.sqrt(np.sum(x * x, axis=1, keepdims=True))\n\nsim_2 = np.dot(norm(v1), norm(v2).T)\n\nprint(\"option 2 : vec norm & dot product\")\nprint(sim_2, \"\\n\")\n\n# Check\nprint(\"outputs are the same :\", np.allclose(sim_1, sim_2))\n\n-- Inputs --\nv1 :\n[[ 1  2  3]\n [ 9  8  7]\n [-1 -4 -2]\n [ 1 -7  2]] \n\nv2 :\n[[-1.39930492  5.1153898   6.63335538]\n [ 5.02945534  8.32623907  3.55335285]\n [-2.1468471  -5.40282568  1.08523362]\n [ 3.98566581 -8.41659442 -1.33038683]] \n\nbatch sizes match : True \n\n-- Outputs --\noption 1 : loop\n[[ 0.90416347  0.83465723 -0.43819935 -0.47839422]\n [ 0.63202915  0.94804086 -0.66704516 -0.31119261]\n [-0.83068087 -0.95751319  0.79653304  0.75022602]\n [-0.38360581 -0.6063967   0.87076444  0.87143998]] \n\noption 2 : vec norm & dot product\n[[ 0.90416347  0.83465723 -0.43819935 -0.47839422]\n [ 0.63202915  0.94804086 -0.66704516 -0.31119261]\n [-0.83068087 -0.95751319  0.79653304  0.75022602]\n [-0.38360581 -0.6063967   0.87076444  0.87143998]] \n\noutputs are the same : True",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Siamese Networks",
      "L2 - Modified Triplet Loss"
    ]
  },
  {
    "objectID": "notes/c3w4/lab02.html#hard-negative-mining",
    "href": "notes/c3w4/lab02.html#hard-negative-mining",
    "title": "Modified Triplet Loss",
    "section": "Hard Negative Mining",
    "text": "Hard Negative Mining\nI’ll now show you how to calculate the mean negative mean\\_neg and the closest negative close\\_neg used in calculating \\mathcal{L_\\mathrm{1}} and \\mathcal{L_\\mathrm{2}}.\n\\mathcal{L_\\mathrm{1}} = \\max{(mean\\_neg -\\mathrm{s}(A,P)  +\\alpha, 0)}\n\\mathcal{L_\\mathrm{2}} = \\max{(closest\\_neg -\\mathrm{s}(A,P)  +\\alpha, 0)}\nYou’ll do this using the matrix of similarity scores you already know how to make, like the example below for a batch size of 4. The diagonal of the matrix contains all the \\mathrm{s}(A,P) values, similarities from duplicate question pairs (aka Positives). This is an important attribute for the calculations to follow.\n\n\nMean Negative\nmean\\_neg is the average of the off diagonals, the \\mathrm{s}(A,N) values, for each row.\n\n\nClosest Negative\nclosest\\_neg is the largest off diagonal value, \\mathrm{s}(A,N), that is smaller than the diagonal \\mathrm{s}(A,P) for each row. * Try using a different matrix of similarity scores.\n\n# Hardcoded matrix of similarity scores\nsim_hardcoded = np.array(\n    [\n        [0.9, -0.8, 0.3, -0.5],\n        [-0.4, 0.5, 0.1, -0.1],\n        [0.3, 0.1, -0.4, -0.8],\n        [-0.5, -0.2, -0.7, 0.5],\n    ]\n)\n\nsim = sim_hardcoded\n### START CODE HERE ###\n# Try using different values for the matrix of similarity scores\n# sim = 2 * np.random.random_sample((b,b)) -1   # random similarity scores between -1 and 1\n# sim = sim_2                                   # the matrix calculated previously\n### END CODE HERE ###\n\n# Batch size\nb = sim.shape[0]\n\nprint(\"-- Inputs --\")\nprint(\"sim :\")\nprint(sim)\nprint(\"shape :\", sim.shape, \"\\n\")\n\n# Positives\n# All the s(A,P) values : similarities from duplicate question pairs (aka Positives)\n# These are along the diagonal\nsim_ap = np.diag(sim)\nprint(\"sim_ap :\")\nprint(np.diag(sim_ap), \"\\n\")\n\n# Negatives\n# all the s(A,N) values : similarities the non duplicate question pairs (aka Negatives)\n# These are in the off diagonals\nsim_an = sim - np.diag(sim_ap)\nprint(\"sim_an :\")\nprint(sim_an, \"\\n\")\n\nprint(\"-- Outputs --\")\n# Mean negative\n# Average of the s(A,N) values for each row\nmean_neg = np.sum(sim_an, axis=1, keepdims=True) / (b - 1)\nprint(\"mean_neg :\")\nprint(mean_neg, \"\\n\")\n\n# Closest negative\n# Max s(A,N) that is &lt;= s(A,P) for each row\nmask_1 = np.identity(b) == 1            # mask to exclude the diagonal\nmask_2 = sim_an &gt; sim_ap.reshape(b, 1)  # mask to exclude sim_an &gt; sim_ap\nmask = mask_1 | mask_2\nsim_an_masked = np.copy(sim_an)         # create a copy to preserve sim_an\nsim_an_masked[mask] = -2\n\nclosest_neg = np.max(sim_an_masked, axis=1, keepdims=True)\nprint(\"closest_neg :\")\nprint(closest_neg, \"\\n\")\n\n-- Inputs --\nsim :\n[[ 0.9 -0.8  0.3 -0.5]\n [-0.4  0.5  0.1 -0.1]\n [ 0.3  0.1 -0.4 -0.8]\n [-0.5 -0.2 -0.7  0.5]]\nshape : (4, 4) \n\nsim_ap :\n[[ 0.9  0.   0.   0. ]\n [ 0.   0.5  0.   0. ]\n [ 0.   0.  -0.4  0. ]\n [ 0.   0.   0.   0.5]] \n\nsim_an :\n[[ 0.  -0.8  0.3 -0.5]\n [-0.4  0.   0.1 -0.1]\n [ 0.3  0.1  0.  -0.8]\n [-0.5 -0.2 -0.7  0. ]] \n\n-- Outputs --\nmean_neg :\n[[-0.33333333]\n [-0.13333333]\n [-0.13333333]\n [-0.46666667]] \n\nclosest_neg :\n[[ 0.3]\n [ 0.1]\n [-0.8]\n [-0.2]]",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Siamese Networks",
      "L2 - Modified Triplet Loss"
    ]
  },
  {
    "objectID": "notes/c3w4/lab02.html#the-loss-functions",
    "href": "notes/c3w4/lab02.html#the-loss-functions",
    "title": "Modified Triplet Loss",
    "section": "The Loss Functions",
    "text": "The Loss Functions\nThe last step is to calculate the loss functions.\n\\mathcal{L_\\mathrm{1}} = \\max{(mean\\_neg -\\mathrm{s}(A,P)  +\\alpha, 0)}\n\\mathcal{L_\\mathrm{2}} = \\max{(closest\\_neg -\\mathrm{s}(A,P)  +\\alpha, 0)}\n\\mathcal{L_\\mathrm{Full}} = \\mathcal{L_\\mathrm{1}} + \\mathcal{L_\\mathrm{2}}\n\n# Alpha margin\nalpha = 0.25\n\n# Modified triplet loss\n# Loss 1\nl_1 = np.maximum(mean_neg - sim_ap.reshape(b, 1) + alpha, 0)\n# Loss 2\nl_2 = np.maximum(closest_neg - sim_ap.reshape(b, 1) + alpha, 0)\n# Loss full\nl_full = l_1 + l_2\n# Cost\ncost = np.sum(l_full)\n\nprint(\"-- Outputs --\")\nprint(\"loss full :\")\nprint(l_full, \"\\n\")\nprint(\"cost :\", \"{:.3f}\".format(cost))\n\n-- Outputs --\nloss full :\n[[0.        ]\n [0.        ]\n [0.51666667]\n [0.        ]] \n\ncost : 0.517",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Siamese Networks",
      "L2 - Modified Triplet Loss"
    ]
  },
  {
    "objectID": "notes/c3w4/lab02.html#summary",
    "href": "notes/c3w4/lab02.html#summary",
    "title": "Modified Triplet Loss",
    "section": "Summary",
    "text": "Summary\nThere were a lot of steps in there, so well done. You now know how to calculate a modified triplet loss, incorporating the mean negative and the closest negative. You also learned how to create a matrix of similarity scores based on cosine similarity.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Siamese Networks",
      "L2 - Modified Triplet Loss"
    ]
  },
  {
    "objectID": "notes/c3w4/lab03.html",
    "href": "notes/c3w4/lab03.html",
    "title": "Evaluate a Siamese model: Ungraded Lecture Notebook",
    "section": "",
    "text": "course banner\nimport trax.fastmath.numpy as np\n\n2025-02-10 16:56:19.897139: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1739199379.909590  124134 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1739199379.913717  124134 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Siamese Networks",
      "L3 - Evaluate a Siamese Model"
    ]
  },
  {
    "objectID": "notes/c3w4/lab03.html#inspecting-the-necessary-elements",
    "href": "notes/c3w4/lab03.html#inspecting-the-necessary-elements",
    "title": "Evaluate a Siamese model: Ungraded Lecture Notebook",
    "section": "Inspecting the necessary elements",
    "text": "Inspecting the necessary elements\nIn this lecture notebook you will learn how to evaluate a Siamese model using the accuracy metric. Because there are many steps before evaluating a Siamese network (as you will see in this week’s assignment) the necessary elements and variables are replicated here using real data from the assignment:\n\nq1: vector with dimension (batch_size X max_length) containing first questions to compare in the test set.\nq2: vector with dimension (batch_size X max_length) containing second questions to compare in the test set.\n\nNotice that for each pair of vectors within a batch ([q1_1, q1_2, q1_3, ...], [q2_1, q2_2,q2_3, ...]) q1_i is associated to q2_k.\n\ny_test: 1 if q1_i and q2_k are duplicates, 0 otherwise.\nv1: output vector from the model’s prediction associated with the first questions.\nv2: output vector from the model’s prediction associated with the second questions.\n\nYou can inspect each one of these variables by running the following cells:\n\nq1 = np.load('q1.npy')\nprint(f'q1 has shape: {q1.shape} \\n\\nAnd it looks like this: \\n\\n {q1}\\n\\n')\n\nq1 has shape: (512, 64) \n\nAnd it looks like this: \n\n [[ 32  38   4 ...   1   1   1]\n [ 30 156  78 ...   1   1   1]\n [ 32  38   4 ...   1   1   1]\n ...\n [ 32  33   4 ...   1   1   1]\n [ 30 156 317 ...   1   1   1]\n [ 30 156   6 ...   1   1   1]]\n\n\n\n\nNotice those 1s on the right-hand side?\nHope you remember that the value of 1 was used for padding.\n\nq2 = np.load('q2.npy')\nprint(f'q2 has shape: {q2.shape} \\n\\nAnd looks like this: \\n\\n {q2}\\n\\n')\n\nq2 has shape: (512, 64) \n\nAnd looks like this: \n\n [[   30   156    78 ...     1     1     1]\n [  283   156    78 ...     1     1     1]\n [   32    38     4 ...     1     1     1]\n ...\n [   32    33     4 ...     1     1     1]\n [   30   156    78 ...     1     1     1]\n [   30   156 10596 ...     1     1     1]]\n\n\n\n\n\ny_test = np.load('y_test.npy')\nprint(f'y_test has shape: {y_test.shape} \\n\\nAnd looks like this: \\n\\n {y_test}\\n\\n')\n\ny_test has shape: (512,) \n\nAnd looks like this: \n\n [0 1 1 0 0 0 0 1 0 1 1 0 0 0 1 1 1 0 1 1 0 0 0 0 1 1 0 0 0 0 1 0 1 1 0 0 0\n 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0\n 0 0 0 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0\n 0 1 0 0 1 1 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0\n 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 1 1\n 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 1 1 0 1 0 0 1 1 0 0 0 1 0 1 0 0 0 0 1 0 0 1\n 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1\n 1 0 1 1 0 0 0 1 0 1 0 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0 1 1 1 0 1 0 1 1 1 0 0\n 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0\n 0 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 0 0 1 1 1 0 1 1 0 1 0 1 1 1 0\n 1 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 0 1 1 1 0 0 0 1 0 1 1 1\n 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1\n 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n\n\n\n\n\nv1 = np.load('v1.npy')\nprint(f'v1 has shape: {v1.shape} \\n\\nAnd looks like this: \\n\\n {v1}\\n\\n')\nv2 = np.load('v2.npy')\nprint(f'v2 has shape: {v2.shape} \\n\\nAnd looks like this: \\n\\n {v2}\\n\\n')\n\nv1 has shape: (512, 128) \n\nAnd looks like this: \n\n [[ 0.01273625 -0.1496373  -0.01982759 ...  0.02205012 -0.00169148\n  -0.01598107]\n [-0.05592084  0.05792497 -0.02226785 ...  0.08156938 -0.02570007\n  -0.00503111]\n [ 0.05686752  0.0294889   0.04522024 ...  0.03141788 -0.08459651\n  -0.00968536]\n ...\n [ 0.15115018  0.17791134  0.02200656 ... -0.00851707  0.00571415\n  -0.00431194]\n [ 0.06995274  0.13110274  0.0202337  ... -0.00902792 -0.01221745\n   0.00505962]\n [-0.16043712 -0.11899089 -0.15950686 ...  0.06544471 -0.01208312\n  -0.01183368]]\n\n\nv2 has shape: (512, 128) \n\nAnd looks like this: \n\n [[ 0.07437647  0.02804951 -0.02974014 ...  0.02378932 -0.01696189\n  -0.01897198]\n [ 0.03270066  0.15122835 -0.02175895 ...  0.00517202 -0.14617395\n   0.00204823]\n [ 0.05635608  0.05454165  0.042222   ...  0.03831453 -0.05387777\n  -0.01447786]\n ...\n [ 0.04727105 -0.06748016  0.04194937 ...  0.07600753 -0.03072828\n   0.00400715]\n [ 0.00269269  0.15222628  0.01714724 ...  0.01482705 -0.0197884\n   0.01389528]\n [-0.15475044 -0.15718803 -0.14732707 ...  0.04299919 -0.01070975\n  -0.01318042]]",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Siamese Networks",
      "L3 - Evaluate a Siamese Model"
    ]
  },
  {
    "objectID": "notes/c3w4/lab03.html#calculating-the-accuracy",
    "href": "notes/c3w4/lab03.html#calculating-the-accuracy",
    "title": "Evaluate a Siamese model: Ungraded Lecture Notebook",
    "section": "Calculating the accuracy",
    "text": "Calculating the accuracy\nYou will calculate the accuracy by iterating over the test set and checking if the model predicts right or wrong.\nThe first step is to set the accuracy to zero:\n\naccuracy = 0\n\nYou will also need the batch size and the threshold that determines if two questions are the same or not.\nNote :A higher threshold means that only very similar questions will be considered as the same question.\n\nbatch_size = 512 # Note: The max it can be is y_test.shape[0] i.e all the samples in test data\nthreshold = 0.7 # You can play around with threshold and then see the change in accuracy.\n\nIn the assignment you will iterate over multiple batches of data but since this is a simplified version only one batch is provided.\nNote: Be careful with the indices when slicing the test data in the assignment!\nThe process is pretty straightforward: - Iterate over each one of the elements in the batch - Compute the cosine similarity between the predictions - For computing the cosine similarity, the two output vectors should have been normalized using L2 normalization meaning their magnitude will be 1. This has been taken care off by the Siamese network you will build in the assignment. Hence the cosine similarity here is just dot product between two vectors. You can check by implementing the usual cosine similarity formula and check if this holds or not. - Determine if this value is greater than the threshold (If it is, consider the two questions as the same and return 1 else 0) - Compare against the actual target and if the prediction matches, add 1 to the accuracy (increment the correct prediction counter) - Divide the accuracy by the number of processed elements\n\nfor j in range(batch_size):        # Iterate over each one of the elements in the batch\n    \n    d = np.dot(v1[j],v2[j])        # Compute the cosine similarity between the predictions as l2 normalized, ||v1[j]||==||v2[j]||==1 so only dot product is needed\n    res = d &gt; threshold            # Determine if this value is greater than the threshold (if it is consider the two questions as the same)\n    accuracy += (y_test[j] == res) # Compare against the actual target and if the prediction matches, add 1 to the accuracy\n\naccuracy = accuracy / batch_size   # Divide the accuracy by the number of processed elements\n\n\nprint(f'The accuracy of the model is: {accuracy}')\n\nThe accuracy of the model is: 0.7421875\n\n\nCongratulations on finishing this lecture notebook!\nNow you should have a clearer understanding of how to evaluate your Siamese language models using the accuracy metric.\nKeep it up!",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Siamese Networks",
      "L3 - Evaluate a Siamese Model"
    ]
  },
  {
    "objectID": "notes/c4w2/index.html",
    "href": "notes/c4w2/index.html",
    "title": "Week 2 - Text Summarization",
    "section": "",
    "text": "deeplearning.ai",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w2/index.html#sec-transformers-vs-rnns",
    "href": "notes/c4w2/index.html#sec-transformers-vs-rnns",
    "title": "Week 2 - Text Summarization",
    "section": "Transformers vs RNNs",
    "text": "Transformers vs RNNs\nRNNs were a big breakthrough and became the state of the art (SOTA) for machine translation (MT).\nThis illustrates a typical RNN that is used to translate the English sentence “How are you?” to its German equivalent, “Wie sind Sie?”.\n\n\n\n\nrnn-non-parallel\n\n\n\n\nlstm\n\n\nThe LSTM which goes a long way to solving the vanishing gradient problems requires three times the memory and cpu steps a the vanilla RNN.\nHowever, as time went by and models got longer and deeper the biggest challenge with improving RNNs, became their use of sequential computation.\n\n\n\n\nseq2seq-steps\n\nWhich entailed that to process the word “you”, the RNN it has to first go through “are” and then “you”. Two other issues with RNNs are the:\n\nInformation loss\nIt becomes harder to keep track of whether the subject is singular or plural as we move further away from the subject.\n\n\n\n\nTransformer\n\ntransformer architecture:\nin the encoder side - lookup layer - the source sequence is converted from one hot encoding to a distributed representation using an embedding. - this is converted to K V matrices in the decoder side\n\n\nVanishing Gradient Problem\nWhen gradients we back-propagate, the gradients can become really small and as a result.\nWith small gradient the model will learn very little.\n\n\n\n\npositional-encoding\n\nTransformers which are based on attention and don’t require any sequential computation per layer, only a single step is needed.\n\n\n\n\nsummary\n\nAdditionally, the gradient steps that need to be taken from the last output to the first input in a transformer is just one. For RNNs, the number of steps increases with longer sequences. Finally, transformers don’t suffer from vanishing gradients problems that are related to the length of the sequences.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w2/index.html#sec-transformer-applications",
    "href": "notes/c4w2/index.html#sec-transformer-applications",
    "title": "Week 2 - Text Summarization",
    "section": "Transformer Applications",
    "text": "Transformer Applications\n Transformers have essentially replaced RNN,LSTM and GRUs in sequence processing.\n\n\n\n\napplication-NLP\n\n\nApplications of Transformers\n\nText summarization\nAutocomplete\nNER\nQ&A\nTranslation\nChat Bots\nSentiment Analyses\nMarket Intelligence\nText Classification\nOCR\nSpell Checking\n\n\n\n\n\nsota\n\n\n\nSOTA Transformers\nTransformers Time Line:\n\nGPT-4:\nElmO\nBERT\nT5",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w2/index.html#sec-t5",
    "href": "notes/c4w2/index.html#sec-t5",
    "title": "Week 2 - Text Summarization",
    "section": "T5 - Text-To-Text Transfer Transformer",
    "text": "T5 - Text-To-Text Transfer Transformer\n\n\n\n\nt5\n\n\n\n\nt5\n\n\n(Raffel et al. 2019) introduced T5 which can do a number of tasks with a single model. While the earlier transformer models were able to score high in many different tasks without specific training. T5 is setup to handle different inputs and respond with output that is relevant to the requested task.\n\nT5 Classification tasks\nThese tasks are selected using the initial string: - Translate English into German - Cola sentence - Question\n\n\n\n\ntext-to-text-transformer\n\n\n\nT5 regression tasks\n\nStbs Sentence1 … Stbs Sentence2 …\nSummarize:\n\nplay trivia against T5 here\n\n\n\n\ntransformers quiz\n\n\n\n\n\n\n\nWarning\n\n\n\nI found this one a little confusing\n\n\nWe are told that the transformers can do in one operation what RNN needed to do in many steps. Also when querying transformers it does one task at a time. It seem that this question is about the ability of multiple heads to do several tasks at once could not do this is not well understood.\n\n\n\n\nsummary of transformers",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w2/index.html#sec-dot-product-attention",
    "href": "notes/c4w2/index.html#sec-dot-product-attention",
    "title": "Week 2 - Text Summarization",
    "section": "Dot-Product Attention",
    "text": "Dot-Product Attention\n\n\n\n\noutline-of-dot-product-attention\n\nDot product attention was introduced in 2015 by Minh-Thang Luong, Hieu Pham, Christopher D. Manning in Effective Approaches to Attention-based Neural Machine Translation which is available at papers with code.\nLook at Review of Effective Approaches to Attention-based NMT\nDot product attention is the main operation in transformers. It is the dot product between the embedding of source and target sequences. The embedding used is a cross language embedding in which distance between equivalent across languages are minimized. This facilitates finding the cosine similarity using the dot product between words.\n\n\n\n\nintro-to-attention\n\nLet’s try to understand dot product attention intuitively by walking over its operations at the word level. The actual algorithm uses linear algebra to perform many operations at once which is fast but more abstract and therefore difficult to understand.\n\nUsing a pre-trained cross-language embedding encode:\n\neach German word vector q_i is placed as a column vector to form the query matrix Q,\neach English word once as k_i and once as v_i, column vectors in the key K and value V matrices. This is more of a preprocessing step.\n\nFor each German word we want to derive a continuous filter function on the English sequence to pick the most relevant words for translation. We build this filter for word q_i by taking its dot product q_i \\cdot k_i with every word vector from the english sequence these products are called the the attention weights.\nnext we convert the rudimentary filter to a probabilistic one by applying a softmax() which is just a differentiable function that converts the attention weights to probabilities by keeping them at the same relative sizes while ensuring they add to one.\nnow that we have a q-filter we want to apply it. This is done by taking the weighed sum of the english words using the attention weights.\n\n\n\\hat q_i = \\sum_{i} softmax(q_i \\cdot k_i) \\times v_i =  \\sum w_a(q_i) * v_i\n\n\nQuery, Key & Value\n\n\n\n\nqueries-keys-values\n\nI find it fascinating that the authors of Attention is all decided to motivate their work on a attention using the language of information retrieval. This makes understanding the concepts a little easier and has also lead to more recent work on LSTM to use this same language. In both case though the authors are interested in the ability of the neural network to use the wghiets that it has learned to process a long sequence of test and put together a coherent output based on distant and often sparse parts of the input. In a named entity recognition task the network has a fairly simple task to do - it needs to classify a few token based a few cues from the immediate context. But for the text summarization task the network has to understand the text and pick up the most salient bit while discarding the rest.\n\n\n\n\n\n\nQuery, Key & Value\n\n\n\nAttention uses three matrices which are formed as shown in the figure The Query Q, Key K and Value V are formed from the source and target (if there is no target then just from the source). Each word is converted into an embedding column vector and these are placed into the matracies as their columns.\nIn Video 1 Dr. Łukasz Kaiser talks about attention and here he is talking about solving the problem of retrieving information from a long sequence. At around 16 minutes in he call Q a query vector and K and V a memory, of all the words we have seen, which we want to access.\n\nThe Query is the matrix formed from the column word vector for the German words.\nThe Key is the matrix formed from the column word vector for the English words.\nThe Value is the matrix formed from the column word vector for the English words.\n\nK and V are the same\n\n\nOnce these are called keys since we use them to are we doing a similarity lookup. And the second time they are called value because we use them in the activation when we apply the weights to them. The input and output sequences are mapped to an embedding layer to become the Q, K and V matrices.\n\n\n\n\n\n\n\nVideo 1: Lukasz Kaiser’s Masterclass on Attention is all you need\n\n\nGiven an input, we transform it into a new representation or a column vector. Depending on the task we are working on, we will end up getting queries, keys, and values. Each column corresponds to a word in the figure above. Hence, when we compute the following:\n\n\n\n\nattention-formula\n\n\nmultiply Q by V.\napply the softmax() to transform to a probability.\nmultiply the softmax by V\n\n\n\n\n\nattention-math\n\nThis is restating the above in a very confusing way. I looked at it many times before I figured out that the square brackets are the dimensions and that we have the following two formulas indicated schematically above:\n\nZ = W_A V\n\nwhere:\n\nZ has size of is a ‘Q length’ \\times ‘Embedding size’ matrix\nor for coders [len(Q),D] dimensional array\n\n\nW_A = softmax(QK^T)\n\nThis concept implies that similar vectors are likely to have a higher score when we dot them with one another. We transform that score into a probability by using a softmax function. We can then multiply the output by\nWe can think of the keys and the values as being the same. Note that both K,V are of dimension L_k, D. Each query q_i picks the most similar key k_j.\n\n\n\n\nattention-formula\n\nQueries are the German words and the keys are the English words. Once we have the attention weights, we can just multiply it by V to get a weighted combination of the input.\n\n\n\n\nattention-quiz\n\n\n\n\nsummary-for-dot-product-attention\n\n\nanother interesting point made in the preceding talk is that dot product attention has O(n^2 *d) complexity but typically d &gt;&gt; n since d ~ 1000 while for n ~ 70. So transformers should perform better then an RNN whose complexity is O(n*d^2). And this is before the advantages of using an efficient transformer like reformer.\nIn (Kasai et al. 2021) there is a reversal of the trend from rnn to transformers. Here the latest results show a an idea of training big transformers and then converting them to RNN to improve performance. (One get an RNN by training a transformer.)",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w2/index.html#sec-causal-attention",
    "href": "notes/c4w2/index.html#sec-causal-attention",
    "title": "Week 2 - Text Summarization",
    "section": "Causal Attention",
    "text": "Causal Attention\n\nWe are interested in three main types of attention.\nWe’ll see a brief overview of causal attention.\nWe’ll discover some mathematical foundations behind the causal attention.\n\n\n\n\n\nthree forms of attention\n\nIn terms of use cases there are three types of attention mechanisms:\n\nScaled dot product attention:\n\nAKA Encoder-Decoder attention.\none sentence in the decoder look at to another one in the encoder.\nuse cases:\n\nseq2seq\nmachine translation.\n\n\n\n\nCausal Attention:\n\nAKA self attention.\nattention is all we need.\nIn the same sentence words attend to previous words.\nFuture words have not been generated yet.\nuse cases:\n\ngeneration\ntext generation\nsummarization.\n\n\n\n\nBi-directional self attention:\n\nIn one sentence words look both at previous and future words.\nuse cases:\n\nmachine comprehension.\nquestion answering\n\n\n\n\n\n\ncausal attention\n\nIn causal attention, queries and keys come from the same sentence. That is why it is often referred to as self-attention. In general, causal attention allows words to attend to other words that are related in various ways.\n\n\n\n\ncausal attention mask\n\nAt a high-level We have K Q V matrices. corresponding However, token should not attend to words in the future since these were not generated yet. Therefore the future token’s data is masked by adding a big negative number.\n\n\n\n\ncausal-attention-math-\n\nMathematically, it looks like this:\n\n\n\n\ncausal-attention-quiz\n\n\n\n\nsummary-for-causal-attention",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w2/index.html#sec-multi-head-attention",
    "href": "notes/c4w2/index.html#sec-multi-head-attention",
    "title": "Week 2 - Text Summarization",
    "section": "V5: Multi-head Attention",
    "text": "V5: Multi-head Attention\n Let’s summarize the intuition behind multi-head attention and scaled dot product attention.\n\n\n\n\nmuti-head-attention\n\nQ. What are multiple attention heads?\n\nMultiple attention heads are simply replicas of the attention mechanism. In this they are analogous to the multiple filters used in a convolutional neural networks (CNN).\nDuring training they specialize by learning different relationships between words.\nDuring inference the operate parallel and independently of each other.\n\n\n\n\n\noverview of muti-head attention\n\nThis is perhaps the most important slide - but it fails to show the critical part of the algorithm.\nLet’s suppose we have k attention heads. We see at the lowest level the K, Q and V being passed into passing through k linear layers. How is this accomplished and more important why. What is actually happening here is the opposite of concatenation. Instead of processing a query embedding from a space of d-dimensions we first split the embedding into k vectors of length D/k. We have now k vectors from a k D/k-dimensional subspace. We now perform a dot product attention on each of these subspaces.\n\nEach of these dot product attention is operating on a difference subspace. It sees different subsets of the data and therefore specializes. How do these heads specializes is anybody’s guess - unless we have a special embedding which has been processed using PCA or some other algorithm to ensure that each subspace corresponds to some interpretable subset of features.\n\n\n\n\nmuti-head attention scaled dot-product\n\nFor example if we used a 1024 dimension embedding which concatenates 4 representations.\n\n[0:256] is an embedding trained on a phonological task\n[256:512] is an embedding trained on a morphological task\n[513:768] is an embedding trained on a syntactical task\n[769:1024] is an embedding trained on a semantic task\n\nWe could devise a number of subspace sampling schemes to give the k different attention heads different areas of specializations.\n\nsample from a single sub-space\n4 heads sample from one subspace and 4 heads sample from 3 different sub-spaces\n5 heads sampling from 2 subspaces different sub-spaces and 3 from 1\n5 heads sampling from 2 subspaces different sub-spaces and 3 from three\n\nEach would specialize on a domain or on a interface between two domain or on all data but one domain. Language is rather redundant so they may be able to reconstruct most of the missing data - but at least they would specialize in a linguistically meaningful way.\nGiven a word, we take its embedding then we multiply it by the Q, K, V matrix to get the corresponding queries, keys and values. When we use multi-head attention, a head can learn different relationships between words from another head.\nHere’s one way to look at it:\n\nFirst, imagine that we have an embedding for a word. We multiply that embedding with Q to get q_1, K to get k_1, and V to get v_1\n\n\n\n\n\nmuti-head-attention-concatenation\n\n\n\n\nmuti-head-attention-math\n\n\n\n\nmuti-head-attention-fotmula\n\n\n\n\nmuti-head-attention-quiz\n\n\n\n\nmuti-head-attention-math\n\n\n\n\nmuti-head-attention-math\n\n\n\n\nmuti-head-attention-math\n\n\n\n\nmuti-head-attention-math\n\n\n\n\n\n\n\n\n\nNext, we feed it to the linear layer, once we go through the linear layer for each word, we need to calculate a score. After that, we end up having an embedding for each word. But we still need to get the score for how much of each word we are going to use. For example, this will tell we how similar two words are q_1 and k_1or even q_1 and k_2 by doing a simple q_1 \\dot k_1. We can take the softmax of those scores (the paper mentions that we have to divide by \\sqrt(d) to get a probability and then we multiply that by the value. That gives we the new representation of the word.) If we have many heads, we can concatenate them and then multiply again by a matrix that is of dimension (dim of each head by num heads - dim of each head) to get one final vector corresponding to each word.\n\nHere is step by step guide, first we get the Q, K, V matrices: Note that the computation above was done for one head. If we have several heads, concretely nn, then we will have Z_1, Z_2, \\ldots, Z_n. In which case, we can just concatenate them and multiply by a W_O matrix as follows:\nHence, the more heads we have, the more Zs we will end up concatenating and as a result, that will change the inner dimension of W_O, which will then project the combined embeddings into one final embedding.\n\n\n\n\nsummary-muti-head-attention",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w2/index.html#sec-transformer-decoder",
    "href": "notes/c4w2/index.html#sec-transformer-decoder",
    "title": "Week 2 - Text Summarization",
    "section": "V6: Transformer Decoder",
    "text": "V6: Transformer Decoder\n\n\n\n\noutline\n\nThere is a learning objective here!\nthe transformer decoder has two parts\n\na decoder block (with multi-head attention) - think feature acquisition.\na feed forward block - think non-parametric regression on the features.\n\n\n\n\n\ntransformer-decoder-overview\n\n\n\n\ntransformer-decoder-explaination\n\n\n\n\ntransformer-decoder-ff\n\n\n\n\ntransformer-decoder-explaination\n\n\n\n\ntransformer-decoder-summary",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w2/index.html#sec-cross-entropy-loss",
    "href": "notes/c4w2/index.html#sec-cross-entropy-loss",
    "title": "Week 2 - Text Summarization",
    "section": "Cross entropy loss",
    "text": "Cross entropy loss\nCross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label.\n\n\n\n\ninference\n\nAfter training GPT2 on summarization data we just treat it like a word model and mine it for summaries. We do this by supply it with an input and predicting the output token by token. A more sophisticated method might be to use a beam search.\nAn even more sophisticated method might be to use an information metric to reject sentences and back track or better yet to do negative sampling from the prediction distribution (i.e. erase some prediction’s probabilities and renormalize)\nOne could do even better by providing hints, especially if we also head some kind of extractive model with a high-level of certainty about the most important sentence and their most significant concepts.\n\n\n\n\nquiz\n\nWe want the model to be penalized if it makes the wrong prediction. In this case it it does not predict the next word in the summary.\nThis may not be ideal for a number of reasons:\n\nthe Big world view “we are interested in a summary not the next word” what if the model is generating a semantically equivalent summary, in such a case it should not be penalized at all.\n\nIn a previous assignment we used a siamese network to check if two queries were equivalent. I think that allowing the network would be beneficial. (A loss that examines a generated sequence and compares it to the output.) But I don’t really know how to back-propagate the outcome for all the words. Well not exactly\nAs we are using teacher forcing we can take a position that we ignore all the mistakes the model made and give it a good output sequence and ask it for the next word. This then allows us to back prop the last word’s loss all by itself.\nIf we do this for each word in the output in sequence we should be able to reuse most of the calculations.\nThere are cases we have multiple summaries:\n\nFor a wikipedia article we often have all version from inception to the current day. This can provide multiple summaries and text along with an a golden version (the current summary). Oh and we may have a better summary in other languages but that is a different story.\nFor IMDB movie plots we often have a long synopsis and multiple shorter summaries. Also we may also have the book or screen play.\n\nI mention these two cases since Curriculum Learning may be able to assist us in training\n\n\n\n\nsummary\n\nI think these is much missing from this lesson about summerization. However there are a number of good source in papers as well as some lectures on YouTube.\nI have quickly summarized one and should link to it from here once it is published.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w2/index.html#sec-lab1-attention",
    "href": "notes/c4w2/index.html#sec-lab1-attention",
    "title": "Week 2 - Text Summarization",
    "section": "Lab1 : Attention",
    "text": "Lab1 : Attention\nThis was a numpy based realization of dot product and multi-head attention. Some of the main assignment required porting this to Jax.\nAttention lab",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w2/index.html#lab2-the-transformer-decoder",
    "href": "notes/c4w2/index.html#lab2-the-transformer-decoder",
    "title": "Week 2 - Text Summarization",
    "section": "Lab2 : The Transformer Decoder",
    "text": "Lab2 : The Transformer Decoder\nthis covered the transformer block\nTransformer block lab",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w2/index.html#assignment-transformer-summarizer",
    "href": "notes/c4w2/index.html#assignment-transformer-summarizer",
    "title": "Week 2 - Text Summarization",
    "section": "Assignment: Transformer Summarizer",
    "text": "Assignment: Transformer Summarizer\n\nThis long assignment primarily focused on dot product attention, multi-head attention and on building the transformer blocks. These were manageable as their theory had been explained in the lectures and their code had already been covered in the labs. It glosses over the parts involving data processing, training, evaluation and the actual summarization task. The summarization is accomplished using maximum likelihood estimate. A beam search might have yielded better results.\nThe date as described by:\n\nWe use the CNN/Daily Mail dataset (Hermann et al., 2015; Nallapati et al., 2016), which contains online news articles (781 tokens on average) paired with multi-sentence summaries (3.75 sentences or 56 tokens on average).\n\n\nSee, Liu, and Manning (2017) Get To The Point §4 (Abigail et all 2017) We used the non anatomized version. However the earlier paper used a preprocessed version which replaced the named entities with token like $entity5. This is probably ideal in other situations like event processing where each event looks quite different unless one anatomizes them rendering them much more similar and hopefully helping the model generalize better by learning from the partitions induced by the equivalency relation.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w2/index.html#sec-expanding-the-lab-to-a-project",
    "href": "notes/c4w2/index.html#sec-expanding-the-lab-to-a-project",
    "title": "Week 2 - Text Summarization",
    "section": "Expanding the lab to a project:",
    "text": "Expanding the lab to a project:\nThis is one of the main areas I’d like to focus on for a project. I have in mind a tool for improving wikipedia article leads. Here is how I’d like to take this project to the next level:\n\nmore data\ntrain it on additional material:\n\npapers and abstracts.\nwikipedia articles (with good first paragraphs. )\nbooks and book summaries (could be problematic due to the book’s length)\nmovie scripts and outlines from IMDB\n\na Storyline\nsummary (paragraph)\na synopsis (longer)\n\n\n\n\nMore algorithms\n\nUsing a reformer to handle longer texts like books.\nBetter summarization using:\n\na beam search to build better summaries.\na bayesian search to avoid repetitions.\n\nuse curriculum learning to speed up training with\n\neasier examples first.\nmultiple summaries per text.\nlearning on anonymized NE before graduating to non-anonymized texts\n\nuse better method for evaluation of summary.\n\nPerhaps an f-score combining precision or recall on\nAttention Activation summed as a Coverage score for each token.\n\nuse of non zero loss-weights layer\n\ndrop to zero as training progresses.\ndepend on the actual length of source and output.\nuse tf-idf to make holes in the mask surrounding essential concepts.\n\n\n\n\nEvaluation\nuse sammy lib with\n\nrouge-n metric\nthe pyramid metric\n\n\n\nExtra features\n\npages for paper reviews\npages for research questions\npages to implement exra code/ experiments.\nvisualize the concepts/sentences/paragraphs/sections covered in the summary.\nestablish a hierarchy of what would go into a longer outline.\ndevelop a f-score metric combining precision and recall for the summary of the abstract.\nin academic writing one sentence per paragraphs should capture the main concept and it is generally the first second or the last. Is such a sentence is available identify it. This would be done by comparing each sentence with the rest of the paragraph.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w2/index.html#sec-open-question",
    "href": "notes/c4w2/index.html#sec-open-question",
    "title": "Week 2 - Text Summarization",
    "section": "Open question",
    "text": "Open question\nFor me the assignment raised a number of questions about what really going on here during training.\nI’ll probably do this assignment again and look for some answers to my many questions. Once I have these I’ll add them in the body of these notes.\n\nQuestions\n\nLoading and prepocessing the data:\n\nWhat is going on after we load the dataset - is there data augmentation?\nWhat this sub word vocab?\nHow to make my own sub word vocab?\nHow are out of vocab words being handled?\nCan we query the model about these beside running decode ?\nHow are these created - I saw several sizes of vocab.\n\nTraining\n\nTraining data seems to be a little mangled - there seems to be missing white space after the first token of the summaries, is there some way to fix this?\nIn not sure but why do we use teacher forcing during training?\n\n\nIt should speed training up, but the setup is unclear.\n\nEvaluation\n\nWhy are we not looking at a summarization metic like pyramid, rouge5 or good old precision and recall.\n\nInference\n\nHow can we tell the model thinks its done?\n\n\nwhen it output and  token\n\n\nHow to generate one sentence for each paragraph/section\n\n\nChop up the input and summarise each section.\nCreate an new dataset that bases it summaries on the last and first sentences of each paragraph. If that’s too long summarize again for each section.\nIntroduce a timed mask that hides [0:t*len/T] where T is total number of tokens being generated.\nmake the mask a Bayesian search mechanism that hides concepts in the output.\n\n\nHow to use multiple summaries like in IMDB?\n\n\nscore using the pyramid scheme or rogue.\n\n\nHow to make the model avoid repeating /rephrasing themselves?\n\n\nuse a metric on new information. for example Maximal marginal relevance. MMR = \\argmax [\\lambda Sim_1(s_i,Q)- (1 - \\lambda) \\max Sim_2(s_i,s_j)] where Q is the query and s are output sentences and try to bake this into the regularization.\na coverage vector seems to be a recommend method.\n\nVisualization\n\n\nIs there a easy way to see the activation for each word in the output?\nIs there a easy way to see which concepts are significant (not too common and not too rare)\nIs there a easy way to see which concepts are salient - aligned to near by concepts.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w2/index.html#references",
    "href": "notes/c4w2/index.html#references",
    "title": "Week 2 - Text Summarization",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w2/index.html#papers",
    "href": "notes/c4w2/index.html#papers",
    "title": "Week 2 - Text Summarization",
    "section": "Papers",
    "text": "Papers\n\nTransformers\n\nExploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (Raffel et al, 2019)\n\nReformer: The Efficient Transformer (Kitaev et al, 2020)\nAttention Is All We Need (Vaswani et al, 2017)\nDeep contextualized word representations (Peters et al, 2018)\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Devlin et al, 2018)\nFinetuning Pretrained Transformers into RNNs (Kasai et all 2021)\n\n\n\nSummarization\n\nA trainable document summarizer. (Kupiec et al., 1995) extractive\nConstructing literature abstracts by computer: techniques and prospects. (Paice, 1990) extractive\nAutomatic text summarization: Past, present and future (Saggion and Poibeau, 2013) extractive\nAbstractive sentence summarization with attentive recurrent neural networks. (Chopra et al., 2016) abstractive summarization\nPointing the unknown words. (Nallapati et al., 2016) abstractive summarization\nA neural attention model for abstractive sentence summarization. (Rush et al.,2015;) abstractive summarization\nEfficient summarization with read-again and copy mechanism(Zeng et al., 2016) abstractive summarization\nGet To The Point: Summarization with Pointer-Generator Networks (Abigail et all 2017) Hybrid summarization. Note: Christopher D. Manning\nExtractive Summarization as Text Matching (Zhong et all 2020)",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w2/index.html#articles",
    "href": "notes/c4w2/index.html#articles",
    "title": "Week 2 - Text Summarization",
    "section": "Articles",
    "text": "Articles\n\nThe Illustrated Transformer (Alammar, 2018)\nThe Illustrated GPT-2 (Alammar, 2019)\nHow GPT3 Works - Visualizations and Animations (Alammar, 2020)\nAttention? Attention! (Lilian Weng, 2018)\nThe Transformer Family (Lilian Weng, 2020)\nTeacher forcing for RNNs",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w2/index.html#links",
    "href": "notes/c4w2/index.html#links",
    "title": "Week 2 - Text Summarization",
    "section": "Links",
    "text": "Links\n\nJax\nTrax\nTrax community on Gitter\nCNN daily mail dataset",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w2/lab01.html",
    "href": "notes/c4w2/lab01.html",
    "title": "The Three Ways of Attention and Dot Product Attention: Ungraded Lab Notebook",
    "section": "",
    "text": "In this notebook you’ll explore the three ways of attention (encoder-decoder attention, causal attention, and bi-directional self attention) and how to implement the latter two with dot product attention.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "L1 - Attention"
    ]
  },
  {
    "objectID": "notes/c4w2/lab01.html#background",
    "href": "notes/c4w2/lab01.html#background",
    "title": "The Three Ways of Attention and Dot Product Attention: Ungraded Lab Notebook",
    "section": "Background",
    "text": "Background\nAs you learned last week, attention models constitute powerful tools in the NLP practitioner’s toolkit. Like LSTMs, they learn which words are most important to phrases, sentences, paragraphs, and so on. Moreover, they mitigate the vanishing gradient problem even better than LSTMs. You’ve already seen how to combine attention with LSTMs to build encoder-decoder models for applications such as machine translation.\n\nThis week, you’ll see how to integrate attention into transformers. Because transformers are not sequence models, they are much easier to parallelize and accelerate. Beyond machine translation, applications of transformers include: * Auto-completion * Named Entity Recognition * Chatbots * Question-Answering * And more!\nAlong with embedding, positional encoding, dense layers, and residual connections, attention is a crucial component of transformers. At the heart of any attention scheme used in a transformer is dot product attention, of which the figures below display a simplified picture:\n\n\nWith basic dot product attention, you capture the interactions between every word (embedding) in your query and every word in your key. If the queries and keys belong to the same sentences, this constitutes bi-directional self-attention. In some situations, however, it’s more appropriate to consider only words which have come before the current one. Such cases, particularly when the queries and keys come from the same sentences, fall into the category of causal attention.\n\nFor causal attention, we add a mask to the argument of our softmax function, as illustrated below:\n\n\nNow let’s see how to implement attention with NumPy. When you integrate attention into a transformer network defined with Trax, you’ll have to use trax.fastmath.numpy instead, since Trax’s arrays are based on JAX DeviceArrays. Fortunately, the function interfaces are often identical.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "L1 - Attention"
    ]
  },
  {
    "objectID": "notes/c4w2/lab01.html#imports",
    "href": "notes/c4w2/lab01.html#imports",
    "title": "The Three Ways of Attention and Dot Product Attention: Ungraded Lab Notebook",
    "section": "Imports",
    "text": "Imports\n\nimport sys\n\nimport numpy as np\nimport scipy.special\n\nimport textwrap\nwrapper = textwrap.TextWrapper(width=70)\n\n# to print the entire np array\nnp.set_printoptions(threshold=sys.maxsize)\n\nHere are some helper functions that will help you create tensors and display useful information:\n\ncreate_tensor() creates a numpy array from a list of lists.\ndisplay_tensor() prints out the shape and the actual tensor.\n\n\ndef create_tensor(t):\n    \"\"\"Create tensor from list of lists\"\"\"\n    return np.array(t)\n\n\ndef display_tensor(t, name):\n    \"\"\"Display shape and tensor\"\"\"\n    print(f'{name} shape: {t.shape}\\n')\n    print(f'{t}\\n')\n\nCreate some tensors and display their shapes. Feel free to experiment with your own tensors. Keep in mind, though, that the query, key, and value arrays must all have the same embedding dimensions (number of columns), and the mask array must have the same shape as np.dot(query, key.T).\n\nq = create_tensor([[1, 0, 0], [0, 1, 0]])\ndisplay_tensor(q, 'query')\nk = create_tensor([[1, 2, 3], [4, 5, 6]])\ndisplay_tensor(k, 'key')\nv = create_tensor([[0, 1, 0], [1, 0, 1]])\ndisplay_tensor(v, 'value')\nm = create_tensor([[0, 0], [-1e9, 0]])\ndisplay_tensor(m, 'mask')\n\nquery shape: (2, 3)\n\n[[1 0 0]\n [0 1 0]]\n\nkey shape: (2, 3)\n\n[[1 2 3]\n [4 5 6]]\n\nvalue shape: (2, 3)\n\n[[0 1 0]\n [1 0 1]]\n\nmask shape: (2, 2)\n\n[[ 0.e+00  0.e+00]\n [-1.e+09  0.e+00]]",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "L1 - Attention"
    ]
  },
  {
    "objectID": "notes/c4w2/lab01.html#dot-product-attention",
    "href": "notes/c4w2/lab01.html#dot-product-attention",
    "title": "The Three Ways of Attention and Dot Product Attention: Ungraded Lab Notebook",
    "section": "Dot product attention",
    "text": "Dot product attention\nHere we come to the crux of this lab, in which we compute \\textrm{softmax} \\left(\\frac{Q K^T}{\\sqrt{d}} + M \\right) V, where the (optional, but default) scaling factor \\sqrt{d} is the square root of the embedding dimension.\n\ndef DotProductAttention(query, key, value, mask, scale=True):\n    \"\"\"Dot product self-attention.\n    Args:\n        query (numpy.ndarray): array of query representations with shape (L_q by d)\n        key (numpy.ndarray): array of key representations with shape (L_k by d)\n        value (numpy.ndarray): array of value representations with shape (L_k by d) where L_v = L_k\n        mask (numpy.ndarray): attention-mask, gates attention with shape (L_q by L_k)\n        scale (bool): whether to scale the dot product of the query and transposed key\n\n    Returns:\n        numpy.ndarray: Self-attention array for q, k, v arrays. (L_q by L_k)\n    \"\"\"\n\n    assert query.shape[-1] == key.shape[-1] == value.shape[-1], \"Embedding dimensions of q, k, v aren't all the same\"\n\n    # Save depth/dimension of the query embedding for scaling down the dot product\n    if scale: \n        depth = query.shape[-1]\n    else:\n        depth = 1\n\n    # Calculate scaled query key dot product according to formula above\n    dots = np.matmul(query, np.swapaxes(key, -1, -2)) / np.sqrt(depth) \n    \n    # Apply the mask\n    if mask is not None:\n        dots = np.where(mask, dots, np.full_like(dots, -1e9)) \n    \n    # Softmax formula implementation\n    # Use scipy.special.logsumexp of masked_qkT to avoid underflow by division by large numbers\n    # Note: softmax = e^(dots - logaddexp(dots)) = E^dots / sumexp(dots)\n    logsumexp = scipy.special.logsumexp(dots, axis=-1, keepdims=True)\n\n    # Take exponential of dots minus logsumexp to get softmax\n    # Use np.exp()\n    dots = np.exp(dots - logsumexp)\n\n    # Multiply dots by value to get self-attention\n    # Use np.matmul()\n    attention = np.matmul(dots, value)\n    \n    return attention\n\nNow let’s implement the masked dot product self-attention (at the heart of causal attention) as a special case of dot product attention\n\ndef dot_product_self_attention(q, k, v, scale=True):\n    \"\"\" Masked dot product self attention.\n    Args:\n        q (numpy.ndarray): queries.\n        k (numpy.ndarray): keys.\n        v (numpy.ndarray): values.\n    Returns:\n        numpy.ndarray: masked dot product self attention tensor.\n    \"\"\"\n    \n    # Size of the penultimate dimension of the query\n    mask_size = q.shape[-2]\n\n    # Creates a matrix with ones below the diagonal and 0s above. It should have shape (1, mask_size, mask_size)\n    # Use np.tril() - Lower triangle of an array and np.ones()\n    mask = np.tril(np.ones((1, mask_size, mask_size), dtype=np.bool_), k=0)  \n        \n    return DotProductAttention(q, k, v, mask, scale=scale)\n\n\ndot_product_self_attention(q, k, v)\n\narray([[[0.        , 1.        , 0.        ],\n        [0.84967455, 0.15032545, 0.84967455]]])",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "L1 - Attention"
    ]
  },
  {
    "objectID": "notes/c2w3/index.html",
    "href": "notes/c2w3/index.html",
    "title": "Autocomplete and Language Models",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\n\nFigure 1\nThese are my notes for Week 3 notes for the Natural Language Processing with Probabilistic Models Course in the Natural Language Processing Specialization Offered by DeepLearning.AI on Coursera.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete & Language Models",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w3/index.html#tldr---autocomplete-and-language-models",
    "href": "notes/c2w3/index.html#tldr---autocomplete-and-language-models",
    "title": "Autocomplete and Language Models",
    "section": "TL;DR - Autocomplete and Language Models",
    "text": "TL;DR - Autocomplete and Language Models\n\n\n\nLanguage Models in a nutshell\n\n\nThis week we learn how to model a language using N-grams. Starting from the definition of conditional probability we develop the probabilities of sequences of words. Next we add start and end of sentences tokens to our word sequence model. Next we add tokens to represent out of vocabulary words. Then we tackle sparsity by implementing smoothing and backoff. Finally we consider how to evaluate our language model.\nIn the labs we preprocess a corpus to create an N-gram language model. We then build the language model and evaluate it using perplexity.\nIn the assignment for this week, we build a language model to generate autocomplete a text fragment. We will also evaluate the perplexity of the model.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete & Language Models",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w3/index.html#n-grams-overview",
    "href": "notes/c2w3/index.html#n-grams-overview",
    "title": "Autocomplete and Language Models",
    "section": "N-Grams Overview",
    "text": "N-Grams Overview\nRecall how Firth suggested a distributional view of semantics?\nPreviously we used a vector space model to represent this idea. Now we dive deeper and develop a model for distributional semantics using probabilities of sequences of words. Once we can estimate these probabilities we can predict the next word in a sentence.\nN-gram refer to howe we model the a sequence of N words. For example, a bigram model would model the probability of a word given the previous word. A trigram model would model the probability of a word given the previous bigram. And so on. These probabilities are derived by counting frequencies in a corpus of texts.\nN-grams are fundamental and give we a foundation that will allow we to understand more complicated models in the specialization. They form the theoretical under pinning for the next two courses.\nN-grams models allow us to predict the probabilities of certain words happening in a specific sequence. Using that, we can build an auto-correct or even a search suggestion tool.\nOther applications of N-gram language modeling include:\n\nSpeech recognition\nSpelling correction\nAugmentative communication\n\n\n\n\n\n\n\n\ncaption\n\n\n\n\nFigure 2: Description\n\n\nThis week we are going to learn to:\n\nProcess a text corpus to N-gram language model\nHandle out of vocabulary words\nImplement smoothing for previously unseen N-grams\nLanguage model evaluation",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete & Language Models",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w3/index.html#n-grams-and-probabilities",
    "href": "notes/c2w3/index.html#n-grams-and-probabilities",
    "title": "Autocomplete and Language Models",
    "section": "N-grams and Probabilities",
    "text": "N-grams and Probabilities\nBefore we start computing probabilities of certain sequences, we need to first define what is an N-gram language model:\n\n\n\n\n\n\n\ncaption\n\n\n\n\nFigure 3: Description\n\n\nNow given the those definitions, we can label a sentence as follows:\n\n\n\n\n\n\n\ncaption\n\n\n\n\nFigure 4: Description\n\n\nIn other notation we can write:\n\nw_1^m = w_1 w_2 w_3 ... w_m\n \nw_1^3 = w_1 w_2 w_3\n \nw_{m-2}^m = w_{m-2} w_{m-1} w_m\n\nGiven the following corpus: I am happy because I am learning.\nSize of corpus m = 7.\n\nP(I) = \\frac{1}{7}\n\n\nP(happy) = \\frac{1}{7}\n\nTo generalize, the probability of a unigram is\n\nP(w) = \\frac{C(w)}{m}\n\nWhere C(w) is the count of the word in the corpus and m is the size of the corpus.\n\nBigram Probability:\n\n\n\n\n\n\n\ncaption\n\n\n\n\nFigure 5: Description\n\n\n\n\nTrigram Probability:\nTo compute the probability of a trigram: \nP(w_3 | w_1^2) = \\frac{C(w_1^2 w_3)}{C(w_1^2)}\n\n\nC(w_1^2 w_3) = C(w_1 w_2 w_3) = C(w_1^3)\n\nN-gram Probability:\n\nP(w_n | w_1^{n-1}) = \\frac{C(w_1^{n-1} w_n)}{C(w_1^{n-1})}\n\n\nC(w_1^{n-1} w_n) = C(w_1^n)",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete & Language Models",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w3/index.html#sequence-probabilities",
    "href": "notes/c2w3/index.html#sequence-probabilities",
    "title": "Autocomplete and Language Models",
    "section": "Sequence Probabilities",
    "text": "Sequence Probabilities\nWe just saw how to compute sequence probabilities, their short comings, and finally how to approximate N-gram probabilities. In doing so, we try to approximate the probability of a sentence. For example, what is the probability of the following sentence: The teacher drinks tea. To compute it, we will make use of the following:\n\nP(B|A) = \\frac{P(A,B)}{P(A)} \\Rightarrow P(A,B) = P(A)P(B|A)\n\n\nP(A,B,C,D) = P(A)P(B|A)P(C|A,B)P(D|A,B,C)\n\nTo compute the probability of a sequence, we can compute the following:\n\nP(\\text{The teacher drinks tea}) = P(\\text{The})P(\\text{teacher}|\\text{The})P(\\text{drinks}|\\text{The teacher})P(\\text{tea}|\\text{The teacher drinks})\n\nOne of the main issues with computing the probabilities above is the corpus rarely contains the exact same phrases as the ones we computed your probabilities on. Hence, we can easily end up getting a probability of 0. The Markov assumption indicates that only the last word matters. Hence:\n\n\\text{Bigram} \\qquad P(w_n | w_1^{n-1}) \\approx P(w_n | w_{n-1})\n\n\n\\text{Trigram} \\qquad P(w_n | w_1^{n-1}) \\approx P(w_n | w_{n-2}^{n-1})\n\n\n\\text{N-gram} \\qquad P(w_n | w_1^{n-1}) \\approx P(w_n | w_{n-N+1}^{n-1})\n\nWe can model the entire sentence as follows:\n\nP(w_1^n) = P(w_1)P(w_2|w_1)P(w_3|w_2)P(w_4|w_3) ... P(w_n|w_{n-1}) = \\prod_{i=1}^{n} P(w_i|w_{i-1})\n\n\nP(w_1^n) = P(w_1)P(w_2|w_1)P(w_3|w_2)P(w_4|w_3) ... P(w_n|w_{n-1}) = \\prod_{i=1}^{n} P(w_i|w_{i-1})",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete & Language Models",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w3/index.html#starting-and-ending-sentences",
    "href": "notes/c2w3/index.html#starting-and-ending-sentences",
    "title": "Autocomplete and Language Models",
    "section": "Starting and Ending Sentences",
    "text": "Starting and Ending Sentences\nWe usually start and end a sentence with the following tokens respectively: &lt;s&gt; &lt;/s&gt;.\nWhen computing probabilities using a unigram, we can append an &lt;s&gt; in the beginning of the sentence. To generalize to an N-gram language model, we can add N-1 start tokens &lt;s&gt;.\nFor the end of sentence token &lt;/s&gt;, we only need one even if it is an N-gram. Here is an example:\n\n\n\n\n\n\n\ncaption\n\n\n\n\nFigure 6: Description\n\n\nMake sure we know how to compute the probabilities above!",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete & Language Models",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w3/index.html#lecture-notebook-corpus-preprocessing-for-n-grams",
    "href": "notes/c2w3/index.html#lecture-notebook-corpus-preprocessing-for-n-grams",
    "title": "Autocomplete and Language Models",
    "section": "Lecture notebook: Corpus preprocessing for N-grams",
    "text": "Lecture notebook: Corpus preprocessing for N-grams\nlab 1 Corpus preprocessing for N-grams",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete & Language Models",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w3/index.html#the-n-gram-language-model",
    "href": "notes/c2w3/index.html#the-n-gram-language-model",
    "title": "Autocomplete and Language Models",
    "section": "The N-gram Language Model",
    "text": "The N-gram Language Model\n\n\n\n\n\n\n\nCount matrix\n\n\n\n\nFigure 7: Count Matrix\n\n\n\n\n\n\n\n\nProbability matrix\n\n\n\n\nFigure 8: Probability Matrix\n\n\n\n\n\n\n\n\nLanguage model\n\n\n\n\nFigure 9: Language model\n\n\n\n\n\n\n\n\nLog probability\n\n\n\n\nFigure 10: Log probability\n\n\n\n\n\nWe covered a lot of concepts in the previous video. We have seen:\n\nCount matrix c.f Figure 7\nProbability matrix c.f Figure 8\nLanguage model c.f. Figure 9\nLog probability c.f Figure 10 to avoid underflow\nGenerative language model c.f. Figure 11\n\nIn the count matrix:\n\nRows correspond to the unique corpus N-1 grams.\nColumns correspond to the unique corpus words.\n\nHere is an example of the count matrix of a bigram.\nTo convert it into a probability matrix, we can use the following formula:\n\nP(w_n \\mid w^{n-1}_{n-N+1}) = \\frac{C(w^{n-1}_{n-N+1}, w_n)}{C(w^{n-1}_{n-N+1})}\n\\tag{1}\n\nsum(row) = \\sum_{w \\in V} C(w^{n-1}_{n-N+1}, w) = C(w^{n-1}_{n-N+1})\n\\tag{2}\nNow given the probability matrix, we can generate the language model. We can compute the sentence probability and the next word prediction.\nTo compute the probability of a sequence, we needed to compute:\n\n\\begin{align*}\nP(w_1^n) &= P(w_1)P(w_2|w_1)P(w_3|w_2)P(w_4|w_3) \\ldots P(w_n|w_{n-1})  \\\\\n&= \\prod_{i=1}^{n} P(w_i | w_{i-1})\n\\end{align*}\n\\tag{3}\nTo avoid underflow, we can multiply by the log.\n\n\\begin{align*}\n\\log P(w_1^n) &= \\log P(w_1) + \\log P(w_2|w_1) + \\log P(w_3|w_2) + \\ldots+ \\log P(w_n|w_{n-1}) \\\\\n& = \\sum_{i=1}^{n} \\log P(w_i|w_{i-1})\n\\end{align*}\n\\tag{4}\nFinally, we can create a generative language model.\n\n\n\n\n\n\n\nGenerative language model\n\n\n\n\nFigure 11: Generative language model",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete & Language Models",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w3/index.html#lecture-notebook-building-the-language-model",
    "href": "notes/c2w3/index.html#lecture-notebook-building-the-language-model",
    "title": "Autocomplete and Language Models",
    "section": "Lecture notebook: Building the language model",
    "text": "Lecture notebook: Building the language model\nlab 2 Building the language model",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete & Language Models",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w3/index.html#language-model-evaluation",
    "href": "notes/c2w3/index.html#language-model-evaluation",
    "title": "Autocomplete and Language Models",
    "section": "Language Model Evaluation",
    "text": "Language Model Evaluation\nSplitting the Data We will now discuss the train/val/test splits and perplexity.\n\nTrain/Val/Test splits\n\n\n\nSmaller Corpora:\nLarger Corpora:\n\n\n\n\ntrain\n80%\n98%\n\n\ntest\n10%\n1%\n\n\nval\n10%\n1%\n\n\n\nThere are two main methods for splitting the data:\n\n\n\n\n\n\n\ncaption\n\n\n\n\nFigure 12: Count Matrix",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete & Language Models",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w3/index.html#perplexity",
    "href": "notes/c2w3/index.html#perplexity",
    "title": "Autocomplete and Language Models",
    "section": "Perplexity",
    "text": "Perplexity\nPerplexity is used to tell us whether a set of sentences look like they were written by humans rather than by a simple program choosing words at random. A text that is written by humans is more likely to have lower perplexity, where a text generated by random word choice would have a higher perplexity.\nConcretely, here are the formulas to calculate perplexity.\n\nPP(W) = P(s_1, s_2, \\ldots, s_m)^{-\\frac{1}{m}}\n\n\nPP(W) = \\sqrt{ \\prod_{i=1}^{m} \\prod_{j=1}^{|s_i|} \\frac{1}{P(w_j^{(i)} | w_{j-1}^{(i)})}}\n\n​w_j^{(i)} corresponds to the jth word in the ith sentence. If we were to concatenate all the sentences then w_i is the ith word in the test set. To compute the log perplexity, we go from:\n\nPP(W) = \\sqrt{ \\prod_{i=1}^{m} \\frac{1}{P(w_i | w_{i-1})}}\n\nTo\n\nlog PP(W) = -\\frac{1}{m} \\sum_{i=1}^{m} \\log_2 P(w_i | w_{i-1})",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete & Language Models",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w3/index.html#out-of-vocabulary-words",
    "href": "notes/c2w3/index.html#out-of-vocabulary-words",
    "title": "Autocomplete and Language Models",
    "section": "Out of Vocabulary Words",
    "text": "Out of Vocabulary Words\nMany times, we will be dealing with unknown words in the corpus. So how do we choose your vocabulary? What is a vocabulary?\nA vocabulary is a set of unique words supported by your language model. In some tasks like speech recognition or question answering, we will encounter and generate words only from a fixed set of words. Hence, a closed vocabulary.\nOpen vocabulary means that we may encounter words from outside the vocabulary, like a name of a new city in the training set. Here is one recipe that would allow we to handle unknown words.\n\nCreate vocabulary V\nReplace any word in corpus and not in V by &lt;UNK&gt;\nCount the probabilities with &lt;UNK&gt; as with any other word\n\n\n\n\n\n\n\n\ncaption\n\n\n\n\nFigure 13: Count Matrix\n\n\nThe example above shows how we can use min_frequency and replace all the words that show up fewer times than min_frequency by UNK. We can then treat UNK as a regular word.\n\nCriteria to create the vocabulary\n\nMin word frequency f\nMax |V|, include words by frequency\nUse &lt;UNK&gt; sparingly (Why?)\nPerplexity - only compare LMs with the same V",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete & Language Models",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w3/index.html#out-of-vocabulary-words-1",
    "href": "notes/c2w3/index.html#out-of-vocabulary-words-1",
    "title": "Autocomplete and Language Models",
    "section": "Out of Vocabulary Words",
    "text": "Out of Vocabulary Words",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete & Language Models",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w3/index.html#smoothing",
    "href": "notes/c2w3/index.html#smoothing",
    "title": "Autocomplete and Language Models",
    "section": "Smoothing",
    "text": "Smoothing\n\n\n\n\n\n\n\nProblem\n\n\n\n\nFigure 14: Missing N-grams\n\n\nThe three main concepts covered here are dealing with missing n-grams, smoothing, and Backoff and interpolation.\n\n\n\n\n\n\n\nSmoothing\n\n\n\n\nFigure 15: Add One Smoothing, Add K Smoothing\n\n\n\nP(w_n | w^{n-1}_{n-N+1}) = \\frac{C(w^{n-1}_{n-N+1}, w_n)}{C(w^{n-1}_{n-N+1})} \\qquad \\text{can be 0}\n\\tag{5}\nHence we can add-1 smoothing as follows to fix that problem:\n\nP(w_n | w_{n-1}) = \\frac{C(w_{n-1}, w_n) + 1}{\\sum_{w \\in V} (C(w_{n-1}, w) + 1)} = \\frac{C(w_{n-1}, w_n) + 1}{C(w_{n-1}) + V} \\qquad\n\\tag{6}\nAdd-k smoothing is very similar:\n\nP(w_n | w_{n-1}) = \\frac{C(w_{n-1}, w_n) + k}{\\sum_{w \\in V} (C(w_{n-1}, w) + k)} =\\frac{C(w_{n-1}, w_n) + k}{C(w_{n-1}) + k\\times V} \\qquad\n\\tag{7}\n\n\n\n\n\n\n\nBackoff\n\n\n\n\nFigure 16: Backoff\n\n\nWhen using back-off:\n\nIf N-gram missing =&gt; use (N-1)-gram, …: Using the lower level N-grams (i.e. (N-1)-gram, (N-2)-gram, down to unigram) distorts the probability distribution. Especially for smaller corpora, some probability needs to be discounted from higher level N-grams to use it for lower level N-grams.\nProbability discounting e.g. Katz backoff: makes use of discounting.\n“Stupid” backoff: If the higher order N-gram probability is missing, the lower order N-gram probability is used, just multiplied by a constant. A constant of about 0.4 was experimentally shown to work well.\n\nHere is a visualization of the backoff process.\nWe can also use interpolation when computing probabilities as follows:\n\n\\hat{P}(w_n | w_{n-2} W_{n-1}) = \\lambda_1 \\times P(w_n | w_{n-2} w_{n-1}) + \\lambda_2 \\times P(w_n | w_{n-1}) + \\lambda_3 \\times P(w_n) \\qquad\n\\tag{8}\nWhere\n\n\\sum_i \\lambda_i = 1",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete & Language Models",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w3/index.html#week-summary",
    "href": "notes/c2w3/index.html#week-summary",
    "title": "Autocomplete and Language Models",
    "section": "Week Summary",
    "text": "Week Summary\nThis week we learned the following concepts\n\nN-Grams and probabilities\nApproximate sentence probability from N-Grams\nBuild a language model from a corpus\nFix missing information\nOut of vocabulary words with &lt;UNK&gt;\nMissing N-Gram in corpus with smoothing, backoff and interpolation\nEvaluate language model with perplexity\nCoding assignment!",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete & Language Models",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w3/index.html#lecture-notebook-language-model-generalization",
    "href": "notes/c2w3/index.html#lecture-notebook-language-model-generalization",
    "title": "Autocomplete and Language Models",
    "section": "Lecture notebook: Language model generalization",
    "text": "Lecture notebook: Language model generalization\nAutocomplete",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete & Language Models",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w3/lab01.html",
    "href": "notes/c2w3/lab01.html",
    "title": "N-grams Corpus preprocessing",
    "section": "",
    "text": "course banner\nThe input corpus in this week’s assignment is a continuous text that needs some preprocessing so that we can start calculating the n-gram probabilities.\nSome common preprocessing steps for the language models include: - lowercasing the text - remove special characters - split text to list of sentences - split sentence into list words\nCan we note the similarities and differences among the preprocessing steps shown during the Course 1 of this specialization?\nimport nltk               # NLP toolkit\nimport re                 # Library for Regular expression operations\n\n#nltk.download('punkt')    # Download the Punkt sentence tokenizer \nnltk.download('punkt_tab')\n\n[nltk_data] Downloading package punkt_tab to /home/oren/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n\n\nTrue",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete & Language Models",
      "L1 - N-grams Corpus preprocessing"
    ]
  },
  {
    "objectID": "notes/c2w3/lab01.html#n-grams",
    "href": "notes/c2w3/lab01.html#n-grams",
    "title": "N-grams Corpus preprocessing",
    "section": "N-grams",
    "text": "N-grams\n\nSentence to n-gram\nThe next step is to build n-grams from the tokenized sentences.\nA sliding window of size n-words can generate the n-grams. The window scans the list of words starting at the sentence beginning, moving by a step of one word until it reaches the end of the sentence.\nHere is an example method that prints all trigrams in the given sentence.\n\ndef sentence_to_trigram(tokenized_sentence):\n    \"\"\"\n    Prints all trigrams in the given tokenized sentence.\n    \n    Args:\n        tokenized_sentence: The words list.\n    \n    Returns:\n        No output\n    \"\"\"\n    # note that the last position of i is 3rd to the end\n    for i in range(len(tokenized_sentence) - 3 + 1):\n        # the sliding window starts at position i and contains 3 words\n        trigram = tokenized_sentence[i : i + 3]\n        print(trigram)\n\ntokenized_sentence = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n\nprint(f'List all trigrams of sentence: {tokenized_sentence}\\n')\nsentence_to_trigram(tokenized_sentence)\n\nList all trigrams of sentence: ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n\n['i', 'am', 'happy']\n['am', 'happy', 'because']\n['happy', 'because', 'i']\n['because', 'i', 'am']\n['i', 'am', 'learning']\n['am', 'learning', '.']\n\n\n ### Prefix of an n-gram\nAs we saw in the lecture, the n-gram probability is often calculated based on the (n-1)-gram counts. The prefix is needed in the formula to calculate the probability of an n-gram.\n\\begin{equation*}\nP(w_n|w_1^{n-1})=\\frac{C(w_1^n)}{C(w_1^{n-1})}\n\\end{equation*}\nThe following code shows how to get an (n-1)-gram prefix from n-gram on an example of getting trigram from a 4-gram.\n\n# get trigram prefix from a 4-gram\nfourgram = ['i', 'am', 'happy','because']\ntrigram = fourgram[0:-1] # Get the elements from 0, included, up to the last element, not included.\nprint(trigram)\n\n['i', 'am', 'happy']\n\n\n\n\nStart and end of sentence word &lt;s&gt; and &lt;/s&gt;\nWe could see in the lecture that we must add some special characters at the beginning and the end of each sentence:\n\n&lt;s&gt; at beginning\n&lt;/s&gt; at the end\n\nFor n-grams, we must prepend n-1 of characters at the begining of the sentence.\nLet us have a look at how we can implement this in code.\n\n# when working with trigrams, we need to prepend 2 &lt;s&gt; and append one &lt;/s&gt;\nn = 3\ntokenized_sentence = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\ntokenized_sentence = [\"&lt;s&gt;\"] * (n - 1) + tokenized_sentence + [\"&lt;/s&gt;\"]\nprint(tokenized_sentence)\n\n['&lt;s&gt;', '&lt;s&gt;', 'i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.', '&lt;/s&gt;']\n\n\nThat’s all for the lab for “N-gram” lesson of week 3.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete & Language Models",
      "L1 - N-grams Corpus preprocessing"
    ]
  },
  {
    "objectID": "notes/c4w4/index.html",
    "href": "notes/c4w4/index.html",
    "title": "Chat Bots",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\nThis week’s slides\n\n\n\n\nSupplementary Figure 1\nMy notes for Week 4 of the Natural Language Processing with Attention Labels Course in the Natural Language Processing Specialization Offered by DeepLearning.AI on Coursera\nDeep learning and A.I. researchers push the field forward by looking for new techniques as well as refinements of old ideas to get better performance on tasks. In this lesson we cover reversible layers which allow us to leverage a time memory tradeoff to process book length sequences and handle contexts over a conversation.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w4/index.html#sec-task-long",
    "href": "notes/c4w4/index.html#sec-task-long",
    "title": "Chat Bots",
    "section": "Tasks with Long Sequences",
    "text": "Tasks with Long Sequences\n\n\n\n\n\n\n\nFigure 1: Context Window\n\n\nThis week we are going to learn about tasks that require processing longer sequences:\n\nWriting books\nStorytelling and understanding\nBuilding intelligent agents for conversations like chat-bots.\n\nMore specifically we will understand how re-former model (AKA the reversible transformer) and reversible layers work.\nThis week we will learn about the bottlenecks in these larger transformer models, and solutions we can use to make them trainable for you. We will also learn about the. Here is what we will be building for your programming assignment: A chatbot!\nIn many ways a Chat bot is very similar to a Q&A system which we built last week and that is also similar to query based summarization another task we covered a week before that. The new challenge is to manage what parts of the new and old context we keep around as the dialogue progresses. Chatbot are smart A.I. agents and much of the techniques developed under the umbrella of knowledge-based AI is also relevant in developing these. For instance carrying out actions on behalf of the user.\nChatbots can also get a very simple ui via the web or as an mobile app, which is another area I have some experience. However an even more powerful paradigm here is the ability to interact using voice which has many additional benefit for example supporting people with disabilities and operating in hands-free mode.\nHere is a link to an AI Storytelling system.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w4/index.html#sec-transformer-complexity",
    "href": "notes/c4w4/index.html#sec-transformer-complexity",
    "title": "Chat Bots",
    "section": "Transformer Complexity",
    "text": "Transformer Complexity\n\n\n\n\n\n\n\nFigure 2: week-4\n\n\nOne of the biggest issues with the transformers is that it takes time and a lot of memory when training. Concretely here are the numbers. If we have a sequence of length L , then we need L^2*N memory to handle the sequence. So if we have N layers, that means your model will take N times more time to complete. As L gets larger, the memory and the time quickly increases.\nPerhaps this is the reason people are looking into converting transformers into RNN after training.\n\n\n\n\n\n\n\nFigure 3: week-4\n\n\nWhen we are handling long sequences, we frequently don’t need to consider all L positions. We can just focus on an area of interest instead. For example, when translating a long text from one language to another, we don’t need to consider every word at once. We can instead focus on a single word being translated, and those immediately around it, by using attention.\nTo overcome the memory requirements we can recompute the activations. As long as we do it efficiently, we will be able to save a good amount of time and memory. We will learn this week how to do it. Instead of storing N layers, we will be able to recompute them when doing the back-propagation. That combined with local attention, will give we a much faster model that works at the same level as the transformer we learned about last week.\n\none area where we can make headway is working with a subsequence of interest.\nduring training we need to keep the activations in memory for the back propagation task. Clearly for inference we may be able to save on memory.\nthe alternative is to discard the activations as we go along and recalculate later. This can allows trading memory for compute time. However with larger models compute time is also a bottleneck.\n\n\n\n\n\n\n\n\nFigure 4: Approximate Nearest Neighbours",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w4/index.html#sec-lsh-attention",
    "href": "notes/c4w4/index.html#sec-lsh-attention",
    "title": "Chat Bots",
    "section": "LSH Attention",
    "text": "LSH Attention\nIn Course 1, we covered how locality sensitive hashing (LSH) works. We learned about:\n\nKNN\nHash Tables and Hash Functions\nLocality Sensitive Hashing\nMultiple Planes\n\nHere are the steps to follow to compute LSH given some vectors, where the vectors may correspond to the transformed word embedding that your transformer outputs.\nAttention is used to try which query (q) and key (k) are the most similar. To do so, we hash q and the keys. This will put similar vectors in the same bucket that we can use. The drawing above shows the lines that separate the buckets. Those could be seen as the planes.\nFirst let’s recall how the standard attention mechanism is defined as follows:\n\nA(Q,K,V) = softmax(QK^T)V\n\\tag{1}\nOnce we hash Q and K we will then compute standard attention on the bins that we have created. We will repeat the same process several times to increase the probability of having the same key in the same bin as the query.\n\n\n\n\n\n\n\nFigure 5: week-4\n\n\n\nGiven the sequence of queries and keys, we hash them into buckets. Check out Course 1 Week 4 for a review of the hashing.\nWe will then sort them by bucket.\nWe split the buckets into chunks (this is a technical detail for parallel computing purposes).\nWe then compute the attention within the same bucket of the chunk we are looking at and the previous chunk.\n\n\nQ. Why do we need to look at the previous chunk?\n\nWe can see in the figure some buckets (both blue and yellow) have been split across two chunks. Looking at the previous chunk will let we attend to the full bucket.\nIn Winograd schemas the resolution of the ambiguous pronoun switches between the two variants of the sentence.\n\nthe animal didn’t cross the street because it was too tired / the animal didn’t cross the street because it was too wide / The city councilmen refused the demonstrators a permit because they feared violence. / The city councilmen refused the demonstrators a permit because they advocated violence. /",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w4/index.html#reformer-lsh",
    "href": "notes/c4w4/index.html#reformer-lsh",
    "title": "Chat Bots",
    "section": "Reformer LSH",
    "text": "Reformer LSH\nReformer LSH",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w4/index.html#sec-reversible",
    "href": "notes/c4w4/index.html#sec-reversible",
    "title": "Chat Bots",
    "section": "Motivation for Reversible Layers: Memory!",
    "text": "Motivation for Reversible Layers: Memory!\n\n\n\n\n\n\n\nFigure 6: Memory efficency\n\n\nFor example in this model:\n\n2 GB for the input\n2 GB are required to compute the Attention\n2 GB for the feed forward. There are 12 attention layers 12 feed forward layers. That is equal to 12 * 2 + 12*2 + 2 (for the input) = 50 GB. That is a lot of memory.\n\nIf N is the sequence length:\n\nTransformers need O(N^2) memory.\n\nEach layer of a transformers has an Attention block and feed-forward block. If we want to process, for example to train a document of length 1 million token with 12 layers we will need 50 GB of ram. As we use residual architecture during prediction we only need the current layers input and the output for the next layer. But during training we need to keep all the copies so we can back-propagate the errors.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w4/index.html#sec-reversible-residual",
    "href": "notes/c4w4/index.html#sec-reversible-residual",
    "title": "Chat Bots",
    "section": "Reversible Residual Layers",
    "text": "Reversible Residual Layers",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w4/index.html#sec-reformer",
    "href": "notes/c4w4/index.html#sec-reformer",
    "title": "Chat Bots",
    "section": "Reformer",
    "text": "Reformer\ncan run 1 million token in 16 gb",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w4/index.html#sec-lab-2",
    "href": "notes/c4w4/index.html#sec-lab-2",
    "title": "Chat Bots",
    "section": "Lab 2: Reversible layers",
    "text": "Lab 2: Reversible layers\n\nFrom the trax documents a Residual, involves first a split and then a merge:\nreturn Serial(\n    Branch(shortcut, layer), # split \n    Add(),                   # merge\n)\nwhere:\n\nBranch(shortcut, layers): makes two copies of the single incoming data stream, passes one copy via the shortcut (typically a no-op), and processes the other copy via the given layers (applied in series). [𝑛_{𝑖𝑛}=1, 𝑛_{𝑜𝑢𝑡}=2]\nAdd(): combines the two streams back into one by adding two tensors element-wise. [𝑛_{𝑖𝑛}=2, 𝑛_{𝑜𝑢𝑡}=1]\n\nIn the Branch operation each layer in the input list copies as many inputs from the stack as it needs, and their outputs are successively combined on stack. Put another way, each element of the branch can have differing numbers of inputs and outputs. Let’s try a more complex example. To work these operations modify the stack by replicating the input needed as well as pushing the outputs (as specified using th out parameters).",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w4/index.html#sec-references",
    "href": "notes/c4w4/index.html#sec-references",
    "title": "Chat Bots",
    "section": "References",
    "text": "References\n\nPractical and Optimal LSH for Angular Distance\n\n\nTokenization\n\nSentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing (Kudo & Richardson 2018) sub-word tokenization\nSubword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates (Kudo 2018) sub-word tokenization\nNeural Machine Translation of Rare Words with Subword Units (Sennrich et all 2016) sub-word tokenization\nSubword tokenizers TF tutorial sub-word tokenization\n[https://blog.floydhub.com/tokenization-nlp/]\nSwivel: Improving Embeddings by Noticing What’s Missing (Shazeer, 2016)\n\n\n\nTransformers\n\n[Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer] (Raffel et al, 2019)\n\n[Reformer: The Efficient Transformer] (Kitaev et al, 2020)\nAttention Is All We Need (Vaswani et al, 2017) Vaswani et al. (2023)\n[Deep contextualized word representations] (Peters et al, 2018)\n[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding] (Devlin et al, 2018)\n[Finetuning Pretrained Transformers into RNNs] (Kasai et all 2021)\n[The Illustrated Transformer] (Alammar, 2018)\n[The Illustrated GPT-2] (Alammar, 2019)\n[How GPT3 Works - Visualizations and Animations] (Alammar, 2020)\nIn Weng (2018) the author covers many attention mechanism Attention? Attention!\n[The Transformer Family] (Lilian Weng, 2020)\nTeacher forcing for RNNs\n\n\n\nQuestion Answering Task:\n\nIn Rush (2015) , a paper titled A Neural Attention Model for Abstractive Sentence Summarization the authors discuss the summarization task.\n\nThe first two videos can be viewed on youtube.\n\n\n\n\n\n\n\nVideo 1: Christopher Manning in Stanford CS224N: NLP with Deep Learning | Winter 2019 | Lecture 10 On Question Answering.\n\n\n\n\n\n\n\n\nVideo 2: Christopher Manning and Danqi Chen in Stanford CS224N NLP with Deep Learning | Winter 2021 | Lecture 12 - Question Answering\n\n\n\n\n\n\n\n\nVideo 3",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w4/index.html#links",
    "href": "notes/c4w4/index.html#links",
    "title": "Chat Bots",
    "section": "Links",
    "text": "Links\n\nJax\nTrax\nTrax community on Gitter\nCNN daily mail dataset\nLei Mao Machine Learning, Artificial Intelligence, Computer Science.\nByte Pair Encoding (Lei Mao 2021)\nExploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\nReformer: The Efficient Transformer\nAttention Is All We Need\nDeep contextualized word representations\nThe Illustrated Transformer\nThe Illustrated GPT-2\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\nHow GPT3 Works - Visualizations and Animations\nAttention? Attention! (Lilian Weng, 2018)\nThe Transformer Family “(Lilian Weng, 2020)”\nFinetuning Pretrained Transformers into RNNs “(Kasai et all 2021)”",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w4/lab01.html",
    "href": "notes/c4w4/lab01.html",
    "title": "Reformer Efficient Attention: Ungraded Lab",
    "section": "",
    "text": "The videos describe two ‘reforms’ made to the Transformer to make it more memory and compute efficient. The Reversible Layers reduce memory and Locality Sensitive Hashing(LSH) reduces the cost of the Dot Product attention for large input sizes. This ungraded lab will look more closely at LSH and how it is used in the Reformer model.\nSpecifically, the notebook has 3 goals",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots",
      "L1 - Reformer LSH"
    ]
  },
  {
    "objectID": "notes/c4w4/lab01.html#outline",
    "href": "notes/c4w4/lab01.html#outline",
    "title": "Reformer Efficient Attention: Ungraded Lab",
    "section": "Outline",
    "text": "Outline\n\nPart 1: Trax Efficient Attention classes\nPart 2: Full Dot Product Self Attention\n\n2.1 Description\n\n2.1.1 our_softmax\n\n2.2 our simple attend\n2.3 Class OurSelfAttention\n\nPart 3: Trax LSHSelfAttention\n\n3.1 Description\n3.2 our_hash_vectors\n3.3 Sorting Buckets\n3.4 Chunked dot product attention\n3.5 OurLSHSelfAttention",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots",
      "L1 - Reformer LSH"
    ]
  },
  {
    "objectID": "notes/c4w4/lab01.html#1",
    "href": "notes/c4w4/lab01.html#1",
    "title": "Reformer Efficient Attention: Ungraded Lab",
    "section": "Part 1.0 Trax Efficient Attention classes",
    "text": "Part 1.0 Trax Efficient Attention classes\nTrax is similar to other popular NN development platforms such as Keras (now integrated into Tensorflow) and Pytorch in that it uses ‘layers’ as a useful level of abstraction. Layers are often represented as classes. We’re going to improve our understanding of Trax by locally extending the classes used in the attention layers. We will extend only the ‘forward’ functions and utilize the existing attention layers as parent classes. The original code can be found at github:trax/layers/Research/Efficient_attention. This link references release 1.3.4 but note that this is under the ‘research’ directory as this is an area of active research. When accessing the code on Github for review on this assignment, be sure you select the 1.3.4 release tag, the master copy may have new changes.:\n\n\n\n\n\n\nFigure 1: Reference Tag 1.3.4 on github\n\n\n\nWhile Trax uses classes liberally, we have not built many classes in the course so far. Let’s spend a few moments reviewing the classes we will be using.\n\n\n\n\n\n\nFigure 2: Classes from Trax/layers/Research/Efficient_Attention.py that we will be utilizing\n\n\n\nStarting on the right in the diagram below you see EfficientAttentionBase. The parent to this class is the base.layer which has the routines used by all layers. EfficientAttentionBase leaves many routines to be overridden by child classes - but it has an important feature in the Forward routine. It supports a use_reference_code capability that selects implementations that limit some of the complexities to provide a more easily understood version of the algorithms. In particular, it implements a nested loop that treats each ‘example, head’ independently. This simplifies our work as we need only worry about matrix operations on one ‘example, head’ at a time. This loop calls forward_unbatched, which is the child process that we will be overriding.\nOn the top left are the outlines of the two child classes we will be using. The SelfAttention layer is a ‘traditional’ implementation of the dot product attention. We will be implementing the forward_unbatched version of this to highlight the differences between this and the LSH implementation.\nBelow that is the LSHSelfAttention. This is the routine used in the Reformer architecture. We will override the forward_unbatched section of this and some of the utility functions it uses to explore its implementation in more detail.\nThe code we will be working with is from the Trax source, and as such has implementation details that will make it a bit harder to follow. However, it will allow use of the results along with the rest of the Trax infrastructure. I will try to briefly describe these as they arise. The Trax documentation can also be referenced.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots",
      "L1 - Reformer LSH"
    ]
  },
  {
    "objectID": "notes/c4w4/lab01.html#1.2",
    "href": "notes/c4w4/lab01.html#1.2",
    "title": "Reformer Efficient Attention: Ungraded Lab",
    "section": "Part 1.2 Trax Details",
    "text": "Part 1.2 Trax Details\nThe goal in this notebook is to override a few routines in the Trax classes with our own versions. To maintain their functionality in a full Trax environment, many of the details we might ignore in example version of routines will be maintained in this code. Here are some of the considerations that may impact our code:\n\nTrax operates with multiple back-end libraries, we will see special cases that will utilize unique features.\n‘Fancy’ numpy indexing is not supported in all backend environments and must be emulated in other ways.\nSome operations don’t have gradients for backprop and must be ignored or include forced re-evaluation.\n\nHere are some of the functions we may see:\n\nAbstracted as fastmath, Trax supports multiple backend’s such as Jax and Tensorflow2\ntie_in: Some non-numeric operations must be invoked during backpropagation. Normally, the gradient compute graph would determine invocation but these functions are not included. To force re-evaluation, they are ‘tied’ to other numeric operations using tie_in.\nstop_gradient: Some operations are intentionally excluded from backprop gradient calculations by setting their gradients to zero.\nBelow we will execute from trax.fastmath import numpy as np, this uses accelerated forms of numpy functions. This is, however a subset of numpy\n\n\nimport os\nimport trax\nfrom trax import layers as tl  # core building block\nimport jax\nfrom trax import fastmath  # uses jax, offers numpy on steroids\n\n# fastmath.use_backend('tensorflow-numpy')\nimport functools\nfrom trax.fastmath import numpy as np  # note, using fastmath subset of numpy!\nfrom trax.layers import (\n    tie_in,\n    length_normalized,\n    apply_broadcasted_dropout,\n    look_adjacent,\n    permute_via_gather,\n    permute_via_sort,\n)\n\n\n# def tie_in(x, y):\n#   if fastmath.backend_name() == 'jax':\n#     return jax.lax.tie_in(x, y)\n#   return y\n\n2025-02-10 16:53:34.595009: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1739199214.607869  121487 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1739199214.611988  121487 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\n\n\n---------------------------------------------------------------------------\nImportError                               Traceback (most recent call last)\nCell In[1], line 10\n      8 import functools\n      9 from trax.fastmath import numpy as np  # note, using fastmath subset of numpy!\n---&gt; 10 from trax.layers import (\n     11     tie_in,\n     12     length_normalized,\n     13     apply_broadcasted_dropout,\n     14     look_adjacent,\n     15     permute_via_gather,\n     16     permute_via_sort,\n     17 )\n     20 # def tie_in(x, y):\n     21 #   if fastmath.backend_name() == 'jax':\n     22 #     return jax.lax.tie_in(x, y)\n     23 #   return y\n\nImportError: cannot import name 'tie_in' from 'trax.layers' (/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/__init__.py)",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots",
      "L1 - Reformer LSH"
    ]
  },
  {
    "objectID": "notes/c4w4/lab01.html#2",
    "href": "notes/c4w4/lab01.html#2",
    "title": "Reformer Efficient Attention: Ungraded Lab",
    "section": "Part 2 Full Dot-Product Self Attention",
    "text": "Part 2 Full Dot-Product Self Attention\n\nPart 2.1 Description\n\n\n\n\n\n\nFigure 3: Project datapath and primary data structures and where they are implemented\n\n\n\nThe diagram above shows many of the familiar data structures and operations related to attention and describes the routines in which they are implemented. We will start by working on our_simple_attend or our simpler version of the original attend function. We will review the steps in performing dot-product attention with more focus on the details of the operations and their significance. This is useful when comparing to LSH attention. Note we will be discussing a single example/head unless otherwise specified.\n\n\n\n\n\n\nFigure 4: Dot-product of Query and Key\n\n\n\nThe attend function receives Query and Key. As a reminder, they are produced by a matrix multiply of all the inputs with a single set of weights. We will describe the inputs as embeddings assuming an NLP application, however, this is not required. This matrix multiply very much like a convolutional network where a set of weights (a filter) slide across the input vectors leaving behind a map of the similarity of the input to the filter. In this case, the filters are the weight matrices W^Q and W^K. The resulting maps are Q and K. Q and K have the dimensions of (n_seq, n_q) where n_seq is the number input embeddings and n_q or n_k is the selected size of the Q or K vectors. Note the shading of Q and K, this reflects the fact that each entry is associated with a particular input embedding. You will note later in the code that K is optional. Apparently, similar results can be achieved using Query alone saving the compute and storage associated with K. In that case, the dot-product in attend is matmul(q,q). Note the resulting dot-product (Dot) entries describe a complete (n_seq,n_seq) map of the similarity of all entries of q vs all entries of k. This is reflected in the notation in the dot-product boxes of w_n,w_m representing word_n, word_m. Note that each row of Dot describes the relationship of an input embedding, say w_0, with every other input.\nIn some applications some values are masked. This can be used, for example to exclude results that occur later in time (causal) or to mask padding or other inputs.\n\n\n\n\n\n\nFigure 5: Masking\n\n\n\nThe routine below mask_self_attention implements a flexible masking capability. The masking is controlled by the information in q_info and kv_info.\n\ndef mask_self_attention(\n    dots, q_info, kv_info, causal=True, exclude_self=True, masked=False\n):\n    \"\"\"Performs masking for self-attention.\"\"\"\n    if causal:\n        mask = fastmath.lt(q_info, kv_info).astype(np.float32)\n        dots = dots - 1e9 * mask\n    if exclude_self:\n        mask = np.equal(q_info, kv_info).astype(np.float32)\n        dots = dots - 1e5 * mask\n    if masked:\n        zeros_like_kv_info = tie_in(kv_info, np.zeros_like(kv_info))\n        mask = fastmath.lt(kv_info, zeros_like_kv_info).astype(np.float32)\n        dots = dots - 1e9 * mask\n    return dots\n\nA SoftMax is applied per row of the Dot matrix to scale the values in the row between 0 and 1.\n\n\n\n\n\n\nFigure 6: SoftMax per row of Dot\n\n\n\n\n\nPart 2.1.1 our_softmax\nThis code uses a separable form of the softmax calculation. Recall the softmax: \nsoftmax(x_i)=\\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\\tag{1}\n\nThis can be alternately implemented as: \nlogsumexp(x)=\\log{({\\sum_j \\exp(x_j)})}\\tag{2}\n\n\nsoftmax(x_i)=\\exp({x_i - logsumexp(x)})\\tag{3}\n\nThe work below will maintain a copy of the logsumexp allowing the softmax to be completed in sections. You will see how this is useful later in the LSHSelfAttention class.\nWe’ll create a routine to implement that here with the addition of a passthrough. The matrix operations we will be working on below are easier to follow if we can maintain integer values. So, for tests, we will skip the softmax in some cases.\n\ndef our_softmax(x, passthrough=False):\n    \"\"\" softmax with passthrough\"\"\"\n    logsumexp = fastmath.logsumexp(x, axis=-1, keepdims=True)\n    o = np.exp(x - logsumexp)\n    if passthrough:\n        return (x, np.zeros_like(logsumexp))\n    else:\n        return (o, logsumexp)\n\nLet’s check our implementation.\n\n## compare softmax(a) using both methods\na = np.array([1.0, 2.0, 3.0, 4.0])\nsma = np.exp(a) / sum(np.exp(a))\nprint(sma)\nsma2, a_logsumexp = our_softmax(a)\nprint(sma2)\nprint(a_logsumexp)\n\n[0.0320586  0.08714432 0.23688282 0.6439142 ]\n[0.0320586  0.0871443  0.23688279 0.64391416]\n[4.44019]\n\n\nThe purpose of the dot-product is to ‘focus attention’ on some of the inputs. Dot now has entries appropriately scaled to enhance some values and reduce others. These are now applied to the V entries.\n\n\n\n\n\n\nFigure 7: Applying Attention to V\n\n\n\nV is of size (n_seq,n_v). Note the shading in the diagram. This is to draw attention to the operation of the matrix multiplication. This is detailed below.\n\n\n\n\n\n\nFigure 8: Matrix Multiply\n\n\n\nV is formed by a matrix multiply of the input embedding with the weight matrix W^v whose values were set by backpropagation. The row entries of V are then related to the corresponding input embedding. The matrix multiply weights first column of V, representing a section of each of the input embeddings, with the first row of Dot, representing the similarity of W_0 and each word of the input embedding and deposits the value in Z\n\n\nPart 2.2 our_simple_attend\nIn this section we’ll work on an implementation of attend whose operations you can see in figure 3. It is a slightly simplified version of the routine in efficient_attention.py. We will fill in a few lines of code. The main goal is to become familiar with the routine. You have implemented similar functionality in a previous assignment.\nInstructions Step 1: matrix multiply (np.matmul) q and the k ‘transpose’ kr. Step 2: use our_softmax() to perform a softmax on masked output of the dot product, dots. Step 3: matrix multiply (np.matmul) dots and v.\n\ndef our_simple_attend(\n    q, k=None, v=None,\n    mask_fn=None, q_info=None, kv_info=None,\n    dropout=0.0, rng=None, verbose=False, passthrough=False\n    ):\n  \"\"\"Dot-product attention,  with masking, without optional chunking and/or.\n\n  Args:\n    q: Query vectors, shape [q_len, d_qk]\n    k: Key vectors, shape [kv_len, d_qk]; or None\n    v: Value vectors, shape [kv_len, d_v]\n    mask_fn: a function reference that implements masking (e.g. mask_self_attention)\n    q_info: Query-associated metadata for masking\n    kv_info: Key-associated metadata for masking\n    dropout: Dropout rate\n    rng: RNG for dropout\n\n  Returns:\n    A tuple (output, dots_logsumexp). The output has shape [q_len, d_v], and\n    dots_logsumexp has shape [q_len]. The logsumexp of the attention\n    probabilities is useful for combining multiple rounds of attention (as in\n    LSH attention).\n  \"\"\"\n  assert v is not None\n  share_qk = (k is None)\n  if share_qk:\n    k = q\n    if kv_info is None:\n      kv_info = q_info\n\n  if share_qk:\n    k = length_normalized(k)\n  k = k / np.sqrt(k.shape[-1])\n\n  # Dot-product attention.\n  kr = np.swapaxes(k, -1, -2)  # note the fancy transpose for later..\n\n## Step 1  ##\n  dots = np.matmul(q, kr )\n  if verbose: print(\"Our attend dots\", dots.shape)\n\n  # Masking\n  if mask_fn is not None:\n    dots = mask_fn(dots, q_info[..., :, None], kv_info[..., None, :])\n\n  # Softmax.\n  #dots_logsumexp = fastmath.logsumexp(dots, axis=-1, keepdims=True)  #original\n  #dots = np.exp(dots - dots_logsumexp)  #original\n  \n## Step 2  ##\n  #replace with our_softmax()\n  dots, dots_logsumexp = our_softmax(dots, passthrough=passthrough)\n  if verbose: print(\"Our attend dots post softmax\", dots.shape, dots_logsumexp.shape)\n\n  if dropout &gt; 0.0:\n    assert rng is not None\n    # Dropout is broadcast across the bin dimension\n    dropout_shape = (dots.shape[-2], dots.shape[-1])\n    keep_prob = tie_in(dots, 1.0 - dropout)\n    keep = fastmath.random.bernoulli(rng, keep_prob, dropout_shape)\n    multiplier = keep.astype(dots.dtype) / tie_in(keep, keep_prob)\n    dots = dots * multiplier\n\n## Step 3  ##\n# The softmax normalizer (dots_logsumexp) is used by multi-round LSH attn.\n  out = np.matmul(dots, v)\n  if verbose: print(\"Our attend out1\", out.shape)\n  out = np.reshape(out, (-1, out.shape[-1]))\n  if verbose: print(\"Our attend out2\", out.shape)\n  dots_logsumexp = np.reshape(dots_logsumexp, (-1,))\n  return out, dots_logsumexp\n\n\nseq_len = 8\nemb_len = 5\nd_qk = 3\nd_v = 4\nwith fastmath.use_backend(\"jax\"):  # specify the backend for consistency\n    rng_attend = fastmath.random.get_prng(1)\n    q = k = jax.random.uniform(rng_attend, (seq_len, d_qk), dtype=np.float32)\n    v = jax.random.uniform(rng_attend, (seq_len, d_v), dtype=np.float32)\n    o, logits = our_simple_attend(\n        q,\n        k,\n        v,\n        mask_fn=None,\n        q_info=None,\n        kv_info=None,\n        dropout=0.0,\n        rng=rng_attend,\n        verbose=True,\n    )\nprint(o, \"\\n\", logits)\n\nOur attend dots (8, 8)\nOur attend dots post softmax (8, 8) (8, 1)\nOur attend out1 (8, 4)\nOur attend out2 (8, 4)\n[[0.5455444  0.4232705  0.62970716 0.45504814]\n [0.5558777  0.4169514  0.6260488  0.45763403]\n [0.5502556  0.42250413 0.6107501  0.4532582 ]\n [0.53680766 0.43004778 0.63048995 0.4492887 ]\n [0.5546176  0.41898918 0.62778664 0.44567773]\n [0.54741716 0.4229177  0.6060424  0.46433902]\n [0.53192824 0.43415833 0.63327026 0.44313937]\n [0.538871   0.42285213 0.6527077  0.44843906]] \n [2.5345023 2.6896586 2.8266857 2.4992957 2.861424  2.6235857 2.5204637\n 2.3627536]\n\n\n\n\n Expected Output \n\n\nExpected Output\nOur attend dots (8, 8)\nOur attend dots post softmax (8, 8) (8, 1)\nOur attend out1 (8, 4)\nOur attend out2 (8, 4)\n[[0.5606324  0.7290605  0.5251243  0.47101074]\n [0.5713517  0.71991956 0.5033342  0.46975708]\n [0.5622886  0.7288458  0.52172124 0.46318397]\n [0.5568317  0.72234154 0.542236   0.4699722 ]\n [0.56504494 0.72274375 0.5204978  0.47231334]\n [0.56175965 0.7216782  0.53293145 0.48003793]\n [0.56753993 0.72232544 0.5141734  0.46625748]\n [0.57100445 0.70785505 0.5325362  0.4590797 ]]\n [2.6512175 2.1914332 2.6630518 2.7792363 2.4583826 2.5421977 2.4145055\n 2.5111294]\n\n\n completed code for reference \n\nThis notebook is ungraded, so for reference, the completed code follows:\n\ndef our_simple_attend(\n    q, k=None, v=None,\n    mask_fn=None, q_info=None, kv_info=None,\n    dropout=0.0, rng=None, verbose=False, passthrough=False\n    ):\n  \"\"\"Dot-product attention,  with masking, without optional chunking and/or.\n\n  Args:\n    q: Query vectors, shape [q_len, d_qk]\n    k: Key vectors, shape [kv_len, d_qk]; or None\n    v: Value vectors, shape [kv_len, d_v]\n    mask_fn: a function reference that implements masking (e.g. mask_self_attention)\n    q_info: Query-associated metadata for masking\n    kv_info: Key-associated metadata for masking\n    dropout: Dropout rate\n    rng: RNG for dropout\n\n  Returns:\n    A tuple (output, dots_logsumexp). The output has shape [q_len, d_v], and\n    dots_logsumexp has shape [q_len]. The logsumexp of the attention\n    probabilities is useful for combining multiple rounds of attention (as in\n    LSH attention).\n  \"\"\"\n  assert v is not None\n  share_qk = (k is None)\n  if share_qk:\n    k = q\n    if kv_info is None:\n      kv_info = q_info\n\n  if share_qk:\n    k = length_normalized(k)\n  k = k / np.sqrt(k.shape[-1])\n\n  # Dot-product attention.\n  kr = np.swapaxes(k, -1, -2)  #note the fancy transpose for later..\n\n## Step 1  ##\n  dots = np.matmul(q, kr )\n  if verbose: print(\"Our attend dots\", dots.shape)\n\n  # Masking\n  if mask_fn is not None:\n    dots = mask_fn(dots, q_info[..., :, None], kv_info[..., None, :])\n\n  # Softmax.\n  #dots_logsumexp = fastmath.logsumexp(dots, axis=-1, keepdims=True)  #original\n  #dots = np.exp(dots - dots_logsumexp)  #original\n## Step 2  ##\n  #replace with our_softmax()\n  dots, dots_logsumexp = our_softmax(dots, passthrough=passthrough)\n  if verbose: print(\"Our attend dots post softmax\", dots.shape, dots_logsumexp.shape)\n\n  if dropout &gt; 0.0:\n    assert rng is not None\n    # Dropout is broadcast across the bin dimension\n    dropout_shape = (dots.shape[-2], dots.shape[-1])\n    keep_prob = tie_in(dots, 1.0 - dropout)\n    keep = fastmath.random.bernoulli(rng, keep_prob, dropout_shape)\n    multiplier = keep.astype(dots.dtype) / tie_in(keep, keep_prob)\n    dots = dots * multiplier\n\n## Step 3  ##\n# The softmax normalizer (dots_logsumexp) is used by multi-round LSH attn.\n  out = np.matmul(dots, v)\n  if verbose: print(\"Our attend out1\", out.shape)\n  out = np.reshape(out, (-1, out.shape[-1]))\n  if verbose: print(\"Our attend out2\", out.shape)\n  dots_logsumexp = np.reshape(dots_logsumexp, (-1,))\n  return out, dots_logsumexp",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots",
      "L1 - Reformer LSH"
    ]
  },
  {
    "objectID": "notes/c4w4/lab01.html#2.3",
    "href": "notes/c4w4/lab01.html#2.3",
    "title": "Reformer Efficient Attention: Ungraded Lab",
    "section": "Part 2.3 Class OurSelfAttention",
    "text": "Part 2.3 Class OurSelfAttention\nHere we create our own self attention layer by creating a class OurSelfAttention. The parent class will be the tl.SelfAttention layer in Trax. We will only override the forward_unbatched routine.\nWe’re not asking you to modify anything in this routine. There are some comments to draw your attention to a few lines.\n\nclass OurSelfAttention(tl.SelfAttention):\n    \"\"\"Our self-attention. Just the Forward Function.\"\"\"\n\n    def forward_unbatched(\n        self, x, mask=None, *, weights, state, rng, update_state, verbose=False\n    ):\n        print(\"ourSelfAttention:forward_unbatched\")\n        del update_state\n        attend_rng, output_rng = fastmath.random.split(rng)\n        if self.bias:\n            if self.share_qk:\n                w_q, w_v, w_o, b_q, b_v = weights\n            else:\n                w_q, w_k, w_v, w_o, b_q, b_k, b_v = weights\n        else:\n            if self.share_qk:\n                w_q, w_v, w_o = weights\n            else:\n                w_q, w_k, w_v, w_o = weights\n\n        print(\"x.shape,w_q.shape\", x.shape, w_q.shape)\n        q = np.matmul(x, w_q)\n        k = None\n        if not self.share_qk:\n            k = np.matmul(x, w_k)\n        v = np.matmul(x, w_v)\n\n        if self.bias:\n            q = q + b_q\n            if not self.share_qk:\n                k = k + b_k\n            v = v + b_v\n\n        mask_fn = functools.partial(\n            mask_self_attention,\n            causal=self.causal,\n            exclude_self=self.share_qk,\n            masked=self.masked,\n        )\n        q_info = kv_info = tie_in(x, np.arange(q.shape[-2], dtype=np.int32))\n\n        assert (mask is not None) == self.masked\n        if self.masked:\n            # mask is a boolean array (True means \"is valid token\")\n            ones_like_mask = tie_in(x, np.ones_like(mask, dtype=np.int32))\n            kv_info = kv_info * np.where(mask, ones_like_mask, -ones_like_mask)\n\n        # Notice, we are callout our vesion of attend\n        o, _ = our_simple_attend(\n            q,\n            k,\n            v,\n            mask_fn=mask_fn,\n            q_info=q_info,\n            kv_info=kv_info,\n            dropout=self.attention_dropout,\n            rng=attend_rng,\n            verbose=True,\n        )\n\n        # Notice, wo weight matrix applied to output of attend in forward_unbatched\n        out = np.matmul(o, w_o)\n        out = apply_broadcasted_dropout(out, self.output_dropout, output_rng)\n        return out, state\n\n\ncausal = False\nmasked = False\nmask = None\nattention_dropout = 0.0\nn_heads = 3\nd_qk = 3\nd_v = 4\nseq_len = 8\nemb_len = 5\nbatch_size = 1\n\nosa = OurSelfAttention(\n    n_heads=n_heads,\n    d_qk=d_qk,\n    d_v=d_v,\n    causal=causal,\n    use_reference_code=True,\n    attention_dropout=attention_dropout,\n    mode=\"train\",\n)\n\nrng_osa = fastmath.random.get_prng(1)\nx = jax.random.uniform(\n    jax.random.PRNGKey(0), (batch_size, seq_len, emb_len), dtype=np.float32\n)\n_, _ = osa.init(tl.shapes.signature(x), rng=rng_osa)\n\n\nosa(x)\n\nourSelfAttention:forward_unbatched\n\n\n\n---------------------------------------------------------------------------\nLayerError                                Traceback (most recent call last)\nCell In[9], line 1\n----&gt; 1 osa(x)\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/base.py:197, in Layer.__call__(self, x, weights, state, rng)\n    195   self.state = state  # Needed if the model wasn't fully initialized.\n    196 state = self.state\n--&gt; 197 outputs, new_state = self.pure_fn(x, weights, state, rng)\n    198 self.state = new_state\n    199 return outputs\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/base.py:605, in Layer.pure_fn(self, x, weights, state, rng, use_cache)\n    602 except Exception:\n    603   # Skipping 3 lines as it's always the uninteresting internal call.\n    604   name, trace = self._name, _short_traceback(skip=3)\n--&gt; 605   raise LayerError(name, 'pure_fn',\n    606                    self._caller, signature(x), trace) from None\n\nLayerError: Exception passing through layer OurSelfAttention (in pure_fn):\n  layer created in file [...]/layers/research/efficient_attention.py, line 1014\n  layer input shapes: ShapeDtype{shape:(1, 8, 5), dtype:float32}\n\n  File [...]/layers/research/efficient_attention.py, line 1323, in forward\n    single_out, single_new_state = self.forward_unbatched(\n\n  File [...]/tmp/ipykernel_121487/1155828790.py, line 10, in forward_unbatched\n    if self.bias:\n\nAttributeError: 'OurSelfAttention' object has no attribute 'bias'. Did you mean: '_bias'?\n\n\n\n\n\n Expected Output \n\nExpected Output Notice a few things:\n\nthe w_q (and w_k) matrices are applied to each row or each embedding on the input. This is similar to the filter operation in convolution\nforward_unbatched is called 3 times. This is because we have 3 heads in this example.\n\nourSelfAttention:forward_unbatched\nx.shape,w_q.shape (8, 5) (5, 3)\nOur attend dots (8, 8)\nOur attend dots post softmax (8, 8) (8, 1)\nOur attend out1 (8, 4)\nOur attend out2 (8, 4)\nourSelfAttention:forward_unbatched\nx.shape,w_q.shape (8, 5) (5, 3)\nOur attend dots (8, 8)\nOur attend dots post softmax (8, 8) (8, 1)\nOur attend out1 (8, 4)\nOur attend out2 (8, 4)\nourSelfAttention:forward_unbatched\nx.shape,w_q.shape (8, 5) (5, 3)\nOur attend dots (8, 8)\nOur attend dots post softmax (8, 8) (8, 1)\nOur attend out1 (8, 4)\nOur attend out2 (8, 4)\nDeviceArray([[[ 6.70414209e-01, -1.04319841e-01, -5.33822298e-01,\n                1.92711830e-01, -4.54187393e-05],\n              [ 6.64090097e-01, -1.01875424e-01, -5.35733163e-01,\n                1.88311756e-01, -6.30629063e-03],\n              [ 6.73380017e-01, -1.06952369e-01, -5.31989932e-01,\n                1.90056816e-01,  1.30271912e-03],\n              [ 6.84564888e-01, -1.13240272e-01, -5.50182462e-01,\n                1.95673436e-01,  5.47635555e-03],\n              [ 6.81435883e-01, -1.11068964e-01, -5.32343209e-01,\n                1.91912338e-01,  5.69400191e-03],\n              [ 6.80724978e-01, -1.08496904e-01, -5.34994125e-01,\n                1.96332246e-01,  5.89773059e-03],\n              [ 6.80933356e-01, -1.14087075e-01, -5.18659890e-01,\n                1.90674081e-01,  1.14096403e-02],\n              [ 6.80265009e-01, -1.09031796e-01, -5.38248718e-01,\n                1.94203183e-01,  4.23943996e-03]]], dtype=float32)\n ## Part 3.0 Trax LSHSelfAttention  ## Part 3.1 Description The larger the matrix multiply in the previous section is, the more context can be taken into account when making the next decision. However, the self attention dot product grows as the size of the input squared. For example, if one wished to have an input size of 1024, that would result in 1024^2 or over a million dot products for each head! As a result, there has been significant research related to reducing the compute requirements. One such approach is Locality Sensitive Hashing(LSH) Self Attention.\nYou may recall, earlier in the course you utilized LSH to find similar tweets without resorting to calculating cosine similarity for each pair of embeddings. We will use a similar approach here. It may be best described with an example. \n\nFigure 9: Example of LSH Self Attention\n\nLSH Self attention uses Queries only, no Keys. Attention then generates a metric of the similarity of each value of Q relative to all the other values in Q. An earlier assignment demonstrated that values which hash to the same bucket are likely to be similar. Further, multiple random hashes can improve the chances of finding entries which are similar. This is the approach taken here, though the hash is implemented a bit differently. The values of Q are hashed into buckets using a randomly generated set of hash vectors. Multiple sets of hash vectors are used, generating multiple hash tables. In the figure above, we have 3 hash tables with 4 buckets in each table. Notionally, following the hash, the values of Q have been replicated 3 times and distributed to their appropriate bucket in each of the 3 tables. To find similarity then, one generates dot-products only between members of the buckets. The result of this operation provides information on which entries are similar. As the operation has been distributed over multiple hash tables, these results need to be combined to form a complete picture and this can be used to generate a reduced dot-product attention array. Its clear that because we do not do a compare of every value vs every other value, the size of Dots will be reduced.\nThe challenge in this approach is getting it to operate efficiently. You may recall from the earlier assignments the buckets were lists of entries and had varying length. This will operate poorly on a vector processing machine such as a GPU or TPU. Ideally, operations are done in large blocks with uniform sizes. While it is straightforward to implement the hash algorithm this way, it is challenging to managed buckets and variable sized dot-products. This will be discussed further below. For now, we will examine and implement the hash function.\n ## Part 3.2 our_hash_vectors\nour_hash_vectors, is a reimplementation of Trax hashvector. It takes in an array of vectors, hashes the entries and returns and array assigning each input vector to n_hash buckets. Hashing is described as creating random rotations, see Practical and Optimal LSH for Angular Distance.\n \n\nFigure 10: Processing steps in our_hash_vectors \n\nNote, in the diagram, sizes relate to our expected input Q while our_hash_vectors is written assuming a generic input vector\nInstructions Step 1 create an array of random normal vectors which will be our hash vectors. Each vector will be hashed into a hash table and into rot_size//2 buckets. We use rot_size//2 to reduce computation. Later in the routine we will form the negative rotations with a simple negation and concatenate to get a full rot_size number of rotations. * use fastmath.random.normal and create an array of random vectors of shape (vec.shape[-1],n_hashes, rot_size//2)\nStep 2 In this step we simply do the matrix multiply. jax has an accelerated version of einsum. Here we will utilize more conventional routines.\nStep 2x * 2a: np.reshape random_rotations into a 2 dimensional array ([-1, n_hashes * (rot_size // 2)]) * 2b: np.dot vecs and random_rotations forming our rotated_vecs * 2c: back to 3 dimension with np.reshape [-1, n_hashes, rot_size//2] * 2d: prepare for concatenating by swapping dimensions np.transpose (1, 0, 2) Step 3 Here we concatenate our rotation vectors getting a fullrot_size number of buckets (note, n_buckets = rotsize) * use np.concatenate, [rotated_vecs, -rotated_vecs], axis=-1 Step 4 This is the exciting step! You have no doubt been wondering how we will turn these vectors into bucket indexes. By performing np.argmax over the rotations for a given entry, you get the index to the best match! We will use this as a bucket index. * np.argmax(…).astype(np.int32); be sure to use the correct axis! Step 5 In this style of hashing, items which land in bucket 0 of hash table 0 are not necessarily similar to those landing in bucket 0 of hash table 1, so we keep them separate. We do this by offsetting the bucket numbers by ‘n_buckets’. * add buckets and offsets and reshape into a one dimensional array This will return a 1D array of size n_hashes * vec.shape[0].\n\ndef our_hash_vectors(vecs, rng, n_buckets, n_hashes, mask=None, verbose=False):\n    \"\"\"\n  Args:\n    vecs: tensor of at least 2 dimension,\n    rng: random number generator\n    n_buckets: number of buckets in each hash table\n    n_hashes: the number of hash tables\n    mask: None indicating no mask or a 1D boolean array of length vecs.shape[0], containing the location of padding value\n    verbose: controls prints for debug\n  Returns:\n    A vector of size n_hashes * vecs.shape[0] containing the buckets associated with each input vector per hash table.\n\n    \"\"\"\n\n    # check for even, integer bucket sizes\n    assert isinstance(n_buckets, int) and n_buckets % 2 == 0\n\n    rng = fastmath.stop_gradient(tie_in(vecs, rng))\n    rot_size = n_buckets\n    ### Start Code Here\n\n    ### Step 1 ###\n    rotations_shape = (vecs.shape[-1], n_hashes, rot_size // 2)\n    random_rotations = fastmath.random.normal(rng, rotations_shape).astype(\n        np.float32)\n    if verbose: print(\"random.rotations.shape\", random_rotations.shape)\n\n    ### Step 2 ###\n    if fastmath.backend_name() == 'jax':\n      rotated_vecs = np.einsum('tf,fhb-&gt;htb', vecs, random_rotations)\n      if verbose: print(\"using jax\")\n    else:\n      #Step 2a\n      random_rotations = np.reshape(random_rotations,\n                                    [-1, n_hashes * (rot_size // 2)])\n      if verbose: print(\"random_rotations reshaped\", random_rotations.shape)\n      #Step 2b\n      rotated_vecs = np.dot(vecs, random_rotations)\n      if verbose: print(\"rotated_vecs1\", rotated_vecs.shape)\n      #Step 2c\n      rotated_vecs = np.reshape(rotated_vecs, [-1, n_hashes, rot_size//2])\n      if verbose: print(\"rotated_vecs2\", rotated_vecs.shape)\n      #Step 2d\n      rotated_vecs = np.transpose(rotated_vecs, (1, 0, 2))\n      if verbose: print(\"rotated_vecs3\", rotated_vecs.shape)\n\n    ### Step 3 ###\n    rotated_vecs = np.concatenate([rotated_vecs, -rotated_vecs], axis=-1)\n    if verbose: print(\"rotated_vecs.shape\", rotated_vecs.shape)\n    ### Step 4 ###\n    buckets = np.argmax(rotated_vecs, axis=-1).astype(np.int32)\n    if verbose: print(\"buckets.shape\", buckets.shape)\n    if verbose: print(\"buckets\", buckets)\n\n    if mask is not None:\n      n_buckets += 1  # Create an extra bucket for padding tokens only\n      buckets = np.where(mask[None, :], buckets, n_buckets - 1)\n\n    # buckets is now (n_hashes, seqlen). Next we add offsets so that\n    # bucket numbers from different hashing rounds don't overlap.\n    offsets = tie_in(buckets, np.arange(n_hashes, dtype=np.int32))\n    offsets = np.reshape(offsets * n_buckets, (-1, 1))\n    ### Step 5 ###\n    buckets = np.reshape(buckets + offsets, (-1,))\n    if verbose: print(\"buckets with offsets\", buckets.shape, \"\\n\", buckets)\n    return buckets\n\n\n# example code. Note for reference, the sizes in this example match the values in the diagram above.\nohv_q = np.ones((8, 5))  # (seq_len=8, n_q=5)\nohv_n_buckets = 4  # even number\nohv_n_hashes = 3\nwith fastmath.use_backend(\"tf\"):\n    ohv_rng = fastmath.random.get_prng(1)\n    ohv = our_hash_vectors(\n        ohv_q, ohv_rng, ohv_n_buckets, ohv_n_hashes, mask=None, verbose=True\n    )\n    print(\"ohv shape\", ohv.shape, \"\\nohv\", ohv)  # (ohv_n_hashes * ohv_n_buckets)\n# note the random number generators do not produce the same results with different backends\nwith fastmath.use_backend(\"jax\"):\n    ohv_rng = fastmath.random.get_prng(1)\n    ohv = our_hash_vectors(ohv_q, ohv_rng, ohv_n_buckets, ohv_n_hashes, mask=None)\n    print(\"ohv shape\", ohv.shape, \"\\nohv\", ohv)  # (ohv_n_hashes * ohv_n_buckets)\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[11], line 5\n      3 ohv_n_buckets = 4  # even number\n      4 ohv_n_hashes = 3\n----&gt; 5 with fastmath.use_backend(\"tf\"):\n      6     ohv_rng = fastmath.random.get_prng(1)\n      7     ohv = our_hash_vectors(\n      8         ohv_q, ohv_rng, ohv_n_buckets, ohv_n_hashes, mask=None, verbose=True\n      9     )\n\nFile /usr/lib/python3.10/contextlib.py:135, in _GeneratorContextManager.__enter__(self)\n    133 del self.args, self.kwds, self.func\n    134 try:\n--&gt; 135     return next(self.gen)\n    136 except StopIteration:\n    137     raise RuntimeError(\"generator didn't yield\") from None\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/fastmath/ops.py:428, in use_backend(name)\n    425 if isinstance(name, Backend):\n    426   name = name.value\n--&gt; 428 _assert_valid_backend_name(name)\n    429 global override_backend\n    430 prev_name_or_backend = override_backend\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/fastmath/ops.py:387, in _assert_valid_backend_name(name)\n    385   if backend_.value == name:\n    386     return\n--&gt; 387 raise ValueError(f'No backend with name {name}')\n\nValueError: No backend with name tf\n\n\n\n\n\n Expected Output \n\nExpected Values\nrandom.rotations.shape (5, 3, 2)\nrandom_rotations reshaped (5, 6)\nrotated_vecs1 (8, 6)\nrotated_vecs2 (8, 3, 2)\nrotated_vecs3 (3, 8, 2)\nrotated_vecs.shape (3, 8, 4)\nbuckets.shape (3, 8)\nbuckets ndarray&lt;tf.Tensor(\n[[3 3 3 3 3 3 3 3]\n [3 3 3 3 3 3 3 3]\n [3 3 3 3 3 3 3 3]], shape=(3, 8), dtype=int32)&gt;\nbuckets with offsets (24,)\n ndarray&lt;tf.Tensor([ 3  3  3  3  3  3  3  3  7  7  7  7  7  7  7  7 11 11 11 11 11 11 11 11], shape=(24,), dtype=int32)&gt;\nohv shape (24,)\nohv ndarray&lt;tf.Tensor([ 3  3  3  3  3  3  3  3  7  7  7  7  7  7  7  7 11 11 11 11 11 11 11 11], shape=(24,), dtype=int32)&gt;\nusing jax\nohv shape (24,)\nohv [ 3  3  3  3  3  3  3  3  5  5  5  5  5  5  5  5 11 11 11 11 11 11 11 11]```\n\n&lt;details&gt;\n&lt;summary&gt;\n    &lt;font size=\"3\" &gt;&lt;b&gt;Completed code for reference &lt;/b&gt;&lt;/font&gt;\n&lt;/summary&gt;",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots",
      "L1 - Reformer LSH"
    ]
  },
  {
    "objectID": "notes/c4w4/lab01.html#3.5",
    "href": "notes/c4w4/lab01.html#3.5",
    "title": "Reformer Efficient Attention: Ungraded Lab",
    "section": "Part 3.5 OurLSHSelfAttention",
    "text": "Part 3.5 OurLSHSelfAttention\nYou can examine the full implementations below. Area’s we did not ‘attend to’ in our implementations above include variable bucket sizes and masking. We will instantiate a layer of the full implementation below. We tried to use the same variable names above to make it easier to decipher the full version. Note that some of the functionality we implemented in our routines is split between attend and forward_unbatched. We’ve inserted our version of hash below, but use the original version of attend.\n\n# original version from trax 1.3.4\ndef attend(\n    q,\n    k=None,\n    v=None,\n    q_chunk_len=None,\n    kv_chunk_len=None,\n    n_chunks_before=0,\n    n_chunks_after=0,\n    mask_fn=None,\n    q_info=None,\n    kv_info=None,\n    dropout=0.0,\n    rng=None,\n):\n    \"\"\"Dot-product attention, with optional chunking and/or masking.\n\n  Args:\n    q: Query vectors, shape [q_len, d_qk]\n    k: Key vectors, shape [kv_len, d_qk]; or None\n    v: Value vectors, shape [kv_len, d_v]\n    q_chunk_len: Set to non-zero to enable chunking for query vectors\n    kv_chunk_len: Set to non-zero to enable chunking for key/value vectors\n    n_chunks_before: Number of adjacent previous chunks to attend to\n    n_chunks_after: Number of adjacent subsequent chunks to attend to\n    mask_fn: TODO(kitaev) doc\n    q_info: Query-associated metadata for masking\n    kv_info: Key-associated metadata for masking\n    dropout: Dropout rate\n    rng: RNG for dropout\n\n  Returns:\n    A tuple (output, dots_logsumexp). The output has shape [q_len, d_v], and\n    dots_logsumexp has shape [q_len]. The logsumexp of the attention\n    probabilities is useful for combining multiple rounds of attention (as in\n    LSH attention).\n  \"\"\"\n    assert v is not None\n    share_qk = k is None\n\n    if q_info is None:\n        q_info = np.arange(q.shape[-2], dtype=np.int32)\n\n    if kv_info is None and not share_qk:\n        kv_info = np.arange(v.shape[-2], dtype=np.int32)\n\n    # Split q/k/v into chunks along the time axis, if desired.\n    if q_chunk_len is not None:\n        q = np.reshape(q, (-1, q_chunk_len, q.shape[-1]))\n        q_info = np.reshape(q_info, (-1, q_chunk_len))\n\n    if share_qk:\n        assert kv_chunk_len is None or kv_chunk_len == q_chunk_len\n        k = q\n        kv_chunk_len = q_chunk_len\n        if kv_info is None:\n            kv_info = q_info\n        elif kv_chunk_len is not None:\n            # kv_info is not None, but reshape as required.\n            kv_info = np.reshape(kv_info, (-1, kv_chunk_len))\n    elif kv_chunk_len is not None:\n        k = np.reshape(k, (-1, kv_chunk_len, k.shape[-1]))\n        kv_info = np.reshape(kv_info, (-1, kv_chunk_len))\n\n    if kv_chunk_len is not None:\n        v = np.reshape(v, (-1, kv_chunk_len, v.shape[-1]))\n\n    if share_qk:\n        k = length_normalized(k)\n    k = k / np.sqrt(k.shape[-1])\n\n    # Optionally include adjacent chunks.\n    if q_chunk_len is not None or kv_chunk_len is not None:\n        assert q_chunk_len is not None and kv_chunk_len is not None\n    else:\n        assert n_chunks_before == 0 and n_chunks_after == 0\n\n    k = look_adjacent(k, n_chunks_before, n_chunks_after)\n    v = look_adjacent(v, n_chunks_before, n_chunks_after)\n    kv_info = look_adjacent(kv_info, n_chunks_before, n_chunks_after)\n\n    # Dot-product attention.\n    dots = np.matmul(q, np.swapaxes(k, -1, -2))\n\n    # Masking\n    if mask_fn is not None:\n        dots = mask_fn(dots, q_info[..., :, None], kv_info[..., None, :])\n\n    # Softmax.\n    dots_logsumexp = fastmath.logsumexp(dots, axis=-1, keepdims=True)\n    dots = np.exp(dots - dots_logsumexp)\n\n    if dropout &gt; 0.0:\n        assert rng is not None\n        # Dropout is broadcast across the bin dimension\n        dropout_shape = (dots.shape[-2], dots.shape[-1])\n        #\n        keep_prob = tie_in(dots, 1.0 - dropout)\n        keep = fastmath.random.bernoulli(rng, keep_prob, dropout_shape)\n        multiplier = keep.astype(dots.dtype) / tie_in(keep, keep_prob)\n        dots = dots * multiplier\n\n    # The softmax normalizer (dots_logsumexp) is used by multi-round LSH attn.\n    out = np.matmul(dots, v)\n    out = np.reshape(out, (-1, out.shape[-1]))\n    dots_logsumexp = np.reshape(dots_logsumexp, (-1,))\n    return out, dots_logsumexp\n\n\nclass OurLSHSelfAttention(tl.LSHSelfAttention):\n    \"\"\"Our simplified LSH self-attention \"\"\"\n\n    def forward_unbatched(self, x, mask=None, *, weights, state, rng, update_state):\n        attend_rng, output_rng = fastmath.random.split(rng)\n        w_q, w_v, w_o = weights\n\n        q = np.matmul(x, w_q)\n        v = np.matmul(x, w_v)\n\n        if update_state:\n            _, old_hash_rng = state\n            hash_rng, hash_subrng = fastmath.random.split(old_hash_rng)\n            #      buckets = self.hash_vectors(q, hash_subrng, mask)  #  original\n            ## use our version of hash\n            buckets = our_hash_vectors(\n                q, hash_subrng, self.n_buckets, self.n_hashes, mask=mask\n            )\n            s_buckets = buckets\n            if self._max_length_for_buckets:\n                length = self.n_hashes * self._max_length_for_buckets\n                if buckets.shape[0] &lt; length:\n                    s_buckets = np.concatenate(\n                        [buckets, np.zeros(length - buckets.shape[0], dtype=np.int32)],\n                        axis=0,\n                    )\n            state = (s_buckets, hash_rng)\n        else:\n            buckets, _ = state\n            if self._max_length_for_buckets:\n                buckets = buckets[: self.n_hashes * x.shape[0]]\n\n        seqlen = x.shape[0]\n        assert int(buckets.shape[0]) == self.n_hashes * seqlen\n\n        ticker = tie_in(x, np.arange(self.n_hashes * seqlen, dtype=np.int32))\n        buckets_and_t = seqlen * buckets + (ticker % seqlen)\n        buckets_and_t = fastmath.stop_gradient(buckets_and_t)\n\n        # Hash-based sort (\"s\" at the start of variable names means \"sorted\")\n        sbuckets_and_t, sticker = fastmath.sort_key_val(\n            buckets_and_t, ticker, dimension=-1\n        )\n        _, undo_sort = fastmath.sort_key_val(sticker, ticker, dimension=-1)\n        sbuckets_and_t = fastmath.stop_gradient(sbuckets_and_t)\n        sticker = fastmath.stop_gradient(sticker)\n        undo_sort = fastmath.stop_gradient(undo_sort)\n\n        st = sticker % seqlen\n        sq = np.take(q, st, axis=0)\n        sv = np.take(v, st, axis=0)\n\n        mask_fn = functools.partial(\n            mask_self_attention,\n            causal=self.causal,\n            exclude_self=True,\n            masked=self.masked,\n        )\n        q_info = st\n\n        assert (mask is not None) == self.masked\n        kv_info = None\n        if self.masked:\n            # mask is a boolean array (True means \"is valid token\")\n            smask = np.take(mask, st, axis=0)\n            ones_like_mask = tie_in(x, np.ones_like(smask, dtype=np.int32))\n            kv_info = q_info * np.where(smask, ones_like_mask, -ones_like_mask)\n\n        ## use original version of attend (could use ours but lacks masks and masking)\n        so, slogits = attend(\n            sq,\n            k=None,\n            v=sv,\n            q_chunk_len=self.chunk_len,\n            n_chunks_before=self.n_chunks_before,\n            n_chunks_after=self.n_chunks_after,\n            mask_fn=mask_fn,\n            q_info=q_info,\n            kv_info=kv_info,\n            dropout=self.attention_dropout,\n            rng=attend_rng,\n        )\n\n        # np.take(so, undo_sort, axis=0); np.take(slogits, undo_sort, axis=0) would\n        # also work, but these helpers include performance optimizations for TPU.\n        o = permute_via_gather(so, undo_sort, sticker, axis=0)\n        logits = permute_via_sort(slogits, sticker, buckets_and_t, axis=-1)\n\n        if self.n_hashes &gt; 1:\n            o = np.reshape(o, (self.n_hashes, seqlen, o.shape[-1]))\n            logits = np.reshape(logits, (self.n_hashes, seqlen, 1))\n            probs = np.exp(logits - fastmath.logsumexp(logits, axis=0, keepdims=True))\n            o = np.sum(o * probs, axis=0)\n\n        assert o.shape == (seqlen, w_v.shape[-1])\n        out = np.matmul(o, w_o)\n        out = apply_broadcasted_dropout(out, self.output_dropout, output_rng)\n        return out, state\n\n\n# Here we're going to try out our LSHSelfAttention\nn_heads = 3\ncausal = False\nmasked = False\nmask = None\nchunk_len = 8\nn_chunks_before = 0\nn_chunks_after = 0\nattention_dropout = 0.0\nn_hashes = 5\nn_buckets = 4\nseq_len = 8\nemb_len = 5\nal = OurLSHSelfAttention(\n    n_heads=n_heads,\n    d_qk=3,\n    d_v=4,\n    causal=causal,\n    chunk_len=8,\n    n_chunks_before=n_chunks_before,\n    n_chunks_after=n_chunks_after,\n    n_hashes=n_hashes,\n    n_buckets=n_buckets,\n    use_reference_code=True,\n    attention_dropout=attention_dropout,\n    mode=\"train\",\n)\n\nx = jax.random.uniform(jax.random.PRNGKey(0), (1, seq_len, emb_len), dtype=np.float32)\nal_osa = fastmath.random.get_prng(1)\n_, _ = al.init(tl.shapes.signature(x), rng=al_osa)\n\n\nal(x)\n\n\n---------------------------------------------------------------------------\nLayerError                                Traceback (most recent call last)\nCell In[20], line 1\n----&gt; 1 al(x)\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/base.py:197, in Layer.__call__(self, x, weights, state, rng)\n    195   self.state = state  # Needed if the model wasn't fully initialized.\n    196 state = self.state\n--&gt; 197 outputs, new_state = self.pure_fn(x, weights, state, rng)\n    198 self.state = new_state\n    199 return outputs\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/base.py:605, in Layer.pure_fn(self, x, weights, state, rng, use_cache)\n    602 except Exception:\n    603   # Skipping 3 lines as it's always the uninteresting internal call.\n    604   name, trace = self._name, _short_traceback(skip=3)\n--&gt; 605   raise LayerError(name, 'pure_fn',\n    606                    self._caller, signature(x), trace) from None\n\nLayerError: Exception passing through layer OurLSHSelfAttention (in pure_fn):\n  layer created in file [...]/layers/research/efficient_attention.py, line 1751\n  layer input shapes: ShapeDtype{shape:(1, 8, 5), dtype:float32}\n\n  File [...]/layers/research/efficient_attention.py, line 2158, in forward\n    single_out, single_new_state = self.forward_unbatched(\n\n  File [...]/tmp/ipykernel_121487/2615489615.py, line 17, in forward_unbatched\n    q, hash_subrng, self.n_buckets, self.n_hashes, mask=mask\n\nAttributeError: 'OurLSHSelfAttention' object has no attribute 'n_buckets'. Did you mean: '_n_buckets'?\n\n\n\n\n\n Expected Output \n\nExpected Values\nusing jax\nusing jax\nusing jax\nDeviceArray([[[ 6.6842824e-01, -1.1364323e-01, -5.4430610e-01,\n                2.1126242e-01, -1.0988623e-02],\n              [ 7.0949769e-01, -1.5455185e-01, -5.9923315e-01,\n                2.2719440e-01,  1.3833776e-02],\n              [ 7.1442688e-01, -1.2046628e-01, -5.3956544e-01,\n                1.7320301e-01, -1.6552269e-02],\n              [ 6.7178929e-01, -7.6611102e-02, -5.9399861e-01,\n                2.1236290e-01,  7.9482794e-04],\n              [ 7.1518433e-01, -1.1359170e-01, -5.7821894e-01,\n                2.1304411e-01,  3.0598268e-02],\n              [ 6.8235350e-01, -9.3979925e-02, -5.5341840e-01,\n                2.1608177e-01, -6.6673756e-04],\n              [ 6.1286640e-01, -8.1027031e-02, -4.8148823e-01,\n                1.9373313e-01,  3.1555295e-02],\n              [ 7.2203505e-01, -1.0199660e-01, -5.5215168e-01,\n                1.7872262e-01, -2.2289157e-02]]], dtype=float32)\n\nCongratuations! you have created a custom layer and have become familiar with LSHSelfAttention.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots",
      "L1 - Reformer LSH"
    ]
  },
  {
    "objectID": "notes/c3w1/lab02.html",
    "href": "notes/c3w1/lab02.html",
    "title": "Classes and subclasses",
    "section": "",
    "text": "course banner\nIn this notebook, I will show you the basics of classes and subclasses in Python. As you’ve seen in the lectures from this week, Trax uses layer classes as building blocks for deep learning models, so it is important to understand how classes and subclasses behave in order to be able to build custom layers when needed.\nBy completing this notebook, you will:",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "L2 - Classes and Subclasses"
    ]
  },
  {
    "objectID": "notes/c3w1/lab02.html#part-1-parameters-methods-and-instances",
    "href": "notes/c3w1/lab02.html#part-1-parameters-methods-and-instances",
    "title": "Classes and subclasses",
    "section": "Part 1: Parameters, methods and instances",
    "text": "Part 1: Parameters, methods and instances\nFirst, let’s define a class My_Class.\n\nclass My_Class: #Definition of My_class\n    x = None    \n\nMy_Class has one parameter x without any value. You can think of parameters as the variables that every object assigned to a class will have. So, at this point, any object of class My_Class would have a variable x equal to None. To check this, I’ll create two instances of that class and get the value of x for both of them.\n\ninstance_a= My_Class() #To create an instance from class \"My_Class\" you have to call \"My_Class\"\ninstance_b= My_Class()\nprint('Parameter x of instance_a: ' + str(instance_a.x)) #To get a parameter 'x' from an instance 'a', write 'a.x'\nprint('Parameter x of instance_b: ' + str(instance_b.x))\n\nParameter x of instance_a: None\nParameter x of instance_b: None\n\n\nFor an existing instance you can assign new values for any of its parameters. In the next cell, assign a value of 5 to the parameter x of instance_a.\n\n### START CODE HERE (1 line) ### \ninstance_a.x = 5\n### END CODE HERE ###\nprint('Parameter x of instance_a: ' + str(instance_a.x))\n\nParameter x of instance_a: 5",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "L2 - Classes and Subclasses"
    ]
  },
  {
    "objectID": "notes/c3w1/lab02.html#the-__init__-method",
    "href": "notes/c3w1/lab02.html#the-__init__-method",
    "title": "Classes and subclasses",
    "section": "1.1 The __init__ method",
    "text": "1.1 The __init__ method\nWhen you want to assign values to the parameters of your class when an instance is created, it is necessary to define a special method: __init__. The __init__ method is called when you create an instance of a class. It can have multiple arguments to initialize the paramenters of your instance. In the next cell I will define My_Class with an __init__ method that takes the instance (self) and an argument y as inputs.\n\nclass My_Class: \n    def __init__(self, y): # The __init__ method takes as input the instance to be initialized and a variable y\n        self.x = y         # Sets parameter x to be equal to y\n\nIn this case, the parameter x of an instance from My_Class would take the value of an argument y. The argument self is used to pass information from the instance being created to the method __init__. In the next cell, create an instance instance_c, with x equal to 10.\n\n### START CODE HERE (1 line) ### \ninstance_c = My_Class(10)\n### END CODE HERE ###\nprint('Parameter x of instance_c: ' + str(instance_c.x))\n\nParameter x of instance_c: 10\n\n\nNote that in this case, you had to pass the argument y from the __init__ method to create an instance of My_Class.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "L2 - Classes and Subclasses"
    ]
  },
  {
    "objectID": "notes/c3w1/lab02.html#the-__call__-method",
    "href": "notes/c3w1/lab02.html#the-__call__-method",
    "title": "Classes and subclasses",
    "section": "1.2 The __call__ method",
    "text": "1.2 The __call__ method\nAnother important method is the __call__ method. It is performed whenever you call an initialized instance of a class. It can have multiple arguments and you can define it to do whatever you want like\n\nChange a parameter,\nPrint a message,\nCreate new variables, etc.\n\nIn the next cell, I’ll define My_Class with the same __init__ method as before and with a __call__ method that adds z to parameter x and prints the result.\n\nclass My_Class: \n    def __init__(self, y): # The __init__ method takes as input the instance to be initialized and a variable y\n        self.x = y         # Sets parameter x to be equal to y\n    def __call__(self, z): # __call__ method with self and z as arguments\n        self.x += z        # Adds z to parameter x when called \n        print(self.x)\n\nLet’s create instance_d with x equal to 5.\n\ninstance_d = My_Class(5)\n\nAnd now, see what happens when instance_d is called with argument 10.\n\ninstance_d(10)\n\n15\n\n\nNow, you are ready to complete the following cell so any instance from My_Class:\n\nIs initialized taking two arguments y and z and assigns them to x_1 and x_2, respectively. And,\nWhen called, takes the values of the parameters x_1 and x_2, sums them, prints and returns the result.\n\n\nclass My_Class: \n    def __init__(self, y, z): #Initialization of x_1 and x_2 with arguments y and z\n        ### START CODE HERE (2 lines) ### \n        self.x_1 = y\n        self.x_2 = z\n        ### END CODE HERE ###\n    def __call__(self):       #When called, adds the values of parameters x_1 and x_2, prints and returns the result \n        ### START CODE HERE (1 line) ### \n        result = self.x_1 + self.x_2 \n        ### END CODE HERE ### \n        print(\"Addition of {} and {} is {}\".format(self.x_1,self.x_2,result))\n        return result\n\nRun the next cell to check your implementation. If everything is correct, you shouldn’t get any errors.\n\ninstance_e = My_Class(10,15)\ndef test_class_definition():\n    \n    assert instance_e.x_1 == 10, \"Check the value assigned to x_1\"\n    assert instance_e.x_2 == 15, \"Check the value assigned to x_2\"\n    assert instance_e() == 25, \"Check the __call__ method\"\n    \n    print(\"\\033[92mAll tests passed!\")\n    \ntest_class_definition()\n\nAddition of 10 and 15 is 25\nAll tests passed!",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "L2 - Classes and Subclasses"
    ]
  },
  {
    "objectID": "notes/c3w1/lab02.html#custom-methods",
    "href": "notes/c3w1/lab02.html#custom-methods",
    "title": "Classes and subclasses",
    "section": "1.3 Custom methods",
    "text": "1.3 Custom methods\nIn addition to the __init__ and __call__ methods, your classes can have custom-built methods to do whatever you want when called. To define a custom method, you have to indicate its input arguments, the instructions that you want it to perform and the values to return (if any). In the next cell, My_Class is defined with my_method that multiplies the values of x_1 and x_2, sums that product with an input w, and returns the result.\n\nclass My_Class: \n    def __init__(self, y, z): #Initialization of x_1 and x_2 with arguments y and z\n        self.x_1 = y\n        self.x_2 = z\n    def __call__(self):       #Performs an operation with x_1 and x_2, and returns the result\n        a = self.x_1 - 2*self.x_2 \n        return a\n    def my_method(self, w):   #Multiplies x_1 and x_2, adds argument w and returns the result\n        result = self.x_1*self.x_2 + w\n        return result\n\nCreate an instance instance_f of My_Class with any integer values that you want for x_1 and x_2. For that instance, see the result of calling My_method, with an argument w equal to 16.\n\n### START CODE HERE (1 line) ### \ninstance_f = My_Class(1,10)\n### END CODE HERE ### \nprint(\"Output of my_method:\",instance_f.my_method(16))\n\nOutput of my_method: 26\n\n\nAs you can corroborate in the previous cell, to call a custom method m, with arguments args, for an instance i you must write i.m(args). With that in mind, methods can call others within a class. In the following cell, try to define new_method which calls my_method with v as input argument. Try to do this on your own in the cell given below.\n\nclass My_Class: \n    def __init__(self, y, z):         #Initialization of x_1 and x_2 with arguments y and z\n        self.x_1 = None\n        self.x_2 = None\n    def __call__(self):               #Performs an operation with x_1 and x_2, and returns the result\n        a = None \n        return a\n    def my_method(self, w):           #Multiplies x_1 and x_2, adds argument w and returns the result\n        b = None\n        return b\n    def new_method(self, v):          #Calls My_method with argument v\n        ### START CODE HERE (1 line) ### \n        result = None\n        ### END CODE HERE ### \n        return result\n\nSPOILER ALERT Solution:\n\n# hidden-cell\nclass My_Class: \n    def __init__(self, y, z):      #Initialization of x_1 and x_2 with arguments y and z\n        self.x_1 = y\n        self.x_2 = z\n    def __call__(self):            #Performs an operation with x_1 and x_2, and returns the result\n        a = self.x_1 - 2*self.x_2 \n        return a\n    def my_method(self, w):        #Multiplies x_1 and x_2, adds argument w and returns the result\n        b = self.x_1*self.x_2 + w\n        return b\n    def new_method(self, v):       #Calls My_method with argument v\n        result = self.my_method(v)\n        return result\n\n\ninstance_g = My_Class(1,10)\nprint(\"Output of my_method:\",instance_g.my_method(16))\nprint(\"Output of new_method:\",instance_g.new_method(16))\n\nOutput of my_method: 26\nOutput of new_method: 26",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "L2 - Classes and Subclasses"
    ]
  },
  {
    "objectID": "notes/c3w1/lab02.html#inheritance",
    "href": "notes/c3w1/lab02.html#inheritance",
    "title": "Classes and subclasses",
    "section": "2.1 Inheritance",
    "text": "2.1 Inheritance\nWhen you define a subclass sub, every method and parameter is inherited from super class, including the __init__ and __call__ methods. This means that any instance from sub can use the methods defined in super. Run the following cell and see for yourself.\n\ninstance_sub_a = sub_c(1,10)\nprint('Parameter x_1 of instance_sub_a: ' + str(instance_sub_a.x_1))\nprint('Parameter x_2 of instance_sub_a: ' + str(instance_sub_a.x_2))\nprint(\"Output of my_method of instance_sub_a:\",instance_sub_a.my_method(16))\n\nParameter x_1 of instance_sub_a: 1\nParameter x_2 of instance_sub_a: 10\nOutput of my_method of instance_sub_a: 26\n\n\nAs you can see, sub_c does not have an initialization method __init__, it is inherited from My_class. However, you can overwrite any method you want by defining it again in the subclass. For instance, in the next cell define a class sub_c with a redefined my_Method that multiplies x_1 and x_2 but does not add any additional argument.\n\nclass sub_c(My_Class):           #Subclass sub_c from My_class\n    def my_method(self):         #Multiplies x_1 and x_2 and returns the result\n        ### START CODE HERE (1 line) ###\n        b = self.x_1*self.x_2 \n        ### END CODE HERE ###\n        return b\n\nTo check your implementation run the following cell.\n\ntest = sub_c(3,10)\nassert test.my_method() == 30, \"The method my_method should return the product between x_1 and x_2\"\n\nprint(\"Output of overridden my_method of test:\",test.my_method()) #notice we didn't pass any parameter to call my_method\n#print(\"Output of overridden my_method of test:\",test.my_method(16)) #try to see what happens if you call it with 1 argument\n\nOutput of overridden my_method of test: 30\n\n\nIn the next cell, two instances are created, one of My_Class and another one of sub_c. The instances are initialized with equal x_1 and x_2 parameters.\n\ny,z= 1,10\ninstance_sub_a = sub_c(y,z)\ninstance_a = My_Class(y,z)\nprint('My_method for an instance of sub_c returns: ' + str(instance_sub_a.my_method()))\nprint('My_method for an instance of My_Class returns: ' + str(instance_a.my_method(10)))\n\nMy_method for an instance of sub_c returns: 10\nMy_method for an instance of My_Class returns: 20\n\n\nAs you can see, even though sub_c is a subclass from My_Class and both instances are initialized with the same values, My_method returns different results for each instance because you overwrote My_method for sub_c.\nCongratulations! You just reviewed the basics behind classes and subclasses. Now you can define your own classes and subclasses, work with instances and overwrite inherited methods. The concepts within this notebook are more than enough to understand how layers in Trax work.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "L2 - Classes and Subclasses"
    ]
  },
  {
    "objectID": "notes/c3w1/lab03.html",
    "href": "notes/c3w1/lab03.html",
    "title": "Data generators",
    "section": "",
    "text": "course banner\nIn Python, a generator is a function that behaves like an iterator. It will return the next item. Here is a link to review python generators. In many AI applications, it is advantageous to have a data generator to handle loading and transforming data for different applications.\nYou will now implement a custom data generator, using a common pattern that you will use during all assignments of this course. In the following example, we use a set of samples a, to derive a new set of samples, with more elements than the original set.\nNote: Pay attention to the use of list lines_index and variable index to traverse the original list.\nimport random \nimport numpy as np\n\n# Example of traversing a list of indexes to create a circular list\na = [1, 2, 3, 4]\nb = [0] * 10\n\na_size = len(a)\nb_size = len(b)\nlines_index = [*range(a_size)] # is equivalent to [i for i in range(0,a_size)], the difference being the advantage of using * to pass values of range iterator to list directly\nindex = 0                      # similar to index in data_generator below\nfor i in range(b_size):        # `b` is longer than `a` forcing a wrap\n    # We wrap by resetting index to 0 so the sequences circle back at the end to point to the first index\n    if index &gt;= a_size:\n        index = 0\n    \n    b[i] = a[lines_index[index]]     #  `indexes_list[index]` point to a index of a. Store the result in b\n    index += 1\n    \nprint(b)\n\n[1, 2, 3, 4, 1, 2, 3, 4, 1, 2]",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "L3 - Data Generators"
    ]
  },
  {
    "objectID": "notes/c3w1/lab03.html#shuffling-the-data-order",
    "href": "notes/c3w1/lab03.html#shuffling-the-data-order",
    "title": "Data generators",
    "section": "Shuffling the data order",
    "text": "Shuffling the data order\nIn the next example, we will do the same as before, but shuffling the order of the elements in the output list. Note that here, our strategy of traversing using lines_index and index becomes very important, because we can simulate a shuffle in the input data, without doing that in reality.\n\n# Example of traversing a list of indexes to create a circular list\na = [1, 2, 3, 4]\nb = []\n\na_size = len(a)\nb_size = 10\nlines_index = [*range(a_size)]\nprint(\"Original order of index:\",lines_index)\n\n# if we shuffle the index_list we can change the order of our circular list\n# without modifying the order or our original data\nrandom.shuffle(lines_index) # Shuffle the order\nprint(\"Shuffled order of index:\",lines_index)\n\nprint(\"New value order for first batch:\",[a[index] for index in lines_index])\nbatch_counter = 1\nindex = 0                # similar to index in data_generator below\nfor i in range(b_size):  # `b` is longer than `a` forcing a wrap\n    # We wrap by resetting index to 0\n    if index &gt;= a_size:\n        index = 0\n        batch_counter += 1\n        random.shuffle(lines_index) # Re-shuffle the order\n        print(\"\\nShuffled Indexes for Batch No.{} :{}\".format(batch_counter,lines_index))\n        print(\"Values for Batch No.{} :{}\".format(batch_counter,[a[index] for index in lines_index]))\n    \n    b.append(a[lines_index[index]])     #  `indexes_list[index]` point to a index of a. Store the result in b\n    index += 1\nprint()    \nprint(\"Final value of b:\",b)\n\nOriginal order of index: [0, 1, 2, 3]\nShuffled order of index: [3, 0, 2, 1]\nNew value order for first batch: [4, 1, 3, 2]\n\nShuffled Indexes for Batch No.2 :[1, 2, 3, 0]\nValues for Batch No.2 :[2, 3, 4, 1]\n\nShuffled Indexes for Batch No.3 :[3, 2, 0, 1]\nValues for Batch No.3 :[4, 3, 1, 2]\n\nFinal value of b: [4, 1, 3, 2, 2, 3, 4, 1, 4, 3]\n\n\nNote: We call an epoch each time that an algorithm passes over all the training examples. Shuffling the examples for each epoch is known to reduce variance, making the models more general and overfit less.\n\nExercise\nInstructions: Implement a data generator function that takes in batch_size, x, y shuffle where x could be a large list of samples, and y is a list of the tags associated with those samples. Return a subset of those inputs in a tuple of two arrays (X,Y). Each is an array of dimension (batch_size). If shuffle=True, the data will be traversed in a random form.\nDetails:\nThis code as an outer loop\nwhile True:  \n...  \nyield((X,Y))  \nWhich runs continuously in the fashion of generators, pausing when yielding the next values. We will generate a batch_size output on each pass of this loop.\nIt has an inner loop that stores in temporal lists (X, Y) the data samples to be included in the next batch.\nThere are three slightly out of the ordinary features.\n\nThe first is the use of a list of a predefined size to store the data for each batch. Using a predefined size list reduces the computation time if the elements in the array are of a fixed size, like numbers. If the elements are of different sizes, it is better to use an empty array and append one element at a time during the loop.\nThe second is tracking the current location in the incoming lists of samples. Generators variables hold their values between invocations, so we create an index variable, initialize to zero, and increment by one for each sample included in a batch. However, we do not use the index to access the positions of the list of sentences directly. Instead, we use it to select one index from a list of indexes. In this way, we can change the order in which we traverse our original list, keeping untouched our original list.\nThe third also relates to wrapping. Because batch_size and the length of the input lists are not aligned, gathering a batch_size group of inputs may involve wrapping back to the beginning of the input loop. In our approach, it is just enough to reset the index to 0. We can re-shuffle the list of indexes to produce different batches each time.\n\n\ndef data_generator(batch_size, data_x, data_y, shuffle=True):\n    '''\n      Input: \n        batch_size - integer describing the batch size\n        data_x - list containing samples\n        data_y - list containing labels\n        shuffle - Shuffle the data order\n      Output:\n        a tuple containing 2 elements:\n        X - list of dim (batch_size) of samples\n        Y - list of dim (batch_size) of labels\n    '''\n    \n    data_lng = len(data_x) # len(data_x) must be equal to len(data_y)\n    index_list = [*range(data_lng)] # Create a list with the ordered indexes of sample data\n    \n    \n    # If shuffle is set to true, we traverse the list in a random way\n    if shuffle:\n        random.shuffle(index_list) # Inplace shuffle of the list\n    \n    index = 0 # Start with the first element\n    # START CODE HERE    \n    # Fill all the None values with code taking reference of what you learned so far\n    while True:\n        X = [0] * batch_size # We can create a list with batch_size elements. \n        Y = [0] * batch_size # We can create a list with batch_size elements. \n        \n        for i in range(batch_size):\n            \n            # Wrap the index each time that we reach the end of the list\n            if index &gt;= data_lng:\n                index = 0\n                # Shuffle the index_list if shuffle is true\n                if shuffle:\n                    rnd.shuffle(index_list) # re-shuffle the order\n            \n            X[i] = data_x[index_list[index]] # We set the corresponding element in x\n            Y[i] = data_y[index_list[index]] # We set the corresponding element in x\n    # END CODE HERE            \n            index += 1\n        \n        yield((X, Y))\n\nIf your function is correct, all the tests must pass.\n\ndef test_data_generator():\n    x = [1, 2, 3, 4]\n    y = [xi ** 2 for xi in x]\n    \n    generator = data_generator(3, x, y, shuffle=False)\n\n    assert np.allclose(next(generator), ([1, 2, 3], [1, 4, 9])),  \"First batch does not match\"\n    assert np.allclose(next(generator), ([4, 1, 2], [16, 1, 4])), \"Second batch does not match\"\n    assert np.allclose(next(generator), ([3, 4, 1], [9, 16, 1])), \"Third batch does not match\"\n    assert np.allclose(next(generator), ([2, 3, 4], [4, 9, 16])), \"Fourth batch does not match\"\n\n    print(\"\\033[92mAll tests passed!\")\n\ntest_data_generator()\n\nAll tests passed!\n\n\nIf you could not solve the exercise, just run the next code to see the answer.\n\nimport base64\n\nsolution = \"ZGVmIGRhdGFfZ2VuZXJhdG9yKGJhdGNoX3NpemUsIGRhdGFfeCwgZGF0YV95LCBzaHVmZmxlPVRydWUpOgoKICAgIGRhdGFfbG5nID0gbGVuKGRhdGFfeCkgIyBsZW4oZGF0YV94KSBtdXN0IGJlIGVxdWFsIHRvIGxlbihkYXRhX3kpCiAgICBpbmRleF9saXN0ID0gWypyYW5nZShkYXRhX2xuZyldICMgQ3JlYXRlIGEgbGlzdCB3aXRoIHRoZSBvcmRlcmVkIGluZGV4ZXMgb2Ygc2FtcGxlIGRhdGEKICAgIAogICAgIyBJZiBzaHVmZmxlIGlzIHNldCB0byB0cnVlLCB3ZSB0cmF2ZXJzZSB0aGUgbGlzdCBpbiBhIHJhbmRvbSB3YXkKICAgIGlmIHNodWZmbGU6CiAgICAgICAgcm5kLnNodWZmbGUoaW5kZXhfbGlzdCkgIyBJbnBsYWNlIHNodWZmbGUgb2YgdGhlIGxpc3QKICAgIAogICAgaW5kZXggPSAwICMgU3RhcnQgd2l0aCB0aGUgZmlyc3QgZWxlbWVudAogICAgd2hpbGUgVHJ1ZToKICAgICAgICBYID0gWzBdICogYmF0Y2hfc2l6ZSAjIFdlIGNhbiBjcmVhdGUgYSBsaXN0IHdpdGggYmF0Y2hfc2l6ZSBlbGVtZW50cy4gCiAgICAgICAgWSA9IFswXSAqIGJhdGNoX3NpemUgIyBXZSBjYW4gY3JlYXRlIGEgbGlzdCB3aXRoIGJhdGNoX3NpemUgZWxlbWVudHMuIAogICAgICAgIAogICAgICAgIGZvciBpIGluIHJhbmdlKGJhdGNoX3NpemUpOgogICAgICAgICAgICAKICAgICAgICAgICAgIyBXcmFwIHRoZSBpbmRleCBlYWNoIHRpbWUgdGhhdCB3ZSByZWFjaCB0aGUgZW5kIG9mIHRoZSBsaXN0CiAgICAgICAgICAgIGlmIGluZGV4ID49IGRhdGFfbG5nOgogICAgICAgICAgICAgICAgaW5kZXggPSAwCiAgICAgICAgICAgICAgICAjIFNodWZmbGUgdGhlIGluZGV4X2xpc3QgaWYgc2h1ZmZsZSBpcyB0cnVlCiAgICAgICAgICAgICAgICBpZiBzaHVmZmxlOgogICAgICAgICAgICAgICAgICAgIHJuZC5zaHVmZmxlKGluZGV4X2xpc3QpICMgcmUtc2h1ZmZsZSB0aGUgb3JkZXIKICAgICAgICAgICAgCiAgICAgICAgICAgIFhbaV0gPSBkYXRhX3hbaW5kZXhfbGlzdFtpbmRleF1dIAogICAgICAgICAgICBZW2ldID0gZGF0YV95W2luZGV4X2xpc3RbaW5kZXhdXSAKICAgICAgICAgICAgCiAgICAgICAgICAgIGluZGV4ICs9IDEKICAgICAgICAKICAgICAgICB5aWVsZCgoWCwgWSkp\"\n\n# Print the solution to the given assignment\nprint(base64.b64decode(solution).decode(\"utf-8\"))\n\ndef data_generator(batch_size, data_x, data_y, shuffle=True):\n\n    data_lng = len(data_x) # len(data_x) must be equal to len(data_y)\n    index_list = [*range(data_lng)] # Create a list with the ordered indexes of sample data\n    \n    # If shuffle is set to true, we traverse the list in a random way\n    if shuffle:\n        rnd.shuffle(index_list) # Inplace shuffle of the list\n    \n    index = 0 # Start with the first element\n    while True:\n        X = [0] * batch_size # We can create a list with batch_size elements. \n        Y = [0] * batch_size # We can create a list with batch_size elements. \n        \n        for i in range(batch_size):\n            \n            # Wrap the index each time that we reach the end of the list\n            if index &gt;= data_lng:\n                index = 0\n                # Shuffle the index_list if shuffle is true\n                if shuffle:\n                    rnd.shuffle(index_list) # re-shuffle the order\n            \n            X[i] = data_x[index_list[index]] \n            Y[i] = data_y[index_list[index]] \n            \n            index += 1\n        \n        yield((X, Y))\n\n\n\n\nHope you enjoyed this tutorial on data generators which will help you with the assignments in this course.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "L3 - Data Generators"
    ]
  },
  {
    "objectID": "notes/c1w4/index.html",
    "href": "notes/c1w4/index.html",
    "title": "Machine Translation and Document Search",
    "section": "",
    "text": "Figure 1: course banner\n\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\nVideo: Overview\nThis week we will be looking at machine translation and document search. We will start by looking at how we can represent words in a vector space. We will then look at how we can use these vectors to translate words from one language to another. We will also look at how we can use these vectors to search for documents that are similar to a given document.\n\n\n\n\n\n\n\ntranslation and search\n\n\n\n\nFigure 3: Using Vector Space Models for Translation and Search\n\n\n\n\n\n\n\n\nObjectives\n\n\n\n\nFigure 4: Learning Objectives\n\n\n\n\n\n\n\n\n\nLearning Objectives\n\n\n\nThis week’s learning objectives include:\n\nGradient descent\nApproximate nearest neighbors\nLocality sensitive hashing\nHash functions\nHash tables\nK nearest neighbors\nDocument search\nMachine translation\nFrobenius norm\n\n\n\n\n\n\nTransforming word vectors\nIn the previous week, I showed we how we can plot word vectors. Now, we will see how we can take a word vector and learn a mapping that will allow we to translate words by learning a “transformation matrix”. Here is a visualization:\n\n\n\n\n\n\n\nCaption\n\n\n\n\nFigure 5: Figure Text\n\n\nNote that the word “chat” in french means cat. We can learn that by taking the vector corresponding to “cat” in english, multiplying it by a matrix that we learn and then we can use cosine similarity between the output and all the french vectors. We should see that the closest result is the vector which corresponds to “chat”.\nHere is a visualization of that showing we the aligned vectors:\nNote that:\n\nX corresponds to the matrix of English word vectors and\nY corresponds to the matrix of French word vectors.\nR is the mapping matrix.\n\n\n\n\n\n\n\n\nCaption\n\n\n\n\nFigure 6: Figure Text\n\n\nSteps required to learn R:\n\nInitialize R\nFor loop​\n\n\n\\text{Loss } \\mathcal{L} = || XR-Y||_F\n\n\ng= \\frac{dLoss}{dR}\n\n\nR = R−\\alpha ∗ g\n\n\n\n\n\n\n\n\nCaption\n\n\n\n\nFigure 7: Figure Text\n\n\nHere is an worked out example to show we how the Frobenius norm works.\n\n\n\n\n\n\n\nCaption\n\n\n\n\nFigure 8: Figure Text\n\n\n\n∥XR−Y∥_F\n\n\nA = \\begin{bmatrix} 2 & 2 \\\\ 2 & 2 \\end{bmatrix}\n\n\n∥A_F∥= \\sqrt{2^2 + 2^2 + 2^2 + 2^2}  = 4\n\n\n∥A∥_F ≡ \\sum _{i=1}^m\\sum _{j=1}^n |a_{ij}|^2\n\nIn summary we are making use of the following:\n\nXR≈Y\nminimize ∥XR−Y∥_F^2 ​\n\n\n\n\n\n\n\n\nCaption\n\n\n\n\nFigure 9: Figure Text\n\n\n\n\n\n\n\n\nCaption\n\n\n\n\nFigure 10: Figure Text\n\n\n\n\n\n\n\n\nCaption\n\n\n\n\nFigure 11: Figure Text\n\n\n\n\n\n\n\nUngraded Lab: Rotation matrices in R2\nRotation matrices in R2\n\n\nVideo: K-nearest neighbors\nAfter we have computed the output of XR we get a vector.\nWe then need to find the most similar vectors to your output. Here is a visual example:\n\n\n\n\n\n\n\nCaption\n\n\n\n\nFigure 12: Finding Translation\n\n\nIn the video, we mentioned if we were in San Francisco, and we had friends all over the world, we would want to find the nearest neighbors. To do that it might be expensive to go over all the countries one at a time. So we will introduce hashing to show we how we can do a look up much faster.\n\n\n\n\n\n\n\nCaption\n\n\n\n\nFigure 13: KNN\n\n\n\n\n\n\n\n\nCaption\n\n\n\n\nFigure 14: KNN\n\n\n\n\n\n\n\n\nCaption\n\n\n\n\nFigure 15: KNN\n\n\n\n\n\n\nHash tables and hash functions\nImagine we had to cluster the following figures into different buckets:\n\n\n\n\n\n\n\nHashing\n\n\n\n\nFigure 16: Hashing\n\n\nNote that the figures blue, red, and gray ones would each be clustered with each other\n\n\n\n\n\n\n\nHash Tables\n\n\n\n\nFigure 17: Hashing\n\n\nWe can think of hash function as a function that takes data of arbitrary sizes and maps it to a fixed value. The values returned are known as hash values or even hashes.\n\n\n\n\n\n\n\nHash Tables\n\n\n\n\nFigure 18: Hashing\n\n\nThe diagram above shows a concrete example of a hash function which takes a vector and returns a value. Then we can mod that value by the number of buckets and put that number in its corresponding bucket. For example, 14 is in the 4th bucker, 17 & 97 are in the 7th bucket. Let’s take a look at how we can do it using some code.\n\n\n\n\n\n\n\nHash Tables\n\n\n\n\nFigure 19: Hashing\n\n\nThe code snippet above creates a basic hash table which consists of hashed values inside their buckets. hash_function takes in value_l (a list of values to be hashed) and n_buckets and mods the value by the buckets. Now to create the hash_table, we first initialize a list to be of dimension n_buckets (each value will go to a bucket). For each value in your list of values, we will feed it into your hash_function, get the hash_value, and append it to the list of values in the corresponding bucket.\nNow given an input, we don’t have to compare it to all the other examples, we can just compare it to all the values in the same hash_bucket that input has been hashed to.\nWhen hashing we sometimes want similar words or similar numbers to be hashed to the same bucket. To do this, we will use “locality sensitive hashing.” Locality is another word for “location”. So locality sensitive hashing is a hashing method that cares very deeply about assigning items based on where they’re located in vector space.\n\n\n\n\n\n\n\nHash Tables\n\n\n\n\nFigure 20: Hashing\n\n\n\n\n\n\n\n\nHash Tables\n\n\n\n\nFigure 21: Hashing\n\n\n\n\n\nLocality sensitive hashing\nLocality sensitive hashing is a technique that allows we to hash similar inputs into the same buckets with high probability.\n\n\n\n\n\n\n\nHash Tables\n\n\n\n\nFigure 22: Locality sensitive hashing\n\n\nInstead of the typical buckets we have been using, we can think of clustering the points by deciding whether they are above or below the line. Now as we go to higher dimensions (say n-dimensional vectors), we would be using planes instead of lines. Let’s look at a concrete example:\n\n\n\n\n\n\n\nHash Tables\n\n\n\n\nFigure 23: Locality sensitive hashing\n\n\nGiven some point located at (1,1) and three vectors V_1=(1,2), V_2=(-1,1), V_3=(-2,-1) we will see what happens when we take the dot product. First note that the dashed line is our plane. The vector with point P=(1,1) is perpendicular to that line (plane). Now any vector above the dashed line that is multiplied by (1,1) would have a positive number. Any vector below the dashed line when dotted with (1,1) will have a negative number. Any vector on the dashed line multiplied by (1,1) will give we a dot product of 0. ​ Here is how to visualize a projection (i.e. a dot product between two vectors):\n\n\n\n\n\n\n\nHash Tables\n\n\n\n\nFigure 24: Locality sensitive hashing\n\n\nWhen we take the dot product of a vector V and a P, then we take the magnitude or length of that vector, we get the black line (labelled as Projection). The sign indicates on which side of the plane the projection vector lies.\n\n\nMultiple Planes\nWe can use multiple planes to get a single hash value. Let’s take a look at the following example:\n\n\n\n\n\n\n\nHash Tables\n\n\n\n\nFigure 25: Locality sensitive hashing\n\n\nGiven some point denoted by v, we can run it through several projections P_1, P_2, P_3 to get one hash value. If we compute P_1v^T P_1v^T we get a positive number, so we set h_1=1. P_2v^T P_2v^T gives we a positive number so we get h_2=1. P_3v^T P_3v^T is a negative number so we set h_3 to be 0. We can then compute the hash value as follows. ​ \nhash=2^0×h_1+2^1×h_2+2^2×h_3 = 1×1+2×1+4×0=3\n\nAnother way to think of it, is at each time we are asking the plane to which side will we find the point (i.e. 1 or 0) until we find your point bounded by the surrounding planes.The hash value is then defined as:\n\nhash_{value}=\\sum_i^H 2^i×h_i\n\nHere is how we can code it up:\n\ndef hash_value_of_vector(P, v):\n    hash_value = 0\n    for i, plane in enumerate(P):\n        sign = side_of_plane(plane, v)\n        hash_i = 1 if sign else 0\n        hash_value += hash_i * 2**i\n    return hash_value\n\nP_l is the list of planes. We initialize the value to 0, and then we iterate over all the planes (P), and we keep track of the index. We get the sign by finding the sign of the dot product between v and your plane P. If it is positive we set it equal to 1, otherwise we set it equal to 0. We then add the score for the ith plane to the hash value by computing 2^i×h_i.\n\n\n\n\n\n\n\n\nhash_multiple_plane()\n\n\n\n\nFigure 26: Code for computing hash value for multiple planes\n\n\n\n\nUngraded Lab: Hash tables\nHash Tables Lab\n\n\nReading: Approximate nearest neighbors\nApproximate nearest neighbors does not give we the full nearest neighbors but gives we an approximation of the nearest neighbors. It usually trades off accuracy for efficiency.\nLook at the following plot:\n\n\n\n\n\n\n\n\nnearest neighnours\n\n\n\n\nFigure 27: Approximate nearest Neighbors\n\n\nWe are trying to find the nearest neighbor for the red vector (point). The first time, the plane gave we green points. We then ran it a second time, but this time we got the blue points. The third time we got the orange points to be the neighbors. So we can see as we do it more times, we are likely to get all the neighbors. Here is the code for one set of random planes. Make sure we understand what is going on.\n\n\n\n\n\n\n\n\nhash_multiple_plane()\n\n\n\n\nFigure 28: Code for computing hash value for multiple planes\n\n\n\n\n\n\n\n\nDocument representation\n\n\n\n\nFigure 29: A combination of word vectors to get document vectors based on Document search with KNN\n\n\n\n\n\nSearching documents\nThe previous video shows we a toy example of how we can actually represent a document as a vector.\n\n\n\n\n\n\n\nhash_multiple_plane()\n\n\n\n\nFigure 30: Code for combining word vectors to get document vectors\n\n\nIn this example, we just add the word vectors of a document to get the document vector. So in summary we should now be familiar with the following concepts:\n\n\n\n\n\n\n\nhash_multiple_plane()\n\n\n\n\nFigure 31: Code for combining word vectors to get document vectors\n\n\n\n\nVideo: Andrew Ng with Kathleen McKeown\n\nI am delighted to have with me here today, Kathy McKeown, who is the Henry and Gertrude Rothschild Professor of Computer Science at Columbia University. Where she is also the founding director of the Institute for Data Sciences and Engineering. She is also an Amazon Scholar and is well known for the work that she’s done over many years on text summarization and many other topics in NLP. Welcome, Kathy, and thanks for joining me.\n\n\nThanks Andrew, and thanks for having me.\n\n\nSo today we lead a large group doing NLP research, but your journey to becoming an AI researcher and an NLP searcher has been an unusual one. If I remember correctly, we actually majored in comparative literature when we were in undergrad, even though you’re also very mathematically oriented at the time. So tell us we story of how we became an NLP researcher.\n\n\nYeah, so when I started out at Brown I didn’t know what I wanted to major in. So I took courses both in math and comparative literature. And as I went on, I became more interested in comparative literature. Probably in part because of the teachers who I had who really influenced me. It was only as I came near the end of my time at Brown that and when I graduated, I took a job as a programmer, which I found actually very boring. And I thought if I was going to have to be working 40 hours a week, I wanted to be doing something that I enjoyed. And it was then that a friend of mine who was a linguistics major at Brown told me about computational linguistics. And so I spent a lot of the year in the library reading about AI and natural language processing. And when I applied for Graduate School the following year I knew that was what I wanted to do. Because it gave me a way to bring together my interest in language and in math.\n\n\nSo that’s fascinating, so as a comparative literature major, we spent a lot of time in the Brown University Library reading about computational linguistics and NLP. Today, with a lot of learners, maybe some watching this video that may not yet be a NLP researcher or AI engineer wanting to break into the field. So I’d love to hear more about what your experience was like reading so much. Were we doing it by yourself? Did we have a study group or what was that like?\n\n\nI was doing it entirely by myself. I really had no guidance in terms of what to look at. I guess this friend that I had made a few suggestions. And then I traced references when I first began reading. I would follow up on references to go further. Yeah, and when I first entered Graduate School, and I had essentially switched fields, I found it very frightening. I was sure that I was and impostor, that I didn’t know enough. And before long they would find out that I really shouldn’t be there after all. Yeah, but that’s something we overcome with time and we learn that it’s not the case and people value your input.\n\n\nThat’s really inspiring. Thank we for sharing that. Would we have any advice for someone maybe today that is trying to do this themselves and wondering if they know enough, or are good enough, or should be in the field? It sounds like we got through that and you’ve been incredibly successful. But what would we say to someone today maybe looking to follow in your footsteps and wondering if this is very for them or if they’ll make it?\n\n\nSo I I guess I have a couple pieces of advice. I do think reaching out to people and talking to people is useful. Until I got to Graduate School I wasn’t in an environment where I had people to talk to. So I do think it’s really helpful, especially to talk to your peers about what they’re doing and what they’re interested in. When we pick problems to work on. And I guess, especially in today’s world of deep learning and neural nets, I would advise choosing problems that are different from what everybody else works on. Sort of strike out in a different direction, choose something new, a new task, and take off from there.\n\n\nAnd I think I would love to come back to the different problems stopping within. And when I speak with learners from around the world, I do hear from some that they feel lonely or isolated. They’re kind of out somewhere or maybe not living in the major tech hub, and they sometimes feel like they’re doing this by themselves. So I find it actually really inspiring that we were able to do that by yourself in a library in Brown University. I don’t know if we have any other thoughts to offer learners that may feel like they’re somewhere in a company or in a city just trying to do this by themselves.\n\n\nI’m not sure I do have a lot more to say about it. I guess read what we enjoy about, if we can be part of a reading group, an online reading group, that would be helpful. There are a lot of reading groups now, and that’s a good way to get sort of insights. There are online videos and course experiences, like yours. And I think that’s a way to find out what’s going on and get in touch with what people are doing. So I think today, the online environment can help people get connected and hear what’s going on. I was lucky, I mean, I was really lucky, I applied to Penn. I didn’t know that at the time it was the best place in natural language processing, and that was totally luck. So I don’t know that I would recommend doing it blind again today, I think getting advice is great.\n\n\nEveryone that’s been successful has had many elements of luck. But the preparation makes we ready to take advantage of when good luck falls into your lap.\n\n\nYeah.\n\n\nThank we for sharing that, that was really inspiring to hear about your early days as a developing researcher. And today we lead a large group at Columbia University doing very interdisciplinary work and doing a lot of work on summarization and other topics. So tell us a bit more about your current work and which excites,\n\n\nSo I mean, summarization has really been the bulk of my work over the most recent years. We’ve done work on summarization of all kinds of different genres, from personal novels to emails. One thread of research in summarization that I’m particularly excited about is work that I’ve done with researchers at Amazon and which was published at ACL. And this is work on summarization of novel chapters. And it’s very new, no one has been working on this task, it’s very challenging, very different from summarization. So the chapters are much longer than the news articles on which most current work in summarization today is done. And that is a challenge for current neural models. And a big problem is that there is an extreme amount of paraphrasing between the input, which is 19th century novels, and the output, which is a summary written in today’s language. None of the current models can handle the kind of paraphrasing that we see there. And that, in general, is a topic that I’m really interested in is a sort of very abstractive summarization. Where the sentences use different words than the input document, where the syntactic structure is different. And that is very different from the vast Majority of work today which is done on summarization of news. And it’s done on summarization of news because that’s where the data is. So some of the other areas that I’m looking at are summarization of personal narratives that we find online where the personal narratives are very informal language and the summary is more formal. Summarization of debates. In past work, we’ve also done summarization of email. Which has some of those same characteristics.\n\n\nWhy did we choose to work on the novel summarization task?\n\n\nWell, so we had done work on novels even earlier. I would say in 2010, when one of my students who was very interested in creative writing, and I really thought he should do a PhD. And so to convince him to stay, we came to a topic that he would be happy with, which was analysis and generation of creative language. And I felt then that my work came full circle, that we collaborated then with a professor in comparative literature. So I acme back to my roots in comparative literature and that was a lot of fun. So, when I first went to Amazon, I knew because of Kindle and online on Amazon, they have a lot of novels. And I thought, what would be more fun than being able to summarize novels?\n\n\nSounds like a fun project, I read a lot on my Kindle, so maybe your work will be a feature on Kindle some day. One aspect of your work that stood out as well is that you’re known for doing highly interdisciplinary work. So rather than focusing narrowly on NLP research, your work spans AI, and the media where I know the Columbia University is a great journalism school. So wonderful journalists we work with there or the application of NLP to social networks. I think we work with medical problems. So tell us a bit about how we think about interdisciplinary work because you’ve done more of it I think than most NLP researchers have.\n\n\nYeah, I really enjoy interdisciplinary work. I think it’s my most favorite kind of research to do. And in part we get a really different perspective on research in the world when we talk to people in other fields. It takes us out of our sort of technical, narrow field.\n\n\nAnd so earlier, we alluded to picking research topics that are novel and I think your research portfolio has certainly touched on a lot of problems that very few others are working on. So can we say more about that? So how do we pick research projects we work on and how do we advise others to pick topics to work on?\n\n\nI think it’s important to pick a task that matters. So that for me is one thing to look at. For example, most of the work in tech summarization today is done on summarization of what’s called single document summarization of news. So take one news article in and generate a summary of that news article. And the reason for that is because that’s where the data is. There’s a huge amount of data that has been pulled together from first the CNN Daily Mail Corpus, and later New York Times, and there are a number of other corpora as well. The problem is that’s not really a task that we need. We’ve known for a long time that the lead of the news article can help people pretty well in serving as a summary of the document. And in fact, for years it was hard to beat. The lead people just worked on, that wasn’t a problem that people worked on in the early years of summarization.\n\n\nThe lead being the first sentence or the first-\n\n\nThe first couple of sentences in the news article. So, Yeah, I mean, people work on a problem like that because that’s where the data is. We have leaderboards. People are competitive. They like to be The leaderboard, but I would question does that one or even half a point in Rouge, which the automated network used to score them, really make a difference? If we look at the output we can see that actually the summaries are quite similar and either one of them might be fine. So I prefer to go in directions that people haven’t gone in before and to choose a task where if we solve it, it’s going to help people. It’s going to be a useful application that you’ve developed. So this is why I have done things like summarization of personal narrative, which we did in the course of disaster. So that we could summarize, think of having a browsing view of summaries of what people have experienced after they’ve lived through a disaster. Or the current work on summarization of novels where it’d be helpful to have a summary of an input chapter. I’d like to go in a different direction, in part because I want to solve the task that matters. But I also like to go in a different direction because in this day in age of deep learning where results come so fast, everybody works on the same problem trying to beat the previous state of the art. It can be hard to be the first one to get there, and if we go in a different direction, nobody else is working on, we are going to be the first one to come to solution. And that’s what I like to do in my research. Overtime I like to be first on a problem. I see, cool yeah. And I feel like, for myself, I have a lot of respect for people that could push that extra half points of performance on the leaderboard because hopefully that advances the whole field. And this all shifts. I also have a lot of respect for people with your creativity in the inside, to charter the new problem that no one else has thought of, and advances the whole field in a different direction. I think the field of AI NLP is broad enough. I think it’s actually not a bad thing if we have lots of people working on lots of different things, including standardized benchmarks. And a bunch of new things.\n\n\nYeah, sure, I feel that there are not as many people who want to go in that new direction and it does take some sort of guts to do it. Because the first thing that happens when we submit a paper is there is no benchmark. There is no baseline of prior work and reviewers have a very hard time dealing with that. How can they judge whether it’s really a good step forward. Whereas if we can show on a leaderboard that you’ve improved by a certain amount and we stay within the traditional trajectory, it’s easier to judge.\n\n\nYeah, I’m with we on that. Actually, I was recently chatting with one of my friends Sharon Joe who mentioned that sometimes the way benchmarks and metrics are established is that some researcher publishing a paper publishes something using some metric. Maybe a good one, maybe an okay one. But to make sure that subsequent papers can compare to earlier work, then everyone, and more and more people end up using the same metric. More for historical reasons and it makes things comparable rather than because it is actually the most useful metric. It’s funny how metrics get established in academia.\n\n\nYeah, I mean that has happened in the summarization field. And I think also in machine translation where we want an automated metric because it’s easier to develop a system to train over and over again. And yet everybody knows that the automated metrics that we currently have are really flawed. And but everyone keeps using them, because that’s what we’ve always done. One of my most stark memories was I remember going to a the Information Retrieval Conference and attending a workshop in text summarization. And I remember being fascinated, but struck that about half of that workshop was on text summarization algorithms and the other half of that workshop was on how to develop metrics to evaluate. Text summarization especially, the development of automated metrics has been challenging.\n\n\nYeah.\n\n\nHey, so in terms of choosing. Lead topics to work on. One of the pieces of work that you’ve been doing that I thought was fascinating, was we were taking texts from the black community from Harlem, near I guess where we teach at Columbia University and analyzing that as well. Tell us about that.\n\n\nThis is where I’m moving with my work with a researcher from social work. And we’re also beginning to involve a linguist who works on African American vernacular. And what we’re doing is we’re looking at what people say, what kind of emotions they express in reaction to major events that are going on today. So, for example, in reaction to Black Lives Matter and in reaction to COVID-19. So this is work that we’re just beginning. We’ve begun with developing an interface where people can post about their experiences with these events and how they’re feeling. And I guess what we’re hoping to do with that in part, so we have two directions to go. One on the natural language side is to be able to understand how people express different kinds of language. Sorry, different kinds of emotion and African American vernacular. And how that difference from how people express it in standard American English. And look at the difference in language and probably even the difference in content in terms of what’s expressed. And this can help us in developing algorithms that are not biased as we move forward. Most of the work in natural language, all of the systems have been trained on language that comes from news like the Wall Street Journal.\n\n\nYeah, that’s great. If this type of work can help fight bias or build bridges between communities or just play some role in understanding and helping to advance the Black Lives Matter movement, that seems to be a wonderful thing to me.\n\n\nYeah, I mean, we also want in that work to look at the impact of trauma. So it’s a different kind of trauma and sometimes it’s not your personal trauma, but the trauma of seeing what has happened to other people who are like you. So yeah, we want to look at how that is expressed in the different kinds of emotions, the intensity of emotion, and so forth.\n\n\nI find it really wonderful that NLP researchers, AI researchers can play an active role in some of these most important societal questions and issues of our time. It feels like the work we do as AI researchers, it could matter in these really important times.\n\n\nYeah, I mean I think so. And I’ve sort of been trying to do that for a while. I think it really attract students to work with you, and often different kinds of students into the field to work with you. On our first work on with analyzing social media posts of gang involved youth, we didn’t have funding. We did that entirely with undergraduates who were just totally amazing. In earlier work we were looking at being able to automatically generate updates about disaster as it unfolded. We did that after Hurricane Sandy hit New York. And again it was something that students came to me and they had seen this happen and they have seen their neighborhoods hurt. Or they lived through the uncertainty of it and they wanted to help. They wanted to know what can we do and that was at that point in time, this whistle pre-neural net we began developing systems that could automatically generate updates as an event unfolded.\n\n\nI think that we don’t need a PhD, don’t need a long publication record, but in undergrad spotting an opportunity with a desire to help. Can step in and start to work on systems that they can make a difference.\n\n\nYes, they’re really passionate about it and There are really good, the work that came out of that was really excellent.\n\n\nYeah, thank you, so Kathy, this is great stuff. And switching tracks a bit, you’ve been working in NLP and associated areas for a long time. In fact, I saw that even way back in 1985 we written an early book on text generation before the modern neural text generation techniques were around. So you’ve been a leader in the field for a long time and seen a lot of things change. I’d love to hear your thoughts on how the field of NLP has evolved over these many years.\n\n\nSure, so when I started, which I got my PhD in 82. so I spent those earlier years at Penn. And there were some characteristics of the field that were salient. So one of them is that there was a lot of interdisciplinary work. There was in developing NLP systems, we drew a lot on work from linguistics, from philosophy, from psychology, from cognitive science. And so when I was at Penn I interacted a lot with faculty from linguistics. Ellen Prince was one of the people or faculty from philosophy. We spent time in these interdisciplinary meetings. And I can remember walking across campus with my advisor to go from the computer science department to the psychology department, for example. I was influenced a lot and I have to mention this, although it’s not exactly what we asked, I was influenced a lot by senior women at the time. If I look back to who was most influential in how I progressed in my early research. My advisor, of course, who was a male Aravan Joshi. But then also Bonnie Weber, who was there in computer science, Eva Hychova from Charles University at Prague who was a linguist. Barbara Gross who lives at Sanford at that time in the CFLI Institute. And Karen Spark Jones was very influential to me. She was from the field of information retrieval. And she and I spent a lot of time talking about summarization. So interdisciplinary is one main feature of that time. A second was drawing on theories from these other areas, so we drew on theories from linguistics. One main kind of theory that we looked at was the focus of attention and how that changed over the course of a discourse. And how that influenced how we made choices and how we realized text in language. So for example, did we use a pronoun or did we use a full noun phrase? What kind of syntactic structure did we use? We might use different syntactic structures to make a concept more prominent in the discourse. We also drew on work from philosophy. So we drew on work from theories from Cyril about intention, and work from Grice about conversational implicature. And so we looked at these theories and we looked at how we could embody them in our natural language approaches.\n\n\nIt’s great to hear about some of your early sources of inspiration. Much as I think today we will be a source of inspiration to many others. So you’ve seen a lot and see a lot in NLP, which continues to be a rapidly evolving field. So I’m actually curious, Kathy, what do we find most exciting in terms of emerging or exciting NLP technologies?\n\n\nFor me, personally, some of the work that I’ve already talked about today on truly abstracted summarization that uses extreme paraphrasing. Work on analyzing the language from diverse community. So we’ve been looking at the black community, but I think there are other communities we could look at as well. I’m interested in looking at how we deal with bias and data. And another very important topic is being able to arrive at what I would call para linguistic meaning. So this pragmatics information about emotion, about intention, would be another important direction to go. And I also think more work on events, being able to understand what events have happened, and to be able to follow them. I also think about often if I look back. Is it okay, if I talk about this now. I look back?\n\n\nOkay. If I look back on my favorite technologies and papers, I can think of papers from thee points in time. The first would be older, and this was very early work in language generation, on how we pick the words in our sentence. And we thought then that it was a hard problem that constraints came from many different sources. And we wrote a paper called Floating Constraints on Lexical Choice. Where we looked at how information from different parts of language, from the discourse, from the lexicon, from syntax, from semantics, will influence what we chose. And we worked in two different domains. One was basketball and one with stock markets. And I give it the example of the floating constraint, where we want to express both the time at which something happened and the manner. In the first example we expressed time and the verb, and the manner and the adverbs. So Wall Street indexes open strongly, open is the time. And in the second weeks press manner in the verb and time and the propositional phrase. So stock index surged at the start of the trading day. And so we wanted to look at how we could control that choice. And I think control is something that’s missing in language generation and summarization. Today using deep learning methods, how do we control what the output is and make sure it’s true to what our intention is? In more recent work, my favorite is work on News Blaster. That’s still about 15 years ago, but it feels recent to me. And that was where we took a real world problem. We did do some collaboration with journalists, and we’ve developed a testbed, where we could identify the events that happened during the day, and produce summary on each event. And then we also looked at how we could track that overtime. And this platform gave us a common sort of application, in which my students could address really hard research questions. And so that was where we looked at, did some of our first work on abstract and summarization. Looking at how we might compress sentences, how we could fuse phrases together, how we might reference, edit references, so that the summary was more coherent. And we also did work on multi lingual summarization. Yeah.\n\n\nCool, Thank you. Lots of exciting, very distinct projects over the years. Do we have any wrap up? Did we have any lost thoughts? We can just say, do we have any final thoughts?\n\n\nWell, I guess I would just say that natural language is a really exciting field today. There’s been a huge amount of progress with deep learning. We’ve seen dramatic increases in accuracy, but we still have a lot of directions to go. And I guess I would like to see more of the interdisciplinary work being brought back in. I’d like to see people looking at the data more and at their output more, rather than just numbers. But I think there are many exciting directions for people to work in, and I hope we’ll see many people joining the field.\n\n\nThank you, that was great. Yeah, I saw the hope that will have a lot more people join NLP and contribute to all of this exciting work. So thanks Kathy, it was cool\n\n\nThanks we much for asking me, it was fun.\n\n\nFor more interviews with NLP thought leaders, check out the DeepLearning.AI YouTube channel, or enroll in the NLP specialization on Coursera.\n\n\n\n\nReading: Bibliography\n\n(Jurafsky and Martin 2025) - Speech and Language Processing\n\n\n\n\n\n\nReferences\n\nJurafsky, Daniel, and James H. Martin. 2025. Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition with Language Models. 3rd ed. https://web.stanford.edu/~jurafsky/slp3/.\n\nCitationBibTeX citation:@online{bochman2020,\n  author = {Bochman, Oren},\n  title = {Machine {Translation} and {Document} {Search}},\n  date = {2020-10-11},\n  url = {https://orenbochman.github.io/notes-nlp/notes/c1w4/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2020. “Machine Translation and Document\nSearch.” October 11, 2020. https://orenbochman.github.io/notes-nlp/notes/c1w4/.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "MT & Document Search via KNN",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w4/lab01.html",
    "href": "notes/c1w4/lab01.html",
    "title": "Vector manipulation in Python",
    "section": "",
    "text": "Figure 1: course banner\nIn this lab, we will have the opportunity to practice once again with the NumPy library. This time, we will explore some advanced operations with arrays and matrices.\nAt the end of the previous module, we used PCA to transform a set of many variables into a set of only two uncorrelated variables. This process was made through a transformation of the data called rotation.\nIn this week’s assignment, we will need to find a transformation matrix from English to French vector space embeddings. Such a transformation matrix is nothing else but a matrix that rotates and scales vector spaces.\nIn this notebook, we will explain in detail the rotation transformation."
  },
  {
    "objectID": "notes/c1w4/lab01.html#transforming-vectors",
    "href": "notes/c1w4/lab01.html#transforming-vectors",
    "title": "Vector manipulation in Python",
    "section": "Transforming vectors",
    "text": "Transforming vectors\nThere are three main vector transformations: * Scaling * Translation * Rotation\nIn previous notebooks, we have applied the first two kinds of transformations. Now, let us learn how to use a fundamental transformation on vectors called rotation.\nThe rotation operation changes the direction of a vector, letting unaffected its dimensionality and its norm. Let us explain with some examples.\nIn the following cells, we will define a NumPy matrix and a NumPy array. Soon we will explain how this is related to matrix rotation.\n\nimport numpy as np                     # Import numpy for array manipulation\nimport matplotlib.pyplot as plt        # Import matplotlib for charts\nfrom utils_nb import plot_vectors      # Function to plot vectors (arrows)\n\n\nExample 1\n\n# Create a 2 x 2 matrix\nR = np.array([[2, 0],\n              [0, -2]])\n\n\nx = np.array([[1, 1]]) # Create a 1 x 2 matrix\n\nThe dot product between a vector and a square matrix produces a rotation and a scaling of the original vector.\nRemember that our recommended way to get the dot product in Python is np.dot(a, b):\n\ny = np.dot(x, R) # Apply the dot product between x and R\ny\n\narray([[ 2, -2]])\n\n\nWe are going to use Pyplot to inspect the effect of the rotation on 2D vectors visually. For that, we have created a function plot_vectors() that takes care of all the intricate parts of the visual formatting. The code for this function is inside the utils_nb.py file.\nNow we can plot the vector \\vec x = [1, 1] in a cartesian plane. The cartesian plane will be centered at [0,0] and its x and y limits will be between [-4, +4]\n\nplot_vectors([x], axes=[4, 4], fname='transform_x.svg')\n\n\n\n\n\n\n\n\nNow, let’s plot in the same system our vector \\vec x = [1, 1] and its dot product with the matrix\nRo = \\begin{bmatrix} 2 & 0 \\\\ 0 & -2 \\end{bmatrix}\ny = x \\cdot Ro = [[-2, 2]]\n\nplot_vectors([x, y], axes=[4, 4], fname='transformx_and_y.svg')\n\n\n\n\n\n\n\n\nNote that the output vector y (blue) is transformed in another vector.\n\n\nExample 2\nWe are going to use Pyplot to inspect the effect of the rotation on 2D vectors visually. For that, we have created a function that takes care of all the intricate parts of the visual formatting. The following procedure plots an arrow within a Pyplot canvas.\nData that is composed of 2 real attributes is telling to belong to a $ RxR $ or $ R^2 $ space. Rotation matrices in R^2 rotate a given vector \\vec x by a counterclockwise angle \\theta in a fixed coordinate system. Rotation matrices are of the form:\nRo = \\begin{bmatrix} cos \\theta & -sin \\theta \\\\ sin \\theta & cos \\theta \\end{bmatrix}\nThe trigonometric functions in Numpy require the angle in radians, not in degrees. In the next cell, we define a rotation matrix that rotates vectors by 45^o.\n\nangle = 100 * (np.pi / 180) #convert degrees to radians\n\nRo = np.array([[np.cos(angle), -np.sin(angle)],\n              [np.sin(angle), np.cos(angle)]])\n\nx2 = np.array([2, 2]).reshape(1, -1) # make it a row vector\ny2 = np.dot(x2, Ro)\n\nprint('Rotation matrix')\nprint(Ro)\nprint('\\nRotated vector')\nprint(y2)\n\nprint('\\n x2 norm', np.linalg.norm(x2))\nprint('\\n y2 norm', np.linalg.norm(y2))\nprint('\\n Rotation matrix norm', np.linalg.norm(Ro))\n\nRotation matrix\n[[-0.17364818 -0.98480775]\n [ 0.98480775 -0.17364818]]\n\nRotated vector\n[[ 1.62231915 -2.31691186]]\n\n x2 norm 2.8284271247461903\n\n y2 norm 2.82842712474619\n\n Rotation matrix norm 1.414213562373095\n\n\n\nplot_vectors([x2, y2], fname='transform_02.svg')\n\n\n\n\n\n\n\n\nSome points to note:\n\nThe norm of the input vector is the same as the norm of the output vector. Rotations matrices do not modify the norm of the vector, only its direction.\nThe norm of any R^2 rotation matrix is always \\sqrt 2 = 1.414221"
  },
  {
    "objectID": "notes/c1w4/lab01.html#frobenius-norm",
    "href": "notes/c1w4/lab01.html#frobenius-norm",
    "title": "Vector manipulation in Python",
    "section": "Frobenius Norm",
    "text": "Frobenius Norm\nThe Frobenius norm is the generalization to R^2 of the already known norm function for vectors\n\\| \\vec a \\| = \\sqrt {{\\vec a} \\cdot {\\vec a}} \nFor a given R^2 matrix A, the frobenius norm is defined as:\n\\|\\mathrm{A}\\|_{F} \\equiv \\sqrt{\\sum_{i=1}^{m} \\sum_{j=1}^{n}\\left|a_{i j}\\right|^{2}}\n\nA = np.array([[2, 2],\n              [2, 2]])\n\nnp.square() is a way to square each element of a matrix. It must be equivalent to use the * operator in Numpy arrays.\n\nA_squared = np.square(A)\nA_squared\n\narray([[4, 4],\n       [4, 4]])\n\n\nNow we can sum over the elements of the resulting array, and then get the square root of the sum.\n\nA_Frobenius = np.sqrt(np.sum(A_squared))\nA_Frobenius\n\nnp.float64(4.0)\n\n\nThat was the extended version of the np.linalg.norm() function. We can check that it yields the same result.\n\nprint('Frobenius norm of the Rotation matrix')\nprint(np.sqrt(np.sum(Ro * Ro)), '== ', np.linalg.norm(Ro))\n\nFrobenius norm of the Rotation matrix\n1.414213562373095 ==  1.414213562373095\n\n\nCongratulations!! We’ve covered a few more matrix operations in this lab. This will come in handy in this week’s programming assignment!"
  },
  {
    "objectID": "notes/c3w3/index.html",
    "href": "notes/c3w3/index.html",
    "title": "LSTMs and Named Entity Recognition",
    "section": "",
    "text": "course banner\n\n\n\n\nFigure 1\n\n\n\n\n\n\n\n\nFigure 2\nMy notes for Week 3 of the Natural Language Processing with Sequence Models Course in the Natural Language Processing Specialization Offered by DeepLearning.AI on Coursera",
    "crumbs": [
      "Home",
      "Sequence Models",
      "LSTMs and Named Entity Recognition",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w3/index.html#rnns-and-vanishing-gradients",
    "href": "notes/c3w3/index.html#rnns-and-vanishing-gradients",
    "title": "LSTMs and Named Entity Recognition",
    "section": "RNNs and Vanishing Gradients",
    "text": "RNNs and Vanishing Gradients\nAdvantages of RNNs RNNs allow us to capture dependancies within a short range and they take up less RAM than other n-gram models.\nDisadvantages of RNNs RNNs struggle with longer term dependencies and are very prone to vanishing or exploding gradients.\n\nNote that as we are back-propagating through time, we end up getting the following:\n\nNote that the sigmoid and tanh functions are bounded by 0 and 1 and -1 and 1 respectively. This eventually leads us to a problem. If we have many numbers that are less than |1|, then as we go through many layers, and we take the product of those numbers, we eventually end up getting a gradient that is very close to 0. This introduces the problem of vanishing gradients.\nSolutions to Vanishing Gradient Problems",
    "crumbs": [
      "Home",
      "Sequence Models",
      "LSTMs and Named Entity Recognition",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w3/index.html#optional-intro-to-optimization-in-deep-learning-gradient-descent",
    "href": "notes/c3w3/index.html#optional-intro-to-optimization-in-deep-learning-gradient-descent",
    "title": "LSTMs and Named Entity Recognition",
    "section": "(Optional) Intro to optimization in deep learning: Gradient Descent",
    "text": "(Optional) Intro to optimization in deep learning: Gradient Descent\nCheck out this blog from Paperspace.io if you’re interested in understanding in more depth some of the challenges in gradient descent.\n\n\nVisual Loss Landscapes For Neural Nets (Paper)",
    "crumbs": [
      "Home",
      "Sequence Models",
      "LSTMs and Named Entity Recognition",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w3/index.html#lab-lecture-notebook-vanishing-gradients",
    "href": "notes/c3w3/index.html#lab-lecture-notebook-vanishing-gradients",
    "title": "LSTMs and Named Entity Recognition",
    "section": "Lab: Lecture Notebook: Vanishing Gradients",
    "text": "Lab: Lecture Notebook: Vanishing Gradients\nVanishing Gradients",
    "crumbs": [
      "Home",
      "Sequence Models",
      "LSTMs and Named Entity Recognition",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w3/index.html#introduction-to-lstms",
    "href": "notes/c3w3/index.html#introduction-to-lstms",
    "title": "LSTMs and Named Entity Recognition",
    "section": "Introduction to LSTMs",
    "text": "Introduction to LSTMs\nThe LSTM allows your model to remember and forget certain inputs. It consists of a cell state and a hidden state with three gates. The gates allow the gradients to flow unchanged. We can think of the three gates as follows:\nInput gate: tells we how much information to input at any time point.\nForget gate: tells we how much information to forget at any time point.\nOutput gate: tells we how much information to pass over at any time point.\nThere are many applications we can use LSTMs for, such as:",
    "crumbs": [
      "Home",
      "Sequence Models",
      "LSTMs and Named Entity Recognition",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w3/index.html#optional-understanding-lstms",
    "href": "notes/c3w3/index.html#optional-understanding-lstms",
    "title": "LSTMs and Named Entity Recognition",
    "section": "(Optional) Understanding LSTMs",
    "text": "(Optional) Understanding LSTMs\nHere’s a classic post on LSTMs with intuitive explanations and diagrams, to complement this week’s material.\n\nUnderstanding LSTM Networks",
    "crumbs": [
      "Home",
      "Sequence Models",
      "LSTMs and Named Entity Recognition",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w3/index.html#lstm-architecture",
    "href": "notes/c3w3/index.html#lstm-architecture",
    "title": "LSTMs and Named Entity Recognition",
    "section": "LSTM Architecture",
    "text": "LSTM Architecture\nThe LSTM architecture could get complicated and don’t worry about it if we do not understand it. I personally prefer looking at the equation, but I will try to give we a visualization for now and later this week we will take a look at the equations.\n\nNote that there is the cell state and the hidden state, and then there is the output state. The forget gate is the first activation in the drawing above. It makes use of the previous hidden state h^{&lt;t_0&gt;} and the input x^{&lt;t_0&gt;}. The input gate makes use of the next two activations, the sigmoid and the tanh. Finally the output gate makes use of the last activation and the tanh right above it. This is just an overview of the architecture, we will dive into the details once we introduce the equations.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "LSTMs and Named Entity Recognition",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w3/index.html#introduction-to-named-entity-recognition",
    "href": "notes/c3w3/index.html#introduction-to-named-entity-recognition",
    "title": "LSTMs and Named Entity Recognition",
    "section": "Introduction to Named Entity Recognition",
    "text": "Introduction to Named Entity Recognition\nNamed Entity Recognition (NER) locates and extracts predefined entities from text. It allows we to find places, organizations, names, time and dates. Here is an example of the model we will be building:\n\nNER systems are being used in search efficiency, recommendation engines, customer service, automatic trading, and many more.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "LSTMs and Named Entity Recognition",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w3/index.html#lstm-equations-optional",
    "href": "notes/c3w3/index.html#lstm-equations-optional",
    "title": "LSTMs and Named Entity Recognition",
    "section": "LSTM equations (Optional)",
    "text": "LSTM equations (Optional)\nThese are the LSTM equations related to the gates I had previously spoken about:\n\nf = \\sigma(W_f[h_{t-1}; x_t] + b_f) \\qquad \\text{Forget}\n\\tag{1}\n\ni = \\sigma(W_i[h_{t-1}; x_t] + b_i) \\qquad \\text{Input}\n\\tag{2}\n\ng = \\tanh(W_g[h_{t-1}; x_t] + b_g) \\qquad \\text{Gate}\n\\tag{3}\n\nc_t = f \\odot c_{t-1} + i \\odot g \\qquad \\text{Cell State}\n\\tag{4}\n\no = \\sigma(W_o[h_{t-1}; x_t] + b_o) \\qquad \\text{Output}\n\\tag{5}\nWe can think of:\n\nThe forget gate as a gate that tells we how much information to forget,\nThe input gate, tells we how much information to pick up.\nThe gate gate as the gate containing information. This is multiplied by the input gate (which tells we how much of that information to keep).",
    "crumbs": [
      "Home",
      "Sequence Models",
      "LSTMs and Named Entity Recognition",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w3/index.html#training-ners-data-processing",
    "href": "notes/c3w3/index.html#training-ners-data-processing",
    "title": "LSTMs and Named Entity Recognition",
    "section": "Training NERs: Data Processing",
    "text": "Training NERs: Data Processing\nProcessing data is one of the most important tasks when training AI algorithms. For NER, we have to:\n\nConvert words and entity classes into arrays:\nPad with tokens: Set sequence length to a certain number and use the  token to fill empty spaces\nCreate a data generator:\n\nOnce we have that, we can assign each class a number, and each word a number.\n\n\n\n\nData Processing\n\nTraining an NER system:\n\nCreate a tensor for each input and its corresponding number\nPut them in a batch ==&gt; 64, 128, 256, 512 …\nFeed it into an LSTM unit\nRun the output through a dense layer\nPredict using a log softmax over K classes\n\nHere is an example of the architecture:\n\nNote that this is just one example of an NER system. Different architectures are possible.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "LSTMs and Named Entity Recognition",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w3/index.html#long-short-term-memory-deep-learning-specialization-c5",
    "href": "notes/c3w3/index.html#long-short-term-memory-deep-learning-specialization-c5",
    "title": "LSTMs and Named Entity Recognition",
    "section": "Long Short-Term Memory (Deep Learning Specialization C5)",
    "text": "Long Short-Term Memory (Deep Learning Specialization C5)\nNote: this section is based on a transcript of the video from the Deep Learning Specialization.\n\n\n\n\n\n\n\nLSTM from GRU or LSTM from RNNs\n\n\n\nIn this video Andrew Ng explains the Long Short-Term Memory (LSTM) as if it was developed from the GRU rather than RNNs. Sure the GRU has one less equation but its just about as complicated as the LSTM … The people who came up with it had 20 years to understand LSTM before they figured these out. We on the other hand have covered neither RNNs not GRUs and have none of that intuition they provide.\nLSTM also have a number of variations, such as the peephole connection, the carousal connection, and the coupled forget and input gates. 😱\nIn the other course on Deep Learning, Ng builds things up using a number of videos starting with notation. RNNs, different types of RNNs, and then the LSTM. The vanishing gradient problem with RNN and then the GRUs.\nI find the notation used very annoying but at least it is explained in the other course and seems to be motivated by time series. In Rnns we process the data in two dimensions. One is for the sequence index used in the super script. The other is for applying multiple layers which we don’t seem to consider.\nRnns learn weights more weights as the sequence grows and though not clear these weights are shared across the RNN units.\n\n\n\n\n\n\n\n\nNotation\n\n\n\n\nX^{(i)&lt;t&gt;} is the input at time t for the ith example.\ny^{(i)&lt;t&gt;} is the output at time t for the ith example.\nT_x^{(i)} is the length of the input sequence for the ith example.\nT_y^{(i)} is the length of the output sequence for the ith example.\nis $T_x^(i) = T_y^(i) not necessarily. (e.g. translation can be longer or shorter, while NER can be one to one.). They will be different for different examples.\nis T_x^(i) = T_x^(j) unlikely as the length of the input sequence will vary from example to example.\n\n\n\n\n\n\n\nGRU v.s. LSTM\n\n\n\n\nLSTM equations\n\n\n\n\nLSTM Schematic\n\n\n\n\nLSTM Rollout\n\n\n\n\nNow that we understand my caveat let’s try to understand by filling the gaps as we go along with what what Ng says and shows!\n\nIf we understand the math we are good to go. Code is just a translation of the math into a programming language. Once we understand the math there are deeper levels of understanding that we can get to but we can’t get there without understanding the math. There are three challenges to understanding the math!\n\nBoth the LSTM and GRU are RNNs so they translate sequences to sequences in the general case let’s imagine we are translating english to german. We start with some input x_0 and we output some out put y_0. For the next word we want to use the previous word to help us translate the next word. This is done by concatenating the current word to the previous (hidden) state. The first take away\nRNN have an internal nonlinearity a tanh but no gating mechanism. The non-linearity is applied to the hidden state concatenated to the input. The hidden state is thus the long term memory of the RNN. For NER we only need a short context to decide but for translation we need to be aware longer context, perhaps a few sentences back. [RNNS, GRUs and LSTMs also have the non-linearity at their core]. That’s the second take away about the math of LSTMS\nThe vanishing and exploding gradients are not the only issues in RNNs there is also a problem of accessing data from many time steps back. By accessing I mean backpropagating the gradients backwards enough time steps. In the LSTM there are not only pathways that let the state pass on unchanged they also allow the gradients to flow back unchanged similar to ResNets. So the two path from Hidden state to hidden state and from Internal State to internal state are what allows the LSTM to handle long term gradient gradients better than RNNS. This is particularly true for most cases where there is a ‘short circuit’ allowing these state to persist. And has a stabilizing effect. That is the third take away from the math of LSTMs.\nThe update, forget, output gates is where the weight and biases are used, thus this is where the learning is taking place and this is happening in an element-wise manner. This is the fourth take away from the math of LSTMs.\nThese three gates also control how much of the new data is incorporated into the internal state c^{&lt;t&gt;} and the hidden state. This is referred to as the gating mechanism. And different variants of LSTM and GRUS make subtle changes to the gating. This is the fifth take away from the math of LSTMs.\nInformation flow in the LST is captured by the dependency between the equations is as follows:\n\nf_t, i_t, g_t, o_t the gate uses see the old stat and the new input.\nc_t the internal state sees f_t, i_t, and g_t and the old state c_t\nh_t sees o_t and c_t which depend on the previous hidden state h_{t-1} and the new input x_t. To sum up the gates only depend on the input an the previous hidden state. The internal state depends on the gates and the previous internal state. The next hidden state depends on the internal state and the output gate.. This is the sixth take away from the math of LSTMs.\n\n\n\n\n\\begin{aligned}\n\\textcolor{red}{f_t} &= \\textcolor{purple}{\\sigma}(\\textcolor{blue}{W_f}[h_{t-1}; x_t] + \\textcolor{blue}{b_f}) \\\\\n\\textcolor{red}{i_t} &= \\textcolor{purple}{\\sigma}(\\textcolor{blue}{W_i}[h_{t-1}; x_t] + \\textcolor{blue}{b_i}) \\\\\n\\textcolor{red}{g_t} &= \\textcolor{purple}{\\tanh}(\\textcolor{blue}{W_g}[h_{t-1}; x_t] + \\textcolor{blue}{b_g})  \\\\\n\\textcolor{red}{o_t} &= \\textcolor{purple}{\\sigma}(\\textcolor{blue}{W_o}[h_{t-1}; x_t] + \\textcolor{blue}{b_o}) \\\\\n\\textcolor{green}{c_t} &= \\textcolor{red}{f_t} \\textcolor{orange}{\\odot} \\textcolor{green}{c_{t-1}} + \\textcolor{red}{i_t} \\textcolor{orange}{\\odot} \\textcolor{red}{g_t}     \\\\\nh_t &= \\textcolor{red}{o_t} \\textcolor{orange}{\\odot} \\textcolor{purple}{\\tanh}(\\textcolor{green}{c_t})\n\\end{aligned}\n\\tag{6}\nkey:\n\nred for gates\nblue for weights\norange for element-wise operations\ngreen for the internal state\npurple for the non-linearity \n\nNext level of understanding is to consider the action of the gating machanism and the relation between internal state and hidden state.\n\n\nThe key things from this unit are that the GRU does not use a forget gate, but uses 1-\\Gamma_u to decide how much of the previous memory cell to keep. In the LSTM, the forget gate instead.\nThere are two aspects to understanding these RNNS.\nThe equations look like simultaneous equations, in reality they are they have a more complex structure as\nThe schematic are emphesise a two other aspects of the LSTM, information flow and gating mechanisms.\n\nhow the equations are wired up to control the information flow and - the idea that we have a gating mechanism that combines the long term memory a in the Hidden state and the uses the memory cell, rather than the hidden state.\n\nWe learned about the GRU, or gated recurrent units, and how that can allow we to learn very long range connections in a sequence. The other type of unit that allows we to do this very well is the LSTM or the long short term memory units, and this is even more powerful than the GRU. Let’s take a look.\nHere are the equations from the previous video for the GRU. And for the GRU, we had a^{&lt;t&gt;} = c^{&lt;t&gt;}, and two gates, the optic gate and the relevance gate, \\tilde{c}^{&lt;t&gt;}, which is a candidate for replacing the memory cell, and then we use the update gate, \\Gamma_u, to decide whether or not to update c^{&lt;t&gt;} using \\tilde{c}^{&lt;t&gt;}.\nThe LSTM is an even slightly more powerful and more general version of the GRU, and is due to Sepp Hochreiter and Jurgen Schmidhuber1, c.f. Hochreiter and Schmidhuber (1997). And this was a really seminal paper, a huge impact on sequence modelling.\n1 with many interesting talks onlineI think this paper is one of the more difficult to read. It goes quite along into theory of vanishing gradients. And so, I think more people have learned about the details of LSTM through maybe other places than from this particular paper even though I think this paper has had a wonderful impact on the Deep Learning community.\nBut these are the equations that govern the LSTM. So, we continue to the memory cell, c, and the candidate value for updating it, \\tilde{c}^{&lt;t&gt;}, will be this, and so on. Notice that for the LSTM, we will no longer have the case that a^{&lt;t&gt;} =c^{&lt;t&gt;}. So, this is what we use. And so, this is just like the equation on the left except that with now, more specially use a^{&lt;t&gt;} there or a^{&lt;t-1&gt;} instead of c^{&lt;t-1&gt;}. And we’re not using this gamma or this relevance gate. Although we could have a variation of the LSTM where we put that back in, but with the more common version of the LSTM, doesn’t bother with that. And then we will have an update gate, same as before. So, W updates and we’re going to use a^{&lt;t-1&gt;} here, x^{&lt;t&gt;} + b_u. And one new property of the LSTM is, instead of having one update gate control, both of these terms, we’re going to have two separate terms. So instead of gamma_u and one minus gamma_u, we’re going have \\Gamma_u here. And forget gate, which we’re going to call \\Gamma_f. So, this gate, \\Gamma_f, is going to be sigmoid of pretty much what you’d expect, x^{&lt;t&gt;}+ b_f. And then, we’re going to have a new output gate which is \\sigma(W_o)+ b_o. And then, the update value to the memory so will be c^{&lt;t&gt;}=\\Gamma_u. And this asterisk denotes element-wise multiplication. This is a vector-vector element-wise multiplication, plus, and instead of one minus gamma u, we’re going to have a separate forget gate, \\Gamma_f * c^{&lt;t-1&gt;}. So this gives the memory cell the option of keeping the old value c^{t-1} and then just adding to it, this new value, \\tilde{c}^{&lt;t&gt;}. So, use a separate update and forget gates. So, this stands for update, forget, and output gate.\nAnd then finally, instead of \na^{&lt;t&gt;} = c^{&lt;t&gt;} \\quad \\text{(GRU)} \\qquad a^{&lt;t&gt;} = \\Gamma_0 * \\tanh( c^{&lt;t&gt;}) \\quad \\text{(LSTM)}\n.\nSo, these are the equations that govern the LSTM and we can tell it has three gates instead of two. So, it’s a bit more complicated and it places the gates into slightly different places. So, here again are the equations governing the behavior of the LSTM.\nOnce again, it’s traditional to explain these things using pictures. So let me draw one here. And if these pictures are too complicated, don’t worry about it. I personally find the equations easier to understand than the picture. But I’ll just show the picture here for the intuitions it conveys. The bigger picture here was very much inspired by a blog post due to Chris Ola, titled Understanding LSTM Network, and the diagram drawing here is quite similar to one that he drew in his blog post. But the key thing is to take away from this picture are maybe that we use a^{&lt;t-1&gt;} and x^{&lt;t&gt;} to compute all the gate values. In this picture, we have a^{&lt;t-1&gt;}, x^{&lt;t&gt;} coming together to compute the forget gate, to compute the update gates, and to compute output gate. And they also go through a tanh to compute \\tilde{c}^{&lt;t&gt;}. And then these values are combined in these complicated ways with element-wise multiplies and so on, to get c^{&lt;t&gt;} from the previous c^{&lt;t-1&gt;}. Now, one element of this is interesting as we have a bunch of these in parallel. So, that’s one of them and we connect them. We then connect these temporally. So it does the input x^{&lt;1&gt;} then x^{&lt;2&gt;}, x^{&lt;3&gt;}. So, we can take these units and just hold them up as follows, where the output a at the previous timestep is the input a at the next timestep, the c. I’ve simplified to diagrams a little bit in the bottom. And one cool thing about this you’ll notice is that there’s this line at the top that shows how, so long as we set the forget and the update gate appropriately, it is relatively easy for the LSTM to have some value c_0 and have that be passed all the way to the right to have your, maybe, c^{&lt;3&gt;}=c^{&lt;0&gt;}. And this is why the LSTM, as well as the GRU, is very good at memorizing certain values even for a long time, for certain real values stored in the memory cell even for many, many timesteps.\nSo, that’s it for the LSTM.\nAs we can imagine, there are also a few variations on this that people use. Perhaps, the most common one is that instead of just having the gate values be dependent only on a_{t-1}, x^{&lt;t&gt;}, sometimes, people also sneak in there the values c_{t-1} as well. This is called a peephole connection, introduced in Gers and Schmidhuber (2000) Not a great name maybe but you’ll see, peephole connection. What that means is that the gate values may depend not just on a^{&lt;t-1&gt;} and on x^{&lt;t&gt;}, but also on the previous memory cell value, and the peephole connection can go into all three of these gates’ computations. So that’s one common variation we see of LSTMs. One technical detail is that these are, say, 100-dimensional vectors. So if we have a 100-dimensional hidden memory cell unit, and so is this. And the, say, fifth element of c^{t-1} affects only the fifth element of the corresponding gates, so that relationship is one-to-one, where not every element of the 100-dimensional c^{&lt;t-1&gt;} can affect all elements of the case. But instead, the first element of c^{&lt;t-1&gt;} affects the first element of the case, second element affects the second element, and so on. But if we ever read the paper and see someone talk about the peephole connection, that’s when they mean that c^{&lt;t-1&gt;} is used to affect the gate value as well. So, that’s it for the LSTM.\nWhen should we use a GRU? And when should we use an LSTM?\nThere isn’t widespread consensus in this. And even though I presented GRUs first, in the history of deep learning, LSTMs actually came much earlier, and then GRUs were relatively recent invention that were maybe derived as Pavia’s simplification of the more complicated LSTM model. Researchers have tried both of these models on many different problems, and on different problems, different algorithms will win out. So, there isn’t a universally-superior algorithm which is why I want to show we both of them. But I feel like when I am using these, the advantage of the GRU is that it’s a simpler model and so it is actually easier to build a much bigger network, it only has two gates, so computationally, it runs a bit faster. So, it scales the building somewhat bigger models but the LSTM is more powerful and more effective since it has three gates instead of two. If we want to pick one to use, I think LSTM has been the historically more proven choice. So, if we had to pick one, I think most people today will still use the LSTM as the default first thing to try. Although, I think in the last few years, GRUs had been gaining a lot of momentum and I feel like more and more teams are also using GRUs because they’re a bit simpler but often work just as well. It might be easier to scale them to even bigger problems. So, that’s it for LSTMs. Well, either GRUs or LSTMs, you’ll be able to build neural network that can capture much longer range dependencies.\nThe correct version of the final equation in the output gate is here:\nhttps://www.coursera.org/learn/nlp-sequence-models/supplement/xdv6z/long-short-term-memory-lstm-correction",
    "crumbs": [
      "Home",
      "Sequence Models",
      "LSTMs and Named Entity Recognition",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w3/index.html#computing-accuracy",
    "href": "notes/c3w3/index.html#computing-accuracy",
    "title": "LSTMs and Named Entity Recognition",
    "section": "Computing Accuracy",
    "text": "Computing Accuracy\nTo compare the accuracy, just follow the following steps:\n\nPass test set through the model\nGet arg max across the prediction array\nMask padded tokens\nCompare with the true labels.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "LSTMs and Named Entity Recognition",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w3/index.html#reflections-on-this-unit",
    "href": "notes/c3w3/index.html#reflections-on-this-unit",
    "title": "LSTMs and Named Entity Recognition",
    "section": "Reflections on this unit",
    "text": "Reflections on this unit\n\n\n\n\n\n\nMain Research Questions\n\n\n\n\nWhat is the vanishing and exploding gradient problem in RNNs?\nHow can we measure the ability of RNN to access data from many time steps back?\nWhat is the nature of the hidden state in RNNs?\n\n\nshort term memory\n\n\nWhat is the nature of the internal state in RNNs?\n\n\nlong term memory\n\n\nHow are gradients updated in the LSTMs?\nwhat is the constant error carousel in LSTMs?\nhow does it solve the vanishing gradient problem?\nHow does gating work in LSTMs?\nAre the gates binary?\nwhat is the idea behind a peekhole LSTM \nwhat is the idea of the bLSTM \nAre all LSTMs stacked, cam we have a single layer LSTM?",
    "crumbs": [
      "Home",
      "Sequence Models",
      "LSTMs and Named Entity Recognition",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w3/index.html#resources",
    "href": "notes/c3w3/index.html#resources",
    "title": "LSTMs and Named Entity Recognition",
    "section": "Resources:",
    "text": "Resources:\n\n(Chadha 2020) Aman Chadha’s Notes\nIbrahim Jelliti’s Notes\nIntro to optimization in deep learning: Gradient Descent (Tutorial) Ayoosh Kathuria\nVisual Loss Landscapes For Neural Nets (Paper)\nArticle on Learning Rate Schedules by Hafidz Zulkifli.\nStochastic Weight Averaging (Paper)\nUnderstanding LSTM Networks\nThe Unreasonable Effectiveness of Recurrent Neural Networks by Andrej Karpathy",
    "crumbs": [
      "Home",
      "Sequence Models",
      "LSTMs and Named Entity Recognition",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w1/lab02.html",
    "href": "notes/c2w1/lab02.html",
    "title": "Candidates from String Edits",
    "section": "",
    "text": "course banner\n\n\nEstimated Time: 20 minutes\nCreate a list of candidate strings by applying an edit operation\n\nImports and Data\n\n# data\nword = 'dearz' # 🦌\n\n\n\nSplits\nFind all the ways we can split a word into 2 parts !\n\n# splits with a loop\nsplits_a = []\nfor i in range(len(word)+1):\n    splits_a.append([word[:i],word[i:]])\n\nfor i in splits_a:\n    print(i)\n\n['', 'dearz']\n['d', 'earz']\n['de', 'arz']\n['dea', 'rz']\n['dear', 'z']\n['dearz', '']\n\n\n\n# same splits, done using a list comprehension\nsplits_b = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n\nfor i in splits_b:\n    print(i)\n\n('', 'dearz')\n('d', 'earz')\n('de', 'arz')\n('dea', 'rz')\n('dear', 'z')\n('dearz', '')\n\n\n\n\nDelete Edit\nDelete a letter from each string in the splits list.\nWhat this does is effectivly delete each possible letter from the original word being edited.\n\n# deletes with a loop\nsplits = splits_a\ndeletes = []\n\nprint('word : ', word)\nfor L,R in splits:\n    if R:\n        print(L + R[1:], ' &lt;-- delete ', R[0])\n\nword :  dearz\nearz  &lt;-- delete  d\ndarz  &lt;-- delete  e\nderz  &lt;-- delete  a\ndeaz  &lt;-- delete  r\ndear  &lt;-- delete  z\n\n\nIt’s worth taking a closer look at how this is excecuting a ‘delete’.\nTaking the first item from the splits list :\n\n# breaking it down\nprint('word : ', word)\none_split = splits[0]\nprint('first item from the splits list : ', one_split)\nL = one_split[0]\nR = one_split[1]\nprint('L : ', L)\nprint('R : ', R)\nprint('*** now implicit delete by excluding the leading letter ***')\nprint('L + R[1:] : ',L + R[1:], ' &lt;-- delete ', R[0])\n\nword :  dearz\nfirst item from the splits list :  ['', 'dearz']\nL :  \nR :  dearz\n*** now implicit delete by excluding the leading letter ***\nL + R[1:] :  earz  &lt;-- delete  d\n\n\nSo the end result transforms ‘dearz’ to ‘earz’ by deleting the first character.\nAnd we use a loop (code block above) or a list comprehension (code block below) to do this for the entire splits list.\n\n# deletes with a list comprehension\nsplits = splits_a\ndeletes = [L + R[1:] for L, R in splits if R]\n\nprint(deletes)\nprint('*** which is the same as ***')\nfor i in deletes:\n    print(i)\n\n['earz', 'darz', 'derz', 'deaz', 'dear']\n*** which is the same as ***\nearz\ndarz\nderz\ndeaz\ndear\n\n\n\n\nUngraded Exercise\nWe now have a list of candidate strings created after performing a delete edit.  Next step will be to filter this list for candidate words found in a vocabulary.  Given the example vocab below, can we think of a way to create a list of candidate words ?  Remember, we already have a list of candidate strings, some of which are certainly not actual words we might find in your vocabulary !   So from the above list earz, darz, derz, deaz, dear.  You’re really only interested in dear.\n\nvocab = ['dean','deer','dear','fries','and','coke']\nedits = list(deletes)\n\nprint('vocab : ', vocab)\nprint('edits : ', edits)\n\ncandidates=[]\n\n### START CODE HERE ###\ncandidates = set(vocab).intersection(edits)  # hint: 'set.intersection'\n### END CODE HERE ###\n\nprint('candidate words : ', candidates)\n\nvocab :  ['dean', 'deer', 'dear', 'fries', 'and', 'coke']\nedits :  ['earz', 'darz', 'derz', 'deaz', 'dear']\ncandidate words :  {'dear'}\n\n\nExpected Outcome:\nvocab : [‘dean’, ‘deer’, ‘dear’, ‘fries’, ‘and’, ‘coke’]\nedits : [‘earz’, ‘darz’, ‘derz’, ‘deaz’, ‘dear’]\ncandidate words : {‘dear’}\n\n\nSummary\nYou’ve unpacked an integral part of the assignment by breaking down splits and edits, specifically looking at deletes here.\nImplementation of the other edit types (insert, replace, switch) follows a similar methodology and should now feel somewhat familiar when we see them.\nThis bit of the code isn’t as intuitive as other sections, so well done!\nWe should now feel confident facing some of the more technical parts of the assignment at the end of the week.\n\n\n\n\nCitationBibTeX citation:@online{bochman2020,\n  author = {Bochman, Oren},\n  title = {Candidates from {String} {Edits}},\n  date = {2020-10-17},\n  url = {https://orenbochman.github.io/notes-nlp/notes/c2w1/lab02.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2020. “Candidates from String Edits.”\nOctober 17, 2020. https://orenbochman.github.io/notes-nlp/notes/c2w1/lab02.html.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocorrect & Dynamic Programming",
      "L2 - Candidates from String Edits"
    ]
  },
  {
    "objectID": "notes/c2w1/lab03.html",
    "href": "notes/c2w1/lab03.html",
    "title": "Assignment 1: Auto Correct",
    "section": "",
    "text": "course banner\n\n\n\n\nFigure 1\nWelcome to the first assignment of Course 2. This assignment will give we a chance to brush up on your python and probability skills. In doing so, we will implement an auto-correct system that is very effective and useful."
  },
  {
    "objectID": "notes/c2w1/lab03.html#outline",
    "href": "notes/c2w1/lab03.html#outline",
    "title": "Assignment 1: Auto Correct",
    "section": "Outline",
    "text": "Outline\n\n0. Overview\n\n0.1 Edit Distance\n\n1. Data Preprocessing\n\n1.1 Exercise 1\n1.2 Exercise 2\n1.3 Exercise 3\n\n2. String Manipulation\n\n2.1 Exercise 4\n2.2 Exercise 5\n2.3 Exercise 6\n2.4 Exercise 7\n\n3. Combining the edits\n\n3.1 Exercise 8\n3.2 Exercise 9\n3.3 Exercise 10\n\n4. Minimum Edit Distance\n\n4.1 Exercise 11\n\n5. Backtrace (Optional)\n\n ## 0. Overview\nWe use autocorrect every day on your cell phone and computer. In this assignment, we will explore what really goes on behind the scenes. Of course, the model we are about to implement is not identical to the one used in your phone, but it is still quite good.\nBy completing this assignment we will learn how to:\n\nGet a word count given a corpus\nGet a word probability in the corpus\nManipulate strings\nFilter strings\nImplement Minimum edit distance to compare strings and to help find the optimal path for the edits.\nUnderstand how dynamic programming works\n\nSimilar systems are used everywhere. - For example, if we type in the word “I am lerningg”, chances are very high that we meant to write “learning”, as shown in Figure 1.\n\n Figure 1\n\n #### 0.1 Edit Distance\nIn this assignment, we will implement models that correct words that are 1 and 2 edit distances away. - We say two words are n edit distance away from each other when we need n edits to change one word into another.\nAn edit could consist of one of the following options:\n\nDelete (remove a letter): ‘hat’ =&gt; ‘at, ha, ht’\nSwitch (swap 2 adjacent letters): ‘eta’ =&gt; ‘eat, tea,…’\nReplace (change 1 letter to another): ‘jat’ =&gt; ‘hat, rat, cat, mat, …’\nInsert (add a letter): ‘te’ =&gt; ‘the, ten, ate, …’\n\nWe will be using the four methods above to implement an Auto-correct. - To do so, we will need to compute probabilities that a certain word is correct given an input.\nThis auto-correct we are about to implement was first created by Peter Norvig in 2007. - His original article may be a useful reference for this assignment.\nThe goal of our spell check model is to compute the following probability:\nP(c|w) = \\frac{P(w|c)\\times P(c)}{P(w)} \\tag{Eqn-1}\nThe equation above is Bayes Rule. - Equation 1 says that the probability of a word being correct $P(c|w) $is equal to the probability of having a certain word w, given that it is correct P(w|c), multiplied by the probability of being correct in general P(C) divided by the probability of that word w appearing P(w) in general. - To compute equation 1, we will first import a data set and then create all the probabilities that we need using that data set.\n # Part 1: Data Preprocessing\n\nimport re\nfrom collections import Counter\nimport numpy as np\nimport pandas as pd\n\nAs in any other machine learning task, the first thing we have to do is process your data set. - Many courses load in pre-processed data for you. - However, in the real world, when we build these NLP systems, we load the datasets and process them. - So let’s get some real world practice in pre-processing the data!\nYour first task is to read in a file called ‘shakespeare.txt’ which is found in your file directory. To look at this file we can go to File ==&gt; Open.\n ### Exercise 1 Implement the function process_data which\n\nReads in a corpus (text file)\nChanges everything to lowercase\nReturns a list of words.\n\n\nOptions and Hints\n\nIf we would like more of a real-life practice, don’t open the ‘Hints’ below (yet) and try searching the web to derive your answer.\nIf we want a little help, click on the green “General Hints” section by clicking on it with your mouse.\nIf we get stuck or are not getting the expected results, click on the green ‘Detailed Hints’ section to get hints for each step that you’ll take to complete this function.\n\n\n\nGeneral Hints\n\n\nGeneral Hints to get started\n\n\nPython input and output\n\n\nPython ‘re’ documentation \n\n\n\n\n\nDetailed Hints\n\n\nDetailed hints if you’re stuck\n\n\nUse ‘with’ syntax to read a file\n\n\nDecide whether to use ‘read()’ or ’readline(). What’s the difference?\n\n\nChoose whether to use either str.lower() or str.lowercase(). What is the difference?\n\n\nUse re.findall(pattern, string)\n\n\nLook for the “Raw String Notation” section in the Python ‘re’ documentation to understand the difference between r’‘, r’’ and ‘\\W’.\n\n\nFor the pattern, decide between using ‘’, ‘’, ‘+’ or ‘+’. What do we think are the differences?\n\n\n\n\n# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: process_data\ndef process_data(file_name):\n    \"\"\"\n    Input: \n        A file_name which is found in your current directory. We just have to read it in. \n    Output: \n        words: a list containing all the words in the corpus (text file we read) in lower case. \n    \"\"\"\n    words = [] # return this variable correctly\n\n    ### START CODE HERE ### \n    words = re.findall(r'\\w+',open(file_name).read().lower())\n    ### END CODE HERE ###\n    \n    return words\n\nNote, in the following cell, ‘words’ is converted to a python set. This eliminates any duplicate entries.\n\n#DO NOT MODIFY THIS CELL\nword_l = process_data('shakespeare.txt')\nvocab = set(word_l)  # this will be your new vocabulary\nprint(f\"The first ten words in the text are: \\n{word_l[0:10]}\")\nprint(f\"There are {len(vocab)} unique words in the vocabulary.\")\n\nThe first ten words in the text are: \n['o', 'for', 'a', 'muse', 'of', 'fire', 'that', 'would', 'ascend', 'the']\nThere are 6116 unique words in the vocabulary.\n\n\n\n\nExpected Output\nThe first ten words in the text are: \n['o', 'for', 'a', 'muse', 'of', 'fire', 'that', 'would', 'ascend', 'the']\nThere are 6116 unique words in the vocabulary.\n ### Exercise 2\nImplement a get_count function that returns a dictionary - The dictionary’s keys are words - The value for each word is the number of times that word appears in the corpus.\nFor example, given the following sentence: “I am happy because I am learning”, your dictionary should return the following:\n\n\n\nKey \n\n\nValue \n\n\n\n\nI\n\n\n2\n\n\n\n\nam\n\n\n2\n\n\n\n\nhappy\n\n\n1\n\n\n\n\nbecause\n\n\n1\n\n\n\n\nlearning\n\n\n1\n\n\n\nInstructions: Implement a get_count which returns a dictionary where the key is a word and the value is the number of times the word appears in the list.\n\n\nHints\n\n\n\n\nTry implementing this using a for loop and a regular dictionary. This may be good practice for similar coding interview questions\n\n\nWe can also use defaultdict instead of a regualr dictionary, along with the for loop\n\n\nOtherwise, to skip using a for loop, we can use Python’s  Counter class\n\n\n\n\n# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# UNIT TEST COMMENT: Candidate for Table Driven Tests\n# GRADED FUNCTION: get_count\ndef get_count(word_l):\n    '''\n    Input:\n        word_l: a set of words representing the corpus. \n    Output:\n        word_count_dict: The wordcount dictionary where key is the word and value is its frequency.\n    '''\n    \n    word_count_dict = {}  # fill this with word counts\n    ### START CODE HERE \n    word_count_dict = Counter(word_l)\n    ### END CODE HERE ### \n    return word_count_dict\n\n\n#DO NOT MODIFY THIS CELL\nword_count_dict = get_count(word_l)\nprint(f\"There are {len(word_count_dict)} key values pairs\")\nprint(f\"The count for the word 'thee' is {word_count_dict.get('thee',0)}\")\n\nThere are 6116 key values pairs\nThe count for the word 'thee' is 240\n\n\n\n\nExpected Output\nThere are 6116 key values pairs\nThe count for the word 'thee' is 240\n ### Exercise 3 Given the dictionary of word counts, compute the probability that each word will appear if randomly selected from the corpus of words.\nP(w_i) = \\frac{C(w_i)}{M} \\tag{Eqn-2} where\nC(w_i) is the total number of times w_i appears in the corpus.\nM is the total number of words in the corpus.\nFor example, the probability of the word ‘am’ in the sentence ‘I am happy because I am learning’ is:\nP(am) = \\frac{C(w_i)}{M} = \\frac {2}{7} \\tag{Eqn-3}.\nInstructions: Implement get_probs function which gives we the probability that a word occurs in a sample. This returns a dictionary where the keys are words, and the value for each word is its probability in the corpus of words.\n\n\nHints\n\n\nGeneral advice\n\n\nUse dictionary.values()\n\n\nUse sum()\n\n\nThe cardinality (number of words in the corpus should be equal to len(word_l). We will calculate this same number, but using the word count dictionary.\n\n\nIf you’re using a for loop:\n\n\nUse dictionary.keys()\n\n\nIf you’re using a dictionary comprehension:\n\n\nUse dictionary.items()\n\n\n\n\n# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: get_probs\ndef get_probs(word_count_dict):\n    '''\n    Input:\n        word_count_dict: The wordcount dictionary where key is the word and value is its frequency.\n    Output:\n        probs: A dictionary where keys are the words and the values are the probability that a word will occur. \n    '''\n    probs = {}  # return this variable correctly\n    \n    ### START CODE HERE ###\n    probs = {k:v/sum(word_count_dict.values()) for k, v in word_count_dict.items()}\n    ### END CODE HERE ###\n    return probs\n\n\n#DO NOT MODIFY THIS CELL\nprobs = get_probs(word_count_dict)\nprint(f\"Length of probs is {len(probs)}\")\nprint(f\"P('thee') is {probs['thee']:.4f}\")\n\nLength of probs is 6116\nP('thee') is 0.0045\n\n\n\n\nExpected Output\nLength of probs is 6116\nP('thee') is 0.0045\n # Part 2: String Manipulations\nNow, that we have computed P(w_i) for all the words in the corpus, we will write a few functions to manipulate strings so that we can edit the erroneous strings and return the right spellings of the words. In this section, we will implement four functions:\n\ndelete_letter: given a word, it returns all the possible strings that have one character removed.\nswitch_letter: given a word, it returns all the possible strings that have two adjacent letters switched.\nreplace_letter: given a word, it returns all the possible strings that have one character replaced by another different letter.\ninsert_letter: given a word, it returns all the possible strings that have an additional character inserted.\n\n\n\nList comprehensions\nString and list manipulation in python will often make use of a python feature called list comprehensions. The routines below will be described as using list comprehensions, but if we would rather implement them in another way, we are free to do so as long as the result is the same. Further, the following section will provide detailed instructions on how to use list comprehensions and how to implement the desired functions. If we are a python expert, feel free to skip the python hints and move to implementing the routines directly.\nPython List Comprehensions embed a looping structure inside of a list declaration, collapsing many lines of code into a single line. If we are not familiar with them, they seem slightly out of order relative to for loops.\n\n Figure 2\n\nThe diagram above shows that the components of a list comprehension are the same components we would find in a typical for loop that appends to a list, but in a different order. With that in mind, we’ll continue the specifics of this assignment. We will be very descriptive for the first function, deletes(), and less so in later functions as we become familiar with list comprehensions.\n ### Exercise 4\nInstructions for delete_letter(): Implement a delete_letter() function that, given a word, returns a list of strings with one character deleted.\nFor example, given the word nice, it would return the set: {‘ice’, ‘nce’, ‘nic’, ‘nie’}.\nStep 1: Create a list of ‘splits’. This is all the ways we can split a word into Left and Right: For example,\n’nice is split into : [('', 'nice'), ('n', 'ice'), ('ni', 'ce'), ('nic', 'e'), ('nice', '')] This is common to all four functions (delete, replace, switch, insert).\n\n Figure 3\n\nStep 2: This is specific to delete_letter. Here, we are generating all words that result from deleting one character.\nThis can be done in a single line with a list comprehension. We can make use of this type of syntax:\n[f(a,b) for a, b in splits if condition]\nFor our ‘nice’ example we get: [‘ice’, ‘nce’, ‘nie’, ‘nic’]\n\n Figure 4\n\n\n\nLevels of assistance\nTry this exercise with these levels of assistance.\n- We hope that this will make it both a meaningful experience but also not a frustrating experience. - Start with level 1, then move onto level 2, and 3 as needed.\n- Level 1. Try to think this through and implement this yourself.\n- Level 2. Click on the \"Level 2 Hints\" section for some hints to get started.\n- Level 3. If we would prefer more guidance, please click on the \"Level 3 Hints\" cell for step by step instructions.\n\nIf we are still stuck, look at the images in the “list comprehensions” section above.\n\n\n\nLevel 2 Hints\n\n\n\n\n Use array slicing like my_string[0:2] \n\n\n Use list comprehensions or for loops \n\n\n\n\n\nLevel 3 Hints\n\n\n\n\nsplits: Use array slicing, like my_str[0:2], to separate a string into two pieces.\n\n\nDo this in a loop or list comprehension, so that we have a list of tuples.\n\nFor example, “cake” can get split into “ca” and “ke”. They’re stored in a tuple (“ca”,“ke”), and the tuple is appended to a list. We’ll refer to these as L and R, so the tuple is (L,R)\n\n&lt;li&gt;When choosing the range for your loop, if we input the word \"cans\" and generate the tuple  ('cans',''), make sure to include an if statement to check the length of that right-side string (R) in the tuple (L,R) &lt;/li&gt;\n&lt;li&gt;deletes: Go through the list of tuples and combine the two strings together. We can use the + operator to combine two strings&lt;/li&gt;\n&lt;li&gt;When combining the tuples, make sure that we leave out a middle character.&lt;/li&gt;\n&lt;li&gt;Use array slicing to leave out the first character of the right substring.&lt;/li&gt;\n\n\n\n# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# UNIT TEST COMMENT: Candidate for Table Driven Tests\n# GRADED FUNCTION: deletes\ndef delete_letter(word, verbose=False):\n    '''\n    Input:\n        word: the string/word for which we will generate all possible words \n                in the vocabulary which have 1 missing character\n    Output:\n        delete_l: a list of all possible strings obtained by deleting 1 character from word\n    '''\n    \n    delete_l = []\n    split_l = []\n    \n    ### START CODE HERE ###\n    split_l = [(word[:i], word[i:]) for i in range(len(word))]\n    delete_l = [l+r[1:] for l, r in split_l]\n    ### END CODE HERE ###\n\n    if verbose: print(f\"input word {word}, \\nsplit_l = {split_l}, \\ndelete_l = {delete_l}\")\n\n    return delete_l\n\n\ndelete_word_l = delete_letter(word=\"cans\",\n                        verbose=True)\n\ninput word cans, \nsplit_l = [('', 'cans'), ('c', 'ans'), ('ca', 'ns'), ('can', 's')], \ndelete_l = ['ans', 'cns', 'cas', 'can']\n\n\n\n\nExpected Output\nNote: We might get a slightly different result with split_l\n\ninput word cans, \nsplit_l = [('', 'cans'), ('c', 'ans'), ('ca', 'ns'), ('can', 's')], \ndelete_l = ['ans', 'cns', 'cas', 'can']\n\n\nNote 1\n\nNotice how it has the extra tuple ('cans', '').\nThis will be fine as long as we have checked the size of the right-side substring in tuple (L,R).\nCan we explain why this will give we the same result for the list of deletion strings (delete_l)?\n\ninput word cans, \nsplit_l = [('', 'cans'), ('c', 'ans'), ('ca', 'ns'), ('can', 's'), ('cans', '')], \ndelete_l = ['ans', 'cns', 'cas', 'can']\n\n\nNote 2\nIf we end up getting the same word as your input word, like this:\ninput word cans, \nsplit_l = [('', 'cans'), ('c', 'ans'), ('ca', 'ns'), ('can', 's'), ('cans', '')], \ndelete_l = ['ans', 'cns', 'cas', 'can', 'cans']\n\nCheck how we set the range.\nSee if we check the length of the string on the right-side of the split.\n\n\n# test # 2\nprint(f\"Number of outputs of delete_letter('at') is {len(delete_letter('at'))}\")\n\nNumber of outputs of delete_letter('at') is 2\n\n\n\n\nExpected output\nNumber of outputs of delete_letter('at') is 2\n ### Exercise 5\nInstructions for switch_letter(): Now implement a function that switches two letters in a word. It takes in a word and returns a list of all the possible switches of two letters that are adjacent to each other. - For example, given the word ‘eta’, it returns {‘eat’, ‘tea’}, but does not return ‘ate’.\nStep 1: is the same as in delete_letter()\nStep 2: A list comprehension or for loop which forms strings by swapping adjacent letters. This is of the form:\n[f(L,R) for L, R in splits if condition] where ‘condition’ will test the length of R in a given iteration. See below.\n\n Figure 5\n\n\n\nLevels of difficulty\nTry this exercise with these levels of difficulty.\n- Level 1. Try to think this through and implement this yourself. - Level 2. Click on the “Level 2 Hints” section for some hints to get started. - Level 3. If we would prefer more guidance, please click on the “Level 3 Hints” cell for step by step instructions.\n\n\nLevel 2 Hints\n\n\n\n\n Use array slicing like my_string[0:2] \n\n\n Use list comprehensions or for loops \n\n\nTo do a switch, think of the whole word as divided into 4 distinct parts. Write out ‘cupcakes’ on a piece of paper and see how we can split it into (‘cupc’, ‘k’, ‘a’, ‘es’)\n\n\n\n\n\nLevel 3 Hints\n\n\n\n\nsplits: Use array slicing, like my_str[0:2], to separate a string into two pieces.\n\n\nSplitting is the same as for delete_letter\n\n\nTo perform the switch, go through the list of tuples and combine four strings together. We can use the + operator to combine strings\n\n\nThe four strings will be the left substring from the split tuple, followed by the first (index 1) character of the right substring, then the zero-th character (index 0) of the right substring, and then the remaining part of the right substring.\n\n\nUnlike delete_letter, we will want to check that your right substring is at least a minimum length. To see why, review the previous hint bullet point (directly before this one).\n\n\n\n\n# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# UNIT TEST COMMENT: Candidate for Table Driven Tests\n# GRADED FUNCTION: switches\ndef switch_letter(word, verbose=False):\n    '''\n    Input:\n        word: input string\n     Output:\n        switches: a list of all possible strings with one adjacent charater switched\n    ''' \n    \n    switch_l = []\n    split_l = []\n    \n    ### START CODE HERE ###\n    split_l = [(word[:i], word[i:]) for i in range(len(word))]\n    switch_l = [l+r[1]+r[0]+r[2:] for l, r in split_l if len(r) &gt; 1]\n    ### END CODE HERE ###\n    \n    if verbose: print(f\"Input word = {word} \\nsplit_l = {split_l} \\nswitch_l = {switch_l}\") \n\n    return switch_l\n\n\nswitch_word_l = switch_letter(word=\"eta\",\n                         verbose=True)\n\nInput word = eta \nsplit_l = [('', 'eta'), ('e', 'ta'), ('et', 'a')] \nswitch_l = ['tea', 'eat']\n\n\n\n\nExpected output\nInput word = eta \nsplit_l = [('', 'eta'), ('e', 'ta'), ('et', 'a')] \nswitch_l = ['tea', 'eat']\n\n\nNote 1\nWe may get this:\nInput word = eta \nsplit_l = [('', 'eta'), ('e', 'ta'), ('et', 'a'), ('eta', '')] \nswitch_l = ['tea', 'eat']\n\nNotice how it has the extra tuple ('eta', '').\nThis is also correct.\nCan we think of why this is the case?\n\n\n\nNote 2\nIf we get an error\nIndexError: string index out of range\n\nPlease see if we have checked the length of the strings when switching characters.\n\n\n# test # 2\nprint(f\"Number of outputs of switch_letter('at') is {len(switch_letter('at'))}\")\n\nNumber of outputs of switch_letter('at') is 1\n\n\n\n\nExpected output\nNumber of outputs of switch_letter('at') is 1\n ### Exercise 6 Instructions for replace_letter(): Now implement a function that takes in a word and returns a list of strings with one replaced letter from the original word.\nStep 1: is the same as in delete_letter()\nStep 2: A list comprehension or for loop which form strings by replacing letters. This can be of the form:\n[f(a,b,c) for a, b in splits if condition for c in string] Note the use of the second for loop.\nIt is expected in this routine that one or more of the replacements will include the original word. For example, replacing the first letter of ‘ear’ with ‘e’ will return ‘ear’.\nStep 3: Remove the original input letter from the output.\n\n\nHints\n\n\n\n\nTo remove a word from a list, first store its contents inside a set()\n\n\nUse set.discard(‘the_word’) to remove a word in a set (if the word does not exist in the set, then it will not throw a KeyError. Using set.remove(‘the_word’) throws a KeyError if the word does not exist in the set.\n\n\n\n\n# UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# UNIT TEST COMMENT: Candidate for Table Driven Tests\n# GRADED FUNCTION: replaces\ndef replace_letter(word, verbose=False):\n    '''\n    Input:\n        word: the input string/word \n    Output:\n        replaces: a list of all possible strings where we replaced one letter from the original word. \n    ''' \n    \n    letters = 'abcdefghijklmnopqrstuvwxyz'\n    replace_l = []\n    split_l = []\n    \n    ### START CODE HERE ###\n    split_l = [(word[:i], word[i:]) for i in range(len(word))]\n    replace_set = set([l+replace+r[1:] for l, r in split_l if len(r) &gt; 0 for replace in letters])\n    replace_set.discard(word)\n    ### END CODE HERE ###\n    \n    # turn the set back into a list and sort it, for easier viewing\n    replace_l = sorted(list(replace_set))\n    \n    if verbose: print(f\"Input word = {word} \\nsplit_l = {split_l} \\nreplace_l {replace_l}\")   \n    \n    return replace_l\n\n\nreplace_l = replace_letter(word='can',\n                              verbose=True)\n\nInput word = can \nsplit_l = [('', 'can'), ('c', 'an'), ('ca', 'n')] \nreplace_l ['aan', 'ban', 'caa', 'cab', 'cac', 'cad', 'cae', 'caf', 'cag', 'cah', 'cai', 'caj', 'cak', 'cal', 'cam', 'cao', 'cap', 'caq', 'car', 'cas', 'cat', 'cau', 'cav', 'caw', 'cax', 'cay', 'caz', 'cbn', 'ccn', 'cdn', 'cen', 'cfn', 'cgn', 'chn', 'cin', 'cjn', 'ckn', 'cln', 'cmn', 'cnn', 'con', 'cpn', 'cqn', 'crn', 'csn', 'ctn', 'cun', 'cvn', 'cwn', 'cxn', 'cyn', 'czn', 'dan', 'ean', 'fan', 'gan', 'han', 'ian', 'jan', 'kan', 'lan', 'man', 'nan', 'oan', 'pan', 'qan', 'ran', 'san', 'tan', 'uan', 'van', 'wan', 'xan', 'yan', 'zan']\n\n\n\n\nExpected Output**:\nInput word = can \nsplit_l = [('', 'can'), ('c', 'an'), ('ca', 'n')] \nreplace_l ['aan', 'ban', 'caa', 'cab', 'cac', 'cad', 'cae', 'caf', 'cag', 'cah', 'cai', 'caj', 'cak', 'cal', 'cam', 'cao', 'cap', 'caq', 'car', 'cas', 'cat', 'cau', 'cav', 'caw', 'cax', 'cay', 'caz', 'cbn', 'ccn', 'cdn', 'cen', 'cfn', 'cgn', 'chn', 'cin', 'cjn', 'ckn', 'cln', 'cmn', 'cnn', 'con', 'cpn', 'cqn', 'crn', 'csn', 'ctn', 'cun', 'cvn', 'cwn', 'cxn', 'cyn', 'czn', 'dan', 'ean', 'fan', 'gan', 'han', 'ian', 'jan', 'kan', 'lan', 'man', 'nan', 'oan', 'pan', 'qan', 'ran', 'san', 'tan', 'uan', 'van', 'wan', 'xan', 'yan', 'zan']\n\nNote how the input word ‘can’ should not be one of the output words.\n\n\n\nNote 1\nIf we get something like this:\nInput word = can \nsplit_l = [('', 'can'), ('c', 'an'), ('ca', 'n'), ('can', '')] \nreplace_l ['aan', 'ban', 'caa', 'cab', 'cac', 'cad', 'cae', 'caf', 'cag', 'cah', 'cai', 'caj', 'cak', 'cal', 'cam', 'cao', 'cap', 'caq', 'car', 'cas', 'cat', 'cau', 'cav', 'caw', 'cax', 'cay', 'caz', 'cbn', 'ccn', 'cdn', 'cen', 'cfn', 'cgn', 'chn', 'cin', 'cjn', 'ckn', 'cln', 'cmn', 'cnn', 'con', 'cpn', 'cqn', 'crn', 'csn', 'ctn', 'cun', 'cvn', 'cwn', 'cxn', 'cyn', 'czn', 'dan', 'ean', 'fan', 'gan', 'han', 'ian', 'jan', 'kan', 'lan', 'man', 'nan', 'oan', 'pan', 'qan', 'ran', 'san', 'tan', 'uan', 'van', 'wan', 'xan', 'yan', 'zan']\n\nNotice how split_l has an extra tuple ('can', ''), but the output is still the same, so this is okay.\n\n\n\nNote 2\nIf we get something like this:\nInput word = can \nsplit_l = [('', 'can'), ('c', 'an'), ('ca', 'n'), ('can', '')] \nreplace_l ['aan', 'ban', 'caa', 'cab', 'cac', 'cad', 'cae', 'caf', 'cag', 'cah', 'cai', 'caj', 'cak', 'cal', 'cam', 'cana', 'canb', 'canc', 'cand', 'cane', 'canf', 'cang', 'canh', 'cani', 'canj', 'cank', 'canl', 'canm', 'cann', 'cano', 'canp', 'canq', 'canr', 'cans', 'cant', 'canu', 'canv', 'canw', 'canx', 'cany', 'canz', 'cao', 'cap', 'caq', 'car', 'cas', 'cat', 'cau', 'cav', 'caw', 'cax', 'cay', 'caz', 'cbn', 'ccn', 'cdn', 'cen', 'cfn', 'cgn', 'chn', 'cin', 'cjn', 'ckn', 'cln', 'cmn', 'cnn', 'con', 'cpn', 'cqn', 'crn', 'csn', 'ctn', 'cun', 'cvn', 'cwn', 'cxn', 'cyn', 'czn', 'dan', 'ean', 'fan', 'gan', 'han', 'ian', 'jan', 'kan', 'lan', 'man', 'nan', 'oan', 'pan', 'qan', 'ran', 'san', 'tan', 'uan', 'van', 'wan', 'xan', 'yan', 'zan']\n\nNotice how there are strings that are 1 letter longer than the original word, such as cana.\nPlease check for the case when there is an empty string '', and if so, do not use that empty string when setting replace_l.\n\n\n# test # 2\nprint(f\"Number of outputs of switch_letter('at') is {len(switch_letter('at'))}\")\n\nNumber of outputs of switch_letter('at') is 1\n\n\n\n\nExpected output\nNumber of outputs of switch_letter('at') is 1\n ### Exercise 7\nInstructions for insert_letter(): Now implement a function that takes in a word and returns a list with a letter inserted at every offset.\nStep 1: is the same as in delete_letter()\nStep 2: This can be a list comprehension of the form:\n[f(a,b,c) for a, b in splits if condition for c in string]\n\n# UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# UNIT TEST COMMENT: Candidate for Table Driven Tests\n# GRADED FUNCTION: inserts\ndef insert_letter(word, verbose=False):\n    '''\n    Input:\n        word: the input string/word \n    Output:\n        inserts: a set of all possible strings with one new letter inserted at every offset\n    ''' \n    letters = 'abcdefghijklmnopqrstuvwxyz'\n    insert_l = []\n    split_l = []\n    \n    ### START CODE HERE ###\n    split_l = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n    insert_l = [l+replace+r for l, r in split_l for replace in letters]\n    ### END CODE HERE ###\n\n    if verbose: print(f\"Input word {word} \\nsplit_l = {split_l} \\ninsert_l = {insert_l}\")\n    \n    return insert_l\n\n\ninsert_l = insert_letter('at', True)\nprint(f\"Number of strings output by insert_letter('at') is {len(insert_l)}\")\n\nInput word at \nsplit_l = [('', 'at'), ('a', 't'), ('at', '')] \ninsert_l = ['aat', 'bat', 'cat', 'dat', 'eat', 'fat', 'gat', 'hat', 'iat', 'jat', 'kat', 'lat', 'mat', 'nat', 'oat', 'pat', 'qat', 'rat', 'sat', 'tat', 'uat', 'vat', 'wat', 'xat', 'yat', 'zat', 'aat', 'abt', 'act', 'adt', 'aet', 'aft', 'agt', 'aht', 'ait', 'ajt', 'akt', 'alt', 'amt', 'ant', 'aot', 'apt', 'aqt', 'art', 'ast', 'att', 'aut', 'avt', 'awt', 'axt', 'ayt', 'azt', 'ata', 'atb', 'atc', 'atd', 'ate', 'atf', 'atg', 'ath', 'ati', 'atj', 'atk', 'atl', 'atm', 'atn', 'ato', 'atp', 'atq', 'atr', 'ats', 'att', 'atu', 'atv', 'atw', 'atx', 'aty', 'atz']\nNumber of strings output by insert_letter('at') is 78\n\n\n\n\nExpected output\nInput word at \nsplit_l = [('', 'at'), ('a', 't'), ('at', '')] \ninsert_l = ['aat', 'bat', 'cat', 'dat', 'eat', 'fat', 'gat', 'hat', 'iat', 'jat', 'kat', 'lat', 'mat', 'nat', 'oat', 'pat', 'qat', 'rat', 'sat', 'tat', 'uat', 'vat', 'wat', 'xat', 'yat', 'zat', 'aat', 'abt', 'act', 'adt', 'aet', 'aft', 'agt', 'aht', 'ait', 'ajt', 'akt', 'alt', 'amt', 'ant', 'aot', 'apt', 'aqt', 'art', 'ast', 'att', 'aut', 'avt', 'awt', 'axt', 'ayt', 'azt', 'ata', 'atb', 'atc', 'atd', 'ate', 'atf', 'atg', 'ath', 'ati', 'atj', 'atk', 'atl', 'atm', 'atn', 'ato', 'atp', 'atq', 'atr', 'ats', 'att', 'atu', 'atv', 'atw', 'atx', 'aty', 'atz']\nNumber of strings output by insert_letter('at') is 78\n\n\nNote 1\nIf we get a split_l like this:\nInput word at \nsplit_l = [('', 'at'), ('a', 't')] \ninsert_l = ['aat', 'bat', 'cat', 'dat', 'eat', 'fat', 'gat', 'hat', 'iat', 'jat', 'kat', 'lat', 'mat', 'nat', 'oat', 'pat', 'qat', 'rat', 'sat', 'tat', 'uat', 'vat', 'wat', 'xat', 'yat', 'zat', 'aat', 'abt', 'act', 'adt', 'aet', 'aft', 'agt', 'aht', 'ait', 'ajt', 'akt', 'alt', 'amt', 'ant', 'aot', 'apt', 'aqt', 'art', 'ast', 'att', 'aut', 'avt', 'awt', 'axt', 'ayt', 'azt']\nNumber of strings output by insert_letter('at') is 52\n\nNotice that split_l is missing the extra tuple (‘at’, ’’). For insertion, we actually WANT this tuple.\nThe function is not creating all the desired output strings.\nCheck the range that we use for the for loop.\n\n\n\nNote 2\nIf we see this:\nInput word at \nsplit_l = [('', 'at'), ('a', 't'), ('at', '')] \ninsert_l = ['aat', 'bat', 'cat', 'dat', 'eat', 'fat', 'gat', 'hat', 'iat', 'jat', 'kat', 'lat', 'mat', 'nat', 'oat', 'pat', 'qat', 'rat', 'sat', 'tat', 'uat', 'vat', 'wat', 'xat', 'yat', 'zat', 'aat', 'abt', 'act', 'adt', 'aet', 'aft', 'agt', 'aht', 'ait', 'ajt', 'akt', 'alt', 'amt', 'ant', 'aot', 'apt', 'aqt', 'art', 'ast', 'att', 'aut', 'avt', 'awt', 'axt', 'ayt', 'azt']\nNumber of strings output by insert_letter('at') is 52\n\nEven though we may have fixed the split_l so that it contains the tuple ('at', ''), notice that you’re still missing some output strings.\n\nNotice that it’s missing strings such as ‘ata’, ‘atb’, ‘atc’ all the way to ‘atz’.\n\nTo fix this, make sure that when we set insert_l, we allow the use of the empty string ''.\n\n\n# test # 2\nprint(f\"Number of outputs of insert_letter('at') is {len(insert_letter('at'))}\")\n\nNumber of outputs of insert_letter('at') is 78\n\n\n\n\nExpected output\nNumber of outputs of insert_letter('at') is 78"
  },
  {
    "objectID": "notes/cs11-737-w08/index.html",
    "href": "notes/cs11-737-w08/index.html",
    "title": "Translation and Translation Data",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\nVideo 1: Lesson Video\n\n\n\n\n\n\n\n\nThis week’s slides\n\n\n\n\nSupplementary Figure 1\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{bochman2022,\n  author = {Bochman, Oren},\n  title = {Translation and {Translation} {Data}},\n  date = {2022-01-24},\n  url = {https://orenbochman.github.io/notes-nlp/notes/cs11-737-w08/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2022. “Translation and Translation Data.”\nJanuary 24, 2022. https://orenbochman.github.io/notes-nlp/notes/cs11-737-w08/."
  },
  {
    "objectID": "notes/c4w3/index.html",
    "href": "notes/c4w3/index.html",
    "title": "Question Answering",
    "section": "",
    "text": "course banner",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w3/index.html#sec-question-answering",
    "href": "notes/c4w3/index.html#sec-question-answering",
    "title": "Question Answering",
    "section": "Question Answering",
    "text": "Question Answering\n\n\n\n\n\n\n\nThis week’s slides\n\n\n\n\nSupplementary Figure 1\n\n\nMy notes for Week 3 of the Natural Language Processing with Attention Labels Course in the Natural Language Processing Specialization Offered by DeepLearning.AI on Coursera\n\n\n\n\n\n\nLearning Objectives:\n\n\n\n\nGain intuition for how transfer learning works in the context of NLP\nIdentify two approaches to transfer learning\nDiscuss the evolution of language models from CBOW to T5 and Bert\nFine-tune BERT on a dataset\nImplement context-based question answering with T5\nInterpret the GLUE benchmark",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w3/index.html#sec-overview",
    "href": "notes/c4w3/index.html#sec-overview",
    "title": "Question Answering",
    "section": "Overview",
    "text": "Overview\n In this week we are going to learn about transfer learning. More specifically we will understand how T5 and BERT actually work.\n\n\n\n\nquestion-answering\n\n\n\n\n\n\n\nDefinitions:\n\n\n\nQ&A comes in two forms:\ncontext based : given a document and a question the model extracts an answer or generates an answer\nclosed book : the model picks an answer from several options (classifier)\n\n\n\n\n\n\ntl\n\n\n\n\nclassical-training\n\n\n\n\ntransfer-learning\n\n\n\n We can see how a model initially trained on some type of sentiment classification, could now be used for question answering. One other model that has state of the art makes use of multi tasking. For example, the same model could be used for sentiment analysis, question answering, and many other things.\n\n\n\n\ngoals\n\nThese new types of models make use of a lot of data. For example the C4 (colossal cleaned crawled corpus) is about 800 GB when all of the english wikipedia is just 13 GB!\n\nC4 is a colossal, cleaned version of Common Crawl’s web crawl corpus. It was based on Common Crawl dataset. It was used to train the T5 text-to-text Transformer models. Introduced by Raffel et al. (2023) in a paper titled “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer” The dataset can be downloaded in a pre-processed form from allennlp. C4 at papers with code",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w3/index.html#sec-video2-transfer-learning-nlp",
    "href": "notes/c4w3/index.html#sec-video2-transfer-learning-nlp",
    "title": "Question Answering",
    "section": "Transfer Learning in NLP",
    "text": "Transfer Learning in NLP\n\n\n\n\ntransfer-learning-options\n\n\n\n\ntl-general-purpose\n\n\n\n\ntl-features-vs-fine-tuning\n\n\n\n\ntl-fine-tuning\n\n\n\n\ntl-pretain-data-performance\n\n\n\n\ntl-pretain-data-supervision\n\n\n\n\ntl-pretain-unsupervised\n\n\n\n\ntl-pretrain-selfsupervised\n\n\n\n\ntl-pretrain-selfsupervised\n\n\n\n\ntl-per-task-fine-tuning\n\n\n\n\ntl-summary\n\n\n\n\n\n\n\n\n\n\n\nThere are three main advantages to transfer learning:\n\nReduce training time\nImprove predictions\nAllows we to use smaller datasets\n\nTwo methods that we can use for transfer learning are the following:\n\npre-training\nfine tuning\n\nIn feature based, we can train word embeddings by running a different model and then using those features (i.e. word vectors) on a different task. When fine tuning, we can use the exact same model and just run it on a different task. Sometimes when fine tuning, we can keep the model weights fixed and just add a new layer that we will train. Other times we can slowly unfreeze the layers one at a time. We can also use unlabelled data when pre-training, by masking words and trying to predict which word was masked.\nFor example, in the drawing above we try to predict the word “friend”. This allows your model to get a grasp of the overall structure of the data and to help the model learn some relationships within the words of a sentence",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w3/index.html#sec-video3-elmo-gpt-bert-t5",
    "href": "notes/c4w3/index.html#sec-video3-elmo-gpt-bert-t5",
    "title": "Question Answering",
    "section": "ELMo, GPT, BERT, T5",
    "text": "ELMo, GPT, BERT, T5\n\n\n\n\noutline\n\n\n\n\n\n\nCBOW-fixed-window\n\nThe models mentioned in the previous video were discovered in the following order.\n\nCBOW in Word2Vec - Issue: Fixed window we want all the context\n\n2013 Word2Vec Google\nCBOW & Skip grams\n\n2014 Glove Stanfor GloVe: Global Vectors for Word ()\n\nElMo - Bidirectional LSTM\n\nSolves: fixed window size using a biderectional RNN\nIssue: weak long term dependency\n\nGPT2 - issue: unidirectional. only looks back\nBERT - just encoder - biderctional, multi mask learning\nT5 - Encoder Decoder - multi-task learning",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w3/index.html#cbow",
    "href": "notes/c4w3/index.html#cbow",
    "title": "Question Answering",
    "section": "CBOW",
    "text": "CBOW\n\n\n\n\nCBOW-issues\n\n\n\n\nELMo-solution\n\n\n\n\nELMo-RNN\n\n\n\n\nGPT-unidirectional\n\n\n\n\nBERT\n\n\n\n\nmulti-mask\n\n\n\n\nBERT-pre-training\n\n\n\n\nt5-encoder-decoder\n\n\n\n\n\n\n\n\nIn CBOW, we want to encode a word as a vector. To do this we used the context before the word and the context after the word and we use that model to learn and creates features for the word. CBOW however uses a fixed window C (for the context).\nthe main isused with CBOW are:\n\nit has a fixed window size\nno concept of order\n\nso what do we do when we need more context to model the concept we are looking at?\nWhat ElMo does, it uses a bi-directional LSTM, which is a version of an RNN that looks at the inputs from the left and the right. This has the added benefit that the context size is no longer constrained. But since it is an RNN it has problems propagating information as sequences grow longer.\nThen Open AI introduced GPT. GPT unfortunately is uni-directional but it makes use of transformers. Although ElMo was bi-directional, it suffered from some issues such as capturing longer-term dependencies.\nBERT was then introduced which stands for the Bi-directional Encoder Representation from Transformers.\nT5 was introduced which makes use of transfer learning and uses the same model to predict on many tasks.\n\nGPT was a transformer decoder\nBERT was a transformer encoder\nT5 is a decoder encoder\n\n\n\n\n\nt5-text-to-text\n\nHere is an illustration of how T5 works:\n\n\n\n\nquestion\n\n\n\n\nsummary\n\n\nSo we can now flesh out the table",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w3/index.html#sec-video4-bert",
    "href": "notes/c4w3/index.html#sec-video4-bert",
    "title": "Question Answering",
    "section": "BERT Bidirectional Encoder Representations from Transformers",
    "text": "BERT Bidirectional Encoder Representations from Transformers\n\n\n\n\nBERT-outline\n\n\n\n\nBERT-question\n\n\n\n\nBERT-summary\n\n\n\nlets dive deeper into BERT\nThere are two steps in the BERT framework: pre-training and fine-tuning. During pre-training, the model is trained on unlabeled data over different pre-training tasks. For fine tuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. For example, in the figure above, we get the corresponding embeddings for the input words, we run it through a few transformer blocks, and then we make the prediction at each time point T_i.\nTraining procedures:\n\nChoose 15% of the tokens at random:\n\nmask them 80% of the time,\nreplace them with a random token 10% of the time,\nkeep as is 10% of the time. There could be multiple masked spans in a sentence. Next sentence prediction is also used when pre-training.\n\n\n\n\n\n\nBERT\n\n\n\n\nBERT-spec\n\n\nSpec and features:\n\n\n\n\nBERT-pre-training",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w3/index.html#sec-video5-bert-objective",
    "href": "notes/c4w3/index.html#sec-video5-bert-objective",
    "title": "Question Answering",
    "section": "BERT Objective",
    "text": "BERT Objective\n\n\n\n\nBERT-outline\n\nMLM - masked language modeling.\nThis is the main unsupervised procedure to train the model with context left and right. It’s not clear how the model handles multiple masked items.\nDoes it try to predict them all at once or each one by considering input as context and unknowns.\n\n\n\n\nBERT-the-input\n\nThe input embeddings are the sum of the token embeddings, the segmentation embeddings and the position embeddings. The input embeddings: we have a CLS token to indicate the beginning of the sentence and a sep to indicate the end of the sentence The segment embeddings: allows we to indicate whether it is sentence a or b. Positional embeddings: allows we to indicate the word’s position in the sentence.\n\n\n\n\nBERT-the-output\n\nThe C token in the image above could be used for classification purposes. The unlabeled sentence A/B pair will depend on what we are trying to predict, it could range from question answering to sentiment. (in which case the second sentence could be just empty).\n\n\n\n\nBERT-objectives\n\nThe BERT objective is defined as follows:\n\n\n\n\nBERT-summary",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w3/index.html#sec-video6-fine-tuning-bert",
    "href": "notes/c4w3/index.html#sec-video6-fine-tuning-bert",
    "title": "Question Answering",
    "section": "Fine tuning BERT",
    "text": "Fine tuning BERT\n\n\n\n\nBERT-fine-tuning-outline\n\nOnce we have a pre-trained model, we can fine tune it on different tasks.\n\n\n\n\ninputs\n\nFor example, given a hypothesis, we can identify the premise. Given a question, we can find the answer. We can also use it for named entity recognition. Here is a summary of the inputs.\n\nWe can replace sentences A/B\nParaphrase from sentence A\nQuestion/passage\nHypothesis premise pairs in entailment\nText and a Ø for classification/sequence tagging\nOutput tokens are fed into a layer for token level tasks otherwise use [CLS] embedding as input.\n\n\n\n\n\nsummary",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w3/index.html#transformer-t5",
    "href": "notes/c4w3/index.html#transformer-t5",
    "title": "Question Answering",
    "section": "Transformer: T5",
    "text": "Transformer: T5\n\n\n\n\nt5-outline\n\n\n\n\nt5-text-to-text\n\n\n\n\nT5-transformer\n\n\n\nOne of the major techniques that allowed the T5 model to reach state of the art is the concept of masking:\nFor example, we represent the “for inviting” with &lt;X&gt; and last with &lt;Y&gt; then the model predicts what the X should be and what the Y should be. This is exactly what we saw in the BERT loss. We can also mask out a few positions, not just one. The loss is only on the mask for BERT, for T5 it is on the target.\n\n\n\n\nT5-architecture\n\nSo we start with the basic encoder-decoder representation. There we have a fully visible attention in the encoder and then causal attention in the decoder. So light gray lines correspond to causal masking. And dark gray lines correspond to the fully visible masking.\nIn the middle we have the language model which consists of a single transformer layer stack. And it’s being fed the concatenation of the inputs and the target. So it uses causal masking throughout as we can see because they’re all gray lines. And we have X_1 going inside, we get X_2, X_2 goes into the model and we get X3 and so forth.\nTo the right, we have prefix language model which corresponds to allowing fully visible masking over the inputs as we can see with the dark arrows. And then causal masking in the rest.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w3/index.html#sec-video8-multi-task-training-strategy",
    "href": "notes/c4w3/index.html#sec-video8-multi-task-training-strategy",
    "title": "Question Answering",
    "section": "Lecture Multi-Task Training Strategy",
    "text": "Lecture Multi-Task Training Strategy\n\n\n\n\nT5-architecture\n\n\n\n\nT5-summary\n\n\n\n\nT5-multi-task-training\n\n\n\nThis is a reminder of how the T5 model works:\nWe can see that we only have to add a small prefix to the input and the model as a result will solve the task for you. There are many tasks that the t5 model can do for you. It is possible to formulate most NLP tasks in a “text-to-text” format – that is, a task where the model is fed some text for context or conditioning and is then asked to produce some output text. This framework provides a consistent training objective both for pre-training and fine-tuning. Specifically, the model is trained with a maximum likelihood objective (using “teacher forcing” ) regardless of the task.\n\nTraining data strategies\n\nExamples-proportional mixing\n\nsample in proportion to the size of each task’s dataset\n\nTemperature scaled mixing\n\nadjust the “temperature”” of the mixing rates. This temperature parameter allows we to weight certain examples more than others. To implement temperature scaling with temperature T, we raise each task’s mixing rate rm to the power of 1⁄T and renormalize the rates so that they sum to 1. When T = 1, this approach is equivalent to examples-proportional mixing and as T increases the proportions become closer to equal mixing\n\nEqual mixing\n\nIn this case, we sample examples from each task with equal probability. Specifically, each example in each batch is sampled uniformly at random from one of the datasets we train on.\n\n\n\n\n\n\nio-format\n\n\n\n\nmulti-task-training\n\n\n\n\ndata-training-strategy\n\n\n\n\nunfreezing-adapter-layers\n\n\n\n\nquestion\n\n\n\n\nfine-tuning\n\n\n\n\n\n\nWe can see how fine tuning on a specific task could work even though we were pre-training on different tasks.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w3/index.html#sec-video9-glue-benchmark",
    "href": "notes/c4w3/index.html#sec-video9-glue-benchmark",
    "title": "Question Answering",
    "section": "GLUE Benchmark",
    "text": "GLUE Benchmark\n\n\n\n\nGLUE-evaluation\n\n\n\n\nGLUE-tasks\n\n\n\n\nGLUE\n\n\n\nGeneral Language Understanding Evaluation (GLUE) is contains:\n\nA collection used to train, evaluate, analyze natural language understanding systems\nDatasets with different genres, and of different sizes and difficulties\nLeaderboard\n\nCurrently T5 is state of the art according to this GLUE benchmark and we will be implementing it for homework this week! This GLUE bench mark is used for research purposes, it is model agnostic, and relies on models that make use of transfer learning.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w3/index.html#sec-video10-question-answering",
    "href": "notes/c4w3/index.html#sec-video10-question-answering",
    "title": "Question Answering",
    "section": "Question Answering",
    "text": "Question Answering\n We will be implementing an encoder this week. Last week we implemented the decoder. So here it is:\n\n\n\n\nBERT-encoder-Block\n\n\n\n\nBERT-blocks\n\n\n\n\nq&a-data-example\n\n\n\n\nq&a-with-t5\n\n\n\n\nt5\n\n\n\n\nt5-question\n\n\n\n\n\n\nWe can see there is a feed forward and the encoder-block above. It makes use of two residual connections, layer normalization, and dropout.\nThe steps we will follow to implement it are:\n\nLoad a pre-trained model\nProcess data to get the required inputs and outputs: “question: Q context: C” as input and “A” as target\nFine tune your model on the new task and input\nPredict using your own model",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w3/index.html#sec-programming-assignment-question-answering",
    "href": "notes/c4w3/index.html#sec-programming-assignment-question-answering",
    "title": "Question Answering",
    "section": "Programming Assignment: Question Answering",
    "text": "Programming Assignment: Question Answering",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w3/index.html#sec-lab-sentencepiece-and-bpe",
    "href": "notes/c4w3/index.html#sec-lab-sentencepiece-and-bpe",
    "title": "Question Answering",
    "section": "Lab: SentencePiece and BPE",
    "text": "Lab: SentencePiece and BPE\n\nNFKC Normalization\nunicode normalization - for accents, diacritics and friends\nfrom unicodedata import normalize\nnorm_eaccent = normalize('NFKC', '\\u00E9')\nnorm_e_accent = normalize('NFKC', '\\u0065\\u0301')\nprint(f'{norm_eaccent} = {norm_e_accent} : {norm_eaccent == norm_e_accent}')",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w3/index.html#lossless-tokenization",
    "href": "notes/c4w3/index.html#lossless-tokenization",
    "title": "Question Answering",
    "section": "lossless tokenization",
    "text": "lossless tokenization\nTo ensure this lossless tokenization it replaces white space with _ (U+2581).\ns_ = s.replace(' ', '\\u2581')\n\nSentencePiece\nTokenization with SentencePiece lab\n\n\nBPE",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w3/index.html#sec-lab-bert-loss",
    "href": "notes/c4w3/index.html#sec-lab-bert-loss",
    "title": "Question Answering",
    "section": "Lab: BERT Loss",
    "text": "Lab: BERT Loss",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w3/index.html#sec-lab-t5",
    "href": "notes/c4w3/index.html#sec-lab-t5",
    "title": "Question Answering",
    "section": "Lab: T5",
    "text": "Lab: T5\nopen in coloab",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w3/index.html#representation.-pdf-bib",
    "href": "notes/c4w3/index.html#representation.-pdf-bib",
    "title": "Question Answering",
    "section": "Representation. [pdf] [bib]",
    "text": "Representation. [pdf] [bib]\n\n2017 fasttext Facebook CBOW\n\nmorphological via sub words Algorithm of fasttext is based on these two papers:[8]\nEnriching Word Vectors with Subword Information , Piotr Bojanowski, Edouard Grave, Armand Joulin and Tomas Mikolov, 2016\nBag of Tricks for Efficient Text Classification, Armand Joulin, Edouard Grave, Piotr Bojanowski, Tomas Mikolov, 2016\n\n2018 ELMO Allen Institute for AI ELMo - Character based Bidirectional LSTM - Issue: long term dependency is weak due to vanishing gradient and information loss.\nGPT Encoder only with left context\nBert uses\n2020 T5 uses a label to specify task uses task specific bidirectional lstm to build the embeddings\nBERT Decoder only\n\nInput Token embedding - the distributed representation of the tokens in one space S with Dim(S)=D\nSegment embedding - because the model cannot tell the segment apart\n\nPosition embedding because the model cannot discriminate the word position. \nNote we are trying to mimic RNN behavior but we don’t have recursion:\nNote these are added - they all live in S. Question: would putting S and P in their own dimensions more interpretable. Questions: how do we know the model does not have embeddings that are similar to E_A and E_0 Output CLS - classification token SEP - separator token convert to embedding C is used for next sentence prediction T_i are used for masked word prediction T\nCross entropy loss + Binary loss\n\ncross entropy loss to compare between two distribution from Softmax\n\nbinary loss - could use cross entropy on two cat.\nPretraining\n        before feeding data we mask 15% of the tokens.\nmask 80% of the time:\ntraining data generator chooses 15%. of these at random for prediction\nreplace with:\nmask .8 of the time a random word .1 of the time\noriginal world otherwise.\n\na sentence may have multiple masks.\n\nnext sentence prediction also used in pre training.\nwhy/how\n(s1,s2) true/false\n\n\nBERT_Base\n12 layers\n12 attention heads\n110 million parameters\nFine tuning BERT\nFine tuning\nT5 like BERT does Transfer learning + fine tuning. classification, MT, NE, Sentiment\nSo we can see over here we have fully visible attention in the encoder and then causal attention in the decoder. \nAnd then we have the general encoder-decoder representation just as \nnotation. \nSo light gray lines correspond to causal masking. \nAnd dark gray lines correspond to the fully visible masking. \nSo on the left as I said again, it's the standard encoder-decoder architecture. \nIn the middle over here what we have, \nwe have the language model which consists of a single transformer layer stack. \nAnd it's being fed the concatenation of the inputs and the target. \nSo it uses causal masking throughout as we can see because they're \nall gray lines. \nAnd we have X1 going inside over here, get at X2, \nX2 goes into the model X3 and so forth. \nNow over here to the right, \nwe have prefix language model which corresponds to allowing fully \nvisible masking over the inputs as we can see here in the dark arrows. \nAnd then causal masking in the rest.\nPlay video starting at :3:2 and follow transcript3:02\nSo as we can see over here, it's doing causal masking. \nSo the model architecture, it uses encoder/decoder stack. \nIt has 12 transformer blocks each. \nSo we can think of it as a dozen eggs and then 220 million parameters. \nSo in summary, you've seen prefix language model attention. \nYou've seen the model architecture for T5. \nAnd you've seen how the pre-training is done similar to birds, but \nwe just use mask language modeling here.\n\n\nencoder/decoder\n1212 transformer blocks 220 million parameters pre training 2^18 steps = 262144",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w3/index.html#tokenization",
    "href": "notes/c4w3/index.html#tokenization",
    "title": "Question Answering",
    "section": "Tokenization",
    "text": "Tokenization\n\nSentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing (Kudo & Richardson 2018) sub-word tokenization\nSubword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates (Kudo 2018) sub-word tokenization\nNeural Machine Translation of Rare Words with Subword Units (Sennrich et all 2016) sub-word tokenization\nSubword tokenizers TF tutorial sub-word tokenization\n[https://blog.floydhub.com/tokenization-nlp/]\nSwivel: Improving Embeddings by Noticing What’s Missing (Shazeer, 2016)",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w3/index.html#transformers",
    "href": "notes/c4w3/index.html#transformers",
    "title": "Question Answering",
    "section": "Transformers",
    "text": "Transformers\n\nExploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (Raffel et al, 2019)\n\nReformer: The Efficient Transformer (Kitaev et al, 2020)\nAttention Is All We Need (Vaswani et al, 2017)\nDeep contextualized word representations (Peters et al, 2018)\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Devlin et al, 2018)\nFinetuning Pretrained Transformers into RNNs (Kasai et all 2021)\nThe Illustrated Transformer (Alammar, 2018)\nThe Illustrated GPT-2 (Alammar, 2019)\nHow GPT3 Works - Visualizations and Animations (Alammar, 2020)\nAttention? Attention! (Lilian Weng, 2018)\nThe Transformer Family (Lilian Weng, 2020)\nTeacher forcing for RNNs\n\n\nQuestion Answering Task:\n\nTitle (Author et al., Year) note",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w3/index.html#links",
    "href": "notes/c4w3/index.html#links",
    "title": "Question Answering",
    "section": "Links",
    "text": "Links\n\nJax\nTrax\nTrax community on Gitter\nCNN daily mail dataset\n\nLei Mao Machine Learning, Artificial Intelligence, Computer Science. [Byte Pair Encoding (Lei Mao 2021)] (https://leimao.github.io/blog/Byte-Pair-Encoding/) videos: Q&A\n\n\nSubword tokenizers\n\n\nSwivel Embeddings\nhttps://youtu.be/hAvtJ516Mw4",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w3/lab01.html",
    "href": "notes/c4w3/lab01.html",
    "title": "SentencePiece and Byte Pair Encoding",
    "section": "",
    "text": "course banner",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "L1 - SentencePiece and BPE"
    ]
  },
  {
    "objectID": "notes/c4w3/lab01.html#introduction-to-tokenization",
    "href": "notes/c4w3/lab01.html#introduction-to-tokenization",
    "title": "SentencePiece and Byte Pair Encoding",
    "section": "Introduction to Tokenization",
    "text": "Introduction to Tokenization\nIn order to process text in neural network models, it is first required to encode text as numbers with ids (such as the embedding vectors we’ve been using in the previous assignments), since the tensor operations act on numbers. Finally, if the output of the network are words, it is required to decode the predicted tokens ids back to text.\nTo encode text, the first decision that has to be made is to what level of granularity are we going to consider the text? Because ultimately, from these tokens, features are going to be created about them. Many different experiments have been carried out using words, morphological units, phonemic units, characters. For example,\n\nTokens are tricky. (raw text)\nTokens are tricky . (words)\nToken s _ are _ trick _ y . (morphemes)\nt oʊ k ə n z _ ɑː _ ˈt r ɪ k i. (phonemes, for STT)\nT o k e n s _ a r e _ t r i c k y . (character)\n\nBut how to identify these units, such as words, are largely determined by the language they come from. For example, in many European languages a space is used to separate words, while in some Asian languages there are no spaces between words. Compare English and Mandarin.\n\nTokens are tricky. (original sentence)\n令牌很棘手 (Mandarin)\nLìng pái hěn jí shǒu (pinyin)\n令牌 很 棘手 (Mandarin with spaces)\n\nSo, the ability to tokenize, i.e. split text into meaningful fundamental units is not always straight-forward.\nAlso, there are practical issues of how large our vocabulary of words, vocab_size, should be, considering memory limitations vs. coverage. A compromise between the finest-grained models employing characters which can be memory and more computationally efficient subword units such as n-grams or larger units need to be made.\nIn SentencePiece unicode characters are grouped together using either a unigram language model (used in this week’s assignment) or BPE, byte-pair encoding. We will discuss BPE, since BERT and many of its variant uses a modified version of BPE and its pseudocode is easy to implement and understand… hopefully!",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "L1 - SentencePiece and BPE"
    ]
  },
  {
    "objectID": "notes/c4w3/lab01.html#sentencepiece-preprocessing",
    "href": "notes/c4w3/lab01.html#sentencepiece-preprocessing",
    "title": "SentencePiece and Byte Pair Encoding",
    "section": "SentencePiece Preprocessing",
    "text": "SentencePiece Preprocessing\n\nNFKC Normalization\nUnsurprisingly, even using unicode to initially tokenize text can be ambiguous, e.g.,\n\neaccent = '\\u00E9'\ne_accent = '\\u0065\\u0301'\nprint(f'{eaccent} = {e_accent} : {eaccent == e_accent}')\n\né = é : False\n\n\nSentencePiece uses the Unicode standard Normalization form, NFKC, so this isn’t an issue. Looking at our example from above again with normalization:\n\nfrom unicodedata import normalize\n\nnorm_eaccent = normalize('NFKC', '\\u00E9')\nnorm_e_accent = normalize('NFKC', '\\u0065\\u0301')\nprint(f'{norm_eaccent} = {norm_e_accent} : {norm_eaccent == norm_e_accent}')\n\né = é : True\n\n\nNormalization has actually changed the unicode code point (unicode unique id) for one of these two characters.\n\ndef get_hex_encoding(s):\n    return ' '.join(hex(ord(c)) for c in s)\n\ndef print_string_and_encoding(s):\n    print(f'{s} : {get_hex_encoding(s)}') \n\n\nfor s in [eaccent, e_accent, norm_eaccent, norm_e_accent]:\n    print_string_and_encoding(s)\n\né : 0xe9\né : 0x65 0x301\né : 0xe9\né : 0xe9\n\n\nThis normalization has other side effects which may be considered useful such as converting curly quotes “ to ” their ASCII equivalent. (Although we now lose directionality of the quote…)\n\n\nLossless Tokenization*\nSentencePiece also ensures that when you tokenize your data and detokenize your data the original position of white space is preserved. (However, tabs and newlines are converted to spaces, please try this experiment yourself later below.)\nTo ensure this lossless tokenization it replaces white space with _ (U+2581). So that a simple join of the replace underscores with spaces can restore the white space, even if there are consecutives symbols. But remember first to normalize and then replace spaces with _ (U+2581). As the following example shows.\n\ns = 'Tokenization is hard.'\ns_ = s.replace(' ', '\\u2581')\ns_n = normalize('NFKC', 'Tokenization is hard.')\n\n\nprint(get_hex_encoding(s))\nprint(get_hex_encoding(s_))\nprint(get_hex_encoding(s_n))\n\n0x54 0x6f 0x6b 0x65 0x6e 0x69 0x7a 0x61 0x74 0x69 0x6f 0x6e 0x20 0x69 0x73 0x20 0x68 0x61 0x72 0x64 0x2e\n0x54 0x6f 0x6b 0x65 0x6e 0x69 0x7a 0x61 0x74 0x69 0x6f 0x6e 0x2581 0x69 0x73 0x2581 0x68 0x61 0x72 0x64 0x2e\n0x54 0x6f 0x6b 0x65 0x6e 0x69 0x7a 0x61 0x74 0x69 0x6f 0x6e 0x20 0x69 0x73 0x20 0x68 0x61 0x72 0x64 0x2e\n\n\nSo the special unicode underscore was replaced by the ASCII unicode. Reversing the order, we see that the special unicode underscore was retained.\n\ns = 'Tokenization is hard.'\nsn = normalize('NFKC', 'Tokenization is hard.')\nsn_ = s.replace(' ', '\\u2581')\n\n\nprint(get_hex_encoding(s))\nprint(get_hex_encoding(sn))\nprint(get_hex_encoding(sn_))\n\n0x54 0x6f 0x6b 0x65 0x6e 0x69 0x7a 0x61 0x74 0x69 0x6f 0x6e 0x20 0x69 0x73 0x20 0x68 0x61 0x72 0x64 0x2e\n0x54 0x6f 0x6b 0x65 0x6e 0x69 0x7a 0x61 0x74 0x69 0x6f 0x6e 0x20 0x69 0x73 0x20 0x68 0x61 0x72 0x64 0x2e\n0x54 0x6f 0x6b 0x65 0x6e 0x69 0x7a 0x61 0x74 0x69 0x6f 0x6e 0x2581 0x69 0x73 0x2581 0x68 0x61 0x72 0x64 0x2e",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "L1 - SentencePiece and BPE"
    ]
  },
  {
    "objectID": "notes/c4w3/lab01.html#bpe-algorithm",
    "href": "notes/c4w3/lab01.html#bpe-algorithm",
    "title": "SentencePiece and Byte Pair Encoding",
    "section": "BPE Algorithm",
    "text": "BPE Algorithm\nNow that we have discussed the preprocessing that SentencePiece performs we will go get our data, preprocess, and apply the BPE algorithm. We will show how this reproduces the tokenization produced by training SentencePiece on our example dataset (from this week’s assignment).\n\nPreparing our Data\nFirst, we get our Squad data and process as above.\n\nimport ast\n\ndef convert_json_examples_to_text(filepath):\n    example_jsons = list(map(ast.literal_eval, open(filepath))) # Read in the json from the example file\n    texts = [example_json['text'].decode('utf-8') for example_json in example_jsons] # Decode the byte sequences\n    text = '\\n\\n'.join(texts)       # Separate different articles by two newlines\n    text = normalize('NFKC', text)  # Normalize the text\n\n    with open('example.txt', 'w') as fw:\n        fw.write(text)\n    \n    return text\n\n\ntext = convert_json_examples_to_text('data.txt')\nprint(text[:900])\n\nBeginners BBQ Class Taking Place in Missoula!\nDo you want to get better at making delicious BBQ? You will have the opportunity, put this on your calendar now. Thursday, September 22nd join World Class BBQ Champion, Tony Balay from Lonestar Smoke Rangers. He will be teaching a beginner level class for everyone who wants to get better with their culinary skills.\nHe will teach you everything you need to know to compete in a KCBS BBQ competition, including techniques, recipes, timelines, meat selection and trimming, plus smoker and fire information.\nThe cost to be in the class is $35 per person, and for spectators it is free. Included in the cost will be either a t-shirt or apron and you will be tasting samples of each meat that is prepared.\n\nDiscussion in 'Mac OS X Lion (10.7)' started by axboi87, Jan 20, 2012.\nI've got a 500gb internal drive and a 240gb SSD.\nWhen trying to restore using di\n\n\nIn the algorithm the vocab variable is actually a frequency dictionary of the words. Further, those words have been prepended with an underscore to indicate that they are the beginning of a word. Finally, the characters have been delimited by spaces so that the BPE algorithm can group the most common characters together in the dictionary in a greedy fashion. We will see how that is exactly done shortly.\n\nfrom collections import Counter\n\nvocab = Counter(['\\u2581' + word for word in text.split()])\nvocab = {' '.join([l for l in word]): freq for word, freq in vocab.items()}\n\n\ndef show_vocab(vocab, end='\\n', limit=20):\n    shown = 0\n    for word, freq in vocab.items():\n        print(f'{word}: {freq}', end=end)\n        shown +=1\n        if shown &gt; limit:\n            break\n\n\nshow_vocab(vocab)\n\n▁ B e g i n n e r s: 1\n▁ B B Q: 3\n▁ C l a s s: 2\n▁ T a k i n g: 1\n▁ P l a c e: 1\n▁ i n: 15\n▁ M i s s o u l a !: 1\n▁ D o: 1\n▁ y o u: 13\n▁ w a n t: 1\n▁ t o: 33\n▁ g e t: 2\n▁ b e t t e r: 2\n▁ a t: 1\n▁ m a k i n g: 2\n▁ d e l i c i o u s: 1\n▁ B B Q ?: 1\n▁ Y o u: 1\n▁ w i l l: 6\n▁ h a v e: 4\n▁ t h e: 31\n\n\nWe check the size of the vocabulary (frequency dictionary) because this is the one hyperparameter that BPE depends on crucially on how far it breaks up a word into SentencePieces. It turns out that for our trained model on our small dataset that 60% of 455 merges of the most frequent characters need to be done to reproduce the upperlimit of a 32K vocab_size over the entire corpus of examples.\n\nprint(f'Total number of unique words: {len(vocab)}')\nprint(f'Number of merges required to reproduce SentencePiece training on the whole corpus: {int(0.60*len(vocab))}')\n\nTotal number of unique words: 455\nNumber of merges required to reproduce SentencePiece training on the whole corpus: 273\n\n\n\n\nBPE Algorithm\nDirectly from the BPE paper we have the following algorithm.\nTo understand what’s going on first take a look at the third function get_sentence_piece_vocab. It takes in the current vocab word-frequency dictionary and the fraction of the total vocab_size to merge characters in the words of the dictionary, num_merges times. Then for each merge operation it get_stats on how many of each pair of character sequences there are. It gets the most frequent pair of symbols as the best pair. Then it merges those pair of symbols (removes the space between them) in each word in the vocab that contains this best (= pair). Consquently, merge_vocab creates a new vocab, v_out. This process is repeated num_merges times and the result is the set of SentencePieces (keys of the final sp_vocab).\nPlease feel free to skip the below if the above description was enough.\nIn a little more detail then, we can see in get_stats we initially create a list of bigram frequencies (two character sequence) from our vocabulary. Later, this may include (trigrams, quadgrams, etc.). Note that the key of the pairs frequency dictionary is actually a 2-tuple, which is just shorthand notation for a pair.\nIn merge_vocab we take in an individual pair (of character sequences, note this is the most frequency best pair) and the current vocab as v_in. We create a new vocab, v_out, from the old by joining together the characters in the pair (removing the space), if they are present in the a word of the dictionary. Warning: the expression (?&lt;!\\S) means that either whitespace character follows before the bigram or there is nothing before (beginning of word) the bigram, similarly for (?!\\S) for preceding whitespace or end of word.\n\nimport re, collections\n\ndef get_stats(vocab):\n    pairs = collections.defaultdict(int)\n    for word, freq in vocab.items():\n        symbols = word.split()\n        for i in range(len(symbols) - 1):\n            pairs[symbols[i], symbols[i+1]] += freq\n    return pairs\n\ndef merge_vocab(pair, v_in):\n    v_out = {}\n    bigram = re.escape(' '.join(pair))\n    p = re.compile(r'(?&lt;!\\S)' + bigram + r'(?!\\S)')\n    for word in v_in:\n        w_out = p.sub(''.join(pair), word)\n        v_out[w_out] = v_in[word]\n    return v_out\n\ndef get_sentence_piece_vocab(vocab, frac_merges=0.60):\n    sp_vocab = vocab.copy()\n    num_merges = int(len(sp_vocab)*frac_merges)\n    \n    for i in range(num_merges):\n        pairs = get_stats(sp_vocab)\n        best = max(pairs, key=pairs.get)\n        sp_vocab = merge_vocab(best, sp_vocab)\n\n    return sp_vocab\n\n\nsp_vocab = get_sentence_piece_vocab(vocab)\nshow_vocab(sp_vocab) \n\n▁B e g in n ers: 1\n▁BBQ: 3\n▁Cl ass: 2\n▁T ak ing: 1\n▁P la ce: 1\n▁in: 15\n▁M is s ou la !: 1\n▁D o: 1\n▁you: 13\n▁w an t: 1\n▁to: 33\n▁g et: 2\n▁be t ter: 2\n▁a t: 1\n▁mak ing: 2\n▁d e l ic i ou s: 1\n▁BBQ ?: 1\n▁ Y ou: 1\n▁will: 6\n▁have: 4\n▁the: 31",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "L1 - SentencePiece and BPE"
    ]
  },
  {
    "objectID": "notes/c4w3/lab01.html#train-sentencepiece-bpe-tokenizer-on-example-data",
    "href": "notes/c4w3/lab01.html#train-sentencepiece-bpe-tokenizer-on-example-data",
    "title": "SentencePiece and Byte Pair Encoding",
    "section": "Train SentencePiece BPE Tokenizer on Example Data",
    "text": "Train SentencePiece BPE Tokenizer on Example Data\n\nExplore SentencePiece Model\nFirst let us explore the SentencePiece model provided with this week’s assignment. Remember you can always use Python’s built in help command to see the documentation for any object or method.\n\nimport sentencepiece as spm\nsp = spm.SentencePieceProcessor(model_file='sentencepiece.model')\n\n\nhelp(sp)\n\nHelp on SentencePieceProcessor in module sentencepiece object:\n\nclass SentencePieceProcessor(builtins.object)\n |  SentencePieceProcessor(model_file=None, model_proto=None, out_type=&lt;class 'int'&gt;, add_bos=False, add_eos=False, reverse=False, emit_unk_piece=False, enable_sampling=False, nbest_size=-1, alpha=0.1, num_threads=-1)\n |  \n |  Methods defined here:\n |  \n |  CalculateEntropy(self, input, alpha, num_threads=None)\n |      Calculate sentence entropy\n |  \n |  Decode(self, input, out_type=&lt;class 'str'&gt;, num_threads=None)\n |      Decode processed id or token sequences.\n |      \n |      Args:\n |        out_type: output type. str, bytes or 'serialized_proto' or 'immutable_proto' (Default = str)\n |        num_threads: the number of threads used in the batch processing (Default = -1).\n |  \n |  DecodeIds(self, input, out_type=&lt;class 'str'&gt;, **kwargs)\n |  \n |  DecodeIdsAsImmutableProto(self, input, out_type='immutable_proto', **kwargs)\n |  \n |  DecodeIdsAsSerializedProto(self, input, out_type='serialized_proto', **kwargs)\n |  \n |  DecodePieces(self, input, out_type=&lt;class 'str'&gt;, **kwargs)\n |  \n |  DecodePiecesAsImmutableProto(self, input, out_type='immutable_proto', **kwargs)\n |  \n |  DecodePiecesAsSerializedProto(self, input, out_type='serialized_proto', **kwargs)\n |  \n |  Detokenize = Decode(self, input, out_type=&lt;class 'str'&gt;, num_threads=None)\n |  \n |  Encode(self, input, out_type=None, add_bos=None, add_eos=None, reverse=None, emit_unk_piece=None, enable_sampling=None, nbest_size=None, alpha=None, num_threads=None)\n |      Encode text input to segmented ids or tokens.\n |      \n |      Args:\n |      input: input string. accepsts list of string.\n |      out_type: output type. int or str.\n |      add_bos: Add &lt;s&gt; to the result (Default = false)\n |      add_eos: Add &lt;/s&gt; to the result (Default = false) &lt;s&gt;/&lt;/s&gt; is added after\n |               reversing (if enabled).\n |      reverse: Reverses the tokenized sequence (Default = false)\n |      emit_unk_piece: Emits the unk literal string (Default = false)\n |      nbest_size: sampling parameters for unigram. Invalid in BPE-Dropout.\n |                  nbest_size = {0,1}: No sampling is performed.\n |                  nbest_size &gt; 1: samples from the nbest_size results.\n |                  nbest_size &lt; 0: assuming that nbest_size is infinite and samples\n |                  from the all hypothesis (lattice) using\n |                  forward-filtering-and-backward-sampling algorithm.\n |      alpha: Soothing parameter for unigram sampling, and merge probability for\n |             BPE-dropout (probablity 'p' in BPE-dropout paper).\n |      num_threads: the number of threads used in the batch processing (Default = -1).\n |  \n |  EncodeAsIds(self, input, **kwargs)\n |  \n |  EncodeAsImmutableProto(self, input, **kwargs)\n |  \n |  EncodeAsPieces(self, input, **kwargs)\n |  \n |  EncodeAsSerializedProto(self, input, **kwargs)\n |  \n |  GetPieceSize(self)\n |  \n |  GetScore = _batched_func(self, arg)\n |  \n |  IdToPiece = _batched_func(self, arg)\n |  \n |  Init(self, model_file=None, model_proto=None, out_type=&lt;class 'int'&gt;, add_bos=False, add_eos=False, reverse=False, emit_unk_piece=False, enable_sampling=False, nbest_size=-1, alpha=0.1, num_threads=-1)\n |      Initialzie sentencepieceProcessor.\n |      \n |      Args:\n |        model_file: The sentencepiece model file path.\n |        model_proto: The sentencepiece model serialized proto.\n |        out_type: output type. int or str.\n |        add_bos: Add &lt;s&gt; to the result (Default = false)\n |        add_eos: Add &lt;/s&gt; to the result (Default = false) &lt;s&gt;/&lt;/s&gt; is added after\n |          reversing (if enabled).\n |        reverse: Reverses the tokenized sequence (Default = false)\n |        emit_unk_piece: Emits the unk literal string (Default = false)\n |        nbest_size: sampling parameters for unigram. Invalid in BPE-Dropout.\n |                    nbest_size = {0,1}: No sampling is performed.\n |                    nbest_size &gt; 1: samples from the nbest_size results.\n |                    nbest_size &lt; 0: assuming that nbest_size is infinite and samples\n |                      from the all hypothesis (lattice) using\n |                      forward-filtering-and-backward-sampling algorithm.\n |        alpha: Soothing parameter for unigram sampling, and dropout probability of\n |               merge operations for BPE-dropout.\n |        num_threads: number of threads in batch processing (Default = -1, auto-detected)\n |  \n |  IsByte = _batched_func(self, arg)\n |  \n |  IsControl = _batched_func(self, arg)\n |  \n |  IsUnknown = _batched_func(self, arg)\n |  \n |  IsUnused = _batched_func(self, arg)\n |  \n |  Load(self, model_file=None, model_proto=None)\n |      Overwride SentencePieceProcessor.Load to support both model_file and model_proto.\n |      \n |      Args:\n |        model_file: The sentencepiece model file path.\n |        model_proto: The sentencepiece model serialized proto. Either `model_file`\n |          or `model_proto` must be set.\n |  \n |  LoadFromFile(self, arg)\n |  \n |  LoadFromSerializedProto(self, serialized)\n |  \n |  LoadVocabulary(self, filename, threshold)\n |  \n |  NBestEncode(self, input, out_type=None, add_bos=None, add_eos=None, reverse=None, emit_unk_piece=None, nbest_size=None)\n |      NBestEncode text input to segmented ids or tokens.\n |      \n |      Args:\n |      input: input string. accepsts list of string.\n |      out_type: output type. int or str.\n |      add_bos: Add &lt;s&gt; to the result (Default = false)\n |      add_eos: Add &lt;/s&gt; to the result (Default = false) &lt;s&gt;/&lt;/s&gt; is added after reversing (if enabled).\n |      reverse: Reverses the tokenized sequence (Default = false)\n |      emit_unk_piece: Emits the unk literal string (Default = false)\n |      nbest_size: nbest size\n |  \n |  NBestEncodeAsIds(self, input, nbest_size=None, **kwargs)\n |  \n |  NBestEncodeAsImmutableProto(self, input, nbest_size=None, **kwargs)\n |  \n |  NBestEncodeAsPieces(self, input, nbest_size=None, **kwargs)\n |  \n |  NBestEncodeAsSerializedProto(self, input, nbest_size=None, **kwargs)\n |  \n |  Normalize(self, input, with_offsets=None)\n |  \n |  OverrideNormalizerSpec(self, **kwargs)\n |  \n |  PieceToId = _batched_func(self, arg)\n |  \n |  ResetVocabulary(self)\n |  \n |  SampleEncodeAndScore(self, input, out_type=None, add_bos=None, add_eos=None, reverse=None, emit_unk_piece=None, num_samples=None, alpha=None, wor=None, include_best=None)\n |      SampleEncodeAndScore text input to segmented ids or tokens.\n |      \n |      Args:\n |      input: input string. accepsts list of string.\n |      out_type: output type. int or str or 'serialized_proto' or 'immutable_proto'\n |      add_bos: Add &lt;s&gt; to the result (Default = false)\n |      add_eos: Add &lt;/s&gt; to the result (Default = false) &lt;s&gt;/&lt;/s&gt; is added after reversing (if enabled).\n |      reverse: Reverses the tokenized sequence (Default = false)\n |      emit_unk_piece: Emits the unk literal string (Default = false)\n |      num_samples: How many samples to return (Default = 1)\n |      alpha: inverse temperature for sampling\n |      wor: whether to sample without replacement (Default = false)\n |      include_best: whether to include the best tokenization, requires wor=True (Default = false)\n |  \n |  SampleEncodeAndScoreAsIds(self, input, num_samples=None, alpha=None, **kwargs)\n |  \n |  SampleEncodeAndScoreAsImmutableProto(self, input, num_samples=None, alpha=None, **kwargs)\n |  \n |  SampleEncodeAndScoreAsPieces(self, input, num_samples=None, alpha=None, **kwargs)\n |  \n |  SampleEncodeAndScoreAsSerializedProto(self, input, num_samples=None, alpha=None, **kwargs)\n |  \n |  SampleEncodeAsIds(self, input, nbest_size=None, alpha=None, **kwargs)\n |  \n |  SampleEncodeAsImmutableProto(self, input, nbest_size=None, alpha=None, **kwargs)\n |  \n |  SampleEncodeAsPieces(self, input, nbest_size=None, alpha=None, **kwargs)\n |  \n |  SampleEncodeAsSerializedProto(self, input, nbest_size=None, alpha=None, **kwargs)\n |  \n |  SetDecodeExtraOptions(self, extra_option)\n |  \n |  SetEncodeExtraOptions(self, extra_option)\n |  \n |  SetVocabulary(self, valid_vocab)\n |  \n |  Tokenize = Encode(self, input, out_type=None, add_bos=None, add_eos=None, reverse=None, emit_unk_piece=None, enable_sampling=None, nbest_size=None, alpha=None, num_threads=None)\n |  \n |  __getitem__(self, piece)\n |  \n |  __getstate__(self)\n |  \n |  __init__ = Init(self, model_file=None, model_proto=None, out_type=&lt;class 'int'&gt;, add_bos=False, add_eos=False, reverse=False, emit_unk_piece=False, enable_sampling=False, nbest_size=-1, alpha=0.1, num_threads=-1)\n |  \n |  __len__(self)\n |  \n |  __repr__ = _swig_repr(self)\n |  \n |  __setstate__(self, serialized_model_proto)\n |  \n |  bos_id(self)\n |  \n |  calculate_entropy = CalculateEntropy(self, input, alpha, num_threads=None)\n |  \n |  decode = Decode(self, input, out_type=&lt;class 'str'&gt;, num_threads=None)\n |  \n |  decode_ids = DecodeIds(self, input, out_type=&lt;class 'str'&gt;, **kwargs)\n |  \n |  decode_ids_as_immutable_proto = DecodeIdsAsImmutableProto(self, input, out_type='immutable_proto', **kwargs)\n |  \n |  decode_ids_as_serialized_proto = DecodeIdsAsSerializedProto(self, input, out_type='serialized_proto', **kwargs)\n |  \n |  decode_pieces = DecodePieces(self, input, out_type=&lt;class 'str'&gt;, **kwargs)\n |  \n |  decode_pieces_as_immutable_proto = DecodePiecesAsImmutableProto(self, input, out_type='immutable_proto', **kwargs)\n |  \n |  decode_pieces_as_serialized_proto = DecodePiecesAsSerializedProto(self, input, out_type='serialized_proto', **kwargs)\n |  \n |  detokenize = Decode(self, input, out_type=&lt;class 'str'&gt;, num_threads=None)\n |  \n |  encode = Encode(self, input, out_type=None, add_bos=None, add_eos=None, reverse=None, emit_unk_piece=None, enable_sampling=None, nbest_size=None, alpha=None, num_threads=None)\n |  \n |  encode_as_ids = EncodeAsIds(self, input, **kwargs)\n |  \n |  encode_as_immutable_proto = EncodeAsImmutableProto(self, input, **kwargs)\n |  \n |  encode_as_pieces = EncodeAsPieces(self, input, **kwargs)\n |  \n |  encode_as_serialized_proto = EncodeAsSerializedProto(self, input, **kwargs)\n |  \n |  eos_id(self)\n |  \n |  get_piece_size = GetPieceSize(self)\n |  \n |  get_score = _batched_func(self, arg)\n |  \n |  id_to_piece = _batched_func(self, arg)\n |  \n |  init = Init(self, model_file=None, model_proto=None, out_type=&lt;class 'int'&gt;, add_bos=False, add_eos=False, reverse=False, emit_unk_piece=False, enable_sampling=False, nbest_size=-1, alpha=0.1, num_threads=-1)\n |  \n |  is_byte = _batched_func(self, arg)\n |  \n |  is_control = _batched_func(self, arg)\n |  \n |  is_unknown = _batched_func(self, arg)\n |  \n |  is_unused = _batched_func(self, arg)\n |  \n |  load = Load(self, model_file=None, model_proto=None)\n |  \n |  load_from_file = LoadFromFile(self, arg)\n |  \n |  load_from_serialized_proto = LoadFromSerializedProto(self, serialized)\n |  \n |  load_vocabulary = LoadVocabulary(self, filename, threshold)\n |  \n |  nbest_encode = NBestEncode(self, input, out_type=None, add_bos=None, add_eos=None, reverse=None, emit_unk_piece=None, nbest_size=None)\n |  \n |  nbest_encode_as_ids = NBestEncodeAsIds(self, input, nbest_size=None, **kwargs)\n |  \n |  nbest_encode_as_immutable_proto = NBestEncodeAsImmutableProto(self, input, nbest_size=None, **kwargs)\n |  \n |  nbest_encode_as_pieces = NBestEncodeAsPieces(self, input, nbest_size=None, **kwargs)\n |  \n |  nbest_encode_as_serialized_proto = NBestEncodeAsSerializedProto(self, input, nbest_size=None, **kwargs)\n |  \n |  normalize = Normalize(self, input, with_offsets=None)\n |  \n |  override_normalizer_spec = OverrideNormalizerSpec(self, **kwargs)\n |  \n |  pad_id(self)\n |  \n |  piece_size(self)\n |  \n |  piece_to_id = _batched_func(self, arg)\n |  \n |  reset_vocabulary = ResetVocabulary(self)\n |  \n |  sample_encode_and_score = SampleEncodeAndScore(self, input, out_type=None, add_bos=None, add_eos=None, reverse=None, emit_unk_piece=None, num_samples=None, alpha=None, wor=None, include_best=None)\n |  \n |  sample_encode_and_score_as_ids = SampleEncodeAndScoreAsIds(self, input, num_samples=None, alpha=None, **kwargs)\n |  \n |  sample_encode_and_score_as_immutable_proto = SampleEncodeAndScoreAsImmutableProto(self, input, num_samples=None, alpha=None, **kwargs)\n |  \n |  sample_encode_and_score_as_pieces = SampleEncodeAndScoreAsPieces(self, input, num_samples=None, alpha=None, **kwargs)\n |  \n |  sample_encode_and_score_as_serialized_proto = SampleEncodeAndScoreAsSerializedProto(self, input, num_samples=None, alpha=None, **kwargs)\n |  \n |  sample_encode_as_ids = SampleEncodeAsIds(self, input, nbest_size=None, alpha=None, **kwargs)\n |  \n |  sample_encode_as_immutable_proto = SampleEncodeAsImmutableProto(self, input, nbest_size=None, alpha=None, **kwargs)\n |  \n |  sample_encode_as_pieces = SampleEncodeAsPieces(self, input, nbest_size=None, alpha=None, **kwargs)\n |  \n |  sample_encode_as_serialized_proto = SampleEncodeAsSerializedProto(self, input, nbest_size=None, alpha=None, **kwargs)\n |  \n |  serialized_model_proto(self)\n |  \n |  set_decode_extra_options = SetDecodeExtraOptions(self, extra_option)\n |  \n |  set_encode_extra_options = SetEncodeExtraOptions(self, extra_option)\n |  \n |  set_vocabulary = SetVocabulary(self, valid_vocab)\n |  \n |  tokenize = Encode(self, input, out_type=None, add_bos=None, add_eos=None, reverse=None, emit_unk_piece=None, enable_sampling=None, nbest_size=None, alpha=None, num_threads=None)\n |  \n |  unk_id(self)\n |  \n |  vocab_size(self)\n |  \n |  ----------------------------------------------------------------------\n |  Static methods defined here:\n |  \n |  __swig_destroy__ = delete_SentencePieceProcessor(...)\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n |  \n |  thisown\n |      The membership flag\n\n\n\nLet’s work with the first sentence of our example text.\n\ns0 = 'Beginners BBQ Class Taking Place in Missoula!'\n\n\n# encode: text =&gt; id\nprint(sp.encode_as_pieces(s0))\nprint(sp.encode_as_ids(s0))\n\n# decode: id =&gt; text\nprint(sp.decode_pieces(sp.encode_as_pieces(s0)))\nprint(sp.decode_ids([12847, 277]))\n\n['▁Beginn', 'ers', '▁BBQ', '▁Class', '▁', 'Taking', '▁Place', '▁in', '▁Miss', 'oul', 'a', '!']\n[12847, 277, 15068, 4501, 3, 12297, 3399, 16, 5964, 7115, 9, 55]\nBeginners BBQ Class Taking Place in Missoula!\nBeginners\n\n\nNotice how SentencePiece breaks the words into seemingly odd parts, but we’ve seen something similar from our work with BPE. But how close were we to this model trained on the whole corpus of examles with a vocab_size of 32,000 instead of 455? Here you can also test what happens to white space, like ‘’.\nBut first let us note that SentencePiece encodes the SentencePieces, the tokens, and has reserved some of the ids as can be seen in this week’s assignment.\n\nuid = 15068\nspiece = \"\\u2581BBQ\"\nunknown = \"__MUST_BE_UNKNOWN__\"\n\n# id &lt;=&gt; piece conversion\nprint(f'SentencePiece for ID {uid}: {sp.id_to_piece(uid)}')\nprint(f'ID for Sentence Piece {spiece}: {sp.piece_to_id(spiece)}')\n\n# returns 0 for unknown tokens (we can change the id for UNK)\nprint(f'ID for unknown text {unknown}: {sp.piece_to_id(unknown)}')\n\nSentencePiece for ID 15068: ▁BBQ\nID for Sentence Piece ▁BBQ: 15068\nID for unknown text __MUST_BE_UNKNOWN__: 2\n\n\n\nprint(f'Beginning of sentence id: {sp.bos_id()}')\nprint(f'Pad id: {sp.pad_id()}')\nprint(f'End of sentence id: {sp.eos_id()}')\nprint(f'Unknown id: {sp.unk_id()}')\nprint(f'Vocab size: {sp.vocab_size()}')\n\nBeginning of sentence id: -1\nPad id: 0\nEnd of sentence id: 1\nUnknown id: 2\nVocab size: 32000\n\n\nWe can also check what are the ids for the first part and last part of the vocabulary.\n\nprint('\\nId\\tSentP\\tControl?')\nprint('------------------------')\n# &lt;unk&gt;, &lt;s&gt;, &lt;/s&gt; are defined by default. Their ids are (0, 1, 2)\n# &lt;s&gt; and &lt;/s&gt; are defined as 'control' symbol.\nfor uid in range(10):\n    print(uid, sp.id_to_piece(uid), sp.is_control(uid), sep='\\t')\n    \n# for uid in range(sp.vocab_size()-10,sp.vocab_size()):\n#     print(uid, sp.id_to_piece(uid), sp.is_control(uid), sep='\\t')\n\n\nId  SentP   Control?\n------------------------\n0   &lt;pad&gt;   True\n1   &lt;/s&gt;    True\n2   &lt;unk&gt;   False\n3   ▁   False\n4   X   False\n5   .   False\n6   ,   False\n7   s   False\n8   ▁the    False\n9   a   False\n\n\n\n\nTrain SentencePiece BPE model with our example.txt\nFinally, let’s train our own BPE model directly from the SentencePiece library and compare it to the results of our implemention of the algorithm from the BPE paper itself.\n\nspm.SentencePieceTrainer.train('--input=example.txt --model_prefix=example_bpe --vocab_size=450 --model_type=bpe')\nsp_bpe = spm.SentencePieceProcessor()\nsp_bpe.load('example_bpe.model')\n\nprint('*** BPE ***')\nprint(sp_bpe.encode_as_pieces(s0))\n\nsentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=example.txt --model_prefix=example_bpe --vocab_size=450 --model_type=bpe\nsentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \ntrainer_spec {\n  input: example.txt\n  input_format: \n  model_prefix: example_bpe\n  model_type: BPE\n  vocab_size: 450\n  self_test_sample_size: 0\n  character_coverage: 0.9995\n  input_sentence_size: 0\n  shuffle_input_sentence: 1\n  seed_sentencepiece_size: 1000000\n  shrinking_factor: 0.75\n  max_sentence_length: 4192\n  num_threads: 16\n  num_sub_iterations: 2\n  max_sentencepiece_length: 16\n  split_by_unicode_script: 1\n  split_by_number: 1\n  split_by_whitespace: 1\n  split_digits: 0\n  pretokenization_delimiter: \n  treat_whitespace_as_suffix: 0\n  allow_whitespace_only_pieces: 0\n  required_chars: \n  byte_fallback: 0\n  vocabulary_output_piece_score: 1\n  train_extremely_large_corpus: 0\n  seed_sentencepieces_file: \n  hard_vocab_limit: 1\n  use_all_vocab: 0\n  unk_id: 0\n  bos_id: 1\n  eos_id: 2\n  pad_id: -1\n  unk_piece: &lt;unk&gt;\n  bos_piece: &lt;s&gt;\n  eos_piece: &lt;/s&gt;\n  pad_piece: &lt;pad&gt;\n  unk_surface:  ⁇ \n  enable_differential_privacy: 0\n  differential_privacy_noise_level: 0\n  differential_privacy_clipping_threshold: 0\n}\nnormalizer_spec {\n  name: nmt_nfkc\n  add_dummy_prefix: 1\n  remove_extra_whitespaces: 1\n  escape_whitespaces: 1\n  normalization_rule_tsv: \n}\ndenormalizer_spec {}\ntrainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\ntrainer_interface.cc(185) LOG(INFO) Loading corpus: example.txt\ntrainer_interface.cc(409) LOG(INFO) Loaded all 26 sentences\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: &lt;unk&gt;\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: &lt;s&gt;\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: &lt;/s&gt;\ntrainer_interface.cc(430) LOG(INFO) Normalizing sentences...\ntrainer_interface.cc(539) LOG(INFO) all chars count=4533\ntrainer_interface.cc(550) LOG(INFO) Done: 99.9559% characters are covered.\ntrainer_interface.cc(560) LOG(INFO) Alphabet size=73\ntrainer_interface.cc(561) LOG(INFO) Final character coverage=0.999559\ntrainer_interface.cc(592) LOG(INFO) Done! preprocessed 26 sentences.\ntrainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 26\ntrainer_interface.cc(609) LOG(INFO) Done! 455\nbpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=99 min_freq=1\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=31 size=20 all=732 active=658 piece=▁w\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=40 all=937 active=863 piece=ch\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=60 all=1014 active=940 piece=▁u\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=80 all=1110 active=1036 piece=me\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=100 all=1166 active=1092 piece=la\nbpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6 min_freq=0\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=120 all=1217 active=1042 piece=SD\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=140 all=1272 active=1097 piece=▁bu\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=160 all=1288 active=1113 piece=▁site\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=180 all=1315 active=1140 piece=ter\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=200 all=1330 active=1155 piece=asure\nbpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=0\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=220 all=1339 active=1008 piece=ge\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=240 all=1371 active=1040 piece=▁sh\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=260 all=1384 active=1053 piece=▁cost\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=280 all=1391 active=1060 piece=de\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=300 all=1405 active=1074 piece=000\nbpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=0\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=320 all=1427 active=1021 piece=▁GB\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=340 all=1438 active=1032 piece=last\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=360 all=1441 active=1035 piece=▁let\ntrainer_interface.cc(687) LOG(INFO) Saving model: example_bpe.model\ntrainer_interface.cc(699) LOG(INFO) Saving vocabs: example_bpe.vocab\n\n\nTrue\n\n\n*** BPE ***\n['▁B', 'e', 'ginn', 'ers', '▁BBQ', '▁Cl', 'ass', '▁T', 'ak', 'ing', '▁P', 'la', 'ce', '▁in', '▁M', 'is', 's', 'ou', 'la', '!']\n\n\n\nshow_vocab(sp_vocab, end = ', ')\n\n▁B e g in n ers: 1, ▁BBQ: 3, ▁Cl ass: 2, ▁T ak ing: 1, ▁P la ce: 1, ▁in: 15, ▁M is s ou la !: 1, ▁D o: 1, ▁you: 13, ▁w an t: 1, ▁to: 33, ▁g et: 2, ▁be t ter: 2, ▁a t: 1, ▁mak ing: 2, ▁d e l ic i ou s: 1, ▁BBQ ?: 1, ▁ Y ou: 1, ▁will: 6, ▁have: 4, ▁the: 31, \n\n\nOur implementation of BPE’s code from the paper matches up pretty well with the library itself! Difference are probably accounted for by the vocab_size. There is also another technical difference in that in the SentencePiece implementation of BPE a priority queue is used to more efficiently keep track of the best pairs. Actually, there is a priority queue in the Python standard library called heapq if you would like to give that a try below!\n\nfrom heapq import heappush, heappop\n\n\ndef heapsort(iterable):\n    h = []\n    for value in iterable:\n        heappush(h, value)\n    return [heappop(h) for i in range(len(h))]\n\n\na = [1,4,3,1,3,2,1,4,2]\nheapsort(a)\n\n[1, 1, 1, 2, 2, 3, 3, 4, 4]\n\n\nFor a more extensive example consider looking at the SentencePiece repo. The last section of this code is repurposed from that tutorial. Thanks for your participation!",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "L1 - SentencePiece and BPE"
    ]
  },
  {
    "objectID": "reviews/book/seret-life-of-pronouns/index.html",
    "href": "reviews/book/seret-life-of-pronouns/index.html",
    "title": "The Secret Life of Pronouns What Our Words Say About Us",
    "section": "",
    "text": "cover"
  },
  {
    "objectID": "reviews/book/seret-life-of-pronouns/index.html#podcast",
    "href": "reviews/book/seret-life-of-pronouns/index.html#podcast",
    "title": "The Secret Life of Pronouns What Our Words Say About Us",
    "section": "Podcast",
    "text": "Podcast\n\n\n\n\n\n\n\n\n\nThe secret life of pronouns in context\n\n\n\nI got interested in this book when I was deep into wikipedia research. I was in a place where I wanted to be able to attribute content to the authors. I had read some disturbing articles with prior work on the topic by some researchers from my Alma Mater. However, I eventually came across this book and it seemed to put the ideas in a better form. Less about ML and more about the type of writers and their personalities.\n\n\nIn (Pennebaker 2013) “The Secret Life of Pronouns: What Our Words Say About Us,” the author explores how the seemingly innocuous function words we use—pronouns, articles, prepositions, and auxiliary verbs—can reveal a great deal about our personalities, emotions, social connections, and psychological states. James W. Pennebaker, the author and a social psychologist, uses computer-based text analysis methods to uncover these hidden patterns."
  },
  {
    "objectID": "reviews/book/seret-life-of-pronouns/index.html#outline",
    "href": "reviews/book/seret-life-of-pronouns/index.html#outline",
    "title": "The Secret Life of Pronouns What Our Words Say About Us",
    "section": "Outline",
    "text": "Outline\n\nKey Questions and Themes:\n\nCan language reveal psychological states? The author is primarily interested in what people’s words say about their psychological states. The book investigates whether language features can reveal how people are thinking and whether this information can be used to change their thinking.\nHow do function words differ from content words? The book distinguishes between content and style of speech, arguing that function words, which reflect language style, reveal more about a person than content words. Function words are tied to the relationship between speaker and listener.\nDo men and women use words differently? The book explores gender differences in language use, noting that women are typically more attentive to the thoughts and feelings of others, while men are less so. It also investigates whether different writing styles reflect different personalities.\nCan language predict behavior? The book examines whether language use can predict behaviors such as success in college, risk of suicide, leadership styles, and likelihood of declaring war.\nHow can language be used as a tool for change? The author examines whether analyzing words can lead to improvements in relationships, teaching, and leadership. Additionally, the book looks at how writing about emotional upheavals can improve mental and physical health, and whether using certain types of words can influence the effectiveness of that writing.\nCan language reveal deception? The book explores linguistic markers of deception, such as the use of first-person singular pronouns and the mention of other individuals.\nCan language analysis help identify authors? The book presents methods for identifying authors using function words, punctuation, and obscure words.\n\nMain Examples and Studies:\n\nExpressive Writing: Early research focused on the benefits of writing about traumatic experiences, leading to the development of the LIWC (Linguistic Inquiry and Word Count) program. This program helps analyze word categories associated with healthy writing such as positive emotion words, cognitive words, and the ability to construct a story.\nThe Bottle and the Two People Pictures: Participants described pictures, and their descriptions were analyzed for function word use and content. For example, people who focused on shadows in the bottle picture tended to be more creative. The second picture revealed common themes such as social connections, clothing, and accessories.\nThinking Styles: The book describes how people’s function words can be statistically grouped into categories reflecting thinking styles, such as formal, analytic, and narrative. These thinking styles can predict how people relate to others and organize their worlds.\n9/11 Blog Analysis: The language used in blog posts was analyzed before and after the 9/11 attacks. Researchers noted an increase in negative emotion words, a decrease in positive emotion words, and a temporary increase in cognitive words.\nCollege Admissions Essays: The study examined whether the writing style in college admissions essays could predict college grades.\nThe Federalist Papers: The book discusses the use of function words to determine the authorship of the anonymous Federalist Papers.\nLanguage Style Matching (LSM): LSM is a technique to determine how similarly two people use function words. It is used in studies of couples, speed dating, and even the writings of historical figures to analyze the relationship between the authors.\nObama’s Pronoun Use: The book analyzes Barack Obama’s use of first-person singular pronouns. His relatively low use of I-words is interpreted as a sign of self-confidence.\n\nAdditional Insights:\n\nStealth Words: The book emphasizes the importance of function words, often called “stealth words,” which are often overlooked.\nThe Role of Computers: Computer technology has revolutionized the study of language, allowing researchers to analyze large datasets and uncover hidden patterns that would be impossible to see with manual analysis.\nLanguage as a Tool: Language can be used not only as a window into the inner workings of people, but also as a tool for guiding thinking and promoting change.\nInterdisciplinary Approach: The author’s work draws on multiple disciplines including psychology, linguistics, computer science, history, and political science."
  },
  {
    "objectID": "reviews/paper/2014-NMT-by-jointly-learning-to-align-and-translate/index.html",
    "href": "reviews/paper/2014-NMT-by-jointly-learning-to-align-and-translate/index.html",
    "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
    "section": "",
    "text": "Litrature review\n\n\n\n\n\n\n\nVideo 1: Roee Aharoni’s Talk covering this paper (Hebrew)\n\n\n\n\n\n\n\n\nRoee Aharoni’s slides\n\n\n\n\nSupplementary Figure 1"
  },
  {
    "objectID": "reviews/paper/2014-NMT-by-jointly-learning-to-align-and-translate/index.html#podcast",
    "href": "reviews/paper/2014-NMT-by-jointly-learning-to-align-and-translate/index.html#podcast",
    "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
    "section": "Podcast",
    "text": "Podcast"
  },
  {
    "objectID": "reviews/paper/2014-NMT-by-jointly-learning-to-align-and-translate/index.html#abstract",
    "href": "reviews/paper/2014-NMT-by-jointly-learning-to-align-and-translate/index.html#abstract",
    "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
    "section": "Abstract",
    "text": "Abstract\n\nNeural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector1 from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition. –(Bahdanau, Cho, and Bengio 2016)\n1 the encoded state"
  },
  {
    "objectID": "reviews/paper/2014-NMT-by-jointly-learning-to-align-and-translate/index.html#outline",
    "href": "reviews/paper/2014-NMT-by-jointly-learning-to-align-and-translate/index.html#outline",
    "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
    "section": "Outline",
    "text": "Outline\n\nIntroduction\n\nPositions NMT as a new approach, contrasting it with traditional phrase-based systems.\nDescribes encoder-decoder models that use a fixed-length vector to encode a source sentence.\nExplains that these models are trained to maximize the probability of a correct translation.\nDiscusses the limitation of compressing all source sentence information into a fixed-length vector2, especially for long sentences.\nIntroduces the extension of the encoder-decoder model that jointly learns to align and translate.\nEmphasizes that the model searches for relevant source positions when generating a target word.\nStates that the new model encodes the input sentence into a sequence of vectors and adaptively chooses a subset during decoding.\nAsserts that the new model performs better with long sentences and that the proposed approach performs better translation compared to the basic encoder-decoder approach.\nNotes that the improvement is apparent with longer sentences and that the model achieves comparable performance to a conventional phrase-based system.\nMentions that the model finds linguistically plausible alignments.\n\nBackground: Neural Machine Translation (NMT)\n\nDefines translation as maximizing the conditional probability of a target sentence given a source sentence. \\text{arg}\\,\\max_{y}\\, {p}(y \\mid x).\nExplains that NMT models learn this conditional distribution using parallel training corpora.\nDescribes NMT models as consisting of an encoder and a decoder.\nNotes that Recurrent Neural Networks (RNNs) have been used for encoding and decoding variable-length sentences.\nPoints out that NMT has shown promising results, and can achieve state-of-the-art performance.\nNotes that adding neural networks to existing systems can boost performance levels.\nRNN Encoder-Decoder\n\nDescribes the RNN Encoder-Decoder framework, where an encoder reads the input sentence into a vector c.\nExplains that the encoder uses an RNN to generate a sequence of hidden states, from which the vector c is generated.\nNotes that the decoder predicts the next word given the context vector and previously predicted words.\nPresents a formula for the conditional probability, which is modeled with an RNN.\n\n\nLearning to Align and Translate\n\nIntroduces a new architecture for NMT using a bidirectional RNN encoder and a decoder that searches through the source sentence.\nDecoder: General Description\n\nPresents a new conditional probability conditioned on a distinct context vector for each target word.\nExplains that the context vector depends on a sequence of annotations from the encoder.\nDefines the context vector as a weighted sum of annotations.\nDescribes how the weights are computed, using an alignment model.\nEmphasizes that the alignment model computes a soft alignment.\nExplains the weighted sum of annotations as computing an expected annotation.\nStates that the probability of the annotation reflects its importance in deciding the next state and generating the target word.\nNotes that this implements an attention mechanism in the decoder.\n\nEncoder: Bidirectional RNN for Annotating Sequences\n\nIntroduces the use of a bidirectional RNN (BiRNN) to summarize preceding and following words.\nDescribes the forward and backward RNNs that comprise the BiRNN.\nExplains how annotations are created by concatenating forward and backward hidden states.\nNotes that the annotation will focus on words around the current word due to the nature of RNNs.\n\n\nExperiment Settings\n\nStates that the proposed approach is evaluated on English-to-French translation using ACL WMT ’14 bilingual corpora.\nCompares the approach with an RNN Encoder-Decoder.\nDataset\n\nDetails the corpora used, their sizes, and the data selection method used.\nNotes that no monolingual data other than the mentioned parallel corpora is used.\nDescribes how the development and test sets were created.\nDetails the tokenization and word shortlist used for training the models.\n\nModels\n\nDetails the two types of models trained, RNN Encoder-Decoder (RNNencdec) and RNNsearch, and the sentence lengths used.\nDescribes the hidden units in the encoder and decoder for both models.\nMentions the use of a multilayer network with a maxout hidden layer.\nDescribes the use of minibatch stochastic gradient descent (SGD) and Adadelta for training, along with the minibatch size and training time.\nExplains the use of beam search to find the translation that maximizes the conditional probability.\nRefers to the appendices for more details on the architectures and training procedure.\n\n\nResults\n\nQuantitative Results\n\nPresents translation performance measured in BLEU scores, showing that RNNsearch outperforms RNNencdec in all cases.\nNotes that the performance of RNNsearch is comparable to that of the phrase-based system, even without using a separate monolingual corpus.\nShows that RNNencdec’s performance decreases with longer sentences.\nDemonstrates that RNNsearch is more robust to sentence length, with no performance drop even with sentences of length 50 or more.\nHighlights the superiority of RNNsearch by noting that RNNsearch-30 outperforms RNNencdec-50.\nPresents a table of BLEU scores for each model.\n\nQualitative Analysis\n\nAlignment\n\nExplains that the approach offers an intuitive way to inspect the soft alignment between words.\nDescribes visualizing annotation weights to see which source positions were considered important.\nNotes the largely monotonic alignment of words, with strong weights along the diagonal.\nHighlights examples of non-trivial alignments and how the model correctly translates phrases.\nExplains the strength of soft-alignment using an example of the phrase “the man” being translated to “l’ homme”.\nNotes that soft alignment deals naturally with phrases of different lengths.\n\nLong Sentences\n\nExplains that the model does not require encoding a long sentence into a fixed-length vector perfectly.\nProvides examples of translations of long sentences, showing that RNNencdec deviates from the source meaning.\nDemonstrates that RNNsearch translates long sentences correctly, preserving the whole meaning.\nConfirms that RNNsearch enables more reliable translation of long sentences than RNNencdec.\n\n\n\nRelated Work\n\nLearning to Align\n\nMentions a similar alignment approach used in handwriting synthesis.\nNotes the key difference: in handwriting synthesis, the modes of the weights of the annotations only move in one direction.\nExplains that, in machine translation, reordering is often needed, and the proposed approach computes annotation weight of every source word for each target word.\n\nNeural Networks for Machine Translation\n\nDiscusses the history of neural networks in machine translation, from providing single features to existing systems to reranking candidate translations.\nMentions examples of neural networks being used as a sub-component of existing translation systems.\nHighlights that the paper focuses on designing a completely new translation system based on neural networks.\n\n\nConclusion\n\nStates that the paper proposes a new architecture that extends the basic model by allowing the model to (soft-)search for input words when generating a target word.\nHighlights the benefit of freeing the model from encoding the whole sentence into a fixed-length vector.\nNotes that all components are jointly trained towards a better log-probability of producing correct translations.\nConfirms that the proposed RNNsearch outperforms the conventional encoder-decoder model and is more robust to sentence length.\nConcludes that the model aligns each target word with the relevant words in the source sentence.\nPoints out that the proposed approach achieved translation performance comparable to the phrase-based statistical machine translation, despite its recent development.\nSuggests that the approach is a promising step toward better machine translation and a better understanding of natural languages.\nIdentifies better handling of unknown or rare words as a challenge for the future.\n\nAppendix A: Model Architecture\n\nArchitectural Choices\n\nDescribes that the scheme is a general framework where the activation functions of RNNs and the alignment model can be defined.\nRecurrent Neural Network\n\nExplains the use of the gated hidden unit for the activation function, which is similar to LSTM units.\nProvides details and equations for the computation of the RNN state using gated hidden units.\nExplains how update and reset gates are computed.\nMentions the use of a multi-layered function with a single hidden layer of maxout units for computing the output probability.\n\n\nAlignment Model\n\nExplains the use of a single-layer multilayer perceptron for the alignment model.\nProvides an equation describing the model and notes that some values can be pre-computed.\n\nDetailed Description of the Model\n\nEncoder\n\nProvides the equations and architecture details of the bidirectional RNN encoder.\n\nDecoder\n\nProvides the equations and architecture details of the decoder with the attention mechanism.\n\nModel Size\n\nSpecifies the sizes of hidden layers, word embeddings, and maxout hidden layer.\n\n\n\nAppendix B: Training Procedure\n\nParameter Initialization\n\nDescribes the initialization of various weight matrices, including the use of random orthogonal matrices and Gaussian distributions.\n\nTraining\n\nExplains that the training is done with stochastic gradient descent (SGD) with Adadelta to adapt the learning rate.\nDescribes how the L2-norm of the gradient was normalized and that minibatches of 80 sentences are used.\nMentions sorting the sentence pairs and splitting them into minibatches.\nPresents a table of learning statistics and related information.\n\n\nAppendix C: Translations of Long Sentences\n\nPresents sample translations generated by RNNenc-50, RNNsearch-50, and Google Translate.\nCompares these translations with a reference (gold-standard) translation for each long source sentence.\n\n\n2 requiring the length to be fixed seems a mistake as sentences can sometimes get very long think hundreds of words"
  },
  {
    "objectID": "reviews/paper/2014-NMT-by-jointly-learning-to-align-and-translate/index.html#reflections",
    "href": "reviews/paper/2014-NMT-by-jointly-learning-to-align-and-translate/index.html#reflections",
    "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
    "section": "Reflections",
    "text": "Reflections\nNaively, translation is an easy task in the sense that we just need to look up each phrase in a statistically generated bi-lingual lexicon and append the translation of the phrase to the target sentence. In reality even when working with parallel text is tricky in that, the translation isn’t one to one but rather is n-m mappings between the source and target words with the number of words being variable. Picking the best entry in the lexicon is not obvious as the source words may be ambiguous and need some attention to other words in the context to disambiguate, and these too may be ambiguous ad infinitum… Finaly the translation may require some reordering of the words in the source sentence to conform to the target language’s grammar.\nTo sum up:\n\nWe need to learn a bi-lingual phrase lexicon.\nEach phrase may have multiple translations.\nTo pick the best entry the source context should be consulted.\nThe target sentence may require reordering to conform to the target language’s grammar.\nAnother challenge is that the target language may require words or even grammatical constructs e.g. gender and gender agreement that are lacking in the source language. These are floating constraints c.f. (McKeown, Elhadad, and Robin (1997)) that the model needs to propagate through the translation process.\n\nEach step of this process is probabilistic and thus prone to mistakes. Though there are two main constructs. The first is the contextual lexicon which needs to be learned using parallel text. The second is the destination grammar that can be learned as part of a language model using monolingual text. However reordering texts isn’t as much of a challenge, if we have sufficient parallel texts then the NMT hidden states should be able to learn the reordering as part of its hidden state."
  },
  {
    "objectID": "reviews/paper/2014-NMT-by-jointly-learning-to-align-and-translate/index.html#the-paper",
    "href": "reviews/paper/2014-NMT-by-jointly-learning-to-align-and-translate/index.html#the-paper",
    "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
    "section": "The Paper",
    "text": "The Paper\n\n\n\npaper"
  },
  {
    "objectID": "reviews/paper/2015-effective-approaches-to-attention-based-NMT/index.html",
    "href": "reviews/paper/2015-effective-approaches-to-attention-based-NMT/index.html",
    "title": "Effective Approaches to Attention-based Neural Machine Translation",
    "section": "",
    "text": "Literature review\nThis review is a bit of a mess, it has gone through at least three editions of this blog and I have learned much about writing and structuring a review since then. I think it needs a bit of an overhaul. This latest version is a step in the right direction.\nThere are many other review of this paper but I think that covering this paper is highly relevant followup to the assignments for NMT as well as the earlier assignment in for MT with KNN in the Classification and Vector Space Models course."
  },
  {
    "objectID": "reviews/paper/2015-effective-approaches-to-attention-based-NMT/index.html#podcast",
    "href": "reviews/paper/2015-effective-approaches-to-attention-based-NMT/index.html#podcast",
    "title": "Effective Approaches to Attention-based Neural Machine Translation",
    "section": "Podcast",
    "text": "Podcast"
  },
  {
    "objectID": "reviews/paper/2015-effective-approaches-to-attention-based-NMT/index.html#effective-approaches-to-attention-based-neural-machine-translation",
    "href": "reviews/paper/2015-effective-approaches-to-attention-based-NMT/index.html#effective-approaches-to-attention-based-neural-machine-translation",
    "title": "Effective Approaches to Attention-based Neural Machine Translation",
    "section": "Effective Approaches to Attention-based Neural Machine Translation",
    "text": "Effective Approaches to Attention-based Neural Machine Translation\n\nAbstract\n\nAn attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches on the WMT translation tasks between English and German in both directions.\n\n\nWith local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems that already incorporate known techniques such as dropout. Our ensemble model using different attention architectures yields a new state-of-the-art result in the WMT’15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker. – (Luong, Pham, and Manning 2015)\n\n\n\nOutline\n\n\nIntroduction\n\nLists the advantages of neural machine translation (NMT)\n\nMinimal domain knowledge\nConceptual simplicity\nAbility to generalize well to long sequences\nSmall memory footprint\nEasy implementation of decoders\n\nDiscusses the concept of attention in neural networks\nMentions different applications of attention in different tasks\nHighlights the application of attention mechanism in NMT by Bahdanau et al. (2015) and the lack of further exploration\nPresents the purpose of the paper, which is to design two novel types of attention-based models\n\nA global approach\nA local approach\n\nPresents the experimental evaluation of the proposed approaches on WMT translation tasks and its analysis\n\nNeural Machine Translation\n\nDescribes the conditional probability of translating a source sentence to a target sentence\nPresents the two components of a basic NMT system an Encoder and a Decoder\nDiscusses the use of recurrent neural network (RNN) architectures in NMT\nPresents the parameterization of the probability of decoding each word in the target sentence\nPresents the training objective used in NMT\n\nAttention-based Models\n\nClassifies the various attention-based models into two broad categories\n\nGlobal attention\nLocal attention\n\nPresents the common process followed by both global and local attention models for deriving the context vector\nDescribes the concatenation of target hidden state and source-side context vector for prediction\n\nGlobal Attention\n\nDescribes the concept of global attention model\nPresents the derivation of a variable-length alignment vector by comparing the current target hidden state with each source hidden state\nPresents three different alternatives for the content-based function used in calculating the alignment vector\nDescribes the location-based function used in early attempts to build attention-based models\nDescribes the calculation of the context vector as the weighted average over all the source hidden states\nPresents a comparison of the proposed global attention approach to the model by Bahdanau et al. (2015)\n\nSimpler architecture\nSimpler computation path\nUse of multiple alignment functions\n\n\nLocal Attention\n\nDescribes the concept of local attention model\nMentions the inspiration from the soft and hard attentional models\nDescribes the selection of a small window of context and its advantages over soft and hard attention models\nPresents two variants of the local attention model\n\nMonotonic alignment\nPredictive alignment\n\nDescribes the comparison to the selective attention mechanism by Gregor et al. (2015)\n\nInput-feeding Approach\n\nDescribes the suboptimal nature of making independent attentional decisions in the global and local approaches\nDiscusses the need for joint alignment decisions taking into account past alignment information\nPresents the input-feeding approach\nMentions the effects of input-feeding approach\n\nMakes the model fully aware of previous alignment choices\nCreates a deep network spanning both horizontally and vertically\n\nPresents the comparison to other related works\n\nUse of context vectors by Bahdanau et al. (2015)\nDoubly attentional approach by Xu et al. (2015)\n\n\nExperiments\n\nDescribes the evaluation setup and datasets used\n\nnewstest2013 as development set\nnewstest2014 and newstest2015 as test sets\n\nMentions the use of case-sensitive BLEU for reporting translation performances\nDescribes the two types of BLEU used\n\nTokenized BLEU\nNIST BLEU\n\n\nTraining Details\n\nDescribes the data used for training NMT systems\n\nWMT’14 training data\n\nPresents the details of vocabulary size and filtering criteria used\nDiscusses the architecture of the LSTM models and training settings\nMentions the training speed and time\n\nEnglish-German Results\n\nDiscusses the different systems used for comparison\nPresents the progressive improvements achieved by\n\nReversing the source sentence\nUsing dropout\nUsing global attention approach\nUsing input-feeding approach\nUsing local attention model with predictive alignments\n\nNotes the correlation between perplexity and translation quality\nMentions the use of unknown replacement technique and the achievement of new SOTA result by ensembling 8 different models\nDescribes the results of testing the models on newstest2015 and the establishment of new SOTA performance\n\nGerman-English Results\n\nMentions the evaluation setup for the German-English translation task\nPresents the results highlighting the effectiveness of\n\nAttentional mechanism\nInput-feeding approach\nContent-based dot product function with dropout\nUnknown word replacement technique\n\n\nAnalysis\n\nDescribes the purpose of conducting extensive analysis\n\nUnderstanding of the learning process\nAbility to handle long sentences\nChoice of attentional architectures\nAlignment quality\n\n\nLearning Curves\n\nPresents the analysis of the learning curves for different models\nNotes the separation between non-attentional and attentional models\nBriefly mentions the effectiveness of input-feeding approach and local attention models\n\nEffects of Translating Long Sentences\n\nBriefly discusses the grouping of sentences based on lengths and computation of BLEU score per group\nMentions the effectiveness of attentional models in handling long sentences\nNotes the superior performance of the best model across all sentence length buckets\n\nChoices of Attentional Architectures\n\nPresents the analysis of different attention models and alignment functions\nHighlights the poor performance of the location-based function\nBriefly mentions the performance of content-based functions\n\nGood performance of dot function for global attention\nBetter performance of general function for local attention\n\nNotes the best performance of local attention model with predictive alignments\n\nAlignment Quality\n\nBriefly discusses the use of alignment error rate (AER) metric to evaluate the alignment quality\nMentions the data used for evaluating alignment quality and the process of extracting one-to-one alignments\nPresents the results of AER evaluation and the comparison to Berkeley aligner\nNotes the better performance of local attention models compared to the global one\nBriefly discusses the AER of the ensemble"
  },
  {
    "objectID": "reviews/paper/2015-effective-approaches-to-attention-based-NMT/index.html#dot-product-attention",
    "href": "reviews/paper/2015-effective-approaches-to-attention-based-NMT/index.html#dot-product-attention",
    "title": "Effective Approaches to Attention-based Neural Machine Translation",
    "section": "Dot-Product Attention",
    "text": "Dot-Product Attention\nDot-Product attention is the first of three attention mechanisms covered in the course and the simplest covered in this paper. Dot-Product Attention is a good fit, in an engineering sense, for a encoder-decoder architecture with tasks where the source source sequence is fully available at the start and the tasks is mapping or transformation the source sequence to an output sequence like in alignment, or translation.\n\n\n\n\n\n\nFigure 1: Neural machine translation – a stacking recurrent architecture for translating a source sequence A B C D into a target sequence X Y Z. Here, &lt;eos&gt; marks the end of a sentence.\n\n\n\nThe first assignment in the course using encoder decoder LSTM model with attention is so similar to the setup disused in this paper, I would not be surprised if it may well have inspired it.\n\nThis is a review of the paper in which scaled dot product attention was introduced in 2015 by Minh-Thang Luong, Hieu Pham, Christopher D. Manning in Effective Approaches to Attention-based Neural Machine Translation which is available at papers with code. In this paper they tried to take the attention mechanism being used in other tasks and to distill it to its essence and at the same time to also find a more general form.\n\n\n\n\n\n\n\n\nGlobal Attention\n\n\n\n\nFigure 2: Global attentional model – at each time step t, the model infers a variable-length alignment weight vector at based on the current target state ht and all source states ̄hs. A global contextvector ct is then computed as the weighted average, according to at, over all the source states.\n\n\n\n\n\n\n\n\n\n\n\n\nLocal attention\n\n\n\n\nFigure 3: Local attention model – the model first predicts a single aligned position p_t or the current target word. A window centered around the source position p_t is then used to compute a context vector c_t, a weighted average of the source hidden states in the window. The weights at are inferred from the current target state h_t and those source states \\bar{h}_s in the window.\n\n\n\n\n\n\n\n\n\n\n\nInput-feeding approach\n\n\n\n\nFigure 4: Input-feeding approach – Attentional vectors \\bar{h}_s are fed as inputs to the next time steps to inform the model about past alignment decisions\n\n\n\n\n\n\n\n\n\nLearning curves\n\n\n\n\nFigure 5: Learning curves – test cost (ln perplexity) on newstest2014 for English-German NMTs as training progresses.\n\n\n\n\n\n\n\n\n\nLength Analysis\n\n\n\n\nFigure 6: Length Analysis – translation qualities of different systems as sentences become longer\n\n\n\nThey also came up with a interesting way to visualize the alignment’s attention mechanism.\n\n\n\n\n\n\nalignment-visulization\n\n\n\n\nFigure 7: Alignment visualizations – shown are images of the attention weights learned by various models: (top left) global, (top right) local-m, and (bottom left) local-p. The gold alignments are displayed at the bottom right corner.\n\n\n\nSo to recap: Luong et all were focused on alignment problem in NMT. When they try to tackle it using attention as function of the content and a function of its location. They came up with a number of ways to distill and generalize the attention mechanism.\n\n\n\n\n\npage 1\n\nAttention was just another engineering technique to improve alignment and it had not yet taken center stage in the models, as it would in Attention Is All We Need (Vaswani et al. (2023)).I find it useful to garner the concepts and intuition which inspired these researchers to adapt attention and how they come up with this form of attention.\nThe abstract begins with:\n\n“An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation.”\n\nwhich was covered in last lesson. The abstract continues with:\n\n“This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time.”\n\ntalks about\n\n\n\n\n\npage 2"
  },
  {
    "objectID": "reviews/paper/2015-effective-approaches-to-attention-based-NMT/index.html#neural-machine-translation",
    "href": "reviews/paper/2015-effective-approaches-to-attention-based-NMT/index.html#neural-machine-translation",
    "title": "Effective Approaches to Attention-based Neural Machine Translation",
    "section": "§2 Neural Machine Translation:",
    "text": "§2 Neural Machine Translation:\nThis section provides a summary of the the NMT task using 4 equations: In particular they note that in the decoder the conditional probability of the target given the source is of the form: \nlog \\space p(y \\vert x) = \\sum_{j=1}^m log \\space p (y_j \\vert y_{&lt;j} , s)\n\nWhere x_i are the source sentence and y_i are the target sentence. \np (y_j \\vert y{&lt;j} , s) = softmax (g(h_j))\n\nHere, h_j is the RNN hidden unit, abstractly computed as: \nh_j = f(h_{j-1},s)\n\nOur training objective is formulated as follows \nJ_t=\\sum_{(x,y)\\in D} -log \\space p(x \\vert y)\n\nWith D being our parallel training corpus."
  },
  {
    "objectID": "reviews/paper/2015-effective-approaches-to-attention-based-NMT/index.html#overview-of-attention",
    "href": "reviews/paper/2015-effective-approaches-to-attention-based-NMT/index.html#overview-of-attention",
    "title": "Effective Approaches to Attention-based Neural Machine Translation",
    "section": "§3 Overview of attention",
    "text": "§3 Overview of attention\n\n\n\n\npage 3\n\nNext they provide a recap of the attention mechanism to set their starting point: &gt;Specifically, given the target hidden state h_t and the source-side context vector c_t, we employ a simple concatenation layer to combine the information from both vectors to produce an attentional hidden state as follows:\n\n\\bar{h}_t = tanh(W_c[c_t;h_t])\n\n\nThe attentional vector \\bar{h}_t is then fed through the softmax layer to produce the predictive distribution formulated as:\n\n\np(y_t|y{&lt;t}, x) = softmax(W_s\\bar{h}_t)"
  },
  {
    "objectID": "reviews/paper/2015-effective-approaches-to-attention-based-NMT/index.html#global-attention",
    "href": "reviews/paper/2015-effective-approaches-to-attention-based-NMT/index.html#global-attention",
    "title": "Effective Approaches to Attention-based Neural Machine Translation",
    "section": "§3.1 Global attention",
    "text": "§3.1 Global attention\nThis is defined in §3.1 of the paper as:\n\n\\begin{align}\n   a_t(s) & = align(h_t,\\bar{h}_s)  \\newline\n   & = \\frac{ e^{score(h_t,\\bar{h}_s)} }{ \\sum_{s'} e^{score(h_t,\\bar{h}_s)} } \\newline\n   & = softmax(score(h_t,\\bar{h}_s))\n\\end{align}\n\nwhere h_t and h_s are the target and source sequences and score() which is referred to as a content-based function as one of three alternative forms provided:\n\nDot product attention:\n\nscore(h_t,\\bar{h}_s)=h_t^T\\bar{h}_s\n This form combines the source and target using a dot product. Geometrically this essentially a projection operation.\n\n\nGeneral attention:\n\nscore(h_t,\\bar{h}_s)=h_t^TW_a\\bar{h}_s\n\nthis form combines the source and target using a dot product after applying a learned attention weights to the source. Geometrically this is a projection of the target on a linear transformation of the source or scaled dot product attention as it is now known\n\n\nConcatenative attention:\n\nscore(h_t,\\bar{h}_s)=v_a^T tanh(W_a [h_t;\\bar{h}_s])\n\nThis is a little puzzling v_a^T is not accounted for and seems to be a learned attention vector which is projected onto the linearly weighted combination of the hidden states of the encoder and decoder. they also mention having considered using a location based function location :\n\na_t = softmax(W_a h_t)\n\nwhich is just a linear transform of the hidden target state h_t"
  },
  {
    "objectID": "reviews/paper/2015-effective-approaches-to-attention-based-NMT/index.html#local-attention",
    "href": "reviews/paper/2015-effective-approaches-to-attention-based-NMT/index.html#local-attention",
    "title": "Effective Approaches to Attention-based Neural Machine Translation",
    "section": "§3.2 Local Attention",
    "text": "§3.2 Local Attention\n\n\n\n\npage 4\n\nin §3.2 they consider a local attention mechanism. This is a resource saving modification of global attention using the simple concept of applying the mechanism within a fixed sized window.\n\nWe propose a local attentional mechanism that chooses to focus only on a small subset of the source positions per target word. This model takes inspiration from the tradeoff between the soft and hard attentional models proposed by Xu et al. (2015) to tackle the image caption generation task.\n\n\nOur local attention mechanism selectively focuses on a small window of context and is differentiable. … In concrete details, the model first generates an aligned position p_t for each target word at time t. The context vector c_t\n\nis then derived as a weighted average over the set of source hidden states within the window [p_t−D, p_t+D]; Where D is empirically selected. The big idea here is to use a fixed window size for this step to conserve resources when translating paragraphs or documents - a laudable notion for times where LSTM gobbled up resources in proportion to the sequence length…\nThey also talk about monotonic alignment where p_t=t and predictive alignment\n\np_t=S\\cdot sigmoid(v_p^Ttanh(W_ph_t))\n\n\na_t(s)=align(h_t,\\bar{h}_s)e^{(-\\frac{(s-p_t)^2}{s\\sigma^2})}\n\nwith align() as defined above and\n\n\\sigma=\\frac{D}{2}\n\n\n\n\n\npage 5\n\nI found the rest of the paper lesser interest\n\n\n\n\npage 6\n\n\n\n\npage 7\n\n\n\n\npage 8\n\n\n\nIn §5.4 In alignment quality\n\n\n\n\npage 9\n\nsome sample translations\n\n\n\n\npage 10\n\nthe references\n\n\n\n\npage 11\n\nThis is appendix A which shows the visualization of alignment weights."
  },
  {
    "objectID": "reviews/paper/2015-effective-approaches-to-attention-based-NMT/index.html#the-paper",
    "href": "reviews/paper/2015-effective-approaches-to-attention-based-NMT/index.html#the-paper",
    "title": "Effective Approaches to Attention-based Neural Machine Translation",
    "section": "The Paper",
    "text": "The Paper\n\n\n\npaper"
  },
  {
    "objectID": "reviews/paper/1997-floating-contraints-in-lexical-choice/index.html",
    "href": "reviews/paper/1997-floating-contraints-in-lexical-choice/index.html",
    "title": "Floating Constraints in Lexical Choice",
    "section": "",
    "text": "Literature review\n\n\n\n\n\n\n\nVideo 1"
  },
  {
    "objectID": "reviews/paper/1997-floating-contraints-in-lexical-choice/index.html#context",
    "href": "reviews/paper/1997-floating-contraints-in-lexical-choice/index.html#context",
    "title": "Floating Constraints in Lexical Choice",
    "section": "Context",
    "text": "Context\nThis is one of the three paper mentioned by Kathleen McKeown in her heroes of NLP interview by Andrew NG. The paper is about the floating constraints in lexical choice.\nLooking quickly over it I noticed a couple of things:\n\n\nI was familiar with Elhadad’s work on generation. I had read his work on FUF Functional Unification Formalism in the 90s which introduced me to the paradigm of unification before I even knew about Prolog or logic programming.\nI found it fascinating that McKeown and Elhadad had worked together on this paper and even a bit curious about what they were looking into here.\nSince McKeown brought it up, it might intimate that there is something worth looking into here. And it is discusing aspects of generative language models."
  },
  {
    "objectID": "reviews/paper/1997-floating-contraints-in-lexical-choice/index.html#abstract",
    "href": "reviews/paper/1997-floating-contraints-in-lexical-choice/index.html#abstract",
    "title": "Floating Constraints in Lexical Choice",
    "section": "Abstract",
    "text": "Abstract\n\nLexical choice is a computationally complex task, requiring a generation system to consider a potentially large number of mappings between concepts and words. Constraints that aid in determining which word is best come from a wide variety of sources, including syntax, semantics, pragmatics, the lexicon, and the underlying domain. Furthermore, in some situations, different constraints come into play early on, while in others, they apply much later. This makes it difficult to determine a systematic ordering in which to apply constraints. In this paper, we present a general approach to lexical choice that can handle multiple, interacting constraints. We focus on the problem of floating constraints, semantic or pragmatic constraints that float, appearing at a variety of different syntactic ranks, often merged with other semantic constraints. This means that multiple content units can be realized by a single surface element, and conversely, that a single content unit can be realized by a variety of surface elements. Our approach uses the Functional Unification Formalism (FuF) to represent a generation lexicon, allowing for declarative and compositional representation of individual constraints. – (McKeown, Elhadad, and Robin 1997)"
  },
  {
    "objectID": "reviews/paper/1997-floating-contraints-in-lexical-choice/index.html#outline",
    "href": "reviews/paper/1997-floating-contraints-in-lexical-choice/index.html#outline",
    "title": "Floating Constraints in Lexical Choice",
    "section": "Outline",
    "text": "Outline\n\nIntroduction\n\nMentions the complexity of lexical choice in language generation.\nNotes the diverse sources of constraints on lexical choice, including syntax, semantics, lexicon, domain, and pragmatics.\nHighlights the challenges posed by floating constraints, which are semantic or pragmatic constraints that can appear at various syntactic ranks and be merged with other semantic constraints.\nPresents examples of floating constraints in sentences.\n\nAn Architecture for Lexical Choice\n\nDiscusses the role of lexical choice within a typical language generation architecture.\nArgues for placing the lexical choice module between the content planner and the surface sentence generator.\nDescribes the input and output of the lexical choice module, emphasizing the need for language-independent conceptual input.\nPresents the two tasks of lexical choice: syntagmatic1 decisions (determining thematic structure) and paradigmatic2 decisions (choosing specific words).\nIntroduces the FUF/SURGE package as the implementation environment.\n\nPhrase Planning in Lexical Choice\n\nDescribes the need for phrase planning to articulate multiple semantic relations into a coherent linguistic structure.\nExplains how the Lexical Chooser selects the head relation and builds its argument structure based on focus and perspective.\nDetails how remaining relations are attached as modifiers, considering options like embedding, subordination, and syntactic forms of modifiers.\n\nCross-ranking and Merged Realizations\n\nDiscusses the non-isomorphism between semantic and syntactic structures, necessitating merging and cross-ranking realizations.\nProvides examples of merging (multiple semantic elements realized by a single word) and cross-ranking (single semantic element realized at different syntactic ranks).\nEmphasizes the need for linguistically neutral input to support this flexibility.\nExplains the implementation of merging and cross-ranking using FUF, highlighting the challenges of search and the use of bk-class for efficient backtracking.\n\nInterlexical Constraints\n\nDefines interlexical constraints as arising from alternative sets of collocations for realizing pairs of content units.\nPresents an example of interlexical constraints with verbs and nouns in the basketball domain.\nDiscusses different strategies for encoding interlexical constraints in the Lexical Chooser.\nIntroduces the :wait mechanism in FUF to delay the choice of one collocate until the other is chosen, preserving modularity and avoiding backtracking.\n\nOther Approaches to Lexical Choice\n\nReviews other systems using FUF for lexical choice, comparing their architectures and features to ADVISOR-II.\nDiscusses non-FUF-based systems, categorizing them by their positioning of lexical choice in the generation process and analyzing their handling of various constraints.\n\nConclusion\n\nSummarizes the contributions of the paper, highlighting the ability to handle a wide range of constraints, the use of FUF for declarative representation and interaction, and the algorithm for lexical selection and syntactic structure building.\nEmphasizes the importance of syntagmatic choice and perspective selection for handling floating constraints.\nNotes the use of lazy evaluation and dependency-directed backtracking for computational efficiency.\nMentions future directions, such as developing domain-independent lexicon modules and methods for organizing large-scale lexica.\n\n\n1 of or denoting the relationship between two or more linguistic units used sequentially to make well-formed structures. The combination of ‘that handsome man + ate + some chicken’ forms a syntagmatic relationship. If the word position is changed, it also changes the meaning of the sentence, eg ’Some chicken ate the handsome man2 Paradigmatic relation describes the relationship between words that can be substituted for words with the same word class (eg replacing a noun with another noun)"
  },
  {
    "objectID": "reviews/paper/1997-floating-contraints-in-lexical-choice/index.html#reflection",
    "href": "reviews/paper/1997-floating-contraints-in-lexical-choice/index.html#reflection",
    "title": "Floating Constraints in Lexical Choice",
    "section": "Reflection",
    "text": "Reflection\n\nIn unification-based approaches, any word is a candidate unless it is ruled out by a constraint. With an LLM a word is selected based on its probability.\nThe idea of constraints is deeply embedded in Linguistics. The two type that immediately come to mind are:\n\nSubcategorization constraints which are rules that govern the selection of complements and adjuncts. These are syntactic constraints.\nSelectional constraints which are rules that govern the selection of arguments. These are semantic constraints\n\nOther constraints that I did not have to deal with as often are pragmatics and these are resolved from the state of the world or common sense knowledge.\nIn FUF there are many more constraints and if care isn’t taken, the system can become over constrained and no words will be selected.\nOne Neat aspect of the approach though is that FUF language could be merged with a query to generate a text.\nUnfortunately as fat as I recall the FUF had to be specified by hand. The only people who could do this were the people who designed the system. They needed to be expert at linguistics and logic programming. Even I was pretty sharp at the time the linguistics being referred was far too challenging. I learned that in reality every computational linguist worth their salt had their own theory of language and their own version of grammar and that FUF was just one of many.\nIt seems that today attention mechanisms within LSTMs or Transformers are capable of learning a pragmatic formalism without a linguist stepping in to specify it.\n\nWhy this is still interesting:\nImagine we want to build a low resource capable language model.\nWe would really like it to be good with grammar Also we would like it to have a lexicon that is as rich as the person it is interacting with. And we would also like to make available to it the common sense knowledge that it would need to use in the context of current and possibly a profile based on past conversations…. On the other hand we don’t want to require a 70 billion parameter model to do this. So what do we do?\nLet imagine we start by training on Wikipedia, after all wikipedia’s mission is to become the sum of all human knowledge. By do we realy need all of the information in Wikipedia in out edge model?\nThe answer is no but the real question is how can we partition the training information and the wights etc so that we have a powerful model with a basic grammar and lexicon but which can load a knowledge graph, extra lexicons and common sense knowledge as needed.\nSo I had this idea from lookin at a Ouhalla (1999). It pointed out there were phrase boundries and that if the sentence was transformed in a number of ways t would still make sense if we did this with respect to a phrase but if we used cucnks that were too small or too big the transofrmation would create malformed sentences. The operations of intrerest were movement and deletion.\nand that can still have a model that is both small but able to access this extra material.\nOne direction seems to me is that we want to apply queries to all the training material as we train the model. Based on the queries that we retrieve each clause or phrase we can decide where we want to learn it and if we want to learn it at all.\n(If it is common sense knowledge we may already know it. If it is semi structured we may want to put it into wikidata. If it is neither we may want to avoid learning it at all. We may still want to use it for improving the grammar and lexicon.) However we may want to store it in a speclised lexicon for domains like medicinee or law.\nWe could also decide if and how to eliminate it from we want to The queries should match each bit of information in the sentence. These are our queries. While the facts may change form\n\nsay we want to decompose a wikipedia article"
  },
  {
    "objectID": "reviews/paper/1997-floating-contraints-in-lexical-choice/index.html#the-paper",
    "href": "reviews/paper/1997-floating-contraints-in-lexical-choice/index.html#the-paper",
    "title": "Floating Constraints in Lexical Choice",
    "section": "The Paper",
    "text": "The Paper\n\n\n\npaper"
  },
  {
    "objectID": "reviews/paper/transforer-to-rnn/index.html",
    "href": "reviews/paper/transforer-to-rnn/index.html",
    "title": "Finetuning Pretrained Transformers into RNNs",
    "section": "",
    "text": "Litrature review\n\n\n\n\n\n\n\nVideo 1: Transformer to RNN (T2RNN) Part-1 by Dr. Niraj Kumar\n\n\n\n\n\n\n\n\nVideo 2: Transformer to RNN (T2RNN) Part-2 by Dr. Niraj Kumar"
  },
  {
    "objectID": "reviews/paper/transforer-to-rnn/index.html#abstract",
    "href": "reviews/paper/transforer-to-rnn/index.html#abstract",
    "title": "Finetuning Pretrained Transformers into RNNs",
    "section": "Abstract",
    "text": "Abstract\n\nTransformers have outperformed recurrent neural networks (RNNs) in natural language generation. But this comes with a significant computational cost, as the attention mechanism’s complexity scales quadratically with sequence length. Efficient transformer variants have received increasing interest in recent works. Among them, a linear-complexity recurrent variant has proven well suited for autoregressive generation. It approximates the softmax attention with randomized or heuristic feature maps, but can be difficult to train and may yield suboptimal accuracy. This work aims to convert a pretrained transformer into its efficient recurrent counterpart, improving efficiency while maintaining accuracy. Specifically, we propose a swap-then-finetune procedure: in an off-the-shelf pretrained transformer, we replace the softmax attention with its linear-complexity recurrent alternative and then finetune. With a learned feature map, our approach provides an improved tradeoff between efficiency and accuracy over the standard transformer and other recurrent variants. We also show that the finetuning process has lower training cost relative to training these recurrent variants from scratch. As many models for natural language tasks are increasingly dependent on large-scale pretrained transformers, this work presents a viable approach to improving inference efficiency without repeating the expensive pretraining process. – (Kasai et al. 2021)"
  },
  {
    "objectID": "reviews/paper/transforer-to-rnn/index.html#the-paper",
    "href": "reviews/paper/transforer-to-rnn/index.html#the-paper",
    "title": "Finetuning Pretrained Transformers into RNNs",
    "section": "The Paper",
    "text": "The Paper\n\n\n\npaper"
  },
  {
    "objectID": "reviews/paper/2023-MBR-all-the-way-dowm/index.html",
    "href": "reviews/paper/2023-MBR-all-the-way-dowm/index.html",
    "title": "It’s MBR All the Way Down: Modern Generation Techniques Through the Lens of Minimum Bayes Risk",
    "section": "",
    "text": "Litrature review"
  },
  {
    "objectID": "reviews/paper/2023-MBR-all-the-way-dowm/index.html#abstract",
    "href": "reviews/paper/2023-MBR-all-the-way-dowm/index.html#abstract",
    "title": "It’s MBR All the Way Down: Modern Generation Techniques Through the Lens of Minimum Bayes Risk",
    "section": "Abstract",
    "text": "Abstract\nMinimum Bayes Risk (MBR) decoding is a method for choosing the outputs of a machine learning system based not on the output with the highest probability, but the output with the lowest risk (expected error) among multiple candidates. It is a simple but powerful method: for an additional cost at inference time, MBR provides reliable several-point improvements across metrics for a wide variety of tasks without any additional data or training. Despite this, MBR is not frequently applied in NLP works, and knowledge of the method itself is limited. We first provide an introduction to the method and the recent literature. We show that several recent methods that do not reference MBR can be written as special cases of MBR; this reformulation provides additional theoretical justification for the performance of these methods, explaining some results that were previously only empirical. We provide theoretical and empirical results about the effectiveness of various MBR variants and make concrete recommendations for the application of MBR in NLP models, including future directions in this area. – (Bertsch et al. 2023)"
  },
  {
    "objectID": "reviews/paper/2023-MBR-all-the-way-dowm/index.html#outline",
    "href": "reviews/paper/2023-MBR-all-the-way-dowm/index.html#outline",
    "title": "It’s MBR All the Way Down: Modern Generation Techniques Through the Lens of Minimum Bayes Risk",
    "section": "Outline",
    "text": "Outline\n\nAbstract\n\nDescribes Minimum Bayes Risk (MBR) decoding.\nPresents MBR as a widely applicable method that often improves performance over beam search and single-sample decoding.\nShows that several recent generation methods can be framed as special cases of MBR, thereby providing theoretical grounding for their empirical success.\n\nIntroduction\n\nPresents Minimum Bayes Risk (MBR) decoding as a simple yet powerful decoding method.\nDiscusses how recent methods in natural language processing (NLP) unknowingly replicate aspects of MBR.\n\nFormalization\n\nStandard decoding\n\nDescribes the standard decoding methods for autoregressive models such as greedy decoding, sampling, and beam search.\n\nMinimum Bayes Risk decoding\n\nDefines the theoretical foundation of Minimum Bayes Risk (MBR) decoding, including the concept of risk based on expected error.\nExplains how risk computation is typically approximated using Monte Carlo methods due to the intractability of calculating the full expectation.\n\n\nTaxonomy of MBR\n\nNotes four key design considerations for implementing an MBR method: the hypothesis set, the evidence set, the gain/error function, and the evidence distribution.\nSampling a hypothesis set\n\nHighlights several works that show improvements in MBR by filtering the hypothesis set to contain only higher-quality candidate outputs.\n\nSampling an evidence set\n\nBriefly discusses various sampling strategies, with most work focusing on drawing unbiased samples from the model distribution.\n\nWhat metric do we want to maximize?\n\nExplores the impact of different gain (or error) functions, noting that using a specific metric as a gain function in MBR tends to lead to improved performance on that metric.\n\nWhat probability distribution should we use to estimate risk?\n\nBriefly discusses the choice of the distribution used to estimate risk in MBR, with most methods using the model’s score distribution over outputs, and some work using alternative distributions like a human or true distribution.\n\n\nMBR as a frame for other methods\n\nHighlights the framing of self-consistency, range voting, output ensembling, and density estimation as special cases of MBR.\nSelf-consistency as MBR\n\nShows how self-consistency, a method where the most frequent answer from multiple model generations is selected, can be formulated as MBR.\nExplains that the best performing sampling strategies for self-consistency are those closest to ancestral sampling due to its unbiased estimation properties.\n\nOutput Ensembling as MBR\n\nPresents output ensembling, where a set of models is used to generate outputs and a combined output is selected, as a form of MBR with a mixture distribution.\n\nMBR as Density Estimation\n\nEstablishes the connection between MBR and kernel density estimation, noting that both can be seen as mode-seeking methods.\n\nRange Voting as MBR\n\nShows that range voting, where candidates are assigned scores by voters, can be formulated as MBR by treating candidates as hypotheses and voters as evidence.\n\n\nDesign Decisions Impact MBR Performance\n\nExamines cases where the choices made in designing an MBR method significantly affect its performance.\nExperimental Details\n\nPresents the datasets and models used for evaluating MBR in abstractive summarization and machine translation tasks.\n\nThe MBR metric matters–but perhaps not as much as the hypothesis set\n\nDemonstrates that MBR using different gain functions (ROUGE-1, BEER, BERTScore) improves abstractive summarization performance.\nNotes that the choice of hypothesis set has a more significant impact than the choice of gain function.\n\nVarying the risk distribution: lessons from beam search don’t translate to MBR\n\nInvestigates the effects of correcting for length bias in the evidence distribution used for estimating risk in MBR.\nFinds that while length correction benefits beam search, it hurts MBR performance, possibly due to a high-variance estimator of risk.\n\n\nMBR applications in NLP\n\nPresents a historical overview of MBR applications in NLP, from its early use in statistical models to its recent resurgence in neural models.\nHistorical context\n\nTraces the roots of MBR to Bayesian decision theory and its use in parsing, speech recognition, and machine translation since the 1990s.\nExplains how early MBR applications were constrained by graph-based model structures, requiring complex algorithms for exact decoding.\n\nRecent usage\n\nDiscusses the revival of MBR in neural text generation tasks, with much of the recent work focusing on machine translation.\nNotes the decline in the explicit use of the term “MBR” in favor of newer terminologies like “self-consistency.”\n\n\nConclusion\n\nDiscusses the terminology drift in NLP that leads to the renaming of MBR as different methods.\nReemphasizes the importance of connecting modern techniques with their historical roots for a better understanding of why they work.\n\nAppendix A: More details on importance sampling for MBR\n\nProvides a detailed explanation of importance sampling and its application to MBR, specifically when estimating risk under a length-corrected distribution.\n\nAppendix B: Contextualizing this work within philosophy of science\n\nExplores the broader implications of the work within the context of meta-analysis of scientific research.\nDiscusses the phenomena of citational amnesia and terminology drift in scientific literature and their possible consequences."
  },
  {
    "objectID": "reviews/paper/2023-MBR-all-the-way-dowm/index.html#the-paper",
    "href": "reviews/paper/2023-MBR-all-the-way-dowm/index.html#the-paper",
    "title": "It’s MBR All the Way Down: Modern Generation Techniques Through the Lens of Minimum Bayes Risk",
    "section": "The Paper",
    "text": "The Paper\n\n\n\npaper"
  },
  {
    "objectID": "reviews/paper/2021-ELMo/index.html",
    "href": "reviews/paper/2021-ELMo/index.html",
    "title": "ELMo - Deep contextualized word representations",
    "section": "",
    "text": "Litrature review"
  },
  {
    "objectID": "reviews/paper/2021-ELMo/index.html#podcast",
    "href": "reviews/paper/2021-ELMo/index.html#podcast",
    "title": "ELMo - Deep contextualized word representations",
    "section": "Podcast",
    "text": "Podcast"
  },
  {
    "objectID": "reviews/paper/2021-ELMo/index.html#abstract",
    "href": "reviews/paper/2021-ELMo/index.html#abstract",
    "title": "ELMo - Deep contextualized word representations",
    "section": "Abstract",
    "text": "Abstract\n\nWe introduce a new type of deep contextualized word representation that models both\n\ncomplex characteristics of word use (e.g., syntax and semantics), and\nhow these uses vary across linguistic contexts (i.e., to model polysemy).\n\nOur word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals."
  },
  {
    "objectID": "reviews/paper/2021-ELMo/index.html#outline",
    "href": "reviews/paper/2021-ELMo/index.html#outline",
    "title": "ELMo - Deep contextualized word representations",
    "section": "Outline",
    "text": "Outline\n\nI. Introduction\n\nIdeally, word representations should model complex characteristics of word use, such as syntax and semantics, and how these uses vary across linguistic contexts, to model polysemy.\nThe authors introduces a new type of deep contextualized word representation (ELMo) that addresses these challenges, integrates easily into existing models, and improves state-of-the-art results.\n\nII. ELMo (Embeddings from Language Models)\n\nELMo representations are functions of the entire input sentence, not just individual tokens.\nThey are computed using a bidirectional LSTM (biLM) trained on a large text corpus with a language model objective.\nELMo representations are deep, in the sense they are a function of all internal layers of the biLM.\nA linear combination of the vectors stacked above each input word is learned for each end task.\nInternal states are combined to create rich word representations\nHigher-level LSTM states capture context-dependent aspects of word meaning (semantics),\nLower-level states model aspects of syntax.\nExposing all of these signals allows learned models to select the most useful types of semi-supervision for each end task.\n\nIII. Bidirectional Language Models (biLM)\n\nA forward language model predicts the next token given the history of previous tokens.\nA backward language model predicts the previous token given the future context.\nA biLM combines both forward and backward LMs, maximizing the log-likelihood of both directions.\nThe biLM uses tied parameters for token representations and the Softmax layer in both directions, but maintains separate parameters for LSTMs in each direction.\n\nIV. ELMo Specifics\n\nFor each token, an L-layer biLM computes a set of 2L+1 representations.\nELMo collapses all biLM layers into a single vector using a task-specific weighting of all layers.\nA scalar parameter scales the entire ELMo vector.\nLayer normalization can be applied to each biLM layer before weighting.\n\nV. Integrating ELMo into Supervised NLP Tasks\n\nThe weights of the pre-trained biLM are frozen, and then the ELMo vector is concatenated with the existing token representation before being passed into the task’s RNN.\nFor some tasks, ELMo is also included at the output of the task RNN.\nDropout is added to ELMo, and sometimes the ELMo weights are regularized.\n\nVI. Pre-trained biLM Architecture\n\nThe biLMs are similar to previous architectures but modified to support joint training of both directions and add a residual connection between LSTM layers.\nThe model uses 2 biLSTM layers with 4096 units and 512-dimension projections, with a residual connection.\nThe context-insensitive type representation uses character n-gram convolutional filters and highway layers.\nThe biLM provides three layers of representation for each input token.\n\nVII. Evaluation\n\nELMo was evaluated on six benchmark NLP tasks, including question answering, textual entailment, and sentiment analysis.\nAdding ELMo significantly improves the state-of-the-art in every case.\nFor tasks where direct comparisons are possible, ELMo outperforms CoVe.\nDeep representations outperform those derived from just the top layer of an LSTM.\n\nVIII. Task-Specific Results\n\nQuestion Answering (SQuAD): ELMo significantly improved the F1 score.\nTextual Entailment (SNLI): ELMo improved accuracy.\nSemantic Role Labeling (SRL): ELMo improved the F1 score.\nCoreference Resolution: ELMo improved the average F1 score.\nNamed Entity Extraction (NER): ELMo enhanced biLSTM-CRF achieved a new state-of-the-art F1 score.\nSentiment Analysis (SST-5): ELMo improved accuracy over the prior state-of-the-art.\n\nIX. Analysis\n\nUsing deep contextual representations improves performance compared to just using the top layer.\nELMo provides better overall performance than representations from a machine translation encoder like CoVe.\nSyntactic information is better represented at lower layers, while semantic information is better captured at higher layers.\nIncluding ELMo at both the input and output layers of the supervised model can improve performance for some tasks.\nELMo increases sample efficiency, requiring fewer training updates and less data to reach the same level of performance.\nThe contextual information captured by ELMo is more important than the sub-word information.\nPre-trained word vectors provide a marginal improvement when used with ELMo.\n\nX. Key Findings\n\nELMo efficiently encodes different types of syntactic and semantic information about words in context.\nUsing all layers of the biLM improves overall task performance.\nELMo provides a general approach for learning high-quality, deep, context-dependent representations."
  },
  {
    "objectID": "reviews/paper/2021-ELMo/index.html#the-paper",
    "href": "reviews/paper/2021-ELMo/index.html#the-paper",
    "title": "ELMo - Deep contextualized word representations",
    "section": "The Paper",
    "text": "The Paper\n\n\n\npaper"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "about.html#resources",
    "href": "about.html#resources",
    "title": "About",
    "section": "Resources",
    "text": "Resources\n\nURL structure best practices for Google SEO\nLighthouse test report"
  },
  {
    "objectID": "about.html#people",
    "href": "about.html#people",
    "title": "About",
    "section": "People",
    "text": "People\n\nChris Manning\nDan Jurafsky\nYoav Goldberg\nGraham Neubig\nGuest Speakers, TA and other contributors\n\nKathleen McKeown\nQuoc Le\nOren Etzioni\nAlon Lavie\nLei Li\nHeng Ji gs\nCraig Stewart\nJonathn Amith\nRoee Aharoni\nYulia Tsvetkov"
  },
  {
    "objectID": "reviews/paper/2019-morphological-embeddings/index.html",
    "href": "reviews/paper/2019-morphological-embeddings/index.html",
    "title": "Morphological Word Embeddings",
    "section": "",
    "text": "Litrature review\nThis is a mentioned in the tokenization lab in course four week 3\nTime permitting I will try and dive deeper into this paper.",
    "crumbs": [
      "Home",
      "Papers",
      "Morphological embeddings"
    ]
  },
  {
    "objectID": "reviews/paper/2019-morphological-embeddings/index.html#podcast",
    "href": "reviews/paper/2019-morphological-embeddings/index.html#podcast",
    "title": "Morphological Word Embeddings",
    "section": "Podcast",
    "text": "Podcast",
    "crumbs": [
      "Home",
      "Papers",
      "Morphological embeddings"
    ]
  },
  {
    "objectID": "reviews/paper/2019-morphological-embeddings/index.html#abstract",
    "href": "reviews/paper/2019-morphological-embeddings/index.html#abstract",
    "title": "Morphological Word Embeddings",
    "section": "Abstract",
    "text": "Abstract\n\nLinguistic similarity is multi-faceted. For instance, two words may be similar with respect to semantics, syntax, or morphology inter alia. Continuous word-embeddings have been shown to capture most of these shades of similarity to some degree. This work considers guiding word-embeddings with morphologically annotated data, a form of semisupervised learning, encouraging the vectors to encode a word’s morphology, i.e., words close in the embedded space share morphological features. We extend the log-bilinear model to this end and show that indeed our learned embeddings achieve this, using German as a case study. – (Cotterell and Schütze 2019)",
    "crumbs": [
      "Home",
      "Papers",
      "Morphological embeddings"
    ]
  },
  {
    "objectID": "reviews/paper/2019-morphological-embeddings/index.html#terminology",
    "href": "reviews/paper/2019-morphological-embeddings/index.html#terminology",
    "title": "Morphological Word Embeddings",
    "section": "Terminology",
    "text": "Terminology\nHere are some terms and concepts discussed in the sources, with explanations:\n\nWord embeddings\n\nThese are vector representations of words in a continuous space, where similar words are located close to each other in the vector space. The goal is to capture linguistic similarity, but the definition of “similarity” can vary (semantic, syntactic, morphological). Word embeddings are typically trained to produce representations that capture linguistic similarity.\n\nLog-bilinear model (LBL)\n\nThis is a language model that learns features along with weights, as opposed to using hand-crafted features. It uses context to predict the next word, and word embeddings fall out as low dimensional representations of the context. The LBL is a generalization of the log-linear model.\n\nMorphology\n\nThe study of word structure, including how words are formed from morphemes (the smallest units of meaning).\n\nMorphological tags\n\nThese are annotations that describe the morphological properties of a word, such as case, gender, number, tense, etc.. They can be very detailed for morphologically rich languages. For example, in German, a word might be tagged to indicate its part of speech, case, gender, and number.\n\nMorphologically rich languages\n\nLanguages with a high morpheme-per-word ratio, where word-internal structure is important for processing. German and Czech are cited as examples.\n\nMorphologically impoverished languages\n\nLanguages with a low morpheme-per-word ratio, such as English.\n\nMulti-task objective\n\nTraining a model to perform multiple tasks simultaneously. In this case, the model is trained to predict both the next word and its morphological tag.\n\nSemi-supervised learning\n\nTraining a model on a partially annotated corpus, using both labeled and unlabeled data. This approach is useful when large amounts of unannotated text are available.\n\nContextual signature\n\nThe words surrounding a given word. The context in which a word appears can be used to determine the word’s meaning and morphological properties.\n\nHamming distance\n\nA measure of the difference between two binary strings, calculated as the number of positions at which the corresponding bits are different. In this context, it is used to compare morphological tags, which are represented as bit vectors.\n\nMORPHOSIM\n\nA metric for evaluating morphologically-driven embeddings that measures how morphologically similar a word is to its nearest neighbors in the embedding space. It is calculated as the average Hamming distance between morphological tags of a word and its neighbors. Lower values are better, as they indicate that the nearest neighbors of each word are closer morphologically.",
    "crumbs": [
      "Home",
      "Papers",
      "Morphological embeddings"
    ]
  },
  {
    "objectID": "reviews/paper/2019-morphological-embeddings/index.html#outline",
    "href": "reviews/paper/2019-morphological-embeddings/index.html#outline",
    "title": "Morphological Word Embeddings",
    "section": "Outline",
    "text": "Outline\n\nIntroduction\n\nPresents the importance of capturing word morphology, especially for morphologically-rich languages.\nHighlights the multifaceted nature of linguistic similarity (semantics, syntax, morphology).\nDiscusses the goal of the paper: to develop word embeddings that specifically encode morphological relationships.\n\nRelated Work\n\nDiscusses previous integration of morphology into language models, including factored language models and neural network-based approaches.\nNotes the role of morphology in computational morphology, particularly in morphological tagging for inflectionally-rich languages.\nHighlights the significance of distributional similarity in morphological analysis.\n\nLog-Bilinear Model\n\nDescribes the log-bilinear model (LBL), a generalization of the log-linear model with learned features.\nPresents the LBL’s energy function and probability distribution in the context of language modeling.\n\nMorph-LBL\n\nProposes a multi-task objective that jointly predicts the next word and its morphological tag.\nDescribes the model’s joint probability distribution, incorporating morphological tag features.\nDiscusses the use of semi-supervised learning, allowing training on partially annotated corpora\n\nEvaluation\n\nMentions the qualitative evaluation using t-SNE, showing clusters reflecting morphological and POS relationships.\nIntroduces MorphoSim, a novel quantitative metric to assess the extent to which similar embeddings are morphologically related.\n\nExperiments and Results\n\nPresents experiments on the German TIGER corpus, comparing Morph-LBL with the original LBL and Word2Vec.\nDescribes two experiments:\n\nExperiment 1: Evaluates the morphological information encoded in embeddings using a k-NN classifier.\nExperiment 2: Compares models using the MorphoSim metric to measure morphological similarity among nearest neighbors.\n\nDiscusses the results, highlighting Morph-LBL’s superior performance in capturing morphological relationships, even without observing all tags during training.\n\nConclusion and Future Work\n\nSummarizes the contributions of the paper: introducing Morph-LBL for inducing morphologically guided embeddings.\nNotes the model’s success in leveraging distributional signatures to capture morphology.\nDiscusses future work on integrating orthographic features for further improvement.\nMentions potential applications in morphological tagging and other NLP tasks.",
    "crumbs": [
      "Home",
      "Papers",
      "Morphological embeddings"
    ]
  },
  {
    "objectID": "reviews/paper/2019-morphological-embeddings/index.html#log-bilinear-model",
    "href": "reviews/paper/2019-morphological-embeddings/index.html#log-bilinear-model",
    "title": "Morphological Word Embeddings",
    "section": "Log-Bilinear Model",
    "text": "Log-Bilinear Model\np(w\\mid h) =\n\\frac{\\exp\\left(s_\\theta(w,h)\\right)}{\\sum_{w'}\n\\exp\\left(s_\\theta(w',h)\\right)} \\qquad\n\nwhere w is a word, h is a history and s_\\theta is an energy function. Following the notation of , in the LBL we define \ns_\\theta(w,h) = \\left(\\sum_{i=1}^{n-1} C_i\nr_{h_i}\\right)^T q_w + b_w \\qquad\n\nwhere n-1 is the history length",
    "crumbs": [
      "Home",
      "Papers",
      "Morphological embeddings"
    ]
  },
  {
    "objectID": "reviews/paper/2019-morphological-embeddings/index.html#morph-lbl",
    "href": "reviews/paper/2019-morphological-embeddings/index.html#morph-lbl",
    "title": "Morphological Word Embeddings",
    "section": "Morph-LBL",
    "text": "Morph-LBL\n\n  p(w, t \\mid h) \\propto \\exp(( f_t^T S  + \\sum_{i=1}^{n-1}C_i r_{h_i})^T q_w + b_{w} ) \\qquad\n\nwhere f_t is a hand-crafted feature vector for a morphological tag t and S is an additional weight matrix.\nUpon inspection, we see that\n\np(t \\mid w,h) \\propto \\exp(S^T f_t q_w) \\qquad\n\nHence given a fixed embedding q_w for word w, we can interpret S as the weights of a conditional log-linear model used to predict the tag t.",
    "crumbs": [
      "Home",
      "Papers",
      "Morphological embeddings"
    ]
  },
  {
    "objectID": "reviews/paper/2019-morphological-embeddings/index.html#the-paper",
    "href": "reviews/paper/2019-morphological-embeddings/index.html#the-paper",
    "title": "Morphological Word Embeddings",
    "section": "The Paper",
    "text": "The Paper\n\n\n\npaper",
    "crumbs": [
      "Home",
      "Papers",
      "Morphological embeddings"
    ]
  },
  {
    "objectID": "reviews/paper/2019-morphological-embeddings/index.html#resources",
    "href": "reviews/paper/2019-morphological-embeddings/index.html#resources",
    "title": "Morphological Word Embeddings",
    "section": "Resources",
    "text": "Resources\n\nLeipzig Glossing Rules which provides a standard way to explain morphological features by examples\nCMU Multilingual NLP 2020 (17): Morphological Analysis and Inflection \nKann, Cotterell, and Schütze (2016) code\nunimorph univorsal morphological database",
    "crumbs": [
      "Home",
      "Papers",
      "Morphological embeddings"
    ]
  },
  {
    "objectID": "reviews/paper/2017-attention-is-all-you-need/index.html",
    "href": "reviews/paper/2017-attention-is-all-you-need/index.html",
    "title": "Attention Is All You Need",
    "section": "",
    "text": "Litrature review",
    "crumbs": [
      "Home",
      "Papers",
      "Attention is all you need"
    ]
  },
  {
    "objectID": "reviews/paper/2017-attention-is-all-you-need/index.html#abstract",
    "href": "reviews/paper/2017-attention-is-all-you-need/index.html#abstract",
    "title": "Attention Is All You Need",
    "section": "Abstract",
    "text": "Abstract\n\nThe dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data –(Vaswani et al. 2023)",
    "crumbs": [
      "Home",
      "Papers",
      "Attention is all you need"
    ]
  },
  {
    "objectID": "reviews/paper/2017-attention-is-all-you-need/index.html#outline",
    "href": "reviews/paper/2017-attention-is-all-you-need/index.html#outline",
    "title": "Attention Is All You Need",
    "section": "Outline",
    "text": "Outline",
    "crumbs": [
      "Home",
      "Papers",
      "Attention is all you need"
    ]
  },
  {
    "objectID": "reviews/paper/2017-attention-is-all-you-need/index.html#the-paper",
    "href": "reviews/paper/2017-attention-is-all-you-need/index.html#the-paper",
    "title": "Attention Is All You Need",
    "section": "The Paper",
    "text": "The Paper\n\n\n\npaper",
    "crumbs": [
      "Home",
      "Papers",
      "Attention is all you need"
    ]
  },
  {
    "objectID": "reviews/paper/2015-LSH/index.html",
    "href": "reviews/paper/2015-LSH/index.html",
    "title": "Practical and Optimal LSH for Angular Distance",
    "section": "",
    "text": "Literature review\n\n\n\n\n\n\n\nVideo 1: Locality-Sensitive Hashing and Beyond\n\n\n\n\n\n\n\n\nVideo 2: Beyond Locality Sensitive Hashing; Alexandr Andoni\nIn the NLP specialization we have covered and used LSH a number of times in at least two courses.  In one sense I have an understanding of what is going on when we implement this. In another sense this seems to be quite confusing. So I don’t fully understand some aspects of LSH.\nOne way to do better is to try and explain it to someone else. This is what I am trying to do here by going back to the source and trying to understand the paper, problems, motivation and finally isolate what it is that is hard to grasp.\nThis paper is a good introduction to LSH for the angular distance.\nIn (Andoni et al. 2015) the authors delve into Locality-Sensitive Hashing (LSH) for the angular distance. They show the existence of an LSH family for the angular distance that yields an approximate Near Neighbor Search algorithm with the asymptotically optimal running time exponent. Unlike earlier algorithms with this property (e.g., Spherical LSH), their algorithm is also practical, improving upon the well-studied hyperplane LSH in practice.\nFor now the two videos which explain in some depth this an the related paper - Optimal Data-Dependent Hashing for Approximate Near Neighbors will have to do.\nTime permitting I will try and dive deeper into this paper.",
    "crumbs": [
      "Home",
      "Papers",
      "Practical & Optimal LSH for Angular Distance"
    ]
  },
  {
    "objectID": "reviews/paper/2015-LSH/index.html#podcast",
    "href": "reviews/paper/2015-LSH/index.html#podcast",
    "title": "Practical and Optimal LSH for Angular Distance",
    "section": "Podcast",
    "text": "Podcast",
    "crumbs": [
      "Home",
      "Papers",
      "Practical & Optimal LSH for Angular Distance"
    ]
  },
  {
    "objectID": "reviews/paper/2015-LSH/index.html#abstract",
    "href": "reviews/paper/2015-LSH/index.html#abstract",
    "title": "Practical and Optimal LSH for Angular Distance",
    "section": "Abstract",
    "text": "Abstract\n\nWe show the existence of a Locality-Sensitive Hashing (LSH) family for the angular distance that yields an approximate Near Neighbor Search algorithm with the asymptotically optimal running time exponent. Unlike earlier algorithms with this property (e.g., Spherical LSH [1 , 2]), our algorithm is also practical, improving upon the well-studied hyperplane LSH [3] in practice.\nWe also introduce a multiprobe version of this algorithm, and conduct experimental evaluation on real and synthetic data sets.\nWe complement the above positive results with a fine-grained lower bound for the quality of any LSH family for angular distance. Our lower bound implies that the above LSH family exhibits a trade-off between evaluation time and quality that is close to optimal for a natural class of LSH functions. – (Andoni et al. 2015)",
    "crumbs": [
      "Home",
      "Papers",
      "Practical & Optimal LSH for Angular Distance"
    ]
  },
  {
    "objectID": "reviews/paper/2015-LSH/index.html#the-paper",
    "href": "reviews/paper/2015-LSH/index.html#the-paper",
    "title": "Practical and Optimal LSH for Angular Distance",
    "section": "The Paper",
    "text": "The Paper\n\n\n\npaper",
    "crumbs": [
      "Home",
      "Papers",
      "Practical & Optimal LSH for Angular Distance"
    ]
  },
  {
    "objectID": "reviews/paper/20180-PTWM-NMT/index.html",
    "href": "reviews/paper/20180-PTWM-NMT/index.html",
    "title": "When and Why are Pre-trained Word Embeddings Useful for Neural Machine Translation?",
    "section": "",
    "text": "Video 1: Talk covering this paper by Roee Aharoni"
  },
  {
    "objectID": "reviews/paper/20180-PTWM-NMT/index.html#podcast",
    "href": "reviews/paper/20180-PTWM-NMT/index.html#podcast",
    "title": "When and Why are Pre-trained Word Embeddings Useful for Neural Machine Translation?",
    "section": "Podcast",
    "text": "Podcast"
  },
  {
    "objectID": "reviews/paper/20180-PTWM-NMT/index.html#abstract",
    "href": "reviews/paper/20180-PTWM-NMT/index.html#abstract",
    "title": "When and Why are Pre-trained Word Embeddings Useful for Neural Machine Translation?",
    "section": "Abstract",
    "text": "Abstract\n\nThe performance of Neural Machine Translation (NMT) systems often suffers in lowresource scenarios where sufficiently largescale parallel corpora cannot be obtained. Pretrained word embeddings have proven to be invaluable for improving performance in natural language analysis tasks, which often suffer from paucity of data. However, their utility for NMT has not been extensively explored. In this work, we perform five sets of experiments that analyze when we can expect pre-trained word embeddings to help in NMT tasks. We show that such embeddings can be surprisingly effective in some cases – providing gains of up to 20 BLEU points in the most favorable setting.. –(Qi et al. 2018)"
  },
  {
    "objectID": "reviews/paper/20180-PTWM-NMT/index.html#outline",
    "href": "reviews/paper/20180-PTWM-NMT/index.html#outline",
    "title": "When and Why are Pre-trained Word Embeddings Useful for Neural Machine Translation?",
    "section": "Outline",
    "text": "Outline\n\n**Introduction\n\nDescribes the problem of low-resource scenarios in Neural Machine Translation (NMT) and the potential utility of pre-trained word embeddings.\nHighlights the success of pre-trained embeddings in natural language analysis tasks and the lack of extensive exploration in NMT.\nPoses five researcb questions:\n\nQ1 Is the behavior of pre-training affected by language families and other linguistic features of source and target languages? (§3)\nQ2 Do pre-trained embeddings help more when the size of the training data is small? (§4)\nQ3 How much does the similarity of the source and target languages affect the efficacy of using pre-trained embeddings? (§5)\nQ4 Is it helpful to align the embedding spaces between the source and target languages? (§6)\nQ5 Do pre-trained embeddings help more in multilingual systems as compared to bilingual systems? (§7)\n\n\nExperimental Setup\n\nDetails the five sets of experiments conducted to evaluate the effectiveness of pre-trained word embeddings in NMT.\nDescribes the datasets used, including the WMT14 English-German and English-French translation tasks.\nOutlines the models and training procedures employed in the experiments.\n\nResults and Analysis\n\nPresents the results of the experiments, showing the impact of pre-trained word embeddings on NMT performance.\nDiscusses the observed gains in BLEU scores and the factors influencing the effectiveness of pre-trained embeddings.\nAnalyzes the relationship between the quality of pre-trained embeddings and the performance of NMT systems.\n\nAnalysis\n\nConsiders the implications of the findings for NMT research and practice.\nDiscusses the potential benefits and limitations of using pre-trained word embeddings in NMT tasks.\n\nConclusion\n\nThe sweet-spot is wgere there is very little training data yet enough to train the system.\nPTWE are more effective if there are more similar translation pairs.\nA priori alignment of embeddings may not be necessary in bilingual scenarios, but is helpful in multi-lingual training scenarios."
  },
  {
    "objectID": "reviews/paper/20180-PTWM-NMT/index.html#the-paper",
    "href": "reviews/paper/20180-PTWM-NMT/index.html#the-paper",
    "title": "When and Why are Pre-trained Word Embeddings Useful for Neural Machine Translation?",
    "section": "The Paper",
    "text": "The Paper\n\n\n\npaper"
  },
  {
    "objectID": "reviews/paper/2024-MBR-decoding/index.html",
    "href": "reviews/paper/2024-MBR-decoding/index.html",
    "title": "Generating Diverse and High-Quality Texts by Minimum Bayes Risk Decoding",
    "section": "",
    "text": "Litrature review"
  },
  {
    "objectID": "reviews/paper/2024-MBR-decoding/index.html#abstract",
    "href": "reviews/paper/2024-MBR-decoding/index.html#abstract",
    "title": "Generating Diverse and High-Quality Texts by Minimum Bayes Risk Decoding",
    "section": "Abstract",
    "text": "Abstract\n\nOne of the most important challenges in text generation systems is to produce outputs that are not only correct but also diverse. Recently, Minimum Bayes-Risk (MBR) decoding has gained prominence for generating sentences of the highest quality among the decoding algorithms. However, existing algorithms proposed to generate diverse outputs are predominantly based on beam search or random sampling, thus their output quality is capped by these underlying decoding algorithms. In this paper, we investigate an alternative approach – we develop diversity-promoting decoding algorithms by enforcing diversity objectives to MBR decoding. We propose two variants of MBR\n\nDiverse MBR (DMBR) that adds a diversity penalty to the decoding objective and\nk-medoids MBR (KMBR) that reformulates the decoding task as a clustering problem.\n\nWe evaluate DMBR and KMBR on a variety of directed text generation tasks using encoder-decoder models and a language model with prompting. The experimental results show that the proposed method achieves a better trade-off than the diverse beam search and sampling algorithms overall – (Jinnai et al. 2024)\n\n\ngithub code repo"
  },
  {
    "objectID": "reviews/paper/2024-MBR-decoding/index.html#outline",
    "href": "reviews/paper/2024-MBR-decoding/index.html#outline",
    "title": "Generating Diverse and High-Quality Texts by Minimum Bayes Risk Decoding",
    "section": "Outline",
    "text": "Outline\n\nIntroduction\n\nDescribes the importance of generating diverse and high-quality texts in various natural language processing tasks.\nHighlights the limitations of existing approaches for diverse text generation, which are typically based on beam search or random sampling.\nPresents the paper’s focus on developing diversity-promoting algorithms using Minimum Bayes Risk (MBR) decoding.\nNotes the advantages of MBR decoding over traditional methods like beam search and random sampling.\nIntroduces:\n\nDiverse MBR (DMBR) and\nk-medoids MBR (KMBR).\n\n\nBackground\n\nDefines the sequence-to-sequence generation task and the goal of decoding in finding the hypothesis that aligns with human preference.\nDiscusses the concept of set decoding, where the aim is to generate a set of sentences that maximizes both human preference and diversity.\nReviews existing diversity-aware decoding algorithms, including random sampling techniques and diverse beam search.\nExplains the principle of Minimum Bayes Risk (MBR) decoding, focusing on its expected utility maximization approach and contrasting it with MAP decoding.\n\nMinimum Bayes Risk Decoding with Diversity\n\nIntroduces the set decoding problem with diversity objective using MBR decoding.\nPresents a naive approach for generating k sentences using MBR and notes its tendency to produce similar sentences.\nProposes Diverse MBR (DMBR) decoding as a solution by adding a diversity penalty to the objective function.\nExplains the formulation of DMBR, including the quality objective and diversity objective, and discusses the use of pairwise similarity as a diversity metric.\nHighlights the computational complexity of DMBR and the deployment of a greedy heuristic algorithm for approximation.\nProposes k-medoids MBR (KMBR) as an alternative method for diversity promotion in MBR decoding.\nExplains how KMBR leverages the k-medoids clustering algorithm to select a set of diverse and high-quality hypotheses.\nNotes the computational challenges of KMBR and the use of the Partition Around Medoids (PAM) algorithm for approximate computation.\n\nExperiments\n\nDescribes the experimental setup, including the tasks (machine translation, image captioning, question generation, common sense reasoning, text summarization), datasets, evaluation metrics, and the choice of BERTScore as the utility function for MBR.\nMentions the use of Huggingface’s Transformers library and other tools for the experiments.\nSummarizes the key results, highlighting that DMBR and KMBR generally achieve better trade-offs between quality and diversity than diverse beam search and sampling algorithms across the tasks.\n\nMachine Translation\n\nExplains the use of the WMT’19 dataset for machine translation experiments, focusing on German-English and Russian-English translation tasks.\nDiscusses the experimental settings, including the number of outputs, comparison baselines (sampling algorithms, diverse beam search), sample size for MBR, and diversity penalty parameters.\nPresents the key findings, emphasizing DMBR’s achievement of higher diversity, flexibility in the quality-diversity trade-off, and higher Oracle scores compared to baselines.\n\nImage Captioning using BLIP-2\n\nDescribes the use of the MS COCO dataset and the BLIP-2 model for evaluating performance on image captioning.\nBriefly notes the experimental settings, including the output size, baselines, and diversity penalty parameters.\nHighlights DMBR’s effectiveness in achieving lower P-BLEU, higher distinct-2, and better semantic diversity as measured by P-SentBERT.\n\nQuestion Generation using Language Model\n\nPresents the evaluation of decoding algorithms for question generation using the SQuADv2 dataset and a language model (Zephyr-7B β) with prompting.\nMentions the experimental settings, including the number of outputs, baselines, sample size for MBR, and diversity penalty parameters.\nDiscusses the findings, noting that DMBR demonstrates advantages in distinct-2 and P-SentBERT but underperforms slightly in P-BLEU compared to DBS.\n\nGenerative Common Sense Reasoning using Language Model\n\nExplains the use of the CommonGen task and the Zephyr-7B β language model for evaluating common sense reasoning abilities.\nNotes the use of prompting and the experimental settings, including the number of outputs, baselines, sample size, and diversity penalty.\nBriefly mentions DMBR’s performance, achieving better distinct-2 but slightly worse P-BLEU compared to DBS, and discusses the low coverage of input concepts in the generations.\n\nText Summarization\n\nDescribes the evaluation of text summarization using the XSum dataset and a BART model pre-trained on XSum.\nMentions the experimental settings, including the output size, baselines, sample size for MBR, and diversity penalty parameters.\nBriefly presents the results, emphasizing DMBR’s better diversity compared to DBS, as measured by P-BLEU and distinct-n.\n\nConclusions\n\nSummarizes the research, highlighting the development and evaluation of DMBR and KMBR for generating diverse and high-quality texts.\nReiterates the better quality-diversity trade-off achieved by these methods compared to diverse beam search, and notes the higher Oracle scores attained by DMBR and KMBR over vanilla MBR.\nDiscusses potential future research directions, including applying the methods to open-ended text generation, conducting human evaluation of diversity, and reducing the inference time of DMBR and KMBR.\n\nLimitations\n\nDiscusses the limitations of the research, including the focus on directed text generation tasks.\nNotes the reliance on automatic evaluation metrics and the need for human evaluation.\nHighlights the slower inference time of DMBR and KMBR compared to DBS, suggesting further research on computational efficiency.\nMentions the use of a simple greedy algorithm for DMBR and the potential for more sophisticated approximation algorithms.\n\nAppendices\n\nAppendix A: Provides a proof of submodularity.\nAppendix B: Evaluates the coverage of input concepts for CommonGen, noting the low coverage in the experiments.\nAppendix C: Shows examples of generations from various decoding algorithms across different tasks.\nAppendix D: Evaluates the oversampling strategy as a baseline, comparing its performance to DMBR.\nAppendix E: Presents additional figures and tables summarizing the experimental results.\nAppendix F: Lists the pre-trained models and codes used in the experiments."
  },
  {
    "objectID": "reviews/paper/2024-MBR-decoding/index.html#the-paper",
    "href": "reviews/paper/2024-MBR-decoding/index.html#the-paper",
    "title": "Generating Diverse and High-Quality Texts by Minimum Bayes Risk Decoding",
    "section": "The Paper",
    "text": "The Paper\n\n\n\npaper"
  },
  {
    "objectID": "reviews/paper/2023-exposing-glitches/index.html",
    "href": "reviews/paper/2023-exposing-glitches/index.html",
    "title": "Exposing Attention Glitches with Flip-Flop Language Modeling",
    "section": "",
    "text": "Literature review"
  },
  {
    "objectID": "reviews/paper/2023-exposing-glitches/index.html#podcast",
    "href": "reviews/paper/2023-exposing-glitches/index.html#podcast",
    "title": "Exposing Attention Glitches with Flip-Flop Language Modeling",
    "section": "Podcast",
    "text": "Podcast"
  },
  {
    "objectID": "reviews/paper/2023-exposing-glitches/index.html#abstract",
    "href": "reviews/paper/2023-exposing-glitches/index.html#abstract",
    "title": "Exposing Attention Glitches with Flip-Flop Language Modeling",
    "section": "Abstract",
    "text": "Abstract\n\nWhy do large language models sometimes output factual inaccuracies and exhibit erroneous reasoning? The brittleness of these models, particularly when executing long chains of reasoning, currently seems to be an inevitable price to pay for their advanced capabilities of coherently synthesizing knowledge, pragmatics, and abstract thought. Towards making sense of this fundamentally unsolved problem, this work identifies and analyzes the phenomenon of attention glitches, in which the Transformer architecture’s inductive biases intermittently fail to capture robust reasoning. To isolate the issue, we introduce flip-flop language modeling (FFLM), a parametric family of synthetic benchmarks designed to probe the extrapolative behavior of neural language models. This simple generative task requires a model to copy binary symbols over long-range dependencies, ignoring the tokens in between. We find that Transformer FFLMs suffer from a long tail of sporadic reasoning errors, some of which we can eliminate using various regularization techniques. Our preliminary mechanistic analyses show why the remaining errors may be very difficult to diagnose and resolve. We hypothesize that attention glitches account for (some of) the closed-domain hallucinations in natural LLMs. – (Liu et al. 2023)"
  },
  {
    "objectID": "reviews/paper/2023-exposing-glitches/index.html#outline",
    "href": "reviews/paper/2023-exposing-glitches/index.html#outline",
    "title": "Exposing Attention Glitches with Flip-Flop Language Modeling",
    "section": "Outline",
    "text": "Outline\n\nIntroduction\n\nDescribes the problem of factual inaccuracies and erroneous reasoning in large language models (LLMs), particularly in long chains of reasoning.\nPresents integer addition problems as a simple example of algorithmic reasoning where LLMs exhibit sporadic errors, highlighting both their capabilities and limitations.\nIntroduces the concept of “attention glitches” as a potential explanation for these errors, suggesting that the Transformer architecture’s inductive biases may intermittently fail to capture robust reasoning.\n\nFlip-flop Automata and the FFLM Task\n\nDefines flip-flop strings and flip-flop languages, focusing on a canonical family parameterized by the probabilities of write, read, and ignore instructions.\nIntroduces the flip-flop language modeling (FFLM) task, which involves training language models to generate or predict continuations of flip-flop strings, emphasizing the importance of perfect read operation accuracy.\nDiscusses the rationale for focusing on flip-flops, highlighting their role as fundamental building blocks of memory and their relevance to various reasoning tasks.\n\nAttention Glitches: A Long Tail of Errors for Transformer FFLMs\n\nPresents the main empirical result: Transformer models trained on FFLM exhibit a long tail of unpredictable reasoning errors (attention glitches), even on simple tasks like remembering one bit.\nHighlights the contrast between Transformers and LSTMs, showing that LSTMs achieve perfect accuracy on FFLM with significantly fewer resources.\nNotes that similar attention glitches are observed in real LLMs when prompted to complete natural language embeddings of flip-flop tasks.\nDiscusses multiple potential mechanisms for attention glitches, including implicit n-gram models, Lipschitz limitations of soft attention, and the difficulty of non-commutative tiebreaking.\n\nMitigations for Attention Glitches\n\nInvestigates various approaches to eliminate attention glitches in Transformer FFLMs, using a 6-layer 19M-parameter model as a canonical baseline.\nDiscusses the effects of training data and scale, showing that training on rare sequences significantly reduces errors, while resource scaling provides weaker improvements.\nExplores indirect algorithmic controls, including standard regularization techniques and attention-sharpening regularizers, finding that some choices improve extrapolation but none completely eliminate glitches.\nPresents a preliminary mechanistic study of trained networks, showing that attention-sharpening promotes hard attention but errors persist due to the complexity and redundancy of attention patterns.\n\nConclusion and Future Challenges\n\nSummarizes the findings, emphasizing that attention glitches represent a systematic architectural flaw in Transformers that may contribute to closed-domain hallucinations in natural LLMs.\nDiscusses the challenges of confirming or refuting the hypothesis that attention glitches cause hallucinations in natural LLMs, highlighting the need for further research.\nSuggests potential paths to hallucination-free Transformers, including data diversity, scale, regularization, and architectural innovations inspired by recurrent models.\nMentions the broader impacts and limitations of the work, emphasizing its foundational nature and the potential for unintended consequences of improved factual reliability in LLMs.\n\nAppendix\n\nProvides deferred background information on flip-flop terminology and history, including the definition of the flip-flop automaton and its transformation monoid.\nDiscusses additional related work on hallucinations, long-range dependencies, explicit memory mechanisms, and Transformers’ performance on algorithmic tasks.\nExplains the rationale for the specific flip-flop language used in the study, highlighting its compatibility with standard language modeling and its parsimonious encoding.\nElaborates on the hypothesis that attention glitches cause hallucinations in natural LLMs, discussing the challenges of formalizing and testing this hypothesis.\nPresents full experimental results, including details for LLM addition prompts, extrapolation failures of standard Transformers, effects of training data and scale, indirect algorithmic controls, and preliminary mechanistic studies.\nProvides proofs for propositions related to the realizability of FFL by small Transformers, the failure of soft attention due to attention dilution, and the failure of hard attention due to bad margins for positional embeddings.\nNotes the software, compute infrastructure, and resource costs associated with the experiments."
  },
  {
    "objectID": "reviews/paper/2023-exposing-glitches/index.html#the-paper",
    "href": "reviews/paper/2023-exposing-glitches/index.html#the-paper",
    "title": "Exposing Attention Glitches with Flip-Flop Language Modeling",
    "section": "The Paper",
    "text": "The Paper\n\n\n\npaper"
  },
  {
    "objectID": "reviews/paper/2025-02-03-xLSTM/index.html",
    "href": "reviews/paper/2025-02-03-xLSTM/index.html",
    "title": "xLSTM: Extended Long Short-Term Memory",
    "section": "",
    "text": "Literature Review\n\n\n\n\n\n\n\nVideo 1: Review by AI Bites\n\n\n\n\n\n\n\n\nVideo 2: Review by Yannic Kilcher\n\n\n\n\n\n\n\n\nVideo 3: LSTM: The Comeback Story? Talk with Sepp Hochreiter after the xLSTM paper"
  },
  {
    "objectID": "reviews/paper/2025-02-03-xLSTM/index.html#introduction",
    "href": "reviews/paper/2025-02-03-xLSTM/index.html#introduction",
    "title": "xLSTM: Extended Long Short-Term Memory",
    "section": "Introduction",
    "text": "Introduction\n\n\n\n\n\n\nMotivation\n\n\n\nSo this is my first review of the RNN papers. I found that since the courses covering LSTM seemed to be over simplified and confusing, I should go back to the source. Since time is limited, it helps to review the papers in reverse chronological order. Each new papers explains the limitations that are generally not apparent when the paper is first published. The authors of the later papers also have the benefit of hindsight which they share in the obligatory literature review wherein they provide lots of context we cannot get by reading the original papers. Given all that there is also a benefit to reading the original paper - the real innovators while harder to understand are remarkable thinkers and often reveal motivations and deep insights that elude later interlocutors.\n\n\n\nPodcast & Other Reviews\nThis paper is a bit heavy on the technical side. But if we are still here, we may enjoy this podcast reviewing the paper. Though AI generated I found it quite useful to get oriented with the paper and understand that the experiments the authors run to validate this new architectures.\nWe also cover something I found absent from the Course on sequence models . The limitations of LSTMs as viewed by the authors when considered in the context of transformers and Mamba.\n\n\n\nAlthough this paper is recent there are a number of other people who cover it.\n\nIn Video 2 Yannic Kilcher covers the paper in his youtube channel. He does a great job of explaining the paper in a way that is easy to understand. Kilcher is all too human, he makes many mistakes, stresses opinions over facts. If we don’t mind all that he is a great resource for understanding the paper. I find he rubs me the wrong way but at least I find this motivating to do better in my own reviews!\n\n\n\n\n\n\n\nWhat can we expect from xLSTM?\n\n\n\n\n\nxLSTM has the potential to considerably impact other fields like Reinforcement Learning, Time Series Prediction, or the modeling of physical systems. – (Beck et al. 2024)\n\nIn his book Understanding Media (McLuhan 1988) Marshal McLuhan introduced his Tetrad. The Tetrad is a mental model for understanding how a technological innovation like the xLSTM might disrupt society. To work the model we ask the following four questions that can be applied to any new technology to understand its impact. The four questions in this instance are:\n\nWhat does the xLSTM enhance or amplify?\n\n\nThe xLSTM enhances the LSTM by introducing exponential gating and novel memory structures, boosting its capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models.\n\n\nWhat does the xLSTM make obsolete or replace?\n\n\nThe xLSTM makes the traditional LSTM obsolete by addressing its limitations, such as the inability to revise storage decisions, limited storage capacities, and lack of parallelizability.\n\n\nWhat does the xLSTM retrieve that was previously obsolesced?\n\n\nThe xLSTM retrieves the LSTM’s relevance in large-scale language modeling by scaling it to billions of parameters and integrating modern LLM techniques while mitigating known limitations. 4. What does the xLSTM reverse or flip into when pushed to its limits?\n\n\nThe xLSTM, when pushed to its limits, can compete with current Large Language Models based on Transformer technology, offering a competitive alternative for sequence modeling tasks.\n\nThe xLSTM paper by (Beck et al. 2024) is a great example of the tetrad in action. The authors take the LSTM, a technology that has stood the test of time, and enhance it with exponential gating and novel memory structures. They address the limitations of traditional LSTMs and push the technology to new limits, creating a new architecture that can compete with state-of-the-art Transformers and State Space Models. Recent work by (Chen et al. 2024) suggest that Transformers and The State Space Models are actually limited in their own ways.\n\n\n\n\n\n\n\n\nTetrad\n\n\n\n\nSupplementary Figure 1"
  },
  {
    "objectID": "reviews/paper/2025-02-03-xLSTM/index.html#abstract",
    "href": "reviews/paper/2025-02-03-xLSTM/index.html#abstract",
    "title": "xLSTM: Extended Long Short-Term Memory",
    "section": "Abstract",
    "text": "Abstract\n\nIn the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining:\n\nsLSTM with a scalar memory, a scalar update, and new memory mixing,\n\nmLSTM that is fully parallelizable with a matrix memory and a covariance update rule.\n\n\nIntegrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling. — (Beck et al. 2024)\n\n\n\n\n\n\n\n\narchitecture\n\n\n\n\nFigure 1: The extended LSTM (xLSTM) family. From left to right:\n1. The original LSTM memory cell with constant error carousel and gating.\n2. New sLSTM and mLSTM memory cells that introduce exponential gating. sLSTM offers a new memory mixing technique. mLSTM is fully parallelizable with a novel matrix memory cell state and new covariance update rule.\n3. mLSTM and sLSTM in residual blocks yield xLSTM blocks.\n4. Stacked xLSTM blocks give an xLSTM\n\n\n\n\n\n\n\n\nblocks\n\n\n\n\nFigure 2: LSTM limitations.\n- Left: Nearest Neighbor Search problem in terms of mean squared error (MSE). Given a reference vector, a sequence is scanned sequentially for the most similar vector with the objective to return its attached value at sequence end. LSTM struggles to revise a stored value when a more similar vector is found. Our new xLSTM overcomes this limitation by exponential gating. - Right: Rare Token Prediction. The perplexity (PPL) of token prediction on Wikitext-103, in partitions of token frequency. LSTM performs worse on predicting rare tokens because of its limited storage capacities, whereas our new xLSTM solves this problem via a matrix memory.\n\n\n\n\n\n\n\n\nblocks\n\n\n\n\nFigure 3: xLSTM blocks.\nLeft: A residual sLSTM block with post up-projection (like Transformers): The input is fed into an sLSTM — with an optional convolution — followed by a gated MLP.\nRight: A residual mLSTM block with pre up-projection (like State Space models): mLSTM is wrapped inside two MLPs, via a convolution, a learnable skip connection, and an output gate that acts componentwise.\n\n\n\n\n\n\n\n\nblocks\n\n\n\n\nFigure 4: xLSTM blocks.\nLeft: A residual sLSTM block with post up-projection (like Transformers): The input is fed into an sLSTM — with an optional convolution — followed by a gated MLP.\nRight: A residual mLSTM block with pre up-projection (like State Space models): mLSTM is wrapped inside two MLPs, via a convolution, a learnable skip connection, and an output gate that acts componentwise.\n\n\n\n\n\n\n\n\nblocks\n\n\n\n\nFigure 5: xLSTM blocks.\nLeft: A residual sLSTM block with post up-projection (like Transformers): The input is fed into an sLSTM — with an optional convolution — followed by a gated MLP.\nRight: A residual mLSTM block with pre up-projection (like State Space models): mLSTM is wrapped inside two MLPs, via a convolution, a learnable skip connection, and an output gate that acts componentwise."
  },
  {
    "objectID": "reviews/paper/2025-02-03-xLSTM/index.html#sec-outline",
    "href": "reviews/paper/2025-02-03-xLSTM/index.html#sec-outline",
    "title": "xLSTM: Extended Long Short-Term Memory",
    "section": "Paper Outline",
    "text": "Paper Outline\n\nIntroduction\n\nDescribes the constant error carousel and gating mechanisms of Long Short-Term Memory (LSTM). &gt; “In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories…”\nDiscusses three main limitations of LSTMs:\n\nThe inability to revise storage decisions.\nLimited storage capacities.\nLack of parallelizability due to memory mixing.\n\nHighlights the emergence of Transformers in language modeling due to these limitations. &gt; Transformer Era: The rise of Transformers with parallelizable self-attention has largely overshadowed LSTMs due to their superior scalability.\n\nExtended Long Short-Term Memory\n\nIntroduces Extended Long Short-Term Memory (xLSTM), which incorporates exponential gating and novel memory structures.\nPresents two new LSTM variants:\n\nsLSTM with a scalar memory, a scalar update, and new memory mixing.\nmLSTM with a matrix memory and a covariance update rule, which is fully parallelizable.\n\nDescribes the integration of these LSTM variants into residual block modules, resulting in xLSTM blocks.\nPresents xLSTM architectures that residually stack xLSTM blocks.\n\nRelated Work:\n\nMentions various linear attention methods to overcome the quadratic complexity of Transformer attention.\nNotes the popularity of State Space Models (SSMs) for language modeling.\nHighlights the use of Recurrent Neural Networks (RNNs) as a replacement for Transformers.\nMentions the use of gating in recent RNN and SSM approaches.\nNotes the use of covariance update rules1 to enhance storage capacities in various models.\n\nExperiments\n\nPresents the experimental evaluation of xLSTM compared to existing methods with a focus on language modeling.\nDiscusses the effectiveness of xLSTM on synthetic tasks, including:\n\nFormal languages.\nMulti-Query Associative Recall.\nLong Range Arena tasks.\n\nPresents a method comparison and ablation study, where different models are trained on 15 billion tokens from the SlimPajama dataset.\nAssesses the scaling behavior of different methods based on validation perplexity.\nConducts a large-scale language modeling experiment:\n\nTraining different model sizes on 300B tokens from SlimPajama.\nEvaluating models on length extrapolation.\nAssessing models on validation perplexity and performance on downstream tasks.\nEvaluating models on 571 text domains of the PALOMA language benchmark dataset.\nAssessing the scaling behavior with increased training data.\n\n\nLimitations\n\nHighlights limitations of xLSTM, including:\n\nLack of parallelizability for sLSTM due to memory mixing.\nUnoptimized CUDA kernels for mLSTM.\nHigh computational complexity of mLSTM’s matrix memory.\nMemory overload for longer contexts due to the independence of mLSTM’s memory from sequence length.\n\n\nConclusion\n\nConcludes that xLSTM performs favorably in language modeling compared to Transformers and State Space Models.\nSuggests that larger xLSTM models will be competitive with current Large Language Models based on Transformer technology.\nNotes the potential impact of xLSTM on other deep learning fields such as reinforcement learning, time series prediction, and physical system modeling.\n\n\n1 explain covariance"
  },
  {
    "objectID": "reviews/paper/2025-02-03-xLSTM/index.html#briefing-xlstm---extended-long-short-term-memory",
    "href": "reviews/paper/2025-02-03-xLSTM/index.html#briefing-xlstm---extended-long-short-term-memory",
    "title": "xLSTM: Extended Long Short-Term Memory",
    "section": "Briefing : xLSTM - Extended Long Short-Term Memory",
    "text": "Briefing : xLSTM - Extended Long Short-Term Memory\n\nIntroduction and Motivation:\nLSTM’s Legacy: The Long Short-Term Memory (LSTM), introduced in the 1990s, has been a cornerstone of deep learning, especially in sequence modeling and early Large Language Models (LLMs). LSTMs are known for their “constant error carousel” and gating mechanisms, designed to combat vanishing gradients in recurrent neural networks.\n\n“In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories…”\n\nTransformer Era: The rise of Transformers with parallelizable self-attention has largely overshadowed LSTMs due to their superior scalability.\n\n“However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale.” xLSTM Question: This research aims to explore the capabilities of LSTMs when scaled to billions of parameters, by integrating modern LLM techniques while addressing known limitations.\n\n\n“We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs?”\n\n\n\nKey Limitations of Traditional LSTMs:\nThe paper identifies three major limitations of traditional LSTMs:\n\nInability to Revise Storage Decisions: LSTMs struggle to update previously stored information when more relevant information arrives later in the sequence. The paper uses a “Nearest Neighbor Search” task to demonstrate this.\n\n\n“LSTM struggles to revise a stored value when a more similar vector is found…”\n\n\nLimited Storage Capacity: LSTMs store information in scalar cell states, which can lead to information compression and loss. This impacts performance on tasks requiring detailed memory, like predicting rare tokens.\n\n\n“LSTM performs worse on predicting rare tokens because of its limited storage capacities…”\n\n\nLack of Parallelizability: LSTMs process data sequentially due to the recurrent connections between hidden states, hindering parallel computation.\n\n\n“Lack of parallelizability due to memory mixing, i.e., the hidden-hidden connections between hidden states from one time step to the next, which enforce sequential processing.”\n\n\nxLSTM Innovations:\n\nThe paper introduces Extended Long Short-Term Memory (xLSTM), designed to address the above limitations, by modifying the original LSTM using two main approaches:\nExponential Gating:Implemented to allow for better revision of storage decisions. Normalization and stabilization techniques are used to avoid overflows in calculations due to the exponential function.\n\n“To empower LSTMs with the ability to revise storage decisions, we introduce exponential gates (red) together with normalization and stabilization.” Novel Memory Structures: Two new LSTM variants are created with these structures:\n\nsLSTM: A scalar memory LSTM with a scalar update and new memory mixing technique (within heads, not across). Features multiple memory cells and multiple heads, with memory mixing within each head.\n\n“The new sLSTM (see Section 2.2) with a scalar memory, a scalar update, and memory mixing…”\n\nmLSTM: A matrix memory LSTM, fully parallelizable, with a covariance update rule for storing key-value pairs. Does not have memory mixing between cells or heads.\n\n“the new mLSTM (see Section 2.3) with a matrix memory and a covariance (outer product) update rule, which is fully parallelizable.”\n\n\nsLSTM Details:\n\n\nExponential Gates: Introduces exponential activation functions for input and forget gates with normalization and a stabilizer state to prevent overflow.\nNormalizer State: A state used to normalize the hidden state output by accumulating a weighted sum of input and forget gates.\nMemory Mixing: sLSTM allows memory mixing between cells, and also introduces a new form of memory mixing through multiple heads, mixing between cells within a head, but not across heads.\n\n\n“The new sLSTM can have multiple heads with memory mixing within each head but not across heads. The introduction of heads for sLSTM together with exponential gating establishes a new way of memory mixing.”\n\n\nmLSTM Details:\n\nMatrix Memory: Replaces the scalar cell state with a matrix, increasing storage capacity.\nKey-Value Storage: The mLSTM memory is based around a key-value storage system inspired by Bidirectional Associative Memories (BAMs) using covariance updates, where keys and values are updated via an outer product.\n\n“At time t, we want to store a pair of vectors, the key k_t ∈ R^d and the value v_t ∈ R^d… The covariance update rule… for storing a key-value pair…”\n\nParallelization: mLSTM is fully parallelizable due to abandoning hidden-hidden recurrent connections and memory mixing. Computations can be performed efficiently on GPUs using standard matrix operations.\n\n“…the mLSTM… which is fully parallelizable.”\n\n\nxLSTM Architecture:\n\nxLSTM Blocks: The sLSTM and mLSTM variants are integrated into residual blocks.\nsLSTM blocks use post up-projection (like Transformers).\nmLSTM blocks use pre up-projection (like State Space Models).\n\n“Integrating these new LSTM variants into residual block modules results in xLSTM blocks…”\n\nResidual Stacking: xLSTM architectures are created by residually stacking these blocks, using pre-LayerNorm backbones similar to current LLMs.\n\n“An xLSTM architecture is constructed by residually stacking build-ing blocks…” Cover’s Theorem: Used to justify mapping information into a higher-dimensional space for better separation of histories or contexts.\n\n\n“We resort to Cover’s Theorem (Cover, 1965), which states that in a higher dimensional space non-linearly embedded patterns can more likely be linearly separated than in the original space.”\n\n\nPerformance and Scaling:\n\nLinear Computation & Constant Memory: Unlike Transformers, xLSTM networks have linear computation and constant memory complexity with respect to sequence length.\n\n“Contrary to Transformers, xLSTM networks have a linear computation and a constant memory complexity with respect to the sequence length.”\n\nSynthetic Tasks: xLSTM showed improved performance over regular LSTMs on “Nearest Neighbor Search” and “Rare Token Prediction” synthetic tasks.\nSlimPajama Experiments: The xLSTM models were trained on 15B and 300B tokens from the SlimPajama dataset for language modelling evaluation.\nCompetitive Performance: xLSTMs demonstrate competitive performance against state-of-the-art Transformers and State Space Models on language modelling benchmarks, both in terms of next token prediction perplexity and downstream tasks.\n\n“Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.”\n\nAblation studies show importance of gating techniques.\n\nMemory & Speed:\n\nsLSTM: Not parallelizable due to memory mixing, although the authors have created a fast CUDA implementation (less than twice as slow as mLSTM).\n\n“While mLSTM is parallelizable analog to FlashAttention… sLSTM is not parallelizable due to the memory mixing…”\n\nmLSTM: Parallelizable, but currently slower than FlashAttention or Mamba due to non-optimized CUDA kernels. Matrix memory is computationally complex but does not require parameters and can be parallelized.\n\n“The memory of mLSTM does not require parameters but is computationally expensive through its d×d matrix memory and d× d update… the computations can be done in parallel on GPUs, therefore these computations have only a minor effect on the wall clock time.”\n\n\nLimitations:\n\nsLSTM Parallelization: sLSTM’s memory mixing is non-parallelizable.\n\n“(i) In contrast to mLSTM, memory mixing of the sLSTM prohibits parallelizable operations, and therefore does not allow a fast parallel implementation.”\n\nmLSTM CUDA Optimization: The mLSTM CUDA kernels are not fully optimized.\n\n“(ii) The CUDA kernels for mLSTM are not optimized, and therefore the current implementation is about 4 times slower than FlashAttention or the scan used in Mamba.”\n\nmLSTM Matrix Memory: High computational complexity for mLSTM due to matrix memory operations.\nForget Gate Initialization: Careful initialization of the forget gates is needed.\nLong Context Memory: The matrix memory is independent of sequence length, and might overload memory for long context sizes.\n\n“(v) Since the matrix memory is independent of the sequence length, increasing the sequence length might overload the memory for longer context sizes.”\n\nHyperparameter Optimization: xLSTM is not yet fully optimized due to computational load, indicating further performance gains are possible.\n\n“(vi) Due to the expensive computational load for large language experiments, we did neither fully optimize the architecture nor the hyperparameters, especially for larger xLSTM architectures.”\n\n\nRelated Work:\n\nThe paper highlights connections of its ideas with the following areas:\nGating: Many recent models such as HGRN, GLA, Gated State Space models, Mamba, and others also utilize gating mechanisms. Covariance Update Rule: The covariance update rule used in mLSTM is related to other methods such as Fast Weight Programmers, Retention, and Linear Transformers.\n\nConclusion:\n\nThe xLSTM presents a compelling alternative to Transformers in sequence modeling, especially in contexts where memory efficiency and longer sequences are required. By combining insights from the original LSTM with exponential gating and novel memory structures, the paper shows that LSTMs can be competitive at scale. While limitations exist in terms of parallelization and hyperparameter tuning, the research shows promising potential for further development. The authors posit further research and improvements could allow xLSTM to reach its full potential."
  },
  {
    "objectID": "reviews/paper/2025-02-03-xLSTM/index.html#my-thoughts",
    "href": "reviews/paper/2025-02-03-xLSTM/index.html#my-thoughts",
    "title": "xLSTM: Extended Long Short-Term Memory",
    "section": "My Thoughts",
    "text": "My Thoughts\n\n\n\n\n\n\n\nVideo 4: Review of the sigmoid function\n\n\n\n\n\n\n\n\nResearch questions\n\n\n\n\nHow does the constant error carousel mitigate the vanishing gradient problem in the LSTM?\n\nThe constant error carousel is a way to keep the error constant over time. This helps to prevent the vanishing gradient problem in the LSTM.\n\nHow are the gates in the original LSTM binary?\n\nSigmoid, saturation and a threshold at 0.5\n\nWhat is the long term memory in the LSTM?\n\nthe cell state c_{t-1}\n\nWhat is the short term memory in the LSTM?\n\nthe hidden state h_{t-1}\n\nHow far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs\n\n\n\n\n\n\n\n\n\n\nSupplementary Figure 2: limits of the sigmoid function\n\n\n\n\n\n\n\n\nbias\n\n\n\n\nSupplementary Figure 3: The inductive bias of the sigmoid decision function.\n\n\n\n\nBinary nature of non exponential Gating Mechanisms\nIf we try to understand why are the gating mechanisms in the original LSTM is described here as binary?\nIt helps to the properties of the sigmoid functions that are explained in Video 4.\n\nSupplementary Figure 1 shows that The sigmoid function has a domain of \\mathbb{R} and a range of (0,1). This means that the sigmoid function can only output values between 0 and 1.\nIt has a very sharp gradient around 0 if the bias is set to 0. This means that the sigmoid function acts as a binary function around 0.\nThe Sigmoid function tends to saturate. Now sigmoid function around 0 isn’t quite quite binary but outside of (-6,+6) it becomes saturated and acts very much as a binary function.\nIf we use it as a loss function or a decision function, the sigmoid function is used with a threshold of 0.5. This means that if the output of the sigmoid function is greater than 0.5, the decision is 1, and if it is less than 0.5, the decision is 0. That is how the sigmoid function can become binary.\nNote that we can use thresholding during inference to make the decision binary and we can keep them as continuous values during training so we can have a smooth gradient for back-propagation.\nEven without thresholding the sigmoid function can become binary this is due to the saturation of the sigmoid function. I recall that Hinton in his Coursera course on Neural Networks and Deep Learning mentioned that activation function that become saturated are problematic. I can say that at last I think I understand what he meant:\nI see that that sigmoid functions can fall off the manifold of the data and this can be a problem. This is why the ReLU function is used in the Gated Recurrent Unit (GRU) and the Transformer. Falling off the manifold of the data means that the sigmoid function becomes saturated (above 6 and below -6). At this point the back-propagation steps are no longer able to change the weight or bias since the gradient is effectively so learning stops.\n\nThis becomes a is a problem in two senses. The sigmoid unit is likely to be stuck in a setting that only representative of part of the training data. The rest of the network which is not stuck now has to learn to compensate for this.\nThis issues may well be the root cause LSTM limitation number 1 - the inability to revise storage decisions. It seems that the authors decided to use the exponential gating function to address this issues.\nThis is why the gating mechanisms in the original LSTM are binary. The sigmoid function is used to control the flow of information in the LSTM by deciding which information to keep and which to discard. The sigmoid function is used to decide how much of the new input information should be added to the cell state and how much of the old cell state should be retained. This binary nature of the sigmoid function is what allows the LSTM to control the flow of information and decide what to remember and what to forget."
  },
  {
    "objectID": "reviews/paper/2025-02-03-xLSTM/index.html#the-paper",
    "href": "reviews/paper/2025-02-03-xLSTM/index.html#the-paper",
    "title": "xLSTM: Extended Long Short-Term Memory",
    "section": "The Paper",
    "text": "The Paper\n\n\n\npaper"
  },
  {
    "objectID": "reviews/paper/2025-02-03-xLSTM/index.html#references",
    "href": "reviews/paper/2025-02-03-xLSTM/index.html#references",
    "title": "xLSTM: Extended Long Short-Term Memory",
    "section": "References",
    "text": "References\n\nThe paper on open review has some additional insights from the authors\nXLSTM — Extended Long Short-Term Memory Networks By Shrinivasan Sankar — May 20, 2024"
  },
  {
    "objectID": "notes/cs11-737-w10/index.html",
    "href": "notes/cs11-737-w10/index.html",
    "title": "Translation and Translation Data",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\nVideo 1: Lesson Video\n\n\n\n\n\n\n\n\nThis week’s slides\n\n\n\n\nSupplementary Figure 1\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{bochman2022,\n  author = {Bochman, Oren},\n  title = {Translation and {Translation} {Data}},\n  date = {2022-01-24},\n  url = {https://orenbochman.github.io/notes-nlp/notes/cs11-737-w10/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2022. “Translation and Translation Data.”\nJanuary 24, 2022. https://orenbochman.github.io/notes-nlp/notes/cs11-737-w10/."
  },
  {
    "objectID": "notes/c4w3/lab03.html",
    "href": "notes/c4w3/lab03.html",
    "title": "Assignment 3 Ungraded Sections - Part 2: T5 SQuAD Model",
    "section": "",
    "text": "course banner\nWelcome to the part 2 of testing the models for this week’s assignment. This time we will perform decoding using the T5 SQuAD model. In this notebook we’ll perform Question Answering by providing a “Question”, its “Context” and see how well we get the “Target” answer.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "L3 - T5"
    ]
  },
  {
    "objectID": "notes/c4w3/lab03.html#colab",
    "href": "notes/c4w3/lab03.html#colab",
    "title": "Assignment 3 Ungraded Sections - Part 2: T5 SQuAD Model",
    "section": "Colab",
    "text": "Colab\nSince this ungraded lab takes a lot of time to run on coursera, as an alternative we have a colab prepared for you.\nT5 SQuAD Model Colab\n\nIf you run into a page that looks similar to the one below, with the option Open with, this would mean you need to download the Colaboratory app. You can do so by Open with -&gt; Connect more apps -&gt; in the search bar write \"Colaboratory\" -&gt; install\n\n\n\nAfter installation it should look like this. Click on Open with Google Colaboratory",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "L3 - T5"
    ]
  },
  {
    "objectID": "notes/c4w3/lab03.html#outline",
    "href": "notes/c4w3/lab03.html#outline",
    "title": "Assignment 3 Ungraded Sections - Part 2: T5 SQuAD Model",
    "section": "Outline",
    "text": "Outline\n\nOverview\nPart 1: Resuming the assignment (T5 SQuAD Model)\nPart 2: Fine-tuning on SQuAD\n\n2.1 Loading in the data and preprocessing\n2.2 Decoding from a fine-tuned model\n\n\n\nOverview\nIn this notebook you will:\n\nImplement the Bidirectional Encoder Representation from Transformer (BERT) loss.\nUse a pretrained version of the model you created in the assignment for inference.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "L3 - T5"
    ]
  },
  {
    "objectID": "notes/c4w3/lab03.html#1",
    "href": "notes/c4w3/lab03.html#1",
    "title": "Assignment 3 Ungraded Sections - Part 2: T5 SQuAD Model",
    "section": "Part 1: Getting ready",
    "text": "Part 1: Getting ready\nRun the code cells below to import the necessary libraries and to define some functions which will be useful for decoding. The code and the functions are the same as the ones you previsouly ran on the graded assignment.\n\nimport string\nimport t5\nimport numpy as np\nimport trax \nfrom trax.supervised import decoding\nimport textwrap \n\nwrapper = textwrap.TextWrapper(width=70)\n\n\n---------------------------------------------------------------------------\nImportError                               Traceback (most recent call last)\nCell In[1], line 2\n      1 import string\n----&gt; 2 import t5\n      3 import numpy as np\n      4 import trax \n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/t5/__init__.py:17\n      1 # Copyright 2023 The T5 Authors.\n      2 #\n      3 # Licensed under the Apache License, Version 2.0 (the \"License\");\n   (...)\n     12 # See the License for the specific language governing permissions and\n     13 # limitations under the License.\n     15 \"\"\"Import API modules.\"\"\"\n---&gt; 17 import t5.data\n     18 import t5.evaluation\n     20 # Version number.\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/t5/data/__init__.py:17\n     15 \"\"\"Import data modules.\"\"\"\n     16 # pylint:disable=wildcard-import,g-bad-import-order\n---&gt; 17 from t5.data.dataset_providers import *\n     18 from t5.data.glue_utils import *\n     19 import t5.data.postprocessors\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/t5/data/dataset_providers.py:28\n     25 from collections.abc import Mapping\n     26 import re\n---&gt; 28 import seqio\n     29 from t5.data import utils\n     30 import tensorflow.compat.v2 as tf\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/seqio/__init__.py:19\n     15 \"\"\"Import to top-level API.\"\"\"\n     17 # pylint:disable=wildcard-import,g-bad-import-order,g-import-not-at-top\n---&gt; 19 from seqio.dataset_providers import *\n     20 from seqio import evaluation\n     21 from seqio import experimental\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/seqio/dataset_providers.py:36\n     33 from typing import Any, Callable, Iterable, List, Mapping, MutableMapping, Optional, Sequence, Set, Tuple, Type, Union\n     35 from absl import logging\n---&gt; 36 import clu.metrics\n     37 import editdistance\n     38 import numpy as np\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/clu/metrics.py:66\n     64 from clu.internal import utils\n     65 import clu.values\n---&gt; 66 import flax\n     67 import jax\n     68 import jax.numpy as jnp\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/flax/__init__.py:19\n      1 # Copyright 2022 The Flax Authors.\n      2 #\n      3 # Licensed under the Apache License, Version 2.0 (the \"License\");\n   (...)\n     14 \n     15 # Lint as: python 3\n     17 \"\"\"Flax API.\"\"\"\n---&gt; 19 from . import core\n     20 from . import linen\n     21 from . import optim\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/flax/core/__init__.py:15\n      1 # Copyright 2022 The Flax Authors.\n      2 #\n      3 # Licensed under the Apache License, Version 2.0 (the \"License\");\n   (...)\n     12 # See the License for the specific language governing permissions and\n     13 # limitations under the License.\n---&gt; 15 from .axes_scan import broadcast\n     16 from .frozen_dict import FrozenDict, freeze, unfreeze\n     17 from .tracers import current_trace, trace_level, check_trace_level\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/flax/core/axes_scan.py:22\n     19 from jax import lax\n     21 from jax.interpreters import partial_eval as pe\n---&gt; 22 from jax import linear_util as lu\n     24 from typing import Union, Optional, Callable, Any\n     26 import numpy as np\n\nImportError: cannot import name 'linear_util' from 'jax' (/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/jax/__init__.py)\n\n\n\n\nPAD, EOS, UNK = 0, 1, 2\n\n\ndef detokenize(np_array):\n    return trax.data.detokenize(\n        np_array,\n        vocab_type='sentencepiece',\n        vocab_file='sentencepiece.model',\n        vocab_dir='.')\n\n\ndef tokenize(s):\n    return next(trax.data.tokenize(\n        iter([s]),\n        vocab_type='sentencepiece',\n        vocab_file='sentencepiece.model',\n        vocab_dir='.'))\n \n    \nvocab_size = trax.data.vocab_size(\n    vocab_type='sentencepiece',\n    vocab_file='sentencepiece.model',\n    vocab_dir='.')\n\n\ndef get_sentinels(vocab_size, display=False):\n    sentinels = {}\n    for i, char in enumerate(reversed(string.ascii_letters), 1):\n        decoded_text = detokenize([vocab_size - i]) \n        # Sentinels, ex: &lt;Z&gt; - &lt;a&gt;\n        sentinels[decoded_text] = f'&lt;{char}&gt;'    \n        if display:\n            print(f'The sentinel is &lt;{char}&gt; and the decoded token is:', decoded_text)\n    return sentinels\n\n\nsentinels = get_sentinels(vocab_size, display=False)    \n\n\ndef pretty_decode(encoded_str_list, sentinels=sentinels):\n    # If already a string, just do the replacements.\n    if isinstance(encoded_str_list, (str, bytes)):\n        for token, char in sentinels.items():\n            encoded_str_list = encoded_str_list.replace(token, char)\n        return encoded_str_list\n  \n    # We need to decode and then prettyfy it.\n    return pretty_decode(detokenize(encoded_str_list))\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[2], line 20\n     12 def tokenize(s):\n     13     return next(trax.data.tokenize(\n     14         iter([s]),\n     15         vocab_type='sentencepiece',\n     16         vocab_file='sentencepiece.model',\n     17         vocab_dir='.'))\n---&gt; 20 vocab_size = trax.data.vocab_size(\n     21     vocab_type='sentencepiece',\n     22     vocab_file='sentencepiece.model',\n     23     vocab_dir='.')\n     26 def get_sentinels(vocab_size, display=False):\n     27     sentinels = {}\n\nNameError: name 'trax' is not defined",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "L3 - T5"
    ]
  },
  {
    "objectID": "notes/c4w3/lab03.html#2",
    "href": "notes/c4w3/lab03.html#2",
    "title": "Assignment 3 Ungraded Sections - Part 2: T5 SQuAD Model",
    "section": "Part 2: Fine-tuning on SQuAD",
    "text": "Part 2: Fine-tuning on SQuAD\nNow let’s try to fine tune on SQuAD and see what becomes of the model.For this, we need to write a function that will create and process the SQuAD tf.data.Dataset. Below is how T5 pre-processes SQuAD dataset as a text2text example. Before we jump in, we will have to first load in the data.\n\n2.1 Loading in the data and preprocessing\nYou first start by loading in the dataset. The text2text example for a SQuAD example looks like:\n{\n  'inputs': 'question: &lt;question&gt; context: &lt;article&gt;',\n  'targets': '&lt;answer_0&gt;',\n}\nThe squad pre-processing function takes in the dataset and processes it using the sentencePiece vocabulary you have seen above. It generates the features from the vocab and encodes the string features. It takes on question, context, and answer, and returns “question: Q context: C” as input and “A” as target.\n\n# Retrieve Question, C, A and return \"question: Q context: C\" as input and \"A\" as target.\ndef squad_preprocess_fn(dataset, mode='train'):\n    return t5.data.preprocessors.squad(dataset)\n\n\n# train generator, this takes about 1 minute\ntrain_generator_fn, eval_generator_fn = trax.data.tf_inputs.data_streams(\n  'squad/plain_text:1.0.0',\n  data_dir='data/',\n  bare_preprocess_fn=squad_preprocess_fn,\n  input_name='inputs',\n  target_name='targets'\n)\n\ntrain_generator = train_generator_fn()\nnext(train_generator)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[4], line 2\n      1 # train generator, this takes about 1 minute\n----&gt; 2 train_generator_fn, eval_generator_fn = trax.data.tf_inputs.data_streams(\n      3   'squad/plain_text:1.0.0',\n      4   data_dir='data/',\n      5   bare_preprocess_fn=squad_preprocess_fn,\n      6   input_name='inputs',\n      7   target_name='targets'\n      8 )\n     10 train_generator = train_generator_fn()\n     11 next(train_generator)\n\nNameError: name 'trax' is not defined\n\n\n\n\n#print example from train_generator\n(inp, out) = next(train_generator)\nprint(inp.decode('utf8').split('context:')[0])\nprint()\nprint('context:', inp.decode('utf8').split('context:')[1])\nprint()\nprint('target:', out.decode('utf8'))\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[5], line 2\n      1 #print example from train_generator\n----&gt; 2 (inp, out) = next(train_generator)\n      3 print(inp.decode('utf8').split('context:')[0])\n      4 print()\n\nNameError: name 'train_generator' is not defined\n\n\n\n\n\n2.2 Decoding from a fine-tuned model\nYou will now use an existing model that we trained for you. You will initialize, then load in your model, and then try with your own input.\n\n# Initialize the model \nmodel = trax.models.Transformer(\n    d_ff = 4096,\n    d_model = 1024,\n    max_len = 2048,\n    n_heads = 16,\n    dropout = 0.1,\n    input_vocab_size = 32000,\n    n_encoder_layers = 24,\n    n_decoder_layers = 24,\n    mode='predict')  # Change to 'eval' for slow decoding.\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[6], line 2\n      1 # Initialize the model \n----&gt; 2 model = trax.models.Transformer(\n      3     d_ff = 4096,\n      4     d_model = 1024,\n      5     max_len = 2048,\n      6     n_heads = 16,\n      7     dropout = 0.1,\n      8     input_vocab_size = 32000,\n      9     n_encoder_layers = 24,\n     10     n_decoder_layers = 24,\n     11     mode='predict')  # Change to 'eval' for slow decoding.\n\nNameError: name 'trax' is not defined\n\n\n\n\n# load in the model\n# this will take a minute\nshape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)\nmodel.init_from_file('model_squad.pkl.gz',\n                     weights_only=True, input_signature=(shape11, shape11))\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[7], line 3\n      1 # load in the model\n      2 # this will take a minute\n----&gt; 3 shape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)\n      4 model.init_from_file('model_squad.pkl.gz',\n      5                      weights_only=True, input_signature=(shape11, shape11))\n\nNameError: name 'trax' is not defined\n\n\n\n\n# create inputs\n# a simple example \n# inputs = 'question: She asked him where is john? context: John was at the game'\n\n# an extensive example\ninputs = 'question: What are some of the colours of a rose? context: A rose is a woody perennial flowering plant of the genus Rosa, in the family Rosaceae, or the flower it bears.There are over three hundred species and tens of thousands of cultivars. They form a group of plants that can be erect shrubs, climbing, or trailing, with stems that are often armed with sharp prickles. Flowers vary in size and shape and are usually large and showy, in colours ranging from white through yellows and reds. Most species are native to Asia, with smaller numbers native to Europe, North America, and northwestern Africa. Species, cultivars and hybrids are all widely grown for their beauty and often are fragrant.'\n\n\n# tokenizing the input so we could feed it for decoding\nprint(tokenize(inputs))\ntest_inputs = tokenize(inputs) \n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[9], line 2\n      1 # tokenizing the input so we could feed it for decoding\n----&gt; 2 print(tokenize(inputs))\n      3 test_inputs = tokenize(inputs) \n\nCell In[2], line 13, in tokenize(s)\n     12 def tokenize(s):\n---&gt; 13     return next(trax.data.tokenize(\n     14         iter([s]),\n     15         vocab_type='sentencepiece',\n     16         vocab_file='sentencepiece.model',\n     17         vocab_dir='.'))\n\nNameError: name 'trax' is not defined\n\n\n\nRun the cell below to decode.\n\n\nNote: This will take some time to run\n\n# Temperature is a parameter for sampling.\n#   # * 0.0: same as argmax, always pick the most probable token\n#   # * 1.0: sampling from the distribution (can sometimes say random things)\n#   # * values inbetween can trade off diversity and quality, try it out!\noutput = decoding.autoregressive_sample(model, inputs=np.array(test_inputs)[None, :],\n                                        temperature=0.0, max_length=5) # originally max_length=10\nprint(wrapper.fill(pretty_decode(output[0])))\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[10], line 5\n      1 # Temperature is a parameter for sampling.\n      2 #   # * 0.0: same as argmax, always pick the most probable token\n      3 #   # * 1.0: sampling from the distribution (can sometimes say random things)\n      4 #   # * values inbetween can trade off diversity and quality, try it out!\n----&gt; 5 output = decoding.autoregressive_sample(model, inputs=np.array(test_inputs)[None, :],\n      6                                         temperature=0.0, max_length=5) # originally max_length=10\n      7 print(wrapper.fill(pretty_decode(output[0])))\n\nNameError: name 'decoding' is not defined\n\n\n\nYou should also be aware that the quality of the decoding is not very good because max_length was downsized from 10 to 5 so that this runs faster within this environment. The colab version uses the original max_length so check that one for the actual decoding.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "L3 - T5"
    ]
  },
  {
    "objectID": "notes/c4w3/lab02.html",
    "href": "notes/c4w3/lab02.html",
    "title": "Assignment 3 Ungraded Sections - Part 1: BERT Loss Model",
    "section": "",
    "text": "course banner\nWelcome to the part 1 of testing the models for this week’s assignment. We will perform decoding using the BERT Loss model. In this notebook we’ll use an input, mask (hide) random word(s) in it and see how well we get the “Target” answer(s).",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "L2 - BERT Loss"
    ]
  },
  {
    "objectID": "notes/c4w3/lab02.html#colab",
    "href": "notes/c4w3/lab02.html#colab",
    "title": "Assignment 3 Ungraded Sections - Part 1: BERT Loss Model",
    "section": "Colab",
    "text": "Colab\nSince this ungraded lab takes a lot of time to run on coursera, as an alternative we have a colab prepared for you.\nBERT Loss Model Colab\n\nIf you run into a page that looks similar to the one below, with the option Open with, this would mean you need to download the Colaboratory app. You can do so by Open with -&gt; Connect more apps -&gt; in the search bar write \"Colaboratory\" -&gt; install\n\n\n\nAfter installation it should look like this. Click on Open with Google Colaboratory",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "L2 - BERT Loss"
    ]
  },
  {
    "objectID": "notes/c4w3/lab02.html#outline",
    "href": "notes/c4w3/lab02.html#outline",
    "title": "Assignment 3 Ungraded Sections - Part 1: BERT Loss Model",
    "section": "Outline",
    "text": "Outline\n\nOverview\nPart 1: Getting ready\nPart 2: BERT Loss\n\n2.1 Decoding\n\n\n\nOverview\nIn this notebook you will:\n\nImplement the Bidirectional Encoder Representation from Transformer (BERT) loss.\nUse a pretrained version of the model you created in the assignment for inference.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "L2 - BERT Loss"
    ]
  },
  {
    "objectID": "notes/c4w3/lab02.html#1",
    "href": "notes/c4w3/lab02.html#1",
    "title": "Assignment 3 Ungraded Sections - Part 1: BERT Loss Model",
    "section": "Part 1: Getting ready",
    "text": "Part 1: Getting ready\nRun the code cells below to import the necessary libraries and to define some functions which will be useful for decoding. The code and the functions are the same as the ones you previsouly ran on the graded assignment.\n\nimport pickle\nimport string\nimport ast\nimport numpy as np\nimport trax \nfrom trax.supervised import decoding\nimport textwrap \n\nwrapper = textwrap.TextWrapper(width=70)\n\n2025-02-10 16:51:29.192037: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1739199089.204440  119292 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1739199089.208671  119292 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\n\n\nexample_jsons = list(map(ast.literal_eval, open('data.txt')))\n\nnatural_language_texts = [example_json['text'] for example_json in example_jsons]\n\nPAD, EOS, UNK = 0, 1, 2\n\ndef detokenize(np_array):\n    return trax.data.detokenize(\n        np_array,\n        vocab_type='sentencepiece',\n        vocab_file='sentencepiece.model',\n        vocab_dir='.')\n\n\ndef tokenize(s):\n    return next(trax.data.tokenize(\n        iter([s]),\n        vocab_type='sentencepiece',\n        vocab_file='sentencepiece.model',\n        vocab_dir='.'))\n \n    \nvocab_size = trax.data.vocab_size(\n    vocab_type='sentencepiece',\n    vocab_file='sentencepiece.model',\n    vocab_dir='.')\n\n\ndef get_sentinels(vocab_size, display=False):\n    sentinels = {}\n    for i, char in enumerate(reversed(string.ascii_letters), 1):\n        decoded_text = detokenize([vocab_size - i]) \n        # Sentinels, ex: &lt;Z&gt; - &lt;a&gt;\n        sentinels[decoded_text] = f'&lt;{char}&gt;'    \n        if display:\n            print(f'The sentinel is &lt;{char}&gt; and the decoded token is:', decoded_text)\n    return sentinels\n\n\nsentinels = get_sentinels(vocab_size, display=False)    \n\n\ndef pretty_decode(encoded_str_list, sentinels=sentinels):\n    # If already a string, just do the replacements.\n    if isinstance(encoded_str_list, (str, bytes)):\n        for token, char in sentinels.items():\n            encoded_str_list = encoded_str_list.replace(token, char)\n        return encoded_str_list\n  \n    # We need to decode and then prettyfy it.\n    return pretty_decode(detokenize(encoded_str_list))\n\n\ninputs_targets_pairs = []\n\n# here you are reading already computed input/target pairs from a file\nwith open ('inputs_targets_pairs_file.txt', 'rb') as fp:\n    inputs_targets_pairs = pickle.load(fp)  \n\n\ndef display_input_target_pairs(inputs_targets_pairs):\n    for i, inp_tgt_pair in enumerate(inputs_targets_pairs, 1):\n        inps, tgts = inp_tgt_pair\n        inps, tgts = pretty_decode(inps), pretty_decode(tgts)\n        print(f'[{i}]\\n'\n              f'inputs:\\n{wrapper.fill(text=inps)}\\n\\n'\n              f'targets:\\n{wrapper.fill(text=tgts)}\\n\\n\\n\\n')\n    \ndisplay_input_target_pairs(inputs_targets_pairs)    \n\n\n---------------------------------------------------------------------------\nImportError                               Traceback (most recent call last)\nCell In[2], line 23\n     15 def tokenize(s):\n     16     return next(trax.data.tokenize(\n     17         iter([s]),\n     18         vocab_type='sentencepiece',\n     19         vocab_file='sentencepiece.model',\n     20         vocab_dir='.'))\n---&gt; 23 vocab_size = trax.data.vocab_size(\n     24     vocab_type='sentencepiece',\n     25     vocab_file='sentencepiece.model',\n     26     vocab_dir='.')\n     29 def get_sentinels(vocab_size, display=False):\n     30     sentinels = {}\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/data/tf_inputs.py:570, in vocab_size(vocab_type, vocab_file, vocab_dir, n_reserved_ids)\n    550 def vocab_size(vocab_type='subword',\n    551                vocab_file=None,\n    552                vocab_dir=None,\n    553                n_reserved_ids=0):\n    554   \"\"\"Returns the size of the vocabulary (number of symbols used).\n    555 \n    556   This function can be used to set the size of the final layers of a model that\n   (...)\n    568     An integer, the number of symbols used (including reserved IDs).\n    569   \"\"\"\n--&gt; 570   vocab = _get_vocab(vocab_type, vocab_file, vocab_dir)\n    571   return vocab.vocab_size + n_reserved_ids\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/data/tf_inputs.py:603, in _get_vocab(vocab_type, vocab_file, vocab_dir, extra_ids)\n    600   return text_encoder.BertEncoder(path, do_lower_case=True)\n    602 assert vocab_type == 'sentencepiece'\n--&gt; 603 return t5_data().SentencePieceVocabulary(sentencepiece_model_file=path,\n    604                                          extra_ids=extra_ids)\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/data/tf_inputs.py:53, in t5_data()\n     51 module = None\n     52 try:\n---&gt; 53   import t5.data  # pylint: disable=g-import-not-at-top\n     54   module = t5.data\n     55 except AttributeError as e:\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/t5/__init__.py:17\n      1 # Copyright 2023 The T5 Authors.\n      2 #\n      3 # Licensed under the Apache License, Version 2.0 (the \"License\");\n   (...)\n     12 # See the License for the specific language governing permissions and\n     13 # limitations under the License.\n     15 \"\"\"Import API modules.\"\"\"\n---&gt; 17 import t5.data\n     18 import t5.evaluation\n     20 # Version number.\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/t5/data/__init__.py:17\n     15 \"\"\"Import data modules.\"\"\"\n     16 # pylint:disable=wildcard-import,g-bad-import-order\n---&gt; 17 from t5.data.dataset_providers import *\n     18 from t5.data.glue_utils import *\n     19 import t5.data.postprocessors\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/t5/data/dataset_providers.py:28\n     25 from collections.abc import Mapping\n     26 import re\n---&gt; 28 import seqio\n     29 from t5.data import utils\n     30 import tensorflow.compat.v2 as tf\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/seqio/__init__.py:19\n     15 \"\"\"Import to top-level API.\"\"\"\n     17 # pylint:disable=wildcard-import,g-bad-import-order,g-import-not-at-top\n---&gt; 19 from seqio.dataset_providers import *\n     20 from seqio import evaluation\n     21 from seqio import experimental\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/seqio/dataset_providers.py:36\n     33 from typing import Any, Callable, Iterable, List, Mapping, MutableMapping, Optional, Sequence, Set, Tuple, Type, Union\n     35 from absl import logging\n---&gt; 36 import clu.metrics\n     37 import editdistance\n     38 import numpy as np\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/clu/metrics.py:66\n     64 from clu.internal import utils\n     65 import clu.values\n---&gt; 66 import flax\n     67 import jax\n     68 import jax.numpy as jnp\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/flax/__init__.py:19\n      1 # Copyright 2022 The Flax Authors.\n      2 #\n      3 # Licensed under the Apache License, Version 2.0 (the \"License\");\n   (...)\n     14 \n     15 # Lint as: python 3\n     17 \"\"\"Flax API.\"\"\"\n---&gt; 19 from . import core\n     20 from . import linen\n     21 from . import optim\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/flax/core/__init__.py:15\n      1 # Copyright 2022 The Flax Authors.\n      2 #\n      3 # Licensed under the Apache License, Version 2.0 (the \"License\");\n   (...)\n     12 # See the License for the specific language governing permissions and\n     13 # limitations under the License.\n---&gt; 15 from .axes_scan import broadcast\n     16 from .frozen_dict import FrozenDict, freeze, unfreeze\n     17 from .tracers import current_trace, trace_level, check_trace_level\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/flax/core/axes_scan.py:22\n     19 from jax import lax\n     21 from jax.interpreters import partial_eval as pe\n---&gt; 22 from jax import linear_util as lu\n     24 from typing import Union, Optional, Callable, Any\n     26 import numpy as np\n\nImportError: cannot import name 'linear_util' from 'jax' (/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/jax/__init__.py)",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "L2 - BERT Loss"
    ]
  },
  {
    "objectID": "notes/c4w3/lab02.html#part-2-bert-loss",
    "href": "notes/c4w3/lab02.html#part-2-bert-loss",
    "title": "Assignment 3 Ungraded Sections - Part 1: BERT Loss Model",
    "section": "Part 2: BERT Loss",
    "text": "Part 2: BERT Loss\nNow that you created the encoder, we will not make you train it. Training it could easily cost you a few days depending on which GPUs/TPUs you are using. Very few people train the full transformer from scratch. Instead, what the majority of people do, they load in a pretrained model, and they fine tune it on a specific task. That is exactly what you are about to do. Let’s start by initializing and then loading in the model.\nInitialize the model from the saved checkpoint.\n\n# Initializing the model\nmodel = trax.models.Transformer(\n    d_ff = 4096,\n    d_model = 1024,\n    max_len = 2048,\n    n_heads = 16,\n    dropout = 0.1,\n    input_vocab_size = 32000,\n    n_encoder_layers = 24,\n    n_decoder_layers = 24,\n    mode='predict')\n\n\n# Now load in the model\n# this takes about 1 minute\nshape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)  # Needed in predict mode.\nmodel.init_from_file('model.pkl.gz',\n                     weights_only=True, input_signature=(shape11, shape11))\n\n\n---------------------------------------------------------------------------\nNotFoundError                             Traceback (most recent call last)\nCell In[4], line 4\n      1 # Now load in the model\n      2 # this takes about 1 minute\n      3 shape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)  # Needed in predict mode.\n----&gt; 4 model.init_from_file('model.pkl.gz',\n      5                      weights_only=True, input_signature=(shape11, shape11))\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/base.py:334, in Layer.init_from_file(self, file_name, weights_only, input_signature)\n    332 with tf.io.gfile.GFile(file_name, 'rb') as f:\n    333   with gzip.GzipFile(fileobj=f, compresslevel=2) as gzipf:\n--&gt; 334     dictionary = pickle.load(gzipf)\n    335 # In the current checkpoint format, we store weights in a separate\n    336 # non-pickled file with the same name but added \".npy\".\n    337 if isinstance(dictionary['flat_weights'], int):\n\nFile /usr/lib/python3.10/gzip.py:321, in GzipFile.peek(self, n)\n    319     import errno\n    320     raise OSError(errno.EBADF, \"peek() on write-only GzipFile object\")\n--&gt; 321 return self._buffer.peek(n)\n\nFile /usr/lib/python3.10/_compression.py:68, in DecompressReader.readinto(self, b)\n     66 def readinto(self, b):\n     67     with memoryview(b) as view, view.cast(\"B\") as byte_view:\n---&gt; 68         data = self.read(len(byte_view))\n     69         byte_view[:len(data)] = data\n     70     return len(data)\n\nFile /usr/lib/python3.10/gzip.py:488, in _GzipReader.read(self, size)\n    484 if self._new_member:\n    485     # If the _new_member flag is set, we have to\n    486     # jump to the next member, if there is one.\n    487     self._init_read()\n--&gt; 488     if not self._read_gzip_header():\n    489         self._size = self._pos\n    490         return b\"\"\n\nFile /usr/lib/python3.10/gzip.py:431, in _GzipReader._read_gzip_header(self)\n    430 def _read_gzip_header(self):\n--&gt; 431     magic = self._fp.read(2)\n    432     if magic == b'':\n    433         return False\n\nFile /usr/lib/python3.10/gzip.py:97, in _PaddedFile.read(self, size)\n     94 read = self._read\n     95 self._read = None\n     96 return self._buffer[read:] + \\\n---&gt; 97        self.file.read(size-self._length+read)\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/tensorflow/python/lib/io/file_io.py:116, in FileIO.read(self, n)\n    104 def read(self, n=-1):\n    105   \"\"\"Returns the contents of a file as a string.\n    106 \n    107   Starts reading from current position in file.\n   (...)\n    114     string if in string (regular) mode.\n    115   \"\"\"\n--&gt; 116   self._preread_check()\n    117   if n == -1:\n    118     length = self.size() - self.tell()\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/tensorflow/python/lib/io/file_io.py:77, in FileIO._preread_check(self)\n     74 if not self._read_check_passed:\n     75   raise errors.PermissionDeniedError(None, None,\n     76                                      \"File isn't open for reading\")\n---&gt; 77 self._read_buf = _pywrap_file_io.BufferedInputStream(\n     78     compat.path_to_str(self.__name), 1024 * 512)\n\nNotFoundError: model.pkl.gz; No such file or directory\n\n\n\n\n\n2.1 Decoding\nNow you will use one of the inputs_targets_pairs for input and as target. Next you will use the pretty_decode to output the input and target. The code to perform all of this has been provided below.\n\n# using the 3rd example\nc4_input = inputs_targets_pairs[2][0]\nc4_target = inputs_targets_pairs[2][1]\n\nprint('pretty_decoded input: \\n\\n', pretty_decode(c4_input))\nprint('\\npretty_decoded target: \\n\\n', pretty_decode(c4_target))\nprint('\\nc4_input:\\n\\n', c4_input)\nprint('\\nc4_target:\\n\\n', c4_target)\nprint(len(c4_target))\nprint(len(pretty_decode(c4_target)))\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[5], line 2\n      1 # using the 3rd example\n----&gt; 2 c4_input = inputs_targets_pairs[2][0]\n      3 c4_target = inputs_targets_pairs[2][1]\n      5 print('pretty_decoded input: \\n\\n', pretty_decode(c4_input))\n\nNameError: name 'inputs_targets_pairs' is not defined\n\n\n\nRun the cell below to decode.\n\n\nNote: This will take some time to run\n\n# Temperature is a parameter for sampling.\n#   # * 0.0: same as argmax, always pick the most probable token\n#   # * 1.0: sampling from the distribution (can sometimes say random things)\n#   # * values inbetween can trade off diversity and quality, try it out!\noutput = decoding.autoregressive_sample(model, inputs=np.array(c4_input)[None, :],\n                                        temperature=0.0, max_length=5) # originally max_length = 50\nprint(wrapper.fill(pretty_decode(output[0])))\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[6], line 5\n      1 # Temperature is a parameter for sampling.\n      2 #   # * 0.0: same as argmax, always pick the most probable token\n      3 #   # * 1.0: sampling from the distribution (can sometimes say random things)\n      4 #   # * values inbetween can trade off diversity and quality, try it out!\n----&gt; 5 output = decoding.autoregressive_sample(model, inputs=np.array(c4_input)[None, :],\n      6                                         temperature=0.0, max_length=5) # originally max_length = 50\n      7 print(wrapper.fill(pretty_decode(output[0])))\n\nNameError: name 'c4_input' is not defined\n\n\n\nAt this point the RAM is almost full, this happens because the model and the decoding is memory heavy. You can run decoding just once. Running it the second time with another example might give you an answer that makes no sense, or repetitive words. If that happens restart the runtime (see how to at the start of the notebook) and run all the cells again.\nYou should also be aware that the quality of the decoding is not very good because max_length was downsized from 50 to 5 so that this runs faster within this environment. The colab version uses the original max_length so check that one for the actual decoding.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "L2 - BERT Loss"
    ]
  },
  {
    "objectID": "notes/cs11-737-w06/index.html",
    "href": "notes/cs11-737-w06/index.html",
    "title": "Translation Models",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\nVideo 1: Lesson Video\n\n\n\n\n\n\n\n\nThis week’s slides\n\n\n\n\nSupplementary Figure 1\n\n\n\n\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\nSequence-to-sequence models w/ attention\nDecoding strategies\nTransformers\n\n\n\n\n\n\n\n\n\nTranscript\n\n\n\n\n\n\n\nlooking forward to talking about machine translation and sequence sequence models today uh as always i’ll welcome discussion on this uh some of this might be this is kind of the basic modeling stuff regarding machine translation so i think some of this might be uh elementary for people who have taken an nlp class before but there are you know some people who might not have covered this so i’d like to go through it after this we’re going to be talking about some concrete code details so looking at code from the annotated transformer and also from the assignment and as always please feel free to ask questions on chat or like live anytime you want so language models are basically generative models of text and uh what they do is they calculate the probability of a text x given um you know calculate a probability of a text x and they allow you to sample from this probability distribution to generate outputs with a high probability and the um so given this uh you know if we train on a particular variety of texts like let’s say we train on harry potter books the sampling distribution will uh generate things that look like harry potter there’s also um conditional language models and conditional language models don’t just generate text but they generate text according to some specifications so if we have our input x uh and our output y our input x could be structured data output y could be a natural language description uh what we’re going to talk about this time is translation so input x could be english output y could be japanese um our input x could be a document output y could be a short description and our input could be an utterance output of response inputs image output text input speech output transcript so you can see that a lot of things a lot of different tasks you know data to text generation or natural language generation translation summarization response generation image captioning and speech recognition all fit within this general paradigm so um because of this you know these are a very uh powerful and widely used variety of models and so i’d like to go a little bit into exactly how we formulate this problem and learn these probabilistic models so when we calculate the probability of a sentence the way we can do that is by taking a sentence which consists of i words and calculating uh the probability by the product of the probabilities of the word in the sentence so if the next word is x i we uh calculate the probability of x i given all of the previous words in the sentence and this is a particular variety of um language model oh sorry there’s a language intense sorry um yeah i’ll find a good place to cut off this description and switch over to the language intent i am totally slipped my mind so [Music] then uh conditioned language models basically what they do is they condition on some added context and they calculate the probability of y given x and um then they do the same thing where you condition on x and all of the previous generated words i talked about uh calculating the probability of words in a sentence and in most cases we’re taking the previous context uh calculating uh the probability of the next word given the previous context these are also called um uh by a variety of names uh one name is an autoregressive uh link model uh where basically that means we’re predicting the next word uh given the previous words in some order specifically uh when the words that you’re predicting the uh the word here um from all come to the left they’re also called left to right language models um and there’s also a term causal language models which i i don’t prefer as much but you’ll also see in the literature um and basically if you have a left to right language model uh we can use something like a recurrent neural network to feed in the previous word predict the next word feed in the previous word predict the next word feed in the previous word predict the next word etc etc and if we want to develop a conditional language model that conditions on some sort of context to predict the probability of the words uh the upcoming words we can do something like feed in the conditioning context into a recurrent neural network and predict the uh the outputs like this so basically we read the conditioning context one by one maybe it’s a japanese sentence and then we predict the probability of the output of the english sentence one by one so we have the encoder and so this is often called an encoder and this is called the decoder so these are often called uh encoder decoder models and uh they’re used pretty widely and you know all of uh nlp nowadays so um i’ll actually skip that part so um now we have this model uh we can train it you know i think a lot of people are everybody in the class is now training uh you know models for part of speech tagging uh now so because of that i i assume you know like about the recurrent neural networks and other things like this the the place where these uh kind of sequence sequence models or translation models become different is that we need to do uh generation from the models and so instead of like taking in individual words and trying to put a tag on them we actually need to generate outputs and the generation problem is basically we form a model of p of y given x and how do we use it to generate a sentence and there’s basically two methods the first method is sampling where we try to generate a random sentence according to the probability distribution and the next method is argmax where we try to generate a sentence with the highest probability and in general in translation which is the main topic of this uh lecture here we we use the arg max and actually maybe i’ll explain why in a few slides after i explain each method so um for sampling uh the most common way to do this is called ancestral sampling where we randomly generate words one by one and the algorithm for doing so looks a little bit like this so we have uh while uh yj minus one is not equal to the end of sentence symbol so basically while we have not yet finished our sentence we calculate the probability of the next word given all the previous words in the conditioning context and we randomly sample a word from the next time step and this is a method for an exact method for sampling from p of y given x so this will give you you know basically samples that exactly follow the probability distribution that the model specifies if you had a really really good model where basically it only assigned probability to why that were you know plausible translations for x then probably the sampling would be fine you would get different translations every time but you know each translation that you sampled would be good however unfortunately our models are not all that great and very often if you sample kind of lower probability things according to this probability distribution they might not be very good translations so uh what we try to do instead is uh do some sort of search for the highest probability translation and one way to do this is doing something like greedy search so basically one by one we pick the single highest probability word in the output so that looks a little bit like this we similarly we go until we reach the end of sentence symbol but instead of sampling a random word according to the probability distribution each time we we take the highest probability word at the next time step each time so uh what this is doing is this is instead of uh picking you know any any old word next time it picks always fix the highest one which moves us closer to a very you know high scoring output unfortunately this is also not exact and it causes real problems so for example um it will often kind of regardless of what the syntax should look like it might try to generate uh the kind of easier words in the sentence first because the easier words in the sentence will have higher probabilities so uh this arg max this greedy arg max we’ll try to choose those words first and another issue that this can bring up is it will prefer multiple common words to one rare word so like for example if we had a choice between new york and pittsburgh even if the product of new in york was lower than the probability of pittsburgh if the probability of new only was higher than pittsburgh this greedy search would kind of make that decision prematurely and not be able to back off and uh and reconsider the decision later so what we do instead is we have a method called beam search there are other methods as well but beam search is the most common method where instead of picking one high probability word at every time step we maintain several paths so this is an example of beam search with size two where basically we um uh instead of like taking the highest probability word at the first time step which would be b we consider two high probability words of the first time step a and b we expand all next words for each of those possibilities and um and then select the highest scoring sequence of length two based on all of these and basically beam search allows you to spend more time computing to look for a high scoring high scoring output and basically allows you to do more precise search and help resolve some of the issues that i talked about on the previous slide cool um there’s also lots of other details if you want to go deep into the details of this like for example there are other methods for sampling that try to kind of alleviate the issues that you get from low probability [Music] low probability outputs by only sampling from the highest probability outputs things like top k sampling and nucleus sampling there’s also um other methods for search that incorporate uh you know like dynamic beam sizes or other things like this but basically these um standard b in machine translation standard beam search tends to be uh the thing that most people use and you usually just pick a beam size of five and forget about it and hope uh you know that that’s good enough and it usually is cool any questions about that before i move on okay i guess not um so actually sorry i need to check a chat so next i’d like to talk about attention so attention is uh very very important in um machine translation and uh one issue with the example uh like machine translation model that i talked about before is basically what we’re asking this model to do here is read in a whole sentence express all of the meaning in that sentence in a single vector and then uh output the appropriate translation based on this and i because i have previously worked as a translator i like to make an analog to what that would be like if you were asking a translator to do something so this is kind of like asking a translator to read a whole sentence memorize that whole sentence put the book that they were looking at away and then tried to translate output the whole sentence uh based entirely on their memory and you know for a lot of sentences a lot of things have people uh you know think of they might be able to do that but it’s a lot harder than if you can go back and reference the uh like original sentence when you’re doing translation and so uh this was kind of eloquently uh stated by ray mooney uh a professor at ut austin where he said you can’t cram the meaning of a whole bleeping sentence into a single bleeping vector and um so the idea of attention is basically to relax this constraint of memorizing all of the content of a sentence in a single vector and put it into multiple vectors one vector for each token in the input so then we have the model that goes back and references each of these vectors one at a time so the basic idea behind attention is that we encode each word in the sentence into a single vector and when decoding we kind of perform a linear combination of these vectors weighted by attention weights and we use this combination from the vectors in picking the next word and i have a graphical example of this here so let’s say we have our encoder here and our encoder calculates a representation for each word in the sentence and our decoder here basically is stepping through the sentence word by word and it’s gotten to the place where it’s at like i hate and that it wants to generate the next word so what a translator would do at this point is they would go back and reference the original sentence maybe after they’ve read through it once and try to decide which word they want to be translating next think about uh what word they want to translate that into and that’s basically what attention does so it takes this query vector down here and it compares that query vector to all of the key vectors in the original included input and it calculates a score or a weight based on the congruence between the query vectors and the key vectors so you put it through this function here that takes in the query vector and the key vector and it spits out a scalar weight and we do this for each pair of the query vector in one of the key vectors in the key matrix basically and uh then we normalize these so they add to one using a softmax function um and basically uh what this looks like is we take in these values and we output something that looks like a probability vector here and i’m sure you know everybody’s familiar with the softmax from the homework that you’re already working on then after we do this we take uh the value vectors which are the things that we want to attend to the types of information that we want to combine and we multiply this by the attention vector here and that gives us a single vector that we can then feed into any part of the model and specifically in the case of machine translation or whatever else we will be feeding this into the decoder in order to calculate the um the probability over the next word so if we have a attention mechanism when we’re doing translation this is a picture from the paper that introduced the attention mechanism basically you’ll see that every time it translates a next word um i believe in french here from english to french it will be attending to um it will be attending to multiple words in the original source documents so and you can see that these align with each other and even uh when like the order reverses then um the uh appropriate words are being attended to so i left the um the actual function that we used to take in the query and the key vector underspecified uh but if we assume that the query and the key q is a query and k is the key we can do various things like feeding the query and the key into a multi-layer perceptron where we concatenate them together multiply them by a weight matrix take a non-linearity like the tan h and multiply by another vector to get this value this is flexible and often very good with larger data sizes another thing that we can do and in fact this is maybe the most common method that we use nowadays we can multiply by the key uh we can have the key and the query and we have a weight matrix uh here that we multiply and this gives us uh the [Music] attention weight here and then we also have um other things like the dot product where we just take the dot product between the two vectors um this is very very simple and and can be effective but the problem is that this uh one issue with this is this definitely requires the key and the query to be the same size because otherwise you can’t take the dot product um so the bi-linear function gives you a bit more flexibility in that you don’t have to assume that the query and the key are in the same kind of like underlying space and it also allows you to compare things of different size and then finally um we can have the scaled dot product and so one problem is that the scale of the dot product increases as the number of dimensions gets larger and so the way of fixing this is basically scaling it by the size of the vector so you take the square root of the size of either the key vector or the query vector it doesn’t really matter which one because they’re both the same size here and this allows you to make sure that even if you have larger or smaller values the scale with which the attention weights varies stays approximately the same okay so this is a a brief overview uh we’ll be going through some concrete examples uh a bit later in the class but are there any questions here i imagine many people will be familiar with this already okay um if not i will go ahead so uh then we have improvements to attention um so attention uh one issue with attention is basically that um neural models often tend to drop or repeat content when they’re doing uh generation in particular for tasks like machine translation where in machine translation we can make the implicit assumption that all content in the source should be translated into the targets we can additionally add biases to that basically model how many times words in the input have been covered and if they have um not been covered you know approximately one time we can give some sort of penalty so basically this is making the assumption that each word should be translated about once or another option is to let the model to tell the model how many times each word has been covered but let it learn to use that information appropriately in making its next uh translation decisions so either way you know this is explicitly or implicitly encouraging the model to you know cover things once and only once or like as many times as necessary another um very common improvement used in most models nowadays is multi-head attention uh where the idea is that we have multiple attention heads that focus on different parts of the sentence so this is an example where essentially what we do this is an example with two attention heads and the two attention heads each have a different function the first attention head is like a normal attention head that is just used to calculate representations of the words in the sentence so kind of like how i explained attention before as a way to combine together information uh at each time step and then the second attention head is basically focusing on words that should be copied from the source to the target and this is used in particular for a summarization task where lots of copying needs to be done so basically it’s summarizing source code into um into function uh names so what you can see is the first attention head because it’s just pulling in information about the input sentence in general it’s kind of spread out across the entire input sentence and the second one because it’s kind of choosing which words to copy to generate outputs is very focused on individual words uh as opposed to being spread out across the whole sentence and uh so i like this example because it gives an example about how like theoretically you might want to have different attention heads with different functions but in standard models nowadays what we use instead is we have multiple attention heads but we just let the attention heads kind of learn on their own what they would like to focus on and here is an example of uh basically this word making attending to other words in the same sentence in self-attention which i’m going to talk about in a second and you can see that making one of the attention heads is attending to making itself there are other attention heads like the blue attention head orange one green red which are attending to more difficult which is kind of the the object of making and then you have another attention head that’s pulling in information from like the previous word to making so you can see that each of these heads has kind of specialized in a different like form or like function that it should be achieving so that’s good because you know there are things like local context like you know the previous word 2009 which could be very useful but there’s also other varieties of context farther away like the object of a verb that could be important for making particular decisions as well so another uh common improvement to attention is supervised training and normally we just train the attention together with the entire model and however sometimes we can get like quote unquote gold standard alignments a priori and um gold standard alignments are basically mit could be manual alignments where we’ve asked a human annotator to say specifically which words um in the source correspond to which words in the target and the reason why we might want to do this is because as you can see here or no um as you can see here even though this is a relatively good example it’s not always the case that attention will focus directly on the words that are being translated like for example here uh the period only has a very vague attention on the period in the input so because of this you can’t necessarily like look at the attention value and immediately tell which words are being translated and that’s even worse in the case of something like multi-headed attention however like as i talked about last class you know if you’re a human translator using machine translation it might be very important or useful to you to know which words were translated which other words so you could check that they’re correct or um something else like this and in that case um doing supervised training with manual alignments or something could make sure that at least you know some of the attention heads uh correspond with human intuition about which words were translated into which other words another way you can get gold standard alignments is by using a strong alignment model so you might have a an alignment model that was trained on manual alignments or something like this and you could also use that to kind of bootstrap the training of your model and this can also improve accuracy overall by making sure that the model is attending to uh like words that are actually aligned uh as well so um exactly how you do the supervised training basically the answer you can do a number of things but the simplest one is you just have another loss function that you incorporate into your model um where the loss is higher if the alignments are like incongruent with the um uh with the annotated alignments and so you can do things like take the the norm uh the l2 norm of the difference between the alignment vectors or something like that okay and uh now i’m going to go into self-attention um which is very widely used in nlp nowadays and self-attention basically the idea is uh up until now we talked about things like recurrent neural networks is a way to gather encodings for each word in the sentence um but another way you can do this is you can have some sort of attention that uses that where you attend each token in the sentence that you’re looking at now attends to the same sentence itself so for example the word this would attend to this is an example the word is would also attend to this is an example uh etc etc and the basic idea here is that this can function is kind of a drop-in replacement for another type of word-level encoder like rnns um and uh and calculate the representation of each word so for example for this um the word this might attend to this a lot is a little bit and the other one’s not very much is might attend to is and its subject and object and other things like this um the motivation for this becomes very obvious if you think about the translation of words like um like the word is um or maybe the word run is a better example so the word run you know depending on whether you’re running a marathon or running a company or running along the road or running along the creek or something like this uh run might be translated differently so if uh the encoding for the word run pulled in the subject information about the subject and the object that would make it a lot easier to perform like lexical selection in translation or something like that um so why would you want to use self-attention instead of an rnn or something like this unlike rnn’s it’s very parallelizable which allows for fast training on gpus you can calculate all of the words in the sentence at the same time so this is probably the most important reason why people like using transformers uh sorry like using self-attention and the transformers that i’m going to be talking about in a second another reason why people like it is unlike both rnn’s and other options like convolutional neural networks it can easily capture global context from the entire sentence so you you’ll note that every time each word is attending to all other words in the sentence so that gives a very direct connection to you know pulling context there um in general uh transformers seem to yield a pretty high accuracy and a large number of uh nlp casks although it’s not 100 clear that they’re actually all that much better with respect to accuracy um uh like there’s this study by chen uh at all that uh compared them with rnns and the results were uh were similar essentially um but the uh but the advantages uh here of like fast training um and being able to easily pull in global contacts are uh are widely appreciated one downside to this is quadratic computation time so essentially um because uh like you can even see this computation time directly from this attention graph here so you can see that if the length is 4 you have 4 times 4 16 attention values here and this can become onerous if you’re using very long sequences to compute so there’s also a lot of different methods to improve over this recently so um now that i’ve introduced a bunch of you know improvements or uh kind of uh elements that go into attentional models uh i’d like to introduce the transformer which is kind of the core element that is used in most state-of-the-art mt uh or nlp models nowadays and basically this is a self um a model that uses both self attention and regular attention between the source and the target um so uh we have a transformer encoder in a transformer decoder where the transformer encoder takes in some word embeddings it adds something called the positional encoding where the positional encoding basically is a another word embedding that tells you not the identity of the word but rather the position of the word in the sentence so instead of saying like this is the word summary it would say this is the first word in the sentence so you add them together and you get um like summary and first word uh in embedding indicating that um then you run this through multi-head self-attention you add in the original input and you do something called layer normalization which basically kind of normalizes the outputs uh to be average of around zero and norm of around one and uh then you run through a feed-forward network um and then you do the addition and normalization and then you do this over and over again you know six times or eight times or ten times or whatever um and then on the the right side over here you have the transformer decoder which is very similar output embeddings positional encodings self-attention but you also have additional cross attention that attends to the input here so that’s kind of the place where you get the attention that i showed before between the source and the target here so that this is in the cross-attention part and then in the self-attention part um you have the encoding of the sentences like you have here so you’re you’re doing both in the transformer so um often we talk about like the transformer is kind of like a monolithic block uh that is used in um in translation models uh but actually you know it contains a lot of the tricks that i talked about before so it contains self-attention multi-headed attention it’s using normalized dot product or actually maybe bilinear attention would be more accurate because it’s multiplying by a a weight matrix for the key in the queries and it has positional encodings it also uses a whole bunch of training tricks such as layer normalization helping ensure that layers remain in a reasonable range it also has a specialized training schedule where you have the atom optimizer but you have a specialized learning rate that kind of gradually ramps up and then slows down in the transformer paper they also used a technique called label smoothing i’m not going to go into a lot of detail about this but basically it kind of smooth smooths out the distribution you’re predicting so you’re not predicting 100 probability on the true next word but you’re giving a little bit of probability to all the other words uh this is pretty effective in a wide variety of neural network based tasks and um you also have a masking method for efficient training and so the way this masking works is basically um you uh decide when you’re calculating the representation for a particular word um you decide which previous uh which context to attend to so when you’re calculating the representation for the word i you only attend to the context in the input when you’re calculating the representation for the word hate you calcul you attend to any of these uh things over here um and similarly throughout the entire uh like calculation um of the representations for the decoders so on the encoder side you always attend to all the other words but on the decoder side you only attend to words in the input and previous words in the output and um so this was about transformers uh taking a step back we can kind of take a unified view of the models that we’ve talked about so far in this class um so we have sequence labeling where uh sequence labeling um has a feature extractor and then given the feature extractor we have a feature for each word and calculate the output so this is what you know everybody is doing for assignment one and then we have sequence sequence modeling where we have a feature x director so these can actually be exactly the same thing and calculate one representation for each word and then we have essentially a masked feature extractor here um where we calculate representations that can ref uh reference any of the things from the input feature extractor but only things that occurred previously in the current output sentence and then we predict the the words here as well cool um so this is all i have in terms of the lecture content and then i’m going to try to go uh through the code for the transformer just a little bit and then patrick’s going to introduce what you need to do for the assignment are there any questions about the lecture content that i talked about here okay i i guess not at the moment so um uh if you if you know about this already this might be a bit of review if you’re not familiar with this it’s very important so uh please do you know ask questions go to office hours other other things like this um and then the final thing i’d like to do is go through a brief code walk through this thing called the annotated transformer and this is a very good thing it’s linked on the class site and basically what it does is it um it demonstrates how you would actually implement the transformer model together with all of the description um from the original paper and so basically [Music] if you look at the model architecture first um the the way it looks is uh we’re talking about in encoder decoder architecture um we have our encoder here we have our decoder here and sorry my computer is very slow uh nowadays because my battery is broken as you heard last time but um i’ll try to do this nonetheless we have our source embedders we have our target embedders and then we have some sort of generation policy so this would be you know like something that helps us do uh sampling or um or helps us predict the probability of the next token and uh we take in and process uh source and target sentences the encoder encodes the source sentence and then the decoder encodes the target sentence and the memory so i believe this is the encoded source sentence and then we have the source mask and the target mask and the generator is basically uh just doing a soft max like you’re all familiar with um and so this is the kind of encoder architecture this is the decoder architecture and then we have our generator here and um one other feature of the uh of the transformer is basically we have uh n times this uh this encoder block in decoder block so uh this method here produces n identical layers uh so it it copies each module so you get n of them so that allows you to do the nx here the core encoder is a stack of n layers where we also have a layer norm after all of them or no sorry this is a final final layer moments and then we pass the uh the input and mask through each layer and turn so we just have a for loop over each of the the layers here and the layer norm what this does is this basically attempts to normalize so i mentioned we’re trying to set the mean uh close to zero and the um and the variance close to uh close to one so you can see that that’s kind of doing that here and we um we add dropout uh oh yes so dropout is uh kind of like regular um regularizing the model to prevent it from overfitting we have layer norm and uh then for each of these blocks you’ll notice that we have the multi-head attention and then we have this add-in norm after this so um this sub-layer what it’s doing is it is doing layer normalization and it’s doing layer normalization it’s calling the uh the sub-layer function here it’s doing dropout over the output and then it’s adding a residual connection so that’s kind of the the yellow block in the transformer stack here and then each layer um has in the encoder has two sub-layers one is the self-attention layer one is the feed-forward layer and then this is cloned you know uh two times uh essentially there then the decoder also has a stack of uh n uh layers basically and uh the decoder is the same except in addition to having self-attention it has a source of tension in the feed forward layer so that’s basically what you can see here in the diagram uh and then it talks a little bit about masking here so uh that was the masking that i talked about in the um in the lecture where you’re basically creating this mask that gets rid of all of the things that are in the future for the decoder and this is applied only to the decoder only to the decoder then finally i’m going to talk about the attention how the attention is done um within the uh the transformer so basically the transformer attention is doing this sort of dot product attention with scaling you take the soft max and you multiply this by the value vector so that’s you know what i talked about in class and that’s all implemented here it’s basically doing a matrix multiplication between the query and the key vector here query and key matrix here it’s taking the square root uh based on the size of the query vector um actually this is dk but this is query so that’s a little bit strange but uh you know it makes no difference as i mentioned before and then you take the soft max and you multiply the attention by the value vector so that’s basically the multiplication here so you can see um you can make a map between the equations of the code and uh then there’s also implementation of multi-head attention um this is a little bit more involved about exactly how this is implemented so i’m not going to go through all the equations here because i want to leave time for patrick but um if you want to look at exactly how the multi-head is uh implemented you can see this here um and yeah that that’s the basic idea of uh what this looks like um i think it’s important to know what’s going on in these models but if you uh want to like actually use this in your code itself um it’s also uh pytorch is also nice enough to just have this uh transformer class for you that you don’t need to um like you don’t need to implement yourself and uh basically if you look at this it’s doing the same implementation that i just showed you but it’s you know maybe more optimized or you know like you know it’s right so you won’t be making mistakes so if you’re actually going to be using this i suggest just using the transformer module but i think it’s important to know what’s going on inside so you can understand the behavior and other things like that um great so that’s all i have here are there any questions about uh the implementation or uh other things yes um isn’t the annotated transformer i seem to remember like it’s slower because it uses for loops for the batches instead of um converting them into like multi-dimensional tensors is that ringing a bell and is that accurate or um let me i didn’t cover batches um so like batching is uh important uh it’s a very uh it’s a very important thing because like basically when you batch together multiple sentences you can get them to uh like all process in a single gpu call instead of in many different gpus calls i’ve i talked about this a little bit more in uh advanced nlp um i don’t see actually i was thinking of the of the multi the multiple heads the attention heads um because i think i think the i think it actually matches by input sequence it definitely watches because he has access to a mask so if he has a mask it’s definitely watching yeah yeah i don’t like i i seem to remember this was actually implemented pretty well pretty efficiently um i don’t um i didn’t actually talk about how the multi-headed attention is implemented but i it’s implemented in a clever way which also makes it hard to read which is why i didn’t go into the details quite as much here but um i i do think this is actually a pretty good example of how you would implement it efficiently as well um although i’d have to go through uh in more detail to make sure for sure but that’s a good question what i can tell you is the pytorch one is definitely implemented efficiently so if you want to um look and see the the best way to do that you can look there um i’ll turn this over to patrick then who uh will i guess go through the uh assignment for assignment two and this is now available on the uh on the website also if you want to follow along and let me share the screen okay do you guys see the right screen yeah cool yeah so i’m going to introduce assignment 2 which was released today uh they’re still the code still needs to be uploaded but all the instructions are there and yeah so let’s get started because we’re short on time so ah and this was developed by me and vijay um which will uh explain the later part of this sign cool so the task for this assignment is machine translation so as gram described given a source language sentence you want to relate to a target language to target language uh so conditional language modeling and the goals of this assignment are uh to understand how the standard data processing pipeline is used in empty to be able to train and empty models both bilingual and multilingual using an empty framework and to learn how these models are evaluated and finally to investigate methods to tackle the data scarcity problem in low resource language pairs which is what you’re going to tackle in this assignment so for requirements you definitely need a machine with gpu uh unlike the previous assignments it will be very very hard to run this on a cpu as in you will run but no invisible amounts of time um i i didn’t test it but i’m pretty sure you can run it on a call out but still if you have access to the credits i would probably recommend doing aws for this and in terms of packages it’s mostly the same as the last assignment you can almost certainly reuse the the environment that you have we tested it and this design will also use firstec as a backbone for training models so first tech is like uh sequence modeling toolkits and and it automates data loading training decoding and a bunch of other small not boring but like you might not want to consider for this assignment and it also supports many other tasks other than translation for the basic requirements you will not need to dive deep into fair sec but if you want to modify for um the actual requirements you probably will need to dive into faresec and and yeah you also need to install soccer blue and combat to evaluate your models and but this is all automated in the instructions of the assignment you just have to follow the the assignment and everything should be installed automatically so the assignment is a collection of scripts which will pre-process you data train your data and score you’re there and we’ll go over them uh in a few slides so for that you will be using the ted talks corpus which contains parallel data between english and 58 languages and you will focus on two low resource language pairs english azerbaijani and ecospeller russian so to pre-process the data um there’s a script that does the following steps we will read the raw parallel data you will learn about byte pairing coding separately for the source and target languages and in particular this assignment will use sentence piece which is a particular implementation of uh bypass encoding we’ll apply bp to all splits there’s also a very minor data cleaning pipeline which just filters based on the size of sentences and finally you’ll binarize the data which makes it efficient for first to trade and crucially for this assignment we simplify the bits sometimes you also do some tokenization using moses prior to learning the bp but in this case we simplify then we just use center space which works fine by itself but sometimes people do moses organization before just as a remark for modeling um in training and generation uh we’ll use the reform architecture as describe diagram and embeddings will be shared between source and targets despite the fact that they use different bps this is still possible uh and this will be trained to minimize cross entropy with adam and decoding will be done using beam search in particular it’s fixed to beam search of five and is an example of what the scripts call and this is just like cli for fersae so you don’t have to worry about too much about training loops or anything for evaluation uh for mt you traditionally don’t look at perplexity because it’s not a good measure of how the models um perform in terms of translation and in this assignment we’ll use two metrics blur which is uh the de facto standard for empty evaluation which uses n-gram overlap between the reference in the target and because there’s some concerns with the reliability of blue and this correlation with human judgment would also consider comet which is part of a new generation of neural neural model based metrics which rely on pre-trained models and are trained to optimize relatively human correlations again you don’t have to think too much about it because there’s a script that already does it it’s just good to know that you’ll evaluate your models based on both okay and i’ll turn it over to vijay thanks patrick so the minimum requirements for this assignment involve first training these mt models on bilingual data and so here we would we you we expect you to train models from azerbaijani to english and vice versa and also bella russian english and vice versa next slide but then the more interesting bit which is also required in the minimal in the in the um our minimal expectations is to train multilingually using a transfer language which in the case of azerbaijani is turkish and we have scripts to do this already for both bilingual and multilingual but you’ll have to modify these scripts minimally in order to support belarusian and we’ve also introduced because nowadays while multilingual training is still very effective there’s a new paradigm that’s become super popular which is fine tuning a massive multilingual model that’s been trained on a large amount of data and to allow you to try this approach which is now pretty predominant uh you can also instead of doing multilingual training you can fine-tune a model that we have provided that has been trained on the floor s 101 data set which is like a massively multi-lingual mt data set and uh you can instead use this strategy and this will also get you to the um to the b b plus grading range next slide uh but uh i think where this assignment gets really exciting and what you need to do to get um an a minus a or a plus is to add extensions beyond what we provided in these scripts and uh i think graham will be talking about a lot of this in the next week some of it he already talked about this week but um uh data augmentation which he hasn’t talked about yet is one uh very promising strategy you could use uh uh these terms will make more sense once uh his lecture is completed next slide you can also try to use better transfer languages we’ve currently suggested two basic transfer languages for both azerbaijani and belarusian but as you might remember from the lecture last week there’s more of a structured way you can choose a good transfer language based on your knowledge of typology or based on learn models and we encourage you to try this and see what what results you can get and then lastly uh uh in terms of the data our baseline models use sentence piece for byte pair encoding which is basically a way of breaking up words into sub words but there’s like many other ways to do this and this is a particularly interesting and active area of research and so we encourage you to consider different variants of this and lastly this is probably the most broad category um feel free to try different models different uh training algorithms different ways of uh doing learning or different uh and uh there’s like a huge space here so we really encourage you to be creative and also when you when you choose something analyze it and try to deepen your understanding of why these things work in machine translation and just for reference like the assignment includes a bunch of references to different techniques that seem to work for this feel free to follow them or do your own literature review of course and in terms of what we ask from you when you submit the homework first you need to submit the code which should include both the minimum requirements for like the scripts that you’ve modified in order to support the um the bilingual and multilingual training for the language pairs in the assignment but also for the additional modifications that you make we also and this is probably the most important part uh we want to see a write-up that provides some solid analysis of uh of the methods describing the assignment and lastly uh we don’t want you to submit the actual model files because these are quite very large and it’ll take a long time to upload but we do want to see you submit your model outputs once you start playing with the software this will it’ll make sense what this is but basically this way we could compare the translations that your model predicts with our references and um reproduce your evaluations just to make sure everyone’s on the same page and these are all submitted on canvas as a tarball like the previous assignment and so i’ve been touching on this but uh the grading tranches are like first for the minimal requirements you just need to reproduce the um modify the scripts that we have for both azerbaijani and belarusian and that will get eub if you couple this with detailed analysis then you’ll get a b plus to get an a minus you need to implement at least one pre-existing method or a new method to try to improve the ability to do this multilingual machine translation next slide and then um to get an a we expect you to do uh two or more several methods to improve multilingual transfer and you might find that uh some of these methods are uh more lightweight than others so uh it might be quite doable to try uh several and uh we we also would hope that one of the methods you try is actually novel something that you’d come up with and then lastly building on that a um if you’ve done something that is particularly extensive or creative or your analysis is like super compelling then that’ll get you an a plus and just to recall that this is a group assignment unlike the first assignment so it’s expected that it will be done in groups of two three uh i’m not exactly sure how this will be arranged but i’m pretty sure gram and you don’t know what’s the best way to do this um but just to apply for expectations yeah regarding groups um feel free to make groups uh you know obviously you’ve already talked to people about um the language intent uh things so that would be one option uh but you’re obviously not obligated to do that if you don’t have a group yet um but would like to form one um please i think one one good idea would be to email the ta mailing list or put a post on piazza um by around the time the first assignment is due which is you know very soon obviously um and so then like when you finish the first assignment if there’s anybody who doesn’t have a group yet we can try to match you up with uh somebody else so um yeah um are there any questions about uh these yeah just uh if you need anything uh feel free to come to our office hours but mine and vijay’s or the other tiers can potentially help you as well and if you want to contact us piazza obviously or feel free to email us or the ta mailing list\n\n\n\n\n\nSee also\nBahdanau, Cho, and Bengio (2016)\nLuong, Pham, and Manning (2015)\nReference: Self Attention (Cheng et al. 2016)\nVaswani et al. (2023)\n\n\n\n\n\nReferences\n\nBahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2016. “Neural Machine Translation by Jointly Learning to Align and Translate.” https://arxiv.org/abs/1409.0473.\n\n\nLuong, Minh-Thang, Hieu Pham, and Christopher D. Manning. 2015. “Effective Approaches to Attention-Based Neural Machine Translation.” https://arxiv.org/abs/1508.04025.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. “Attention Is All You Need.” https://arxiv.org/abs/1706.03762.\n\nCitationBibTeX citation:@online{bochman2022,\n  author = {Bochman, Oren},\n  title = {Translation {Models}},\n  date = {2022-02-24},\n  url = {https://orenbochman.github.io/notes-nlp/notes/cs11-737-w06/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2022. “Translation Models.” February 24,\n2022. https://orenbochman.github.io/notes-nlp/notes/cs11-737-w06/."
  },
  {
    "objectID": "notes/c2w1/lab01.html",
    "href": "notes/c2w1/lab01.html",
    "title": "Building the vocabulary",
    "section": "",
    "text": "course banner\n\n\nEstimated Time: 10 minutes\n\nVocabulary Creation\nCreate a tiny vocabulary from a tiny corpus\nIt’s time to start small !\n\nImports and Data\n\n# imports\nimport re # regular expression library; for tokenization of words\nfrom collections import Counter # collections library; counter: dict subclass for counting hashable objects\nimport matplotlib.pyplot as plt # for data visualization\n\n\n# the tiny corpus of text ! \ntext = 'red pink pink blue blue yellow ORANGE BLUE BLUE PINK' # 🌈\nprint(text)\nprint('string length : ',len(text))\n\nred pink pink blue blue yellow ORANGE BLUE BLUE PINK\nstring length :  52\n\n\n\n\nPreprocessing\n\n# convert all letters to lower case\ntext_lowercase = text.lower()\nprint(text_lowercase)\nprint('string length : ',len(text_lowercase))\n\nred pink pink blue blue yellow orange blue blue pink\nstring length :  52\n\n\n\n# some regex to tokenize the string to words and return them in a list\nwords = re.findall(r'\\w+', text_lowercase)\nprint(words)\nprint('count : ',len(words))\n\n['red', 'pink', 'pink', 'blue', 'blue', 'yellow', 'orange', 'blue', 'blue', 'pink']\ncount :  10\n\n\n\n\nCreate Vocabulary\nOption 1 : A set of distinct words from the text\n\n# create vocab\nvocab = set(words)\nprint(vocab)\nprint('count : ',len(vocab))\n\n{'blue', 'red', 'yellow', 'orange', 'pink'}\ncount :  5\n\n\n\n\nAdd Information with Word Counts\nOption 2 : Two alternatives for including the word count as well\n\n# create vocab including word count\ncounts_a = dict()\nfor w in words:\n    counts_a[w] = counts_a.get(w,0)+1\nprint(counts_a)\nprint('count : ',len(counts_a))\n\n{'red': 1, 'pink': 3, 'blue': 4, 'yellow': 1, 'orange': 1}\ncount :  5\n\n\n\n# create vocab including word count using collections.Counter\ncounts_b = dict()\ncounts_b = Counter(words)\nprint(counts_b)\nprint('count : ',len(counts_b))\n\nCounter({'blue': 4, 'pink': 3, 'red': 1, 'yellow': 1, 'orange': 1})\ncount :  5\n\n\n\n# barchart of sorted word counts\nd = {'blue': counts_b['blue'], 'pink': counts_b['pink'], 'red': counts_b['red'], 'yellow': counts_b['yellow'], 'orange': counts_b['orange']}\nplt.bar(range(len(d)), list(d.values()), align='center', color=d.keys())\n_ = plt.xticks(range(len(d)), list(d.keys()))\n\n\n\n\n\n\n\n\n\n\nUngraded Exercise\nNote that counts_b, above, returned by collections.Counter is sorted by word count\nCan we modify the tiny corpus of text so that a new color appears between pink and red in counts_b ?\nDo we need to run all the cells again, or just specific ones ?\n\nprint('counts_b : ', counts_b)\nprint('count : ', len(counts_b))\n\ncounts_b :  Counter({'blue': 4, 'pink': 3, 'red': 1, 'yellow': 1, 'orange': 1})\ncount :  5\n\n\nExpected Outcome:\ncounts_b : Counter({‘blue’: 4, ‘pink’: 3, ‘your_new_color_here’: 2, red’: 1, ‘yellow’: 1, ‘orange’: 1})  count : 6\n\n\nSummary\nThis is a tiny example but the methodology scales very well.  In the assignment we will create a large vocabulary of thousands of words, from a corpus  of tens of thousands or words! But the mechanics are exactly the same.  The only extra things to pay attention to should be; run time, memory management and the vocab data structure.  So the choice of approach used in code blocks counts_a vs counts_b, above, will be important.\n\n\n\n\n\nCitationBibTeX citation:@online{bochman2020,\n  author = {Bochman, Oren},\n  title = {Building the Vocabulary},\n  date = {2020-10-16},\n  url = {https://orenbochman.github.io/notes-nlp/notes/c2w1/lab01.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2020. “Building the Vocabulary.” October 16,\n2020. https://orenbochman.github.io/notes-nlp/notes/c2w1/lab01.html.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocorrect & Dynamic Programming",
      "L1 - Building the vocabulary"
    ]
  },
  {
    "objectID": "notes/c2w1/index.html",
    "href": "notes/c2w1/index.html",
    "title": "Autocorrect and minimum edit distance",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\n\nFigure 1\nMy notes for Week 1 of the Natural Language Processing with Probabilistic Models Course in the Natural Language Processing Specialization Offered by DeepLearning.AI on Coursera",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocorrect & Dynamic Programming",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w1/index.html#overview",
    "href": "notes/c2w1/index.html#overview",
    "title": "Autocorrect and minimum edit distance",
    "section": "Overview",
    "text": "Overview\nWe use auto-correct everyday. When we send your friend a text message, or when we make a mistake in a query, there is an autocorrect behind the scenes that corrects the sentence for you. This week we are also going to learn about minimum edit distance, which tells we the minimum amount of edits to change one word into another. In doing that, we will learn about dynamic programming which is an important programming concept which frequently comes up in interviews and could be used to solve a lot of optimization problems.\n\n\n\n\n\n\n\nautocorrect\n\n\n\n\nFigure 2: Learning Objectives",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocorrect & Dynamic Programming",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w1/index.html#autocorrect",
    "href": "notes/c2w1/index.html#autocorrect",
    "title": "Autocorrect and minimum edit distance",
    "section": "Autocorrect",
    "text": "Autocorrect\nAutocorrects are used everywhere. We use them in your phones, tablets, and computers.\n\n\n\n\n\n\n\nfind candidates\n\n\n\n\nFigure 3: What is autocorrect\n\n\nTo implement autocorrect in this week’s assignment, we have to follow these steps:\n\nIdentify a misspelled word\nFind strings n edit distance away: (these could be random strings)\nFilter candidates: (keep only the real words from the previous steps)\nCalculate word probabilities: (choose the word that is most likely to occur in that context)\n\n\n\n\n\n\n\n\nfind candidates\n\n\n\n\nFigure 4: Find candidates\n\n\nBuilding the model:\n\nIdentify the misspelled word\n\n\nWhen identifying the misspelled word, we can check whether it is in the vocabulary. If we don’t find it, then it is probably a typo.\n\n\nFind strings n edit distance away\nFilter candidates\n\n\nIn this step, we want to take all the words generated above and then only keep the actual words that make sense and that we can find in your vocabulary.\n\n\n\n\n\n\n\n\nfilter\n\n\n\n\nFigure 5: Filter candidates",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocorrect & Dynamic Programming",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w1/index.html#lab-building-the-vocabulary",
    "href": "notes/c2w1/index.html#lab-building-the-vocabulary",
    "title": "Autocorrect and minimum edit distance",
    "section": "Lab: Building the vocabulary",
    "text": "Lab: Building the vocabulary\nBuilding the vocabulary",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocorrect & Dynamic Programming",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w1/index.html#building-the-model-ii",
    "href": "notes/c2w1/index.html#building-the-model-ii",
    "title": "Autocorrect and minimum edit distance",
    "section": "Building the model II",
    "text": "Building the model II\n\nCalculating word probabilities\n\n\n\n\n\n\n\n\nword probabilities\n\n\n\n\nFigure 6: calculating word probabilities\n\n\nNote that we are storing the count of words and then we can use that to generate the probabilities. For this week, we will be counting the probabilities of words occurring. If we want to build a slightly more sophisticated auto-correct we can keep track of two words occurring next to each other instead. We can then use the previous word to decide. For example which combo is more likely, there friend or their friend? For this week however we will be implementing the probabilities by just using the word frequencies.\nHere is a summary of everything we have seen before in the previous two videos.\n\n\n\n\n\n\n\nsummary\n\n\n\n\nFigure 7: summary of first four autocorrect steps",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocorrect & Dynamic Programming",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w1/index.html#lab-candidates-from-edits",
    "href": "notes/c2w1/index.html#lab-candidates-from-edits",
    "title": "Autocorrect and minimum edit distance",
    "section": "Lab: Candidates from Edits",
    "text": "Lab: Candidates from Edits\nCandidates from Edits",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocorrect & Dynamic Programming",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w1/index.html#minimum-edit-distance",
    "href": "notes/c2w1/index.html#minimum-edit-distance",
    "title": "Autocorrect and minimum edit distance",
    "section": "Minimum edit distance",
    "text": "Minimum edit distance\nMinimum edit distance allows we to:\n\nEvaluate similarity between two strings\nFind the minimum number of edits between two strings\nImplement spelling correction, document similarity, machine translation, DNA sequencing, and more\n\nRemember that the edits include:\n\nInsert (add a letter) ‘to’: ‘top’, ‘two’ …\nDelete (remove a letter) ‘hat’: ‘ha’, ‘at’, ‘ht’\nReplace (change 1 letter to another) ‘jaw’: ‘jar’, ‘paw’, …\n\nHere is a concrete example where we calculate the cost (i.e. edit distance) between two strings.\n\n\n\n\n\n\n\nMinimum edit distance\n\n\n\n\nFigure 8: Minimum edit distance\n\n\nNote that as your strings get larger it gets much harder to calculate the minimum edit distance. Hence we will now learn about the minimum edit distance algorithm!",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocorrect & Dynamic Programming",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w1/index.html#minimum-edit-distance-algorithm",
    "href": "notes/c2w1/index.html#minimum-edit-distance-algorithm",
    "title": "Autocorrect and minimum edit distance",
    "section": "Minimum edit distance algorithm",
    "text": "Minimum edit distance algorithm\nWhen computing the minimum edit distance, we would start with a source word and transform it into the target word. Let’s look at the following example:\n\n\n\n\n\n\n\nMinimum edit distance\n\n\n\n\nFigure 9: Minimum edit distance algorithm\n\n\nTo go:\n\nfrom # → # has a cost of 0\nfrom p → # has a cost of 1, because that is the cost of a delete.\n\nfrom p → s has a cost of 2, delete p and insert s.\n\nWe can keep going this way by populating one element at a time, but it turns out there is a faster way to do this.\nWee will learn about it next.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocorrect & Dynamic Programming",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w1/index.html#minimum-edit-distance-algorithm-ii",
    "href": "notes/c2w1/index.html#minimum-edit-distance-algorithm-ii",
    "title": "Autocorrect and minimum edit distance",
    "section": "Minimum edit distance algorithm II",
    "text": "Minimum edit distance algorithm II\nTo populate the following table:\n\n\n\n\n\n\n\nMinimum edit distance\n\n\n\n\nFigure 10: Minimum edit distance algorithm\n\n\nThere are three equations:\nD[i,j] = D[i-1, j] + del_cost: this indicates we want to populate the current cell (i,j) by using the cost in the cell found directly above.\nD[i,j] = D[i, j-1] + ins_cost: this indicates we want to populate the current cell (i,j) by using the cost in the cell found directly to its left.\nD[i,j] = D[i-1, j-1] + rep_cost: the rep cost can be 2 or 0 depending if we are going to actually replace it or not.\nAt every time step we check the three possible paths where we can come from and we select the least expensive one. Once we are done, we get the following:\n\n\n\n\n\n\n\nMinimum edit distance\n\n\n\n\nFigure 11: Minimum edit distance algorithm",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocorrect & Dynamic Programming",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w1/index.html#minimum-edit-distance-iii",
    "href": "notes/c2w1/index.html#minimum-edit-distance-iii",
    "title": "Autocorrect and minimum edit distance",
    "section": "Minimum edit distance III",
    "text": "Minimum edit distance III\n\n\n\n\n\n\n\nMinimum edit distance\n\n\n\n\nFigure 12: Minimum edit distance algorithm\n\n\nTo summarize, we have seen the levenshtein distance which specifies the cost per operation. If we need to reconstruct the path of how we got from one string to the other, we can use a backtrace. We should keep a simple pointer in each cell letting we know where we came from to get there. So we know the path taken across the table from the top left corner, to the bottom right corner. We can then reconstruct it.\nThis method for computation instead of brute force is a technique known as dynamic programming. We first solve the smallest subproblem first and then reusing that result we solve the next biggest subproblem, saving that result, reusing it again, and so on. This is exactly what we did by populating each cell from the top right to the bottom left. It’s a well-known technique in computer science!",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocorrect & Dynamic Programming",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w3/lab01.html",
    "href": "notes/c3w3/lab01.html",
    "title": "Vanishing Gradients",
    "section": "",
    "text": "course banner\nIn this notebook you’ll take another look at vanishing gradients, from an intuitive standpoint.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "LSTMs and Named Entity Recognition",
      "L1 - Vanishing Gradients"
    ]
  },
  {
    "objectID": "notes/c3w3/lab01.html#background",
    "href": "notes/c3w3/lab01.html#background",
    "title": "Vanishing Gradients",
    "section": "Background",
    "text": "Background\nAdding layers to a neural network introduces multiplicative effects in both forward and backward propagation. The back prop in particular presents a problem as the gradient of activation functions can be very small. Multiplied together across many layers, their product can be vanishingly small! This results in weights not being updated in the front layers and training not progressing.\n Gradients of the sigmoid function, for example, are in the range 0 to 0.25. To calculate gradients for the front layers of a neural network the chain rule is used. This means that these tiny values are multiplied starting at the last layer, working backwards to the first layer, with the gradients shrinking exponentially at each step. ## Imports\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n%matplotlib inline",
    "crumbs": [
      "Home",
      "Sequence Models",
      "LSTMs and Named Entity Recognition",
      "L1 - Vanishing Gradients"
    ]
  },
  {
    "objectID": "notes/c3w3/lab01.html#data-activation-gradient",
    "href": "notes/c3w3/lab01.html#data-activation-gradient",
    "title": "Vanishing Gradients",
    "section": "Data, Activation & Gradient",
    "text": "Data, Activation & Gradient\n\nData\nI’ll start be creating some data, nothing special going on here. Just some values spread across the interval -5 to 5.\n\nTry changing the range of values in the data to see how it impacts the plots that follow.\n\n\n\nActivation\nThe example here is sigmoid() to squish the data x into the interval 0 to 1.\n\n\nGradient\nThis is the derivative of the sigmoid() activation function. It has a maximum of 0.25 at x = 0, the steepest point on the sigmoid plot.\n\nTry changing the x value for finding the tangent line in the plot.\n\n\n\n# Data\n# Interval [-5, 5]\n### START CODE HERE ###\nx = np.linspace(-5, 5, 100)  # try changing the range of values in the data. eg: (-100,100,1000)\n### END CODE HERE ###\n# Activation\n# Interval [0, 1]\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\nactivations = sigmoid(x)\n\n# Gradient\n# Interval [0, 0.25]\ndef sigmoid_gradient(x):\n    return (x) * (1 - x)\n\ngradients = sigmoid_gradient(activations)\n\n# Plot sigmoid with tangent line\nplt.plot(x, activations)\nplt.title(\"Sigmoid Steepest Point\")\nplt.xlabel(\"x input data\")\nplt.ylabel(\"sigmoid(x)\")\n\n# Add the tangent line\n### START CODE HERE ###\nx_tan = 0   # x value to find the tangent. try different values within x declared above. eg: 2  \n### END CODE HERE ###\ny_tan = sigmoid(x_tan)  # y value\nspan = 1.7              # line span along x axis\ndata_tan = np.linspace(x_tan - span, x_tan + span)  # x values to plot\ngradient_tan = sigmoid_gradient(sigmoid(x_tan))     # gradient of the tangent\ntan = y_tan + gradient_tan * (data_tan - x_tan)     # y values to plot\nplt.plot(x_tan, y_tan, marker=\"o\", color=\"orange\", label=True)  # marker\nplt.plot(data_tan, tan, linestyle=\"--\", color=\"orange\")         # line\nplt.show()\n\nText(0.5, 1.0, 'Sigmoid Steepest Point')\n\n\nText(0.5, 0, 'x input data')\n\n\nText(0, 0.5, 'sigmoid(x)')",
    "crumbs": [
      "Home",
      "Sequence Models",
      "LSTMs and Named Entity Recognition",
      "L1 - Vanishing Gradients"
    ]
  },
  {
    "objectID": "notes/c3w3/lab01.html#plots",
    "href": "notes/c3w3/lab01.html#plots",
    "title": "Vanishing Gradients",
    "section": "Plots",
    "text": "Plots\n\nSub Plots\nData values along the x-axis of the plots on the interval chosen for x, -5 to 5. Subplots: - x vs x - sigmoid of x - gradient of sigmoid\nNotice how the y axis keeps compressing from the left plot to the right plot. The interval range has shrunk from 10 to 1 to 0.25. How did this happen? As |x| gets larger the sigmoid approaches asymptotes at 0 and 1, and the sigmoid gradient shrinks towards 0. * Try changing the range of values in the code block above to see how it impacts the plots.\n\n# Sub plots\nfig, axs = plt.subplots(1, 3, figsize=(15, 4), sharex=True)\n\n# X values\naxs[0].plot(x, x)\naxs[0].set_title(\"x values\")\naxs[0].set_ylabel(\"y=x\")\naxs[0].set_xlabel(\"x input data\")\n\n# Sigmoid\naxs[1].plot(x, activations)\naxs[1].set_title(\"sigmoid\")\naxs[1].set_ylabel(\"sigmoid\")\naxs[1].set_xlabel(\"x input data\")\n\n# Sigmoid gradient\naxs[2].plot(x, gradients)\naxs[2].set_title(\"sigmoid gradient\")\naxs[2].set_ylabel(\"gradient\")\naxs[2].set_xlabel(\"x input data\")\n\nfig.show()\n\nText(0.5, 1.0, 'x values')\n\n\nText(0, 0.5, 'y=x')\n\n\nText(0.5, 0, 'x input data')\n\n\nText(0.5, 1.0, 'sigmoid')\n\n\nText(0, 0.5, 'sigmoid')\n\n\nText(0.5, 0, 'x input data')\n\n\nText(0.5, 1.0, 'sigmoid gradient')\n\n\nText(0, 0.5, 'gradient')\n\n\nText(0.5, 0, 'x input data')\n\n\n/tmp/ipykernel_120031/2272590038.py:22: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n  fig.show()\n\n\n\n\n\n\n\n\n\n\n\nSingle Plo\nt Putting all 3 series on a single plot can help visualize the compression. Notice how hard it is to interpret because sigmoid and sigmoid gradient are so small compared to the scale of the input data x.\n\nTrying changing the plot ylim to zoom in.\n\n\n# Single plot\nplt.plot(x, x, label=\"data\")\nplt.plot(x, activations, label=\"sigmoid\")\nplt.plot(x, gradients, label=\"sigmoid gradient\")\nplt.legend(loc=\"upper left\")\nplt.title(\"Visualizing Compression\")\nplt.xlabel(\"x input data\")\nplt.ylabel(\"range\")\n### START CODE HERE ###\n# plt.ylim(-.5, 1.5)    # try shrinking the y axis limit for better visualization. eg: uncomment this line\n### END CODE HERE ###\nplt.show()\n\n# Max, Min of each array\nprint(\"\")\nprint(\"Max of x data :\", np.max(x))\nprint(\"Min of x data :\", np.min(x), \"\\n\")\nprint(\"Max of sigmoid :\", \"{:.3f}\".format(np.max(activations)))\nprint(\"Min of sigmoid :\", \"{:.3f}\".format(np.min(activations)), \"\\n\")\nprint(\"Max of gradients :\", \"{:.3f}\".format(np.max(gradients)))\nprint(\"Min of gradients :\", \"{:.3f}\".format(np.min(gradients)))\n\nText(0.5, 1.0, 'Visualizing Compression')\n\n\nText(0.5, 0, 'x input data')\n\n\nText(0, 0.5, 'range')\n\n\n\n\n\n\n\n\n\n\nMax of x data : 5.0\nMin of x data : -5.0 \n\nMax of sigmoid : 0.993\nMin of sigmoid : 0.007 \n\nMax of gradients : 0.250\nMin of gradients : 0.007",
    "crumbs": [
      "Home",
      "Sequence Models",
      "LSTMs and Named Entity Recognition",
      "L1 - Vanishing Gradients"
    ]
  },
  {
    "objectID": "notes/c3w3/lab01.html#numerical-impact",
    "href": "notes/c3w3/lab01.html#numerical-impact",
    "title": "Vanishing Gradients",
    "section": "Numerical Impact",
    "text": "Numerical Impact\n\nMultiplication & Decay\nMultiplying numbers smaller than 1 results in smaller and smaller numbers. Below is an example that finds the gradient for an input x = 0 and multiplies it over n steps. Look how quickly it ‘Vanishes’ to almost zero. Yet sigmoid(x=0)=0.5 which has a sigmoid gradient of 0.25 and that happens to be the largest sigmoid gradient possible!  (Note: This is NOT an implementation of back propagation.)\n\nTry changing the number of steps n.\nTry changing the input value x. Consider the impact on sigmoid and sigmoid gradient.\n\n\n# Simulate decay\n# Inputs\n### START CODE HERE ###\nn = 6  # number of steps : try changing this\nx = 0  # value for input x : try changing this\n### END CODE HERE ###\ngrad = sigmoid_gradient(sigmoid(x))\nsteps = np.arange(1, n + 1)\nprint(\"-- Inputs --\")\nprint(\"steps :\", n)\nprint(\"x value :\", x)\nprint(\"sigmoid :\", \"{:.5f}\".format(sigmoid(x)))\nprint(\"gradient :\", \"{:.5f}\".format(grad), \"\\n\")\n\n# Loop to calculate cumulative total\nprint(\"-- Loop --\")\nvals = []\ntotal_grad = 1  # initialize to 1 to satisfy first loop below\nfor s in steps:\n    total_grad = total_grad * grad\n    vals.append(total_grad)\n    print(\"step\", s, \":\", total_grad)\n\nprint(\"\")\n\n# Plot\nplt.plot(steps, vals)\nplt.xticks(steps)\nplt.title(\"Multiplying Small Numbers\")\nplt.xlabel(\"Steps\")\nplt.ylabel(\"Cumulative Gradient\")\nplt.show()\n\n-- Inputs --\nsteps : 6\nx value : 0\nsigmoid : 0.50000\ngradient : 0.25000 \n\n-- Loop --\nstep 1 : 0.25\nstep 2 : 0.0625\nstep 3 : 0.015625\nstep 4 : 0.00390625\nstep 5 : 0.0009765625\nstep 6 : 0.000244140625\n\n\n\n([&lt;matplotlib.axis.XTick at 0x7edb9179aa10&gt;,\n  &lt;matplotlib.axis.XTick at 0x7edb917989d0&gt;,\n  &lt;matplotlib.axis.XTick at 0x7edb917ba920&gt;,\n  &lt;matplotlib.axis.XTick at 0x7edb917bb610&gt;,\n  &lt;matplotlib.axis.XTick at 0x7edb917e0340&gt;,\n  &lt;matplotlib.axis.XTick at 0x7edb917ba170&gt;],\n [Text(1, 0, '1'),\n  Text(2, 0, '2'),\n  Text(3, 0, '3'),\n  Text(4, 0, '4'),\n  Text(5, 0, '5'),\n  Text(6, 0, '6')])\n\n\nText(0.5, 1.0, 'Multiplying Small Numbers')\n\n\nText(0.5, 0, 'Steps')\n\n\nText(0, 0.5, 'Cumulative Gradient')",
    "crumbs": [
      "Home",
      "Sequence Models",
      "LSTMs and Named Entity Recognition",
      "L1 - Vanishing Gradients"
    ]
  },
  {
    "objectID": "notes/c3w3/lab01.html#solution",
    "href": "notes/c3w3/lab01.html#solution",
    "title": "Vanishing Gradients",
    "section": "Solution",
    "text": "Solution\nOne solution is to use activation functions that don’t have tiny gradients. Other solutions involve more sophisticated model design. But they’re both discussions for another time.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "LSTMs and Named Entity Recognition",
      "L1 - Vanishing Gradients"
    ]
  },
  {
    "objectID": "notes/c1w4/lab02.html",
    "href": "notes/c1w4/lab02.html",
    "title": "Hash functions and multiplanes",
    "section": "",
    "text": "Figure 1: course banner\nIn this lab, we are going to practice the most important concepts related to the hash functions explained in the videos. We will be using these in this week’s assignment.\nA key point for the lookup using hash functions is the calculation of the hash key or bucket id that we assign for a given entry. In this notebook, we will cover:"
  },
  {
    "objectID": "notes/c1w4/lab02.html#basic-hash-tables",
    "href": "notes/c1w4/lab02.html#basic-hash-tables",
    "title": "Hash functions and multiplanes",
    "section": "Basic Hash tables",
    "text": "Basic Hash tables\nHash tables are data structures that allow indexing data to make lookup tasks more efficient. In this part, we will see the implementation of the simplest hash function.\n\nimport numpy as np                # library for array and matrix manipulation\nimport pprint                     # utilities for console printing \nfrom utils_nb import plot_vectors # helper function to plot vectors\nimport matplotlib.pyplot as plt   # visualization library\n\npp = pprint.PrettyPrinter(indent=4) # Instantiate a pretty printer\n\nIn the next cell, we will define a straightforward hash function for integer numbers. The function will receive a list of integer numbers and the desired amount of buckets. The function will produce a hash table stored as a dictionary, where keys contain the hash keys, and the values will provide the hashed elements of the input list.\nThe hash function is just the remainder of the integer division between each element and the desired number of buckets.\n\ndef basic_hash_table(value_l, n_buckets):\n    \n    def hash_function(value, n_buckets):\n        return int(value) % n_buckets\n    \n    hash_table = {i:[] for i in range(n_buckets)} # Initialize all the buckets in the hash table as empty lists\n\n    for value in value_l:\n        hash_value = hash_function(value,n_buckets) # Get the hash key for the given value\n        hash_table[hash_value].append(value) # Add the element to the corresponding bucket\n    \n    return hash_table\n\nNow let’s see the hash table function in action. The pretty print function (pprint()) will produce a visually appealing output.\n\nvalue_l = [100, 10, 14, 17, 97] # Set of values to hash\nhash_table_example = basic_hash_table(value_l, n_buckets=10)\npp.pprint(hash_table_example)\n\n{   0: [100, 10],\n    1: [],\n    2: [],\n    3: [],\n    4: [14],\n    5: [],\n    6: [],\n    7: [17, 97],\n    8: [],\n    9: []}\n\n\nIn this case, the bucket key must be the rightmost digit of each number."
  },
  {
    "objectID": "notes/c1w4/lab02.html#planes",
    "href": "notes/c1w4/lab02.html#planes",
    "title": "Hash functions and multiplanes",
    "section": "Planes",
    "text": "Planes\nMultiplanes hash functions are other types of hash functions. Multiplanes hash functions are based on the idea of numbering every single region that is formed by the intersection of n planes. In the following code, we show the most basic forms of the multiplanes principle. First, with a single plane:\n\nP = np.array([[1, 1]]) # Define a single plane. \nfig, ax1 = plt.subplots(figsize=(8, 8)) # Create a plot\n\nplot_vectors([P], axes=[2, 2], ax=ax1) # Plot the plane P as a vector\n\n# Plot  random points. \nfor i in range(0, 10):\n        v1 = np.array(np.random.uniform(-2, 2, 2)) # Get a pair of random numbers between -4 and 4 \n        side_of_plane = np.sign(np.dot(P, v1.T)) \n        \n        # Color the points depending on the sign of the result of np.dot(P, point.T)\n        if side_of_plane == 1:\n            ax1.plot([v1[0]], [v1[1]], 'bo') # Plot blue points\n        else:\n            ax1.plot([v1[0]], [v1[1]], 'ro') # Plot red points\n\nplt.show()\n\n\n\n\n\n\n\n\nThe first thing to note is that the vector that defines the plane does not mark the boundary between the two sides of the plane. It marks the direction in which we find the ‘positive’ side of the plane. Not intuitive at all!\nIf we want to plot the separation plane, we need to plot a line that is perpendicular to our vector P. We can get such a line using a 90^o rotation matrix.\nFeel free to change the direction of the plane P.\n\nP = np.array([[1, 2]])  # Define a single plane. We may change the direction\n\n# Get a new plane perpendicular to P. We use a rotation matrix\nPT = np.dot([[0, 1], [-1, 0]], P.T).T  \n\nfig, ax1 = plt.subplots(figsize=(8, 8)) # Create a plot with custom size\n\nplot_vectors([P], colors=['b'], axes=[2, 2], ax=ax1) # Plot the plane P as a vector\n\n# Plot the plane P as a 2 vectors. \n# We scale by 2 just to get the arrows outside the current box\nplot_vectors([PT * 4, PT * -4], colors=['k', 'k'], axes=[4, 4], ax=ax1)\n\n# Plot 20 random points. \nfor i in range(0, 20):\n        v1 = np.array(np.random.uniform(-4, 4, 2)) # Get a pair of random numbers between -4 and 4 \n        side_of_plane = np.sign(np.dot(P, v1.T)) # Get the sign of the dot product with P\n        # Color the points depending on the sign of the result of np.dot(P, point.T)\n        if side_of_plane == 1:\n            ax1.plot([v1[0]], [v1[1]], 'bo') # Plot a blue point\n        else:\n            ax1.plot([v1[0]], [v1[1]], 'ro') # Plot a red point\n\nplt.show()\n\n\n\n\n\n\n\n\nNow, let us see what is inside the code that color the points.\n\nP = np.array([[1, 1]])      # Single plane\nv1 = np.array([[1, 2]])     # Sample point 1\nv2 = np.array([[-1, 1]])    # Sample point 2\nv3 = np.array([[-2, -1]])   # Sample point 3\n\n\nnp.dot(P, v1.T)\n\narray([[3]])\n\n\n\nnp.dot(P, v2.T)\n\narray([[0]])\n\n\n\nnp.dot(P, v3.T)\n\narray([[-3]])\n\n\nThe function below checks in which side of the plane P is located the vector v\n\ndef side_of_plane(P, v):\n    dotproduct = np.dot(P, v.T) # Get the dot product P * v'\n    sign_of_dot_product = np.sign(dotproduct) # The sign of the elements of the dotproduct matrix \n    sign_of_dot_product_scalar = sign_of_dot_product.item() # The value of the first item\n    return sign_of_dot_product_scalar\n\n\nside_of_plane(P, v1) # In which side is [1, 2]\n\n1\n\n\n\nside_of_plane(P, v2) # In which side is [-1, 1]\n\n0\n\n\n\nside_of_plane(P, v3) # In which side is [-2, -1]\n\n-1"
  },
  {
    "objectID": "notes/c1w4/lab02.html#hash-function-with-multiple-planes",
    "href": "notes/c1w4/lab02.html#hash-function-with-multiple-planes",
    "title": "Hash functions and multiplanes",
    "section": "Hash Function with multiple planes",
    "text": "Hash Function with multiple planes\nIn the following section, we are going to define a hash function with a list of three custom planes in 2D.\n\nP1 = np.array([[1, 1]])   # First plane 2D\nP2 = np.array([[-1, 1]])  # Second plane 2D\nP3 = np.array([[-1, -1]]) # Third plane 2D\nP_l = [P1, P2, P3]  # List of arrays. It is the multi plane\n\n# Vector to search\nv = np.array([[2, 2]])\n\nThe next function creates a hash value based on a set of planes. The output value is a combination of the side of the plane where the vector is localized with respect to the collection of planes.\nWe can think of this list of planes as a set of basic hash functions, each of which can produce only 1 or 0 as output.\n\ndef hash_multi_plane(P_l, v):\n    hash_value = 0\n    for i, P in enumerate(P_l):\n        sign = side_of_plane(P,v)\n        hash_i = 1 if sign &gt;=0 else 0\n        hash_value += 2**i * hash_i\n    return hash_value\n\n\nhash_multi_plane(P_l, v) # Find the number of the plane that containes this value\n\n3"
  },
  {
    "objectID": "notes/c1w4/lab02.html#random-planes",
    "href": "notes/c1w4/lab02.html#random-planes",
    "title": "Hash functions and multiplanes",
    "section": "Random Planes",
    "text": "Random Planes\nIn the cell below, we create a set of three random planes\n\nnp.random.seed(0)\nnum_dimensions = 2 # is 300 in assignment\nnum_planes = 3 # is 10 in assignment\nrandom_planes_matrix = np.random.normal(\n                       size=(num_planes,\n                             num_dimensions))\nprint(random_planes_matrix)\n\n[[ 1.76405235  0.40015721]\n [ 0.97873798  2.2408932 ]\n [ 1.86755799 -0.97727788]]\n\n\n\nv = np.array([[2, 2]])\n\nThe next function is similar to the side_of_plane() function, but it evaluates more than a plane each time. The result is an array with the side of the plane of v, for the set of planes P\n\n# Side of the plane function. The result is a matrix\ndef side_of_plane_matrix(P, v):\n    dotproduct = np.dot(P, v.T)\n    sign_of_dot_product = np.sign(dotproduct) # Get a boolean value telling if the value in the cell is positive or negative\n    return sign_of_dot_product\n\nGet the side of the plane of the vector [2, 2] for the set of random planes.\n\nsides_l = side_of_plane_matrix(\n            random_planes_matrix, v)\nsides_l\n\narray([[1.],\n       [1.],\n       [1.]])\n\n\nNow, let us use the former function to define our multiplane hash function\n\ndef hash_multi_plane_matrix(P, v, num_planes):\n    sides_matrix = side_of_plane_matrix(P, v) # Get the side of planes for P and v\n    hash_value = 0\n    for i in range(num_planes):\n        sign = sides_matrix[i].item() # Get the value inside the matrix cell\n        hash_i = 1 if sign &gt;=0 else 0\n        hash_value += 2**i * hash_i # sum 2^i * hash_i\n        \n    return hash_value\n\nPrint the bucket hash for the vector v = [2, 2].\n\nhash_multi_plane_matrix(random_planes_matrix, v, num_planes)\n\n7\n\n\n\nNote\nThis showed we how to make one set of random planes. We will make multiple sets of random planes in order to make the approximate nearest neighbors more accurate."
  },
  {
    "objectID": "notes/c1w4/lab02.html#document-vectors",
    "href": "notes/c1w4/lab02.html#document-vectors",
    "title": "Hash functions and multiplanes",
    "section": "Document vectors",
    "text": "Document vectors\nBefore we finish this lab, remember that we can represent a document as a vector by adding up the word vectors for the words inside the document. In this example, our embedding contains only three words, each represented by a 3D array.\n\nword_embedding = {\"I\": np.array([1,0,1]),\n                   \"love\": np.array([-1,0,1]),\n                   \"learning\": np.array([1,0,1])\n                  }\nwords_in_document = ['I', 'love', 'learning', 'not_a_word']\ndocument_embedding = np.array([0,0,0])\nfor word in words_in_document:\n    document_embedding += word_embedding.get(word,0)\n    \nprint(document_embedding)\n\n[1 0 3]\n\n\nCongratulations! You’ve now completed this lab on hash functions and multiplanes!"
  },
  {
    "objectID": "notes/c3w1/lab01.html",
    "href": "notes/c3w1/lab01.html",
    "title": "Trax : Ungraded Lecture Notebook",
    "section": "",
    "text": "course banner\nIn this notebook you’ll get to know about the Trax framework and learn about some of its basic building blocks.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "L1 - Introduction to Trax"
    ]
  },
  {
    "objectID": "notes/c3w1/lab01.html#background",
    "href": "notes/c3w1/lab01.html#background",
    "title": "Trax : Ungraded Lecture Notebook",
    "section": "Background",
    "text": "Background\n\nWhy Trax and not TensorFlow or PyTorch?\nTensorFlow and PyTorch are both extensive frameworks that can do almost anything in deep learning. They offer a lot of flexibility, but that often means verbosity of syntax and extra time to code.\nTrax is much more concise. It runs on a TensorFlow backend but allows you to train models with 1 line commands. Trax also runs end to end, allowing you to get data, model and train all with a single terse statements. This means you can focus on learning, instead of spending hours on the idiosyncrasies of big framework implementation.\n\n\nWhy not Keras then?\nKeras is now part of Tensorflow itself from 2.0 onwards. Also, Trax is good for implementing new state of the art algorithms like Transformers, Reformers, BERT because it is actively maintained by Google Brain Team for advanced deep learning tasks. It runs smoothly on CPUs, GPUs and TPUs as well with comparatively lesser modifications in code.\n\n\nHow to Code in Trax\nBuilding models in Traxrelies on 2 key concepts:\n\nlayers and\ncombinators.\n\nTrax layers are simple objects that process data and perform computations. They can be chained together into composite layers using Trax combinators, allowing you to build layers and models of any complexity.\n\n\nTrax, JAX, TensorFlow and Tensor2Tensor\nYou already know that Trax uses Tensorflow as a backend, but it also uses the JAX library to speed up computation too. You can view JAX as an enhanced and optimized version of numpy.\nWatch out for assignments which import import trax.fastmath.numpy as np. If you see this line, remember that when calling np you are really calling Trax’s version of numpy that is compatible with JAX.\nAs a result of this, where you used to encounter the type numpy.ndarray now you will find the type jax.interpreters.xla.DeviceArray.\nTensor2Tensor is another name you might have heard. It started as an end to end solution much like how Trax is designed, but it grew unwieldy and complicated. So you can view Trax as the new improved version that operates much faster and simpler.\n\n\nResources\n\nTrax source code can be found on Github: Trax\nJAX library: JAX",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "L1 - Introduction to Trax"
    ]
  },
  {
    "objectID": "notes/c3w1/lab01.html#installing-trax",
    "href": "notes/c3w1/lab01.html#installing-trax",
    "title": "Trax : Ungraded Lecture Notebook",
    "section": "Installing Trax",
    "text": "Installing Trax\nTrax has dependencies on JAX and some libraries like JAX which are yet to be supported in Windows but work well in Ubuntu and MacOS. We would suggest that if you are working on Windows, try to install Trax on WSL2.\nOfficial maintained documentation - trax-ml not to be confused with this TraX\n\n#!pip install trax==1.3.1 Use this version for this notebook",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "L1 - Introduction to Trax"
    ]
  },
  {
    "objectID": "notes/c3w1/lab01.html#imports",
    "href": "notes/c3w1/lab01.html#imports",
    "title": "Trax : Ungraded Lecture Notebook",
    "section": "Imports",
    "text": "Imports\n\nimport numpy as np  # regular ol' numpy\n\nfrom trax import layers as tl  # core building block\nfrom trax import shapes  # data signatures: dimensionality and type\nfrom trax import fastmath  # uses jax, offers numpy on steroids\n\n2025-02-10 16:53:07.573380: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1739199187.587843  120958 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1739199187.592157  120958 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\n\n\n# Trax version 1.3.1 or better \n!pip list | grep trax\n\ntrax                         1.4.1",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "L1 - Introduction to Trax"
    ]
  },
  {
    "objectID": "notes/c3w1/lab01.html#layers",
    "href": "notes/c3w1/lab01.html#layers",
    "title": "Trax : Ungraded Lecture Notebook",
    "section": "Layers",
    "text": "Layers\nLayers are the core building blocks in Trax or as mentioned in the lectures, they are the base classes.\nThey take inputs, compute functions/custom calculations and return outputs.\nYou can also inspect layer properties. Let me show you some examples.\n\nRelu Layer\nFirst I’ll show you how to build a relu activation function as a layer. A layer like this is one of the simplest types. Notice there is no object initialization so it works just like a math function.\nNote: Activation functions are also layers in Trax, which might look odd if you have been using other frameworks for a longer time.\n\n# Layers\n# Create a relu trax layer\nrelu = tl.Relu()\n\n# Inspect properties\nprint(\"-- Properties --\")\nprint(\"name :\", relu.name)\nprint(\"expected inputs :\", relu.n_in)\nprint(\"promised outputs :\", relu.n_out, \"\\n\")\n\n# Inputs\nx = np.array([-2, -1, 0, 1, 2])\nprint(\"-- Inputs --\")\nprint(\"x :\", x, \"\\n\")\n\n# Outputs\ny = relu(x)\nprint(\"-- Outputs --\")\nprint(\"y :\", y)\n\n-- Properties --\nname : Serial\nexpected inputs : 1\npromised outputs : 1 \n\n-- Inputs --\nx : [-2 -1  0  1  2] \n\n-- Outputs --\ny : [0 0 0 1 2]\n\n\n\n\nConcatenate Layer\nNow I’ll show you how to build a layer that takes 2 inputs. Notice the change in the expected inputs property from 1 to 2.\n\n# Create a concatenate trax layer\nconcat = tl.Concatenate()\nprint(\"-- Properties --\")\nprint(\"name :\", concat.name)\nprint(\"expected inputs :\", concat.n_in)\nprint(\"promised outputs :\", concat.n_out, \"\\n\")\n\n# Inputs\nx1 = np.array([-10, -20, -30])\nx2 = x1 / -10\nprint(\"-- Inputs --\")\nprint(\"x1 :\", x1)\nprint(\"x2 :\", x2, \"\\n\")\n\n# Outputs\ny = concat([x1, x2])\nprint(\"-- Outputs --\")\nprint(\"y :\", y)\n\n-- Properties --\nname : Concatenate\nexpected inputs : 2\npromised outputs : 1 \n\n-- Inputs --\nx1 : [-10 -20 -30]\nx2 : [1. 2. 3.] \n\n-- Outputs --\ny : [-10. -20. -30.   1.   2.   3.]",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "L1 - Introduction to Trax"
    ]
  },
  {
    "objectID": "notes/c3w1/lab01.html#layers-are-configurable",
    "href": "notes/c3w1/lab01.html#layers-are-configurable",
    "title": "Trax : Ungraded Lecture Notebook",
    "section": "Layers are Configurable",
    "text": "Layers are Configurable\nYou can change the default settings of layers. For example, you can change the expected inputs for a concatenate layer from 2 to 3 using the optional parameter n_items.\n\n# Configure a concatenate layer\nconcat_3 = tl.Concatenate(n_items=3)  # configure the layer's expected inputs\nprint(\"-- Properties --\")\nprint(\"name :\", concat_3.name)\nprint(\"expected inputs :\", concat_3.n_in)\nprint(\"promised outputs :\", concat_3.n_out, \"\\n\")\n\n# Inputs\nx1 = np.array([-10, -20, -30])\nx2 = x1 / -10\nx3 = x2 * 0.99\nprint(\"-- Inputs --\")\nprint(\"x1 :\", x1)\nprint(\"x2 :\", x2)\nprint(\"x3 :\", x3, \"\\n\")\n\n# Outputs\ny = concat_3([x1, x2, x3])\nprint(\"-- Outputs --\")\nprint(\"y :\", y)\n\n-- Properties --\nname : Concatenate\nexpected inputs : 3\npromised outputs : 1 \n\n-- Inputs --\nx1 : [-10 -20 -30]\nx2 : [1. 2. 3.]\nx3 : [0.99 1.98 2.97] \n\n-- Outputs --\ny : [-10.   -20.   -30.     1.     2.     3.     0.99   1.98   2.97]\n\n\nNote: At any point,if you want to refer the function help/ look up the documentation or use help function.\n\n#help(tl.Concatenate) #Uncomment this to see the function docstring with explaination",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "L1 - Introduction to Trax"
    ]
  },
  {
    "objectID": "notes/c3w1/lab01.html#layers-can-have-weights",
    "href": "notes/c3w1/lab01.html#layers-can-have-weights",
    "title": "Trax : Ungraded Lecture Notebook",
    "section": "Layers can have Weights",
    "text": "Layers can have Weights\nSome layer types include mutable weights and biases that are used in computation and training. Layers of this type require initialization before use.\nFor example the LayerNorm layer calculates normalized data, that is also scaled by weights and biases. During initialization you pass the data shape and data type of the inputs, so the layer can initialize compatible arrays of weights and biases.\n\n# Uncomment any of them to see information regarding the function\n# help(tl.LayerNorm)\n# help(shapes.signature)\n\n\n# Layer initialization\nnorm = tl.LayerNorm()\n# You first must know what the input data will look like\nx = np.array([0, 1, 2, 3], dtype=\"float\")\n\n# Use the input data signature to get shape and type for initializing weights and biases\nnorm.init(shapes.signature(x)) # We need to convert the input datatype from usual tuple to trax ShapeDtype\n\nprint(\"Normal shape:\",x.shape, \"Data Type:\",type(x.shape))\nprint(\"Shapes Trax:\",shapes.signature(x),\"Data Type:\",type(shapes.signature(x)))\n\n# Inspect properties\nprint(\"-- Properties --\")\nprint(\"name :\", norm.name)\nprint(\"expected inputs :\", norm.n_in)\nprint(\"promised outputs :\", norm.n_out)\n# Weights and biases\nprint(\"weights :\", norm.weights[0])\nprint(\"biases :\", norm.weights[1], \"\\n\")\n\n# Inputs\nprint(\"-- Inputs --\")\nprint(\"x :\", x)\n\n# Outputs\ny = norm(x)\nprint(\"-- Outputs --\")\nprint(\"y :\", y)\n\n/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/normalization.py:141: UserWarning: Explicitly requested dtype float64 requested in ones is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.\n  scale = jnp.ones(features, dtype=input_signature.dtype)\n/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/normalization.py:142: UserWarning: Explicitly requested dtype float64 requested in zeros is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.\n  bias = jnp.zeros(features, dtype=input_signature.dtype)\n\n\n((Array([1., 1., 1., 1.], dtype=float32),\n  Array([0., 0., 0., 0.], dtype=float32)),\n ())\n\n\nNormal shape: (4,) Data Type: &lt;class 'tuple'&gt;\nShapes Trax: ShapeDtype{shape:(4,), dtype:float64} Data Type: &lt;class 'trax.shapes.ShapeDtype'&gt;\n-- Properties --\nname : LayerNorm\nexpected inputs : 1\npromised outputs : 1\nweights : [1. 1. 1. 1.]\nbiases : [0. 0. 0. 0.] \n\n-- Inputs --\nx : [0. 1. 2. 3.]\n-- Outputs --\ny : [-1.3416404  -0.44721344  0.44721344  1.3416404 ]",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "L1 - Introduction to Trax"
    ]
  },
  {
    "objectID": "notes/c3w1/lab01.html#custom-layers",
    "href": "notes/c3w1/lab01.html#custom-layers",
    "title": "Trax : Ungraded Lecture Notebook",
    "section": "Custom Layers",
    "text": "Custom Layers\nThis is where things start getting more interesting! You can create your own custom layers too and define custom functions for computations by using tl.Fn. Let me show you how.\n\nhelp(tl.Fn)\n\nHelp on function Fn in module trax.layers.base:\n\nFn(name, f, n_out=1)\n    Returns a layer with no weights that applies the function `f`.\n    \n    `f` can take and return any number of arguments, and takes only positional\n    arguments -- no default or keyword arguments. It often uses JAX-numpy (`jnp`).\n    The following, for example, would create a layer that takes two inputs and\n    returns two outputs -- element-wise sums and maxima:\n    \n        `Fn('SumAndMax', lambda x0, x1: (x0 + x1, jnp.maximum(x0, x1)), n_out=2)`\n    \n    The layer's number of inputs (`n_in`) is automatically set to number of\n    positional arguments in `f`, but you must explicitly set the number of\n    outputs (`n_out`) whenever it's not the default value 1.\n    \n    Args:\n      name: Class-like name for the resulting layer; for use in debugging.\n      f: Pure function from input tensors to output tensors, where each input\n          tensor is a separate positional arg, e.g., `f(x0, x1) --&gt; x0 + x1`.\n          Output tensors must be packaged as specified in the `Layer` class\n          docstring.\n      n_out: Number of outputs promised by the layer; default value 1.\n    \n    Returns:\n      Layer executing the function `f`.\n\n\n\n\n# Define a custom layer\n# In this example you will create a layer to calculate the input times 2\n\ndef TimesTwo():\n    layer_name = \"TimesTwo\" #don't forget to give your custom layer a name to identify\n\n    # Custom function for the custom layer\n    def func(x):\n        return x * 2\n\n    return tl.Fn(layer_name, func)\n\n\n# Test it\ntimes_two = TimesTwo()\n\n# Inspect properties\nprint(\"-- Properties --\")\nprint(\"name :\", times_two.name)\nprint(\"expected inputs :\", times_two.n_in)\nprint(\"promised outputs :\", times_two.n_out, \"\\n\")\n\n# Inputs\nx = np.array([1, 2, 3])\nprint(\"-- Inputs --\")\nprint(\"x :\", x, \"\\n\")\n\n# Outputs\ny = times_two(x)\nprint(\"-- Outputs --\")\nprint(\"y :\", y)\n\n-- Properties --\nname : TimesTwo\nexpected inputs : 1\npromised outputs : 1 \n\n-- Inputs --\nx : [1 2 3] \n\n-- Outputs --\ny : [2 4 6]",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "L1 - Introduction to Trax"
    ]
  },
  {
    "objectID": "notes/c3w1/lab01.html#combinators",
    "href": "notes/c3w1/lab01.html#combinators",
    "title": "Trax : Ungraded Lecture Notebook",
    "section": "Combinators",
    "text": "Combinators\nYou can combine layers to build more complex layers. Trax provides a set of objects named combinator layers to make this happen. Combinators are themselves layers, so behavior commutes.\n\nSerial Combinator\nThis is the most common and easiest to use. For example could build a simple neural network by combining layers into a single layer using the Serial combinator. This new layer then acts just like a single layer, so you can inspect intputs, outputs and weights. Or even combine it into another layer! Combinators can then be used as trainable models. Try adding more layers\nNote:As you must have guessed, if there is serial combinator, there must be a parallel combinator as well. Do try to explore about combinators and other layers from the trax documentation and look at the repo to understand how these layers are written.\n\n# help(tl.Serial)\n# help(tl.Parallel)\n\n\n# Serial combinator\nserial = tl.Serial(\n    tl.LayerNorm(),         # normalize input\n    tl.Relu(),              # convert negative values to zero\n    times_two,              # the custom layer you created above, multiplies the input recieved from above by 2\n    \n    ### START CODE HERE\n#     tl.Dense(n_units=2),  # try adding more layers. eg uncomment these lines\n#     tl.Dense(n_units=1),  # Binary classification, maybe? uncomment at your own peril\n#     tl.LogSoftmax()       # Yes, LogSoftmax is also a layer\n    ### END CODE HERE\n)\n\n# Initialization\nx = np.array([-2, -1, 0, 1, 2]) #input\nserial.init(shapes.signature(x)) #initialising serial instance\n\nprint(\"-- Serial Model --\")\nprint(serial,\"\\n\")\nprint(\"-- Properties --\")\nprint(\"name :\", serial.name)\nprint(\"sublayers :\", serial.sublayers)\nprint(\"expected inputs :\", serial.n_in)\nprint(\"promised outputs :\", serial.n_out)\nprint(\"weights & biases:\", serial.weights, \"\\n\")\n\n# Inputs\nprint(\"-- Inputs --\")\nprint(\"x :\", x, \"\\n\")\n\n# Outputs\ny = serial(x)\nprint(\"-- Outputs --\")\nprint(\"y :\", y)\n\n/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/normalization.py:141: UserWarning: Explicitly requested dtype int64 requested in ones is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.\n  scale = jnp.ones(features, dtype=input_signature.dtype)\n/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/normalization.py:142: UserWarning: Explicitly requested dtype int64 requested in zeros is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.\n  bias = jnp.zeros(features, dtype=input_signature.dtype)\n\n\n(((Array([1, 1, 1, 1, 1], dtype=int32), Array([0, 0, 0, 0, 0], dtype=int32)),\n  ((), (), ()),\n  ()),\n ((), ((), (), ()), ()))\n\n\n-- Serial Model --\nSerial[\n  LayerNorm\n  Serial[\n    Relu\n  ]\n  TimesTwo\n] \n\n-- Properties --\nname : Serial\nsublayers : [LayerNorm, Serial[\n  Relu\n], TimesTwo]\nexpected inputs : 1\npromised outputs : 1\nweights & biases: ((Array([1, 1, 1, 1, 1], dtype=int32), Array([0, 0, 0, 0, 0], dtype=int32)), ((), (), ()), ()) \n\n-- Inputs --\nx : [-2 -1  0  1  2] \n\n-- Outputs --\ny : [0.        0.        0.        1.4142132 2.8284264]",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "L1 - Introduction to Trax"
    ]
  },
  {
    "objectID": "notes/c3w1/lab01.html#jax",
    "href": "notes/c3w1/lab01.html#jax",
    "title": "Trax : Ungraded Lecture Notebook",
    "section": "JAX",
    "text": "JAX\nJust remember to lookout for which numpy you are using, the regular ol’ numpy or Trax’s JAX compatible numpy. Both tend to use the alias np so watch those import blocks.\nNote:There are certain things which are still not possible in fastmath.numpy which can be done in numpy so you will see in assignments we will switch between them to get our work done.\n\n# Numpy vs fastmath.numpy have different data types\n# Regular ol' numpy\nx_numpy = np.array([1, 2, 3])\nprint(\"good old numpy : \", type(x_numpy), \"\\n\")\n\n# Fastmath and jax numpy\nx_jax = fastmath.numpy.array([1, 2, 3])\nprint(\"jax trax numpy : \", type(x_jax))\n\ngood old numpy :  &lt;class 'numpy.ndarray'&gt; \n\njax trax numpy :  &lt;class 'jaxlib.xla_extension.ArrayImpl'&gt;",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "L1 - Introduction to Trax"
    ]
  },
  {
    "objectID": "notes/c3w1/lab01.html#summary",
    "href": "notes/c3w1/lab01.html#summary",
    "title": "Trax : Ungraded Lecture Notebook",
    "section": "Summary",
    "text": "Summary\nTrax is a concise framework, built on TensorFlow, for end to end machine learning. The key building blocks are layers and combinators. This notebook is just a taste, but sets you up with some key inuitions to take forward into the rest of the course and assignments where you will build end to end models.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "L1 - Introduction to Trax"
    ]
  },
  {
    "objectID": "notes/c3w1/index.html",
    "href": "notes/c3w1/index.html",
    "title": "Neural Networks for Sentiment Analysis",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\nThis week’s slides\n\n\n\n\nFigure 1\nMy notes for Week 1 of the Natural Language Processing with Sequence Models Course in the Natural Language Processing Specialization Offered by DeepLearning.AI on Coursera",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w1/index.html#sec-course-intro",
    "href": "notes/c3w1/index.html#sec-course-intro",
    "title": "Neural Networks for Sentiment Analysis",
    "section": "Course Intro",
    "text": "Course Intro\n\n\n\n\n\n\nCourse Intro Video Transcript\n\n\n\n\n\n\nWelcome to the third course of this specialization. This course is called natural language processing with sequence models. In this course, here are some of the things you get to build. First, you take sentiment analysis to the next level with deep neural networks. You also build a language generator using RNNs or recurrent neural networks. You apply LSTM units. LSTM stands for long short term memory. You apply LCM units to the problem of named entity recognition. And, finally you use Siamese networks to identify duplicate questions like say if there’s an online discussion forum and different people are asking questions. Can you figure out if two different people ask essentially the same question but with different wording? With the skills you develop in this course, you’ll be able to build powerful NLP systems to solve problems across a wide range of industries. It is my pleasure to welcome Lukasz and Younes as your instructors for this course and they’re thrilled to dive into these topics with you. Younes, perhaps you could say a bit more about the applications that learners will build in this course.\n\n\n\nSure thing. Thanks, Andrew. Well, I’d like to start by saying that in the first two courses of the specialization, you build a powerful foundation that will provide you with both the context and the fundamental skills you need to tackle this course. For example, you’ve already done sentiment analysis in course one with a simple naive Bayes classifier. But now, you will leverage the power of deep neural networks to build a much more robust sentiment analysis classifier. You’ve also seen how to do things like predict the next word in a sequence, using relatively simple n-gram language models in course two. In this course, you’ll create an advanced model using recurrent neural networks to generate text. You can think of this course as taking the step from foundational skills into building real world NLP applications.\n\n\n\n\nVery cool, Younes. I think that’s a great way to think about it. Lukasz, maybe you could say a bit more about the applications learners will get to build.\n\n\n\n\nSure, thank you, Andrew. Well, as you saw in the first course, the problem of sentiment analysis is a really tricky one. But in many applications you want to determine the sentiment of a sentence. So it’s a really good problem to work on as well. With language modeling, which you’ll tackle in week two of this course, the problems you can solve are almost infinite. From translation to autocomplete to generating text from scratch. In week three of this course, you’ll work on named entity recognition, which is the problem of separating named entities in sentences, like people and places. This is a building block of many important NLP systems. Finally, in week four you will tackle the problem of identifying duplicates. The question whether two pieces of texts are duplicates of each other might not sound very interesting at first, but as you will see, it’s actually core building block of things like online forums and search engines and we’ll show you how to solve it. So we’re really excited to show you these applications and bring your skills to the next level.\n\n\n\n\nThank you Lukasz and thank you Younes. This is going to be an exciting course, so let’s get started.\n\n\n\n\nGood luck.\n\n\n\n\nHave fun.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w1/index.html#neural-networks-for-sentiment-analysis",
    "href": "notes/c3w1/index.html#neural-networks-for-sentiment-analysis",
    "title": "Neural Networks for Sentiment Analysis",
    "section": "Neural Networks for Sentiment Analysis",
    "text": "Neural Networks for Sentiment Analysis\nPreviously in the course we did sentiment analysis with logistic regression and naive Bayes. Those models were in a sense more naive, and are not able to catch the sentiment off a tweet like: “I am not happy” or “If only it was a good day”. When using a neural network to predict the sentiment of a sentence, we can use the following. Note that the image below has three outputs, in this case we might want to predict, “positive”, “neutral”, or “negative”.\n\nNote that the network above has three layers. To go from one layer to another we can use a W matrix to propagate to the next layer. Hence, we call this concept of going from the input until the final layer, forward propagation. To represent a tweet, we can use the following:\nNote, that we add zeros for padding to match the size of the longest tweet.\n\n\n\n\n\n\n\nVideo Transcript\n\n\n\n\n\n\nThis week I’ll show you how to create neural networks using layers. This simplifies the task a lot as you will see. Let’s dive in. First, you’ll revisit the general structure of neural networks and how they make predictions. I’ll show you the structure you’ll be using to perform sentiment analysis during this week. Neural networks are computational structures that, in a very simplistic way, attempt to mimic the way the human brain recognizes patterns. They’re used in many applications of artificial intelligence and have proven very effective on a variety of tasks, including those in NLP. Have a look at this example of a simple neural network with n input parameters, two hidden layers, and three output units. As inputs, this neural network receives a data representation x with n features, then performs computations in its hidden layers. Finally, it delivers an output which in this case has size 3. Let’s take a look at how it works mathematically. All the nodes every activation layer as a_i, where i is the layer’s number. First, define a_0 to be the input vector x. To get the values for each layer’s activation, a, you have to compute the value for z_i, which depends on both the weights matrix for that layer and the activations, a, from the previous layer. Finally, you get the values for each layer by applying an activation function, g, to the value of z. As you can see, this computation moves forward through the left of the neural network towards the right. That’s why this process is called forward propagation. For this module’s assignments, you’re going to implement a neural network that looks like this. As inputs, it will receive a simple vector representation of your tweets. It will have an embedding layer that will transform your representation into an optimal one for this task. Finally, it will have a hidden layer with a ReLU activation function and then output layer with the softmax function that will give you the probabilities for whether a tweet has a positive or negative sentiment. This neural network will allow you to predict sentiments for complex tweets, such as a tweet like this one that says, “This movie was almost good.” That you wouldn’t have been able to classify correctly using simpler methods such as Naive Bayes because they missed important information. The initial representation, x, that you’ll use for this neural network will be a vector of integers. Similar to your previous work with sentiment analysis, you will first need to list all of your words from your vocabulary. Next for this application, you’ll assign an integer index to each of them. Then for each word in your tweets add the index from your vocabulary to construct a vector like this one for every tweet. After you have all the vector representations of your tweets, you will need to identify the maximum vector size and fill every vector with zeros to match that size. This process is called padding and ensures that all of your vectors have the same size even if your tweets don’t. Let’s do a quick recap. At this point, you’re familiar with the general structure of the neural network that you’ll be using to classify sentiments for a set of complex nuance tweets. You also reviewed the integer representation that’s going to be used in this module. Next, I’ll introduce the tracks library for neural networks and demonstrate how the embedding layer works. I’ll see you later.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w1/index.html#sec-trax-nn",
    "href": "notes/c3w1/index.html#sec-trax-nn",
    "title": "Neural Networks for Sentiment Analysis",
    "section": "Trax: Neural Networks",
    "text": "Trax: Neural Networks\nTrax has several advantages:\n\nRuns fast on CPUs, GPUs and TPUs\nParallel computing\nRecord algebraic computations for gradient evaluation\n\nHere is an example of how we can code a neural network in Trax:\n\n\n\n\n\n\n\nVideo Transcript",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w1/index.html#sec-trax-jax",
    "href": "notes/c3w1/index.html#sec-trax-jax",
    "title": "Neural Networks for Sentiment Analysis",
    "section": "Trax and JAX, docs and code",
    "text": "Trax and JAX, docs and code\nOfficial Trax documentation maintained by the Google Brain team:\n\nhttps://trax-ml.readthedocs.io/en/latest/\n\nTrax source code on GitHub:\n\nhttps://github.com/google/trax\n\nJAX library:\n\nhttps://jax.readthedocs.io/en/latest/index.html",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w1/index.html#sec-intro-trax",
    "href": "notes/c3w1/index.html#sec-intro-trax",
    "title": "Neural Networks for Sentiment Analysis",
    "section": "Lab: Introduction to Trax",
    "text": "Lab: Introduction to Trax\nIntroduction to Trax",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w1/index.html#sec-trax-layers",
    "href": "notes/c3w1/index.html#sec-trax-layers",
    "title": "Neural Networks for Sentiment Analysis",
    "section": "Trax: Layers",
    "text": "Trax: Layers\nTrax makes use of classes. If we are not familiar with classes in python, don’t worry about it, here is an example.\n\nIn the example above, we can see that a class takes in an __init__ and a __call__ method.\nThese methods allow we to initialize your internal variables and allow we to execute your function when called.\nTo the right we can see how we can initialize your class. When we call MyClass(7) , we are setting the y variable to 7. Now when we call f(3) we are adding 7 + 3.\nWe can change the my_method function to do whatever we want, and we can have as many methods as we want in a class.\n\n\n\n\n\n\nVideo Transcript\n\n\n\n\n\n\nTo create a neural network out of layers, you need to put them together. In a deep network, you run them one after the other in a sequential way. Let me show you how you can do this. First, you will observe how a basic neural network is defined in Trax. Then I’ll show you some of the benefits of using a framework like TensorFlow, which is the framework that Trax is built on. Let’s take this network architecture as an example. In this model, you have two hidden layers with sigmoid activation functions and an output layer with softmax activation. In Trax, you’ll need to specify the type of model architecture.\nFor simple architectures like this one, you’ll use a serial model. To start, list the layers from left to right, or from your input variables to the output layer. In this case, first you have a dense layer with four units, and then assign the sigmoid activation function to that layer. After that, repeat the process for the second hidden layer and the output layer. You can specify any architecture you like in the simple way. Note that, this way to specify your models architecture, follows the order in which the computations are made in your neural network. There are several advantages to using libraries like Trax, such as they’re designed to perform computations efficiently in hardware like CPUs, GPUs, and even TPUs. They allow you to easily perform parallel computing by running gear models on multiple machines or course simultaneously. They keep a record of all the algebraic operations on your neural net in the order of computation. So they are able to compute the gradients of your model automatically. There are many open source frameworks out there, and Trax is one of the latest.\nIt’s based on TensorFlow. You might be already familiar with TensorFlow, PyTorch, and JAX. If you’re not familiar with those, don’t worry. I’ll show you the basics of Trax and you’ll be able to implement amazing NLP models. So far, I showed you how to define a model in Trax with the simple sequential architecture, and I pointed out some of the advantages to be had, like computational efficiency and parallel computing.\n\n\nNext, I’ll get into more detail on how to use Trax.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w1/index.html#sec-why-trax",
    "href": "notes/c3w1/index.html#sec-why-trax",
    "title": "Neural Networks for Sentiment Analysis",
    "section": "Why we recommend Trax",
    "text": "Why we recommend Trax\n\n\n\n\n\n\nVideo Transcript\n\n\n\n\n\n\nHi. My name is Lukasz, and I want to tell you in this video why we made the machine learning library Trax. This is a little bit of a personal story for me. I’ve been at Google for about seven years now. I’m a researcher in the Google Brain Team. But before I was a researcher, I was a software engineer. I worked on a lot of machine learning projects and frameworks. This journey for me ended in the Trax library. I believe Trax is currently the best library to learn and to productionize machine learning research and machine learning models, especially sequenced models, models like transformer and models that are used in natural language processing. The reasons I believe that come from a personal journey that I took that led me here. I will tell you a little bit about myself and how I came here, and then I’ll tell you why I think Trax is the best thing to use currently for machine learning, especially in natural language processing. My journey with machine learning and machine learning frameworks started around 2014-15 when we were making TensorFlow. TensorFlow is, you probably know, is a big machine learning systems, it has about 100 million downloads by now. It was released in November 2015. It was a very emotional moment for all of us when we were releasing it. At that point, we were not sure if deep learning will become as big as it did. We were not sure how many users there will be. What we wanted to do was a system that’s primarily very fast, that can run distributed machine learning systems, large-scale fast training. The main focus was speed. A secondary focus was to make it easy to program the systems that wasn’t a reader, but it was not the most important thing. After releasing TensorFlow, I worked on machine translation and especially on the Google’s Neural Machine Translation System. This was the first system using deep sequence models that was used by the Google Translate team that was actually released as a product. It’s handling all of Google translations these days. Every language that we have has a neural model. It started with LSTMs and RNN models, and now it’s a lot of transformers. We released that in 2016 based on the TensorFlow framework. These models, they’re amazing. They’re much better than the previous phrase-based translation models, but they took a long time to train. They were training for days on clusters of GPUs at that time. This was not practical for anyone else to do rather than Google. This was only because we had this TensorFlow system, a large group of engineers who would ferry very well, and we were training for days and days. That was great. But I felt like this is not satisfactory because no one else can do that. It’s not possible to be done at the university. You cannot launch a startup doing that, because it was impossible if you were not Google, or maybe from Microsoft, but no one else. I wanted to change that. To do that, we created the Tensor2Tensor Library. The Tensor2Tensor Library, which was released in 2017, started with the thought that we should make this deep learning research, especially for sequence models, widely accessible. This was not working with these large RNN models, but while writing the library, we created this transformer model. This transformer has taken NLP by storm because it allows you to train much faster. At that time within a few days, now, it’s less than a day in a matter of hours on an 8 GPU system. You can create translation models that surpass any RNN models. The Tensor2Tensor library has become already widely used. It’s used in production Google systems. It’s used by some very large companies in the world and it has led to a number of startups that they know about that basically exists thanks to this library. You can say, well, this is done and this is good, but, the problem is, it’s become complicated and it’s not nice to learn and it’s become very hard to do new researcher. Around 2018, we decided it’s time to improve. As time moves on, we need to do even better, and this is how we created Trax. Trax is a deep-learning library that’s focused on clear code and speed. Let me tell you why, so, if you think carefully what you want from a deep-learning library, there are really two things that matters. You want the the programmers to be efficient and you want the code to run fast, and this is because what costs you is the time of the programmer, and the money you need to pay for running your training code. Programmer’s time is very important. You need to use it efficiently, but in deep learning you’re training big models and these costs money too. For example, using eight GPUs on-demand from the Cloud, can cost $20 an hour almost. But using the preemptible eight could TPU costs only $1.40. In Trax, you can use one or the other without changing a single character in your code. How does Trax make programmers sufficient? Well, it was redesigned from the bottom-up to be easy to debug and understand. You can literally read Trax code and understand what’s going to come. This is not the case in some other libraries, this is unluckily of the case anymore in TensorFlow. But, you can say, well it used to be the case, but nowadays TensorFlow, even when we clean up the code, it needs to be backwards compatible. It carries the weight of these years of development, and this is crazy errors of Machine Learning. There is a lot of baggage that it just has to carry because it’s backward compatible. What we do in Trax is we break the backwards compatibility. This means you need to learn new things. This carries some price. But what you get for that price, is that it’s a newly cleanly designed library which has four models, not just primitives to build them, but also four models with dataset bindings, we regression test these models daily because we use these libraries, so we know every day these monster running. It’s like a new programming language. It costs a little bit to learn, this is a new thing, but it makes your life much more efficient. To make this point point clear, the Adam Optimizer, the most popular optimizer in machine learning timesteps. On the left, you see a screenshot from the paper that introduced data, and you see it has like about seven lines. Next is just a part of the Adam implementation and patronage, which is one of the cleanest ones actually and you need to know way more, you need to know what are parameter groups, you need to know secret keys into these groups that key parameters by some means, you need to do seven stick initialization and some conditional to introduce either and other things. On the right, you see the Adam optimizer in TensorFlow and Keras and as you’ll see it’s even longer. You need to apply it to resource variables and two non-research variables and you need to know what these are. The reason they exist is historical. Currently we only use resource variables, but we have to support people who used the old non-research variables too. There are a lot of things that in 2020 you actually don’t need anymore, but they have to be there and painted and in TensorFlow code. While if you go to Trax code, this is the full code of Adam and Trax. It’s very similar for the paper. That’s the whole point. Because if you’re implementing a new paper or if you’re learning and you want to find, in the code of the framework, where are the equations from the paper, you can really do with this here. So that is the benefit of Trax. The price of this benefit is that you’re using a new thing. But there is a huge gain that comes to you when you’re actually debugging your code. When you’re debugging your code, you will hit lines that are in the framework. So you will actually need to understand these lines, which means you need to understand all of these PyTorch and all of these TensorFlow if you use those. But in Trax, you only need to understand these Trax lines. It’s much easier to debug, which makes programmers more efficient. Now this efficiency would not be worth that much if the code is running slow. Hey, there’s a lot of beautiful things where you can program things in a few line, but the run so slowly that it’s actually useless. Not so in Trax because we use the just-in-time compiler technology that was built in the last six years of TensorFlow. It’s called XLA, and we use it on top of Trax. These teams have put tremendous effort to make this coat the fastest code on the planet. There is an industry competition called MLPerf. In 2020, JAX actually won this competition, being the fastest transformer to ever be benchmarked independently. So JAX transformer ran in 0.26 of a minute, so in about 16 seconds, I think, while the fastest TensorFlow transformer on the same hardware took 0.35 minutes. So you see, it’s almost 50 percent slower. The fastest PyTorch, but this was not on TPU, took 0.62. So being two times faster is significant game. It’s not clear you’ll get the same gain in any model on other hardware. There was a lot of work to tune it for this particular model hardware. But in general, Trax runs fast. This means, you’ll pay less for the TPUs and GPUs you’ll be running on Cloud. It’s also tested with TPUs on Colab. Colabs are the IPython notebooks that Google gives you for free. You can select a hardware accelerator, you can select TPU and run the same code with no changes. It’s GPU, TPU, or CPU, on this Colab, where you’re getting an eight-code TPU for free. So you can test your code there and then run it on Cloud for much cheaper than other frameworks, and it really runs fast. So these are the reasons to use Trax, and for me, Trax is also super fun. It’s super fun to learn, it’s super fun to use, because we had the liberty to do things from scratch using many years of experience now. You can write model using combinators. This is a whole transformer language model on the left. On the right, you can see it’s from a README. This is everything you need to run a pre-trained model and get your translations. So this gave us the opportunity to clean up the framework, clean up the code, make sure it runs really fast. It’s a lot of fun to use. So I encourage you come check it out. See how you can use Trax for your own machine learning endeavors, both for research. If you want to start a startup or if you want to run it for a big company, I think Trax will be there for you.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w1/index.html#lab-classes-and-subclasses",
    "href": "notes/c3w1/index.html#lab-classes-and-subclasses",
    "title": "Neural Networks for Sentiment Analysis",
    "section": "Lab: Classes and Subclasses",
    "text": "Lab: Classes and Subclasses\nClasses and Subclasses",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w1/index.html#dense-and-relu-layer",
    "href": "notes/c3w1/index.html#dense-and-relu-layer",
    "title": "Neural Networks for Sentiment Analysis",
    "section": "Dense and ReLU layer",
    "text": "Dense and ReLU layer\nThe Dense layer is the computation of the inner product between a set of trainable weights (weight matrix) and an input vector. The visualization of the dense layer could be seen in the image below.\n\nThe orange box shows the dense layer. An activation layer is the set of blue nodes. Concretely one of the most commonly used activation layers is the rectified linear unit (ReLU).\n\nReLU(x) is defined as max(0,x) for any input x.\n\n\n\n\n\n\nVideo Transcript\n\n\n\n\n\n\nNow, I’ll show you dense and ReLU layers. First, I’ll show you dense layers and then ReLU layers. Suppose that you have a simple serial model like this one. Let’s focus on the first parts of the model. Here, you have an input vector X fully connected to a layer of activations. To get the value of z, let’s go into the activations. You will have to compute the inner products between a set of trainable weights and the input vector. This single computation is called a dense layer. The ReLU layer is much simpler. Let’s take the same model you’ve been working with.\nThe ReLU layer is an activation layer that typically follows a dense fully connected layer, and transforms any negative values to zero before sending them onto the next layer. To do this, the ReLU layer computes the function g, which returns a value of zero for all negative values of z, and z for all positive ones. You’ve now seen the dense layer and the ReLU layer. Next, I’ll show you how to put a model together.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w1/index.html#serial-layer",
    "href": "notes/c3w1/index.html#serial-layer",
    "title": "Neural Networks for Sentiment Analysis",
    "section": "Serial Layer",
    "text": "Serial Layer\nA serial layer allows we to compose layers in a serial arrangement:\n\nIt is a composition of sublayers. These layers are usually dense layers followed by activation layers.\n\n\n\n\n\n\nVideo Transcript\n\n\n\n\n\n\nTo create a neural network out of layers, you need to put them together. In a deep network, you run them one after the other in a sequential way. Let me show you how you can do this. Previously, you saw how to define layers, and I showed you how the dense and ReLU layers performed single steps of forward propagation. Now, I’ll show you how to define a serial neural network as a composition of layers thats operates in a sequence. Imagine, a basic neural network like this one. You have some dense layers and activation layers, and the sequential arrangements of those layers is done in tracts, when you define a serial layer. You could think of this new serial layer as your whole neural network model in one layer. Let’s summarize what you just learned. A serial layer is a composition of sublayers that operates sequentially to perform the forward computation of your entire model. Coming up, I’ll introduce some additional layers and the training procedure.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w1/index.html#other-layers",
    "href": "notes/c3w1/index.html#other-layers",
    "title": "Neural Networks for Sentiment Analysis",
    "section": "Other Layers",
    "text": "Other Layers\nOther layers could include embedding layers and mean layers. For example, we can learn word embeddings for each word in your vocabulary as follows:\n\nThe mean layer allows we to take the average of the embeddings. We can visualize it as follows:\n\nThis layer does not have any trainable parameters.\n\n\n\n\n\n\nVideo Transcript\n\n\n\n\n\n\nEarlier, you worked with a few different ways to represent text data. When using neural networks for NLP tasks, embedding layers are often included in the model. Going forward, I will introduce embedding layers, and explain why you will need to include layers such as the mean layer in serial models directly after your embedding layer. Let’s now dive into the embedding layer. In NLP, you usually have a set of unique words called the vocabulary. An embedding layer takes an index assigned to each word from your vocabulary and maps it to a representation of that word with a determined dimension. In this example, embedding of size equal to two. For instance, the embedding layer in this example, we’ll return a vector equal to 0.020, and 0.006 for the word I, and negative 0.009 and 0.050 for the word NLP. Every value from this part of the table is trainable in tracks. When using an embedding layer in your model, you will learn the representation that gives the best performance for your NLP task. For the embedding layer in your model, you’d have to learn a matrix of weights, of size equal to your vocabulary times the dimension of the embedding. The size of the embedding could be treated as a hyperparameter in your model. With this layer, your model can learn or improve the word embeddings for your NLP task like it improves the weights matrices on each layer.\nThe embedding layer is able to map words to embeddings. If you had a series of words, like a tweet that says, “I am happy,” the embedding layer will map each of those words to their corresponding embedding, and return a matrix of word embeddings. If you had padded vectors representing your tweets, you could unroll this matrix and feed its values to the next layer on the neural network. But in doing this, you might end up with lots of parameters to train. As an alternative, you could just take the mean of each feature from the embedding, and that’s exactly what the mean layer does in tracks. After the mean layer, you will have the same number of features as you’re embedding size. Even for sequences of texts, those are very long. Note that this layer doesn’t have any trainable parameters because it’s only computing the mean of the word embeddings.\nAt this point, you are familiar with what’s embedding layers, and mean layers are, and how they work. An important takeaway here is that using an embedding layer in your model allows you to learn a good representation of your vocabulary for the specific task you’re working on, and that’s the mean layer takes a matrix of embeddings, and returns a vector representation for a set of words, like tweets.\n\n\n\n\n\nTraining\nIn Trax, the function grad allows we to compute the gradient. We can use it as follows:\n\nNow if we were to evaluate grad_f at a certain value, namely z, it would be the same as computing 6z+1. Now to do the training, it becomes very simple:\n\nWe simply compute the gradients by feeding in y.forward (the latest value of y), the weights, and the input x, and then it does the back-propagation for we in a single line. We can then have the loop that allows we to update the weights (i.e. gradient descent!).\n\n\n\n\n\n\nVideo Transcript\n\n\n\n\n\n\nTo train a neural network, you need to compute gradients. You’ve done it by hand earlier in this specialization, and you’ve seen that it can be quite complex. But you know what? You don’t have to necessarily do it by yourself. Deep learning frameworks can do it for you. Let’s dive in. I will show you how the trax grad function allows you to easily compute gradients, which will allow you to perform back-propagation and train your model. You will see how easy it is compared to back propagating by hand. Computing ingredients using Trax is pretty straightforward. Suppose that you have the following equation, f of x, whose gradient with respect to x is equal to this derivative. To get that derivative in Trax, define the function f that takes in x, and then just call the grad function with f as its single parameter. The function grad will return a function that computes the gradient of f. That’s nice.\nUsing the grad function to train a model is similarly painless. Suppose that you have a neural network model y. To get the gradient of your model, just apply the grad function with the forward method of your model as a single parameter. Then evaluate the gradient with your weights and inputs. Notice the double sets of parentheses.\nThe first one passes the arguments for the grad function, and the second one, the arguments for the function returned by grad. After you have the gradients for your model, just iterate until convergence is reached.\nThat’s it, forward and back-propagation performed in a single line. This final block is gradient descent. You can always change the optimization algorithm if necessary. So let’s summarize. Having a defined model in Trax, make training significantly easier than computing forward and back-propagation by hand because the built-in grad function automatically computes everything you need.\n\n\nIn the programming assignments from this module, you’ll be able to define and train a neural network using Trax.\nGood luck, and have fun. This was the last video of the week. You now know how to create neural networks with layers and how to train them.\nNext, you will learn even more complex layers and built networks that perform better.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w1/index.html#lab-data-generators",
    "href": "notes/c3w1/index.html#lab-data-generators",
    "title": "Neural Networks for Sentiment Analysis",
    "section": "Lab: Data Generators",
    "text": "Lab: Data Generators\nData Generators",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w4/lab02.html",
    "href": "notes/c4w4/lab02.html",
    "title": "Putting the “Re” in Reformer: Ungraded Lab",
    "section": "",
    "text": "This ungraded lab will explore Reversible Residual Networks. You will use these networks in this week’s assignment that utilizes the Reformer model. It is based on on the Transformer model you already know, but with two unique features. * Locality Sensitive Hashing (LSH) Attention to reduce the compute cost of the dot product attention and * Reversible Residual Networks (RevNets) organization to reduce the storage requirements when doing backpropagation in training.\nIn this ungraded lab we’ll start with a quick review of Residual Networks and their implementation in Trax. Then we will discuss the Revnet architecture and its use in Reformer.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots",
      "L2 - Revnet"
    ]
  },
  {
    "objectID": "notes/c4w4/lab02.html#outline",
    "href": "notes/c4w4/lab02.html#outline",
    "title": "Putting the “Re” in Reformer: Ungraded Lab",
    "section": "Outline",
    "text": "Outline\n\nPart 1: Residual Networks\n\n1.1 Branch\n1.2 Residual Model\n\nPart 2: Reversible Residual Networks\n\n2.1 Trax Reversible Layers\n2.2 Residual Model\n\n\n\nimport trax\nfrom trax import layers as tl               # core building block\nimport numpy as np                          # regular ol' numpy\nfrom trax.models.reformer.reformer import (\n    ReversibleHalfResidualV2 as ReversibleHalfResidual,\n)                                           # unique spot\nfrom trax import fastmath                   # uses jax, offers numpy on steroids\nfrom trax import shapes                     # data signatures: dimensionality and type\nfrom trax.fastmath import numpy as jnp      # For use in defining new layer types.\nfrom trax.shapes import ShapeDtype\nfrom trax.shapes import signature\n\n2025-02-10 16:54:01.601593: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1739199241.613582  121997 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1739199241.617462  121997 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\n\n\n---------------------------------------------------------------------------\nImportError                               Traceback (most recent call last)\nCell In[1], line 4\n      2 from trax import layers as tl               # core building block\n      3 import numpy as np                          # regular ol' numpy\n----&gt; 4 from trax.models.reformer.reformer import (\n      5     ReversibleHalfResidualV2 as ReversibleHalfResidual,\n      6 )                                           # unique spot\n      7 from trax import fastmath                   # uses jax, offers numpy on steroids\n      8 from trax import shapes                     # data signatures: dimensionality and type\n\nImportError: cannot import name 'ReversibleHalfResidualV2' from 'trax.models.reformer.reformer' (/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/models/reformer/reformer.py)",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots",
      "L2 - Revnet"
    ]
  },
  {
    "objectID": "notes/c4w4/lab02.html#1",
    "href": "notes/c4w4/lab02.html#1",
    "title": "Putting the “Re” in Reformer: Ungraded Lab",
    "section": "Part 1.0 Residual Networks",
    "text": "Part 1.0 Residual Networks\nDeep Residual Networks (Resnets) were introduced to improve convergence in deep networks. Residual Networks introduce a shortcut connection around one or more layers in a deep network as shown in the diagram below from the original paper.\n\n\n\n\n\n\nFigure 1: Residual Network diagram from original paper\n\n\n\nThe Trax documentation describes an implementation of Resnets using branch. We’ll explore that here by implementing a simple resnet built from simple function based layers. Specifically, we’ll build a 4 layer network based on two functions, ‘F’ and ‘G’.\n\n\n\n\n\n\nFigure 2: 4 stage Residual network\n\n\n\nDon’t worry about the lengthy equations. Those are simply there to be referenced later in the notebook.\n\nPart 1.1 Branch\nTrax branch figures prominently in the residual network layer so we will first examine it. You can see from the figure above that we will need a function that will copy an input and send it down multiple paths. This is accomplished with a branch layer, one of the Trax ‘combinators’. Branch is a combinator that applies a list of layers in parallel to copies of inputs. Lets try it out! First we will need some layers to play with. Let’s build some from functions.\n\n# simple function taking one input and one output\nbl_add1 = tl.Fn(\"add1\", lambda x0: (x0 + 1), n_out=1)\nbl_add2 = tl.Fn(\"add2\", lambda x0: (x0 + 2), n_out=1)\nbl_add3 = tl.Fn(\"add3\", lambda x0: (x0 + 3), n_out=1)\n# try them out\nx = np.array([1])\nprint(bl_add1(x), bl_add2(x), bl_add3(x))\n# some information about our new layers\nprint(\n    \"name:\",\n    bl_add1.name,\n    \"number of inputs:\",\n    bl_add1.n_in,\n    \"number of outputs:\",\n    bl_add1.n_out,\n)\n\n[2] [3] [4]\nname: add1 number of inputs: 1 number of outputs: 1\n\n\n\nbl_3add1s = tl.Branch(bl_add1, bl_add2, bl_add3)\nbl_3add1s\n\nBranch_out3[\n  add1\n  add2\n  add3\n]\n\n\nTrax uses the concept of a ‘stack’ to transfer data between layers. For Branch, for each of its layer arguments, it copies the n_in inputs from the stack and provides them to the layer, tracking the max_n_in, or the largest n_in required. It then pops the max_n_in elements from the stack. \n\nFigure 3: One in, one out Branch\n\nOn output, each layer, in succession pushes its results onto the stack. Note that the push/pull operations impact the top of the stack. Elements that are not part of the operation (n, and m in the diagram) remain intact.\n\n# n_in = 1, Each bl_addx pushes n_out = 1 elements onto the stack\nbl_3add1s(x)\n\n(array([2]), array([3]), array([4]))\n\n\n\n# n = np.array([10]); m = np.array([20])  # n, m will remain on the stack\nn = \"n\"\nm = \"m\"  # n, m will remain on the stack\nbl_3add1s([x, n, m]) \n\n(array([2]), array([3]), array([4]), 'n', 'm')\n\n\nEach layer in the input list copies as many inputs from the stack as it needs, and their outputs are successively combined on stack. Put another way, each element of the branch can have differing numbers of inputs and outputs. Let’s try a more complex example.\n\nbl_addab = tl.Fn(\n    \"addab\", lambda x0, x1: (x0 + x1), n_out=1\n)  # Trax figures out how many inputs there are\nbl_rep3x = tl.Fn(\n    \"add2x\", lambda x0: (x0, x0, x0), n_out=3\n)  # but you have to tell it how many outputs there are\nbl_3ops = tl.Branch(bl_add1, bl_addab, bl_rep3x)\n\nIn this case, the number if inputs being copied from the stack varies with the layer \n\nFigure 4: variable in, variable out Branch\n\nThe stack when the operation is finished is 5 entries reflecting the total from each layer.\n\n# Before Running this cell, what is the output you are expecting?\ny = np.array([3])\nbl_3ops([x, y, n, m])\n\n(array([2]), array([4]), array([1]), array([1]), array([1]), 'n', 'm')\n\n\nBranch has a special feature to support Residual Network. If an argument is ‘None’, it will pull the top of stack and push it (at its location in the sequence) onto the output stack \n\nFigure 5: Branch for Residual\n\n\nbl_2ops = tl.Branch(bl_add1, None)\nbl_2ops([x, n, m])\n\n(array([2]), array([1]), 'n', 'm')\n\n\n ### Part 1.2 Residual Model OK, your turn. Write a function ‘MyResidual’, that uses tl.Branch and tl.Add to build a residual layer. If you are curious about the Trax implementation, you can see the code here.\n\ndef MyResidual(layer):\n    return tl.Serial(\n        ### START CODE HERE ###\n        tl.Branch(layer, None),\n        tl.Add(),\n        ### END CODE HERE ###\n    )\n\n\n# Lets Try it\nmr = MyResidual(bl_add1)\nx = np.array([1])\nmr([x, n, m])\n\n(array([3]), 'n', 'm')\n\n\nExpected Result (array([3]), ‘n’, ‘m’)\nGreat! Now, let’s build the 4 layer residual Network in Figure 2. You can use MyResidual, or if you prefer, the tl.Residual in Trax, or a combination!\n\nFl = tl.Fn(\"F\", lambda x0: (2 * x0), n_out=1)\nGl = tl.Fn(\"G\", lambda x0: (10 * x0), n_out=1)\nx1 = np.array([1])\n\n\nresfg = tl.Serial(\n    ### START CODE HERE ###\n    # None,  #Fl    # x + F(x)\n    # None,  #Gl    # x + F(x) + G(x + F(x)) etc\n    # None,  #Fl\n    # None,  #Gl\n    ### END CODE HERE ###\n)\n\n\n# Lets try it\nresfg([x1, n, m])\n\n[array([1]), 'n', 'm']\n\n\nExpected Results (array([1089]), ‘n’, ‘m’)\n ## Part 2.0 Reversible Residual Networks The Reformer utilized RevNets to reduce the storage requirements for performing backpropagation. \n\nFigure 6: Reversible Residual Networks \n\nThe standard approach on the left above requires one to store the outputs of each stage for use during backprop. By using the organization to the right, one need only store the outputs of the last stage, y1, y2 in the diagram. Using those values and running the algorithm in reverse, one can reproduce the values required for backprop. This trades additional computation for memory space which is at a premium with the current generation of GPU’s/TPU’s.\nOne thing to note is that the forward functions produced by two networks are similar, but they are not equivalent. Note for example the asymmetry in the output equations after two stages of operation. \n\nFigure 7: ‘Normal’ Residual network (Top) vs REversible Residual Network \n\n\n\nPart 2.1 Trax Reversible Layers\nLet’s take a look at how this is used in the Reformer.\n\nrefm = trax.models.reformer.ReformerLM(\n    vocab_size=33000, n_layers=2, mode=\"train\"  # Add more options.\n)\nrefm\n\nSerial[\n  Serial[\n    ShiftRight(1)\n  ]\n  Embedding_33000_512\n  Dropout\n  Serial[\n    PositionalEncoding\n  ]\n  Dup_out2\n  ReversibleSerial_in2_out2[\n    ReversibleHalfResidualDecoderAttn_in2_out2[\n      Serial[\n        LayerNorm\n      ]\n      SelfAttention\n    ]\n    ReversibleSwap_in2_out2\n    ReversibleHalfResidualDecoderFF_in2_out2[\n      Serial[\n        LayerNorm\n        Dense_2048\n        Dropout\n        Serial[\n          FastGelu\n        ]\n        Dense_512\n        Dropout\n      ]\n    ]\n    ReversibleSwap_in2_out2\n    ReversibleHalfResidualDecoderAttn_in2_out2[\n      Serial[\n        LayerNorm\n      ]\n      SelfAttention\n    ]\n    ReversibleSwap_in2_out2\n    ReversibleHalfResidualDecoderFF_in2_out2[\n      Serial[\n        LayerNorm\n        Dense_2048\n        Dropout\n        Serial[\n          FastGelu\n        ]\n        Dense_512\n        Dropout\n      ]\n    ]\n    ReversibleSwap_in2_out2\n  ]\n  Concatenate_in2\n  LayerNorm\n  Dropout\n  Serial[\n    Dense_33000\n  ]\n]\n\n\nEliminating some of the detail, we can see the structure of the network. \n\nFigure 8: Key Structure of Reformer Reversible Network Layers in Trax \n\nWe’ll review the Trax layers used to implement the Reversible section of the Reformer. First we can note that not all of the reformer is reversible. Only the section in the ReversibleSerial layer is reversible. In a large Reformer model, that section is repeated many times making up the majority of the model. \n\nFigure 9: Functional Diagram of Trax elements in Reformer \n\nThe implementation starts by duplicating the input to allow the two paths that are part of the reversible residual organization with Dup. Note that this is accomplished by copying the top of stack and pushing two copies of it onto the stack. This then feeds into the ReversibleHalfResidual layer which we’ll review in more detail below. This is followed by ReversibleSwap. As the name implies, this performs a swap, in this case, the two topmost entries in the stack. This pattern is repeated until we reach the end of the ReversibleSerial section. At that point, the topmost 2 entries of the stack represent the two paths through the network. These are concatenated and pushed onto the stack. The result is an entry that is twice the size of the non-reversible version.\nLet’s look more closely at the ReversibleHalfResidual. This layer is responsible for executing the layer or layers provided as arguments and adding the output of those layers, the ‘residual’, to the top of the stack. Below is the ‘forward’ routine which implements this. \n\nFigure 10: ReversibleHalfResidual code and diagram \n\nUnlike the previous residual function, the value that is added is from the second path rather than the input to the set of sublayers in this layer. Note that the Layers called by the ReversibleHalfResidual forward function are not modified to support reverse functionality. This layer provides them a ‘normal’ view of the stack and takes care of reverse operation.\nLet’s try out some of these layers! We’ll start with the ones that just operate on the stack, Dup() and Swap().\n\nx1 = np.array([1])\nx2 = np.array([5])\n# Dup() duplicates the Top of Stack and returns the stack\ndl = tl.Dup()\ndl(x1)\n\n(array([1]), array([1]))\n\n\n\n# ReversibleSwap() duplicates the Top of Stack and returns the stack\nsl = tl.ReversibleSwap()\nsl([x1, x2])\n\n(array([5]), array([1]))\n\n\nYou are no doubt wondering “How is ReversibleSwap different from Swap?”. Good question! Lets look: \n\nFigure 11: Two versions of Swap() \n\nThe ReverseXYZ functions include a “reverse” compliment to their “forward” function that provides the functionality to run in reverse when doing backpropagation. It can also be run in reverse by simply calling ‘reverse’.\n\n# Demonstrate reverse swap\nprint(x1, x2, sl.reverse([x1, x2]))\n\n[1] [5] (array([5]), array([1]))\n\n\nLet’s try ReversibleHalfResidual, First we’ll need some layers..\n\nFl = tl.Fn(\"F\", lambda x0: (2 * x0), n_out=1)\nGl = tl.Fn(\"G\", lambda x0: (10 * x0), n_out=1)\n\nJust a note about ReversibleHalfResidual. As this is written, it resides in the Reformer model and is a layer. It is invoked a bit differently that other layers. Rather than tl.XYZ, it is just ReversibleHalfResidual(layers..) as shown below. This may change in the future.\n\nhalf_res_F = ReversibleHalfResidual(Fl)\nprint(type(half_res_F), \"\\n\", half_res_F)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[19], line 1\n----&gt; 1 half_res_F = ReversibleHalfResidual(Fl)\n      2 print(type(half_res_F), \"\\n\", half_res_F)\n\nNameError: name 'ReversibleHalfResidual' is not defined\n\n\n\n\nhalf_res_F([x1, x1])  # this is going to produce an error - why?\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[20], line 1\n----&gt; 1 half_res_F([x1, x1])  # this is going to produce an error - why?\n\nNameError: name 'half_res_F' is not defined\n\n\n\n\n# we have to initialize the ReversibleHalfResidual layer to let it know what the input is going to look like\nhalf_res_F.init(shapes.signature([x1, x1]))\nhalf_res_F([x1, x1])\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[21], line 2\n      1 # we have to initialize the ReversibleHalfResidual layer to let it know what the input is going to look like\n----&gt; 2 half_res_F.init(shapes.signature([x1, x1]))\n      3 half_res_F([x1, x1])\n\nNameError: name 'half_res_F' is not defined\n\n\n\nNotice the output: (DeviceArray([3], dtype=int32), array([1])). The first value, (DeviceArray([3], dtype=int32) is the output of the “Fl” layer and has been converted to a ‘Jax’ DeviceArray. The second array([1]) is just passed through (recall the diagram of ReversibleHalfResidual above).\nThe final layer we need is the ReversibleSerial Layer. This is the reversible equivalent of the Serial layer and is used in the same manner to build a sequence of layers.\n ### Part 2.2 Build a reversible model We now have all the layers we need to build the model shown below. Let’s build it in two parts. First we’ll build ‘blk’ and then a list of blk’s. And then ‘mod’.\n\n\n\n\nFigure 12: Reversible Model we will build using Trax components \n\n\nblk = [  # a list of the 4 layers shown above\n    ### START CODE HERE ###\n    None,\n    None,\n    None,\n    None,\n]\nblks = [None, None]\n### END CODE HERE ###\n\n\nmod = tl.Serial(\n    ### START CODE HERE ###\n    None,\n    None,\n    None,\n    ### END CODE HERE ###\n)\nmod\n\n/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/combinators.py:437: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n  if self._mode == 'predict' and self._state[1] is not ():  # pylint: disable=literal-comparison\n/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/combinators.py:910: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n  if state[0] is ():  # pylint: disable=literal-comparison\n/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/combinators.py:437: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n  if self._mode == 'predict' and self._state[1] is not ():  # pylint: disable=literal-comparison\n/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/combinators.py:910: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n  if state[0] is ():  # pylint: disable=literal-comparison\n\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[23], line 1\n----&gt; 1 mod = tl.Serial(\n      2     ### START CODE HERE ###\n      3     None,\n      4     None,\n      5     None,\n      6     ### END CODE HERE ###\n      7 )\n      8 mod\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/combinators.py:59, in Serial.__init__(self, name, sublayers_to_print, *sublayers)\n     55 def __init__(self, *sublayers, name=None, sublayers_to_print=None):\n     56   super().__init__(\n     57       name=name, sublayers_to_print=sublayers_to_print)\n---&gt; 59   sublayers = _ensure_flat(sublayers)\n     60   self._sublayers = sublayers\n     61   self._n_layers = len(sublayers)\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/combinators.py:1110, in _ensure_flat(layers)\n   1108 for obj in layers:\n   1109   if not isinstance(obj, base.Layer):\n-&gt; 1110     raise ValueError(\n   1111         f'Found nonlayer object ({obj}) in layers: {layers}')\n   1112 return layers\n\nValueError: Found nonlayer object (None) in layers: [None, None, None]\n\n\n\nExpected Output\nSerial[\n  Dup_out2\n  ReversibleSerial_in2_out2[\n    ReversibleHalfResidualV2_in2_out2[\n      Serial[\n        F\n      ]\n    ]\n    ReversibleSwap_in2_out2\n    ReversibleHalfResidualV2_in2_out2[\n      Serial[\n        G\n      ]\n    ]\n    ReversibleSwap_in2_out2\n    ReversibleHalfResidualV2_in2_out2[\n      Serial[\n        F\n      ]\n    ]\n    ReversibleSwap_in2_out2\n    ReversibleHalfResidualV2_in2_out2[\n      Serial[\n        G\n      ]\n    ]\n    ReversibleSwap_in2_out2\n  ]\n  Concatenate_in2\n]\n\nmod.init(shapes.signature(x1))\nout = mod(x1)\nout\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[24], line 1\n----&gt; 1 mod.init(shapes.signature(x1))\n      2 out = mod(x1)\n      3 out\n\nNameError: name 'mod' is not defined\n\n\n\nExpected Result DeviceArray([ 65, 681], dtype=int32)\nOK, now you have had a chance to try all the ‘Reversible’ functions in Trax. On to the Assignment!",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots",
      "L2 - Revnet"
    ]
  },
  {
    "objectID": "notes/c2w3/lab03.html",
    "href": "notes/c2w3/lab03.html",
    "title": "Out of vocabulary words (OOV)",
    "section": "",
    "text": "course banner\n\n\n\nVocabulary\nIn the video about the out of vocabulary words, we saw that the first step in dealing with the unknown words is to decide which words belong to the vocabulary.\nIn the code assignment, we will try the method based on minimum frequency - all words appearing in the training set with frequency &gt;= minimum frequency are added to the vocabulary.\nHere is a code for the other method, where the target size of the vocabulary is known in advance and the vocabulary is filled with words based on their frequency in the training set.\n\n# build the vocabulary from M most frequent words\n# use Counter object from the collections library to find M most common words\nfrom collections import Counter\n\n# the target size of the vocabulary\nM = 3\n\n# pre-calculated word counts\n# Counter could be used to build this dictionary from the source corpus\nword_counts = {'happy': 5, 'because': 3, 'i': 2, 'am': 2, 'learning': 3, '.': 1}\n\nvocabulary = Counter(word_counts).most_common(M)\n\n# remove the frequencies and leave just the words\nvocabulary = [w[0] for w in vocabulary]\n\nprint(f\"the new vocabulary containing {M} most frequent words: {vocabulary}\\n\") \n\nthe new vocabulary containing 3 most frequent words: ['happy', 'because', 'learning']\n\n\n\nNow that the vocabulary is ready, we can use it to replace the OOV words with &lt;UNK&gt; as we saw in the lecture.\n\n# test if words in the input sentences are in the vocabulary, if OOV, print &lt;UNK&gt;\nsentence = ['am', 'i', 'learning']\noutput_sentence = []\nprint(f\"input sentence: {sentence}\")\n\nfor w in sentence:\n    # test if word w is in vocabulary\n    if w in vocabulary:\n        output_sentence.append(w)\n    else:\n        output_sentence.append('&lt;UNK&gt;')\n        \nprint(f\"output sentence: {output_sentence}\")\n\ninput sentence: ['am', 'i', 'learning']\noutput sentence: ['&lt;UNK&gt;', '&lt;UNK&gt;', 'learning']\n\n\nWhen building the vocabulary in the code assignment, we will need to know how to iterate through the word counts dictionary.\nHere is an example of a similar task showing how to go through all the word counts and print out only the words with the frequency equal to f. \n\n# iterate through all word counts and print words with given frequency f\nf = 3\n\nword_counts = {'happy': 5, 'because': 3, 'i': 2, 'am': 2, 'learning':3, '.': 1}\n\nfor word, freq in word_counts.items():\n    if freq == f:\n        print(word)\n\nbecause\nlearning\n\n\nAs mentioned in the videos, if there are many &lt;UNK&gt; replacements in your train and test set, we may get a very low perplexity even though the model itself wouldn’t be very helpful.\nHere is a sample code showing this unwanted effect.\n\n# many &lt;unk&gt; low perplexity \ntraining_set = ['i', 'am', 'happy', 'because','i', 'am', 'learning', '.']\ntraining_set_unk = ['i', 'am', '&lt;UNK&gt;', '&lt;UNK&gt;','i', 'am', '&lt;UNK&gt;', '&lt;UNK&gt;']\n\ntest_set = ['i', 'am', 'learning']\ntest_set_unk = ['i', 'am', '&lt;UNK&gt;']\n\nM = len(test_set)\nprobability = 1\nprobability_unk = 1\n\n# pre-calculated probabilities\nbigram_probabilities = {('i', 'am'): 1.0, ('am', 'happy'): 0.5, ('happy', 'because'): 1.0, ('because', 'i'): 1.0, ('am', 'learning'): 0.5, ('learning', '.'): 1.0}\nbigram_probabilities_unk = {('i', 'am'): 1.0, ('am', '&lt;UNK&gt;'): 1.0, ('&lt;UNK&gt;', '&lt;UNK&gt;'): 0.5, ('&lt;UNK&gt;', 'i'): 0.25}\n\n# got through the test set and calculate its bigram probability\nfor i in range(len(test_set) - 2 + 1):\n    bigram = tuple(test_set[i: i + 2])\n    probability = probability * bigram_probabilities[bigram]\n        \n    bigram_unk = tuple(test_set_unk[i: i + 2])\n    probability_unk = probability_unk * bigram_probabilities_unk[bigram_unk]\n\n# calculate perplexity for both original test set and test set with &lt;UNK&gt;\nperplexity = probability ** (-1 / M)\nperplexity_unk = probability_unk ** (-1 / M)\n\nprint(f\"perplexity for the training set: {perplexity}\")\nprint(f\"perplexity for the training set with &lt;UNK&gt;: {perplexity_unk}\")\n\nperplexity for the training set: 1.2599210498948732\nperplexity for the training set with &lt;UNK&gt;: 1.0\n\n\n\n\nSmoothing\nAdd-k smoothing was described as a method for smoothing of the probabilities for previously unseen n-grams.\nHere is an example code that shows how to implement add-k smoothing but also highlights a disadvantage of this method. The downside is that n-grams not previously seen in the training dataset get too high probability.\nIn the code output bellow you’ll see that a phrase that is in the training set gets the same probability as an unknown phrase.\n\ndef add_k_smooting_probability(k, vocabulary_size, n_gram_count, n_gram_prefix_count):\n    numerator = n_gram_count + k\n    denominator = n_gram_prefix_count + k * vocabulary_size\n    return numerator / denominator\n\ntrigram_probabilities = {('i', 'am', 'happy') : 2}\nbigram_probabilities = {( 'am', 'happy') : 10}\nvocabulary_size = 5\nk = 1\n\nprobability_known_trigram = add_k_smooting_probability(k, vocabulary_size, trigram_probabilities[('i', 'am', 'happy')], \n                           bigram_probabilities[( 'am', 'happy')])\n\nprobability_unknown_trigram = add_k_smooting_probability(k, vocabulary_size, 0, 0)\n\nprint(f\"probability_known_trigram: {probability_known_trigram}\")\nprint(f\"probability_unknown_trigram: {probability_unknown_trigram}\")\n\nprobability_known_trigram: 0.2\nprobability_unknown_trigram: 0.2\n\n\n\n\nBack-off\nBack-off is a model generalization method that leverages information from lower order n-grams in case information about the high order n-grams is missing. For example, if the probability of an trigram is missing, use bigram information and so on.\nHere we can see an example of a simple back-off technique.\n\n# pre-calculated probabilities of all types of n-grams\ntrigram_probabilities = {('i', 'am', 'happy'): 0}\nbigram_probabilities = {( 'am', 'happy'): 0.3}\nunigram_probabilities = {'happy': 0.4}\n\n# this is the input trigram we need to estimate\ntrigram = ('are', 'you', 'happy')\n\n# find the last bigram and unigram of the input\nbigram = trigram[1: 3]\nunigram = trigram[2]\nprint(f\"besides the trigram {trigram} we also use bigram {bigram} and unigram ({unigram})\\n\")\n\n# 0.4 is used as an example, experimentally found for web-scale corpuses when using the \"stupid\" back-off\nlambda_factor = 0.4\nprobability_hat_trigram = 0\n\n# search for first non-zero probability starting with trigram\n# to generalize this for any order of n-gram hierarchy, \n# we could loop through the probability dictionaries instead of if/else cascade\nif trigram not in trigram_probabilities or trigram_probabilities[trigram] == 0:\n    print(f\"probability for trigram {trigram} not found\")\n    \n    if bigram not in bigram_probabilities or bigram_probabilities[bigram] == 0:\n        print(f\"probability for bigram {bigram} not found\")\n        \n        if unigram in unigram_probabilities:\n            print(f\"probability for unigram {unigram} found\\n\")\n            probability_hat_trigram = lambda_factor * lambda_factor * unigram_probabilities[unigram]\n        else:\n            probability_hat_trigram = 0\n    else:\n        probability_hat_trigram = lambda_factor * bigram_probabilities[bigram]\nelse:\n    probability_hat_trigram = trigram_probabilities[trigram]\n\nprint(f\"probability for trigram {trigram} estimated as {probability_hat_trigram}\")\n\nbesides the trigram ('are', 'you', 'happy') we also use bigram ('you', 'happy') and unigram (happy)\n\nprobability for trigram ('are', 'you', 'happy') not found\nprobability for bigram ('you', 'happy') not found\nprobability for unigram happy found\n\nprobability for trigram ('are', 'you', 'happy') estimated as 0.06400000000000002\n\n\n\n\nInterpolation\nThe other method for using probabilities of lower order n-grams is the interpolation. In this case, we use weighted probabilities of n-grams of all orders every time, not just when high order information is missing.\nFor example, we always combine trigram, bigram and unigram probability. We can see how this in the following code snippet.\n\n# pre-calculated probabilities of all types of n-grams\ntrigram_probabilities = {('i', 'am', 'happy'): 0.15}\nbigram_probabilities = {( 'am', 'happy'): 0.3}\nunigram_probabilities = {'happy': 0.4}\n\n# the weights come from optimization on a validation set\nlambda_1 = 0.8\nlambda_2 = 0.15\nlambda_3 = 0.05\n\n# this is the input trigram we need to estimate\ntrigram = ('i', 'am', 'happy')\n\n# find the last bigram and unigram of the input\nbigram = trigram[1: 3]\nunigram = trigram[2]\nprint(f\"besides the trigram {trigram} we also use bigram {bigram} and unigram ({unigram})\\n\")\n\n# in the production code, we would need to check if the probability n-gram dictionary contains the n-gram\nprobability_hat_trigram = lambda_1 * trigram_probabilities[trigram] \n+ lambda_2 * bigram_probabilities[bigram]\n+ lambda_3 * unigram_probabilities[unigram]\n\nprint(f\"estimated probability of the input trigram {trigram} is {probability_hat_trigram}\")\n\nbesides the trigram ('i', 'am', 'happy') we also use bigram ('am', 'happy') and unigram (happy)\n\n\n\n0.045\n\n\n0.020000000000000004\n\n\nestimated probability of the input trigram ('i', 'am', 'happy') is 0.12\n\n\nThat’s it for week 3, we should be ready now for the code assignment.\n\n\n\n\nCitationBibTeX citation:@online{bochman2020,\n  author = {Bochman, Oren},\n  title = {Out of Vocabulary Words {(OOV)}},\n  date = {2020-10-27},\n  url = {https://orenbochman.github.io/notes-nlp/notes/c2w3/lab03.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2020. “Out of Vocabulary Words (OOV).”\nOctober 27, 2020. https://orenbochman.github.io/notes-nlp/notes/c2w3/lab03.html.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete & Language Models",
      "L3 - Out of vocabulary words"
    ]
  },
  {
    "objectID": "notes/c2w3/lab02.html",
    "href": "notes/c2w3/lab02.html",
    "title": "Building the language model",
    "section": "",
    "text": "course banner",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete & Language Models",
      "L2 - Building the language model"
    ]
  },
  {
    "objectID": "notes/c2w3/lab02.html#language-model-evaluation",
    "href": "notes/c2w3/lab02.html#language-model-evaluation",
    "title": "Building the language model",
    "section": "Language model evaluation",
    "text": "Language model evaluation\n\nTrain/validation/test split\nIn the videos, we saw that to evaluate language models, we need to keep some of the corpus data for validation and testing.\nThe choice of the test and validation data should correspond as much as possible to the distribution of the data coming from the actual application. If nothing but the input corpus is known, then random sampling from the corpus is used to define the test and validation subset.\nHere is a code similar to what you’ll see in the code assignment. The following function allows we to randomly sample the input data and return train/validation/test subsets in a split given by the method parameters.\n\n# we only need train and validation %, test is the remainder\nimport random\ndef train_validation_test_split(data, train_percent, validation_percent):\n    \"\"\"\n    Splits the input data to  train/validation/test according to the percentage provided\n    \n    Args:\n        data: Pre-processed and tokenized corpus, i.e. list of sentences.\n        train_percent: integer 0-100, defines the portion of input corpus allocated for training\n        validation_percent: integer 0-100, defines the portion of input corpus allocated for validation\n        \n        Note: train_percent + validation_percent need to be &lt;=100\n              the reminder to 100 is allocated for the test set\n    \n    Returns:\n        train_data: list of sentences, the training part of the corpus\n        validation_data: list of sentences, the validation part of the corpus\n        test_data: list of sentences, the test part of the corpus\n    \"\"\"\n    # fixed seed here for reproducibility\n    random.seed(87)\n    \n    # reshuffle all input sentences\n    random.shuffle(data)\n\n    train_size = int(len(data) * train_percent / 100)\n    train_data = data[0:train_size]\n    \n    validation_size = int(len(data) * validation_percent / 100)\n    validation_data = data[train_size:train_size + validation_size]\n    \n    test_data = data[train_size + validation_size:]\n    \n    return train_data, validation_data, test_data\n\ndata = [x for x in range (0, 100)]\n\ntrain_data, validation_data, test_data = train_validation_test_split(data, 80, 10)\nprint(\"split 80/10/10:\\n\",f\"train data:{train_data}\\n\", f\"validation data:{validation_data}\\n\", \n      f\"test data:{test_data}\\n\")\n\ntrain_data, validation_data, test_data = train_validation_test_split(data, 98, 1)\nprint(\"split 98/1/1:\\n\",f\"train data:{train_data}\\n\", f\"validation data:{validation_data}\\n\", \n      f\"test data:{test_data}\\n\")\n\nsplit 80/10/10:\n train data:[28, 76, 5, 0, 62, 29, 54, 95, 88, 58, 4, 22, 92, 14, 50, 77, 47, 33, 75, 68, 56, 74, 43, 80, 83, 84, 73, 93, 66, 87, 9, 91, 64, 79, 20, 51, 17, 27, 12, 31, 67, 81, 7, 34, 45, 72, 38, 30, 16, 60, 40, 86, 48, 21, 70, 59, 6, 19, 2, 99, 37, 36, 52, 61, 97, 44, 26, 57, 89, 55, 53, 85, 3, 39, 10, 71, 23, 32, 25, 8]\n validation data:[78, 65, 63, 11, 49, 98, 1, 46, 15, 41]\n test data:[90, 96, 82, 42, 35, 13, 69, 24, 94, 18]\n\nsplit 98/1/1:\n train data:[66, 23, 29, 28, 52, 87, 70, 13, 15, 2, 62, 43, 82, 50, 40, 32, 30, 79, 71, 89, 6, 10, 34, 78, 11, 49, 39, 42, 26, 46, 58, 96, 97, 8, 56, 86, 33, 93, 92, 91, 57, 65, 95, 20, 72, 3, 12, 9, 47, 37, 67, 1, 16, 74, 53, 99, 54, 68, 5, 18, 27, 17, 48, 36, 24, 45, 73, 19, 41, 59, 21, 98, 0, 31, 4, 85, 80, 64, 84, 88, 25, 44, 61, 22, 60, 94, 76, 38, 77, 81, 90, 69, 63, 7, 51, 14, 55, 83]\n validation data:[35]\n test data:[75]\n\n\n\n\n\nPerplexity\nIn order to implement the perplexity formula, you’ll need to know how to implement m-th order root of a variable.\n\\begin{equation*}\nPP(W)=\\sqrt[M]{\\prod_{i=1}^{m}{\\frac{1}{P(w_i|w_{i-1})}}}\n\\end{equation*}\nRemember from calculus:\n\\begin{equation*}\n\\sqrt[M]{\\frac{1}{x}} = x^{-\\frac{1}{M}}\n\\end{equation*}\nHere is a code that will help we with the formula.\n\n# to calculate the exponent, use the following syntax\np = 10 ** (-250)\nM = 100\nperplexity = p ** (-1 / M)\nprint(perplexity)\n\n316.22776601683796\n\n\nThat’s all for the lab for “N-gram language model” lesson of week 3.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete & Language Models",
      "L2 - Building the language model"
    ]
  },
  {
    "objectID": "notes/c4w2/lab02.html",
    "href": "notes/c4w2/lab02.html",
    "title": "The Transformer Decoder: Ungraded Lab Notebook",
    "section": "",
    "text": "In this notebook, you’ll explore the transformer decoder and how to implement it with Trax.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "L2 - The Transformer Decoder"
    ]
  },
  {
    "objectID": "notes/c4w2/lab02.html#background",
    "href": "notes/c4w2/lab02.html#background",
    "title": "The Transformer Decoder: Ungraded Lab Notebook",
    "section": "Background",
    "text": "Background\nIn the last lecture notebook, you saw how to translate the mathematics of attention into NumPy code. Here, you’ll see how multi-head causal attention fits into a GPT-2 transformer decoder, and how to build one with Trax layers. In the assignment notebook, you’ll implement causal attention from scratch, but here, you’ll exploit the handy-dandy tl.CausalAttention() layer.\nThe schematic below illustrates the components and flow of a transformer decoder. Note that while the algorithm diagram flows from the bottom to the top, the overview and subsequent Trax layer codes are top-down.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "L2 - The Transformer Decoder"
    ]
  },
  {
    "objectID": "notes/c4w2/lab02.html#imports",
    "href": "notes/c4w2/lab02.html#imports",
    "title": "The Transformer Decoder: Ungraded Lab Notebook",
    "section": "Imports",
    "text": "Imports\n\nimport sys\nimport os\n\nimport time\nimport numpy as np\nimport gin\n\nimport textwrap\nwrapper = textwrap.TextWrapper(width=70)\n\nimport trax\nfrom trax import layers as tl\nfrom trax.fastmath import numpy as jnp\n\n# to print the entire np array\nnp.set_printoptions(threshold=sys.maxsize)\n\n2025-02-10 16:55:22.489737: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1739199322.502312  123480 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1739199322.506307  123480 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "L2 - The Transformer Decoder"
    ]
  },
  {
    "objectID": "notes/c4w2/lab02.html#sentence-gets-embedded-add-positional-encoding",
    "href": "notes/c4w2/lab02.html#sentence-gets-embedded-add-positional-encoding",
    "title": "The Transformer Decoder: Ungraded Lab Notebook",
    "section": "Sentence gets embedded, add positional encoding",
    "text": "Sentence gets embedded, add positional encoding\nEmbed the words, then create vectors representing each word’s position in each sentence \\in \\{ 0, 1, 2, \\ldots , K\\} = range(max_len), where max_len = K+1)\n\ndef PositionalEncoder(vocab_size, d_model, dropout, max_len, mode):\n    \"\"\"Returns a list of layers that: \n    1. takes a block of text as input, \n    2. embeds the words in that text, and \n    3. adds positional encoding, \n       i.e. associates a number in range(max_len) with \n       each word in each sentence of embedded input text \n    \n    The input is a list of tokenized blocks of text\n    \n    Args:\n        vocab_size (int): vocab size.\n        d_model (int):  depth of embedding.\n        dropout (float): dropout rate (how much to drop out).\n        max_len (int): maximum symbol length for positional encoding.\n        mode (str): 'train' or 'eval'.\n    \"\"\"\n    # Embedding inputs and positional encoder\n    return [ \n        # Add embedding layer of dimension (vocab_size, d_model)\n        tl.Embedding(vocab_size, d_model),  \n        # Use dropout with rate and mode specified\n        tl.Dropout(rate=dropout, mode=mode), \n        # Add positional encoding layer with maximum input length and mode specified\n        tl.PositionalEncoding(max_len=max_len, mode=mode)]",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "L2 - The Transformer Decoder"
    ]
  },
  {
    "objectID": "notes/c4w2/lab02.html#multi-head-causal-attention",
    "href": "notes/c4w2/lab02.html#multi-head-causal-attention",
    "title": "The Transformer Decoder: Ungraded Lab Notebook",
    "section": "Multi-head causal attention",
    "text": "Multi-head causal attention\nThe layers and array dimensions involved in multi-head causal attention (which looks at previous words in the input text) are summarized in the figure below:\n\ntl.CausalAttention() does all of this for you! You might be wondering, though, whether you need to pass in your input text 3 times, since for causal attention, the queries Q, keys K, and values V all come from the same source. Fortunately, tl.CausalAttention() handles this as well by making use of the tl.Branch() combinator layer. In general, each branch within a tl.Branch() layer performs parallel operations on copies of the layer’s inputs. For causal attention, each branch (representing Q, K, and V) applies a linear transformation (i.e. a dense layer without a subsequent activation) to its copy of the input, then splits that result into heads. You can see the syntax for this in the screenshot from the trax.layers.attention.py source code below:",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "L2 - The Transformer Decoder"
    ]
  },
  {
    "objectID": "notes/c4w2/lab02.html#feed-forward-layer",
    "href": "notes/c4w2/lab02.html#feed-forward-layer",
    "title": "The Transformer Decoder: Ungraded Lab Notebook",
    "section": "Feed-forward layer",
    "text": "Feed-forward layer\n\nTypically ends with a ReLU activation, but we’ll leave open the possibility of a different activation\nMost of the parameters are here\n\n\ndef FeedForward(d_model, d_ff, dropout, mode, ff_activation):\n    \"\"\"Returns a list of layers that implements a feed-forward block.\n\n    The input is an activation tensor.\n\n    Args:\n        d_model (int):  depth of embedding.\n        d_ff (int): depth of feed-forward layer.\n        dropout (float): dropout rate (how much to drop out).\n        mode (str): 'train' or 'eval'.\n        ff_activation (function): the non-linearity in feed-forward layer.\n\n    Returns:\n        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\n    \"\"\"\n    \n    # Create feed-forward block (list) with two dense layers with dropout and input normalized\n    return [ \n        # Normalize layer inputs\n        tl.LayerNorm(), \n        # Add first feed forward (dense) layer (don't forget to set the correct value for n_units)\n        tl.Dense(d_ff), \n        # Add activation function passed in as a parameter (you need to call it!)\n        ff_activation(),  # Generally ReLU\n        # Add dropout with rate and mode specified (i.e., don't use dropout during evaluation)\n        tl.Dropout(rate=dropout, mode=mode), \n        # Add second feed forward layer (don't forget to set the correct value for n_units)\n        tl.Dense(d_model), \n        # Add dropout with rate and mode specified (i.e., don't use dropout during evaluation)\n        tl.Dropout(rate=dropout, mode=mode) \n    ]",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "L2 - The Transformer Decoder"
    ]
  },
  {
    "objectID": "notes/c4w2/lab02.html#decoder-block",
    "href": "notes/c4w2/lab02.html#decoder-block",
    "title": "The Transformer Decoder: Ungraded Lab Notebook",
    "section": "Decoder block",
    "text": "Decoder block\nHere, we return a list containing two residual blocks. The first wraps around the causal attention layer, whose inputs are normalized and to which we apply dropout regulation. The second wraps around the feed-forward layer. You may notice that the second call to tl.Residual() doesn’t call a normalization layer before calling the feed-forward layer. This is because the normalization layer is included in the feed-forward layer.\n\ndef DecoderBlock(d_model, d_ff, n_heads,\n                 dropout, mode, ff_activation):\n    \"\"\"Returns a list of layers that implements a Transformer decoder block.\n\n    The input is an activation tensor.\n\n    Args:\n        d_model (int):  depth of embedding.\n        d_ff (int): depth of feed-forward layer.\n        n_heads (int): number of attention heads.\n        dropout (float): dropout rate (how much to drop out).\n        mode (str): 'train' or 'eval'.\n        ff_activation (function): the non-linearity in feed-forward layer.\n\n    Returns:\n        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\n    \"\"\"\n        \n    # Add list of two Residual blocks: the attention with normalization and dropout and feed-forward blocks\n    return [\n      tl.Residual(\n          # Normalize layer input\n          tl.LayerNorm(), \n          # Add causal attention \n          tl.CausalAttention(d_feature, n_heads=n_heads, dropout=dropout, mode=mode) \n        ),\n      tl.Residual(\n          # Add feed-forward block\n          # We don't need to normalize the layer inputs here. The feed-forward block takes care of that for us.\n          FeedForward(d_model, d_ff, dropout, mode, ff_activation)\n        ),\n      ]",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "L2 - The Transformer Decoder"
    ]
  },
  {
    "objectID": "notes/c4w2/lab02.html#the-transformer-decoder-putting-it-all-together",
    "href": "notes/c4w2/lab02.html#the-transformer-decoder-putting-it-all-together",
    "title": "The Transformer Decoder: Ungraded Lab Notebook",
    "section": "The transformer decoder: putting it all together",
    "text": "The transformer decoder: putting it all together",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "L2 - The Transformer Decoder"
    ]
  },
  {
    "objectID": "notes/c4w2/lab02.html#a.k.a.-repeat-n-times-dense-layer-and-softmax-for-output",
    "href": "notes/c4w2/lab02.html#a.k.a.-repeat-n-times-dense-layer-and-softmax-for-output",
    "title": "The Transformer Decoder: Ungraded Lab Notebook",
    "section": "A.k.a. repeat N times, dense layer and softmax for output",
    "text": "A.k.a. repeat N times, dense layer and softmax for output\n\ndef TransformerLM(vocab_size=33300,\n                  d_model=512,\n                  d_ff=2048,\n                  n_layers=6,\n                  n_heads=8,\n                  dropout=0.1,\n                  max_len=4096,\n                  mode='train',\n                  ff_activation=tl.Relu):\n    \"\"\"Returns a Transformer language model.\n\n    The input to the model is a tensor of tokens. (This model uses only the\n    decoder part of the overall Transformer.)\n\n    Args:\n        vocab_size (int): vocab size.\n        d_model (int):  depth of embedding.\n        d_ff (int): depth of feed-forward layer.\n        n_layers (int): number of decoder layers.\n        n_heads (int): number of attention heads.\n        dropout (float): dropout rate (how much to drop out).\n        max_len (int): maximum symbol length for positional encoding.\n        mode (str): 'train', 'eval' or 'predict', predict mode is for fast inference.\n        ff_activation (function): the non-linearity in feed-forward layer.\n\n    Returns:\n        trax.layers.combinators.Serial: A Transformer language model as a layer that maps from a tensor of tokens\n        to activations over a vocab set.\n    \"\"\"\n    \n    # Create stack (list) of decoder blocks with n_layers with necessary parameters\n    decoder_blocks = [ \n        DecoderBlock(d_model, d_ff, n_heads, dropout, mode, ff_activation) for _ in range(n_layers)] \n\n    # Create the complete model as written in the figure\n    return tl.Serial(\n        # Use teacher forcing (feed output of previous step to current step)\n        tl.ShiftRight(mode=mode), \n        # Add embedding inputs and positional encoder\n        PositionalEncoder(vocab_size, d_model, dropout, max_len, mode),\n        # Add decoder blocks\n        decoder_blocks, \n        # Normalize layer\n        tl.LayerNorm(), \n\n        # Add dense layer of vocab_size (since need to select a word to translate to)\n        # (a.k.a., logits layer. Note: activation already set by ff_activation)\n        tl.Dense(vocab_size), \n        # Get probabilities with Logsoftmax\n        tl.LogSoftmax() \n    )",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "L2 - The Transformer Decoder"
    ]
  },
  {
    "objectID": "notes/c4w2/lab02.html#concluding-remarks",
    "href": "notes/c4w2/lab02.html#concluding-remarks",
    "title": "The Transformer Decoder: Ungraded Lab Notebook",
    "section": "Concluding remarks",
    "text": "Concluding remarks\nIn this week’s assignment, you’ll see how to train a transformer decoder on the cnn_dailymail dataset, available from TensorFlow Datasets (part of TensorFlow Data Services). Because training such a model from scratch is time-intensive, you’ll use a pre-trained model to summarize documents later in the assignment. Due to time and storage concerns, we will also not train the decoder on a different summarization dataset in this lab. If you have the time and space, we encourage you to explore the other summarization datasets at TensorFlow Datasets. Which of them might suit your purposes better than the cnn_dailymail dataset? Where else can you find datasets for text summarization models?",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "L2 - The Transformer Decoder"
    ]
  },
  {
    "objectID": "notes/c3w4/lab01.html",
    "href": "notes/c3w4/lab01.html",
    "title": "Creating a Siamese model using Trax: Ungraded Lecture Notebook",
    "section": "",
    "text": "course banner\nimport trax\nfrom trax import layers as tl\nimport trax.fastmath.numpy as np\nimport numpy\n\n# Setting random seeds\n# set random seeds to make this notebook easier to replicate\nfrom trax import fastmath\nseed=10\nrng = fastmath.random.get_prng(seed)\n#trax.supervised.trainer_lib.init_random_number_generators(10)\nnumpy.random.seed(seed)\n\n2025-02-10 16:56:47.210910: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1739199407.224698  124526 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1739199407.228897  124526 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Siamese Networks",
      "L1 - Creating a Siamese Model using Trax"
    ]
  },
  {
    "objectID": "notes/c3w4/lab01.html#l2-normalization",
    "href": "notes/c3w4/lab01.html#l2-normalization",
    "title": "Creating a Siamese model using Trax: Ungraded Lecture Notebook",
    "section": "L2 Normalization",
    "text": "L2 Normalization\nBefore building the model you will need to define a function that applies L2 normalization to a tensor. This is very important because in this week’s assignment you will create a custom loss function which expects the tensors it receives to be normalized. Luckily this is pretty straightforward:\n\ndef normalize(x):\n    return x / np.sqrt(np.sum(x * x, axis=-1, keepdims=True))\n\nNotice that the denominator can be replaced by np.linalg.norm(x, axis=-1, keepdims=True) to achieve the same results and that Trax’s numpy is being used within the function.\n\ntensor = numpy.random.random((2,5))\nprint(f'The tensor is of type: {type(tensor)}\\n\\nAnd looks like this:\\n\\n {tensor}')\n\nThe tensor is of type: &lt;class 'numpy.ndarray'&gt;\n\nAnd looks like this:\n\n [[0.77132064 0.02075195 0.63364823 0.74880388 0.49850701]\n [0.22479665 0.19806286 0.76053071 0.16911084 0.08833981]]\n\n\n\nnorm_tensor = normalize(tensor)\nprint(f'The normalized tensor is of type: {type(norm_tensor)}\\n\\nAnd looks like this:\\n\\n {norm_tensor}')\n\nThe normalized tensor is of type: &lt;class 'jaxlib.xla_extension.ArrayImpl'&gt;\n\nAnd looks like this:\n\n [[0.5739379  0.01544148 0.4714962  0.5571832  0.37093794]\n [0.26781026 0.23596111 0.9060541  0.20146926 0.10524315]]\n\n\nNotice that the initial tensor was converted from a numpy array to a jax array in the process.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Siamese Networks",
      "L1 - Creating a Siamese Model using Trax"
    ]
  },
  {
    "objectID": "notes/c3w4/lab01.html#siamese-model",
    "href": "notes/c3w4/lab01.html#siamese-model",
    "title": "Creating a Siamese model using Trax: Ungraded Lecture Notebook",
    "section": "Siamese Model",
    "text": "Siamese Model\nTo create a Siamese model you will first need to create a LSTM model using the Serial combinator layer and then use another combinator layer called Parallel to create the Siamese model. You should be familiar with the following layers (notice each layer can be clicked to go to the docs): - Serial A combinator layer that allows to stack layers serially using function composition. - Embedding Maps discrete tokens to vectors. It will have shape (vocabulary length X dimension of output vectors). The dimension of output vectors (also called d_feature) is the number of elements in the word embedding. - LSTM The LSTM layer. It leverages another Trax layer called LSTMCell. The number of units should be specified and should match the number of elements in the word embedding. - Mean Computes the mean across a desired axis. Mean uses one tensor axis to form groups of values and replaces each group with the mean value of that group. - Fn Layer with no weights that applies the function f, which should be specified using a lambda syntax. - Parallel It is a combinator layer (like Serial) that applies a list of layers in parallel to its inputs.\nPutting everything together the Siamese model will look like this:\n\nvocab_size = 500\nmodel_dimension = 128\n\n# Define the LSTM model\nLSTM = tl.Serial(\n        tl.Embedding(vocab_size=vocab_size, d_feature=model_dimension),\n        tl.LSTM(model_dimension),\n        tl.Mean(axis=1),\n        tl.Fn('Normalize', lambda x: normalize(x))\n    )\n\n# Use the Parallel combinator to create a Siamese model out of the LSTM \nSiamese = tl.Parallel(LSTM, LSTM)\n\nNext is a helper function that prints information for every layer (sublayer within Serial):\n\ndef show_layers(model, layer_prefix):\n    print(f\"Total layers: {len(model.sublayers)}\\n\")\n    for i in range(len(model.sublayers)):\n        print('========')\n        print(f'{layer_prefix}_{i}: {model.sublayers[i]}\\n')\n\nprint('Siamese model:\\n')\nshow_layers(Siamese, 'Parallel.sublayers')\n\nprint('Detail of LSTM models:\\n')\nshow_layers(LSTM, 'Serial.sublayers')\n\nSiamese model:\n\nTotal layers: 2\n\n========\nParallel.sublayers_0: Serial[\n  Embedding_500_128\n  LSTM_128\n  Mean\n  Normalize\n]\n\n========\nParallel.sublayers_1: Serial[\n  Embedding_500_128\n  LSTM_128\n  Mean\n  Normalize\n]\n\nDetail of LSTM models:\n\nTotal layers: 4\n\n========\nSerial.sublayers_0: Embedding_500_128\n\n========\nSerial.sublayers_1: LSTM_128\n\n========\nSerial.sublayers_2: Mean\n\n========\nSerial.sublayers_3: Normalize\n\n\n\nTry changing the parameters defined before the Siamese model and see how it changes!\nYou will actually train this model in this week’s assignment. For now you should be more familiarized with creating Siamese models using Trax.\nKeep it up!",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Siamese Networks",
      "L1 - Creating a Siamese Model using Trax"
    ]
  },
  {
    "objectID": "notes/c3w4/index.html",
    "href": "notes/c3w4/index.html",
    "title": "Siamese Networks",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\n\nThis week’s slides\n\n\n\n\nSupplementary Figure 1\nMy notes for Week 4 of the Natural Language Processing with Sequence Models Course in the Natural Language Processing Specialization Offered by DeepLearning.AI on Coursera",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Siamese Networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w4/index.html#sec-siamese-network",
    "href": "notes/c3w4/index.html#sec-siamese-network",
    "title": "Siamese Networks",
    "section": "Siamese Network",
    "text": "Siamese Network\nIt is best to describe what a Siamese network is through an example.\n\n\n\n\n\n\n\nFigure 1: Comparisons questions pairs\n\n\nNote that in the first example above, the two sentences mean the same thing but have completely different words. While in the second case, the two sentences mean completely different things but they have very similar words.\n\nClassification: learns what makes an input what it is.\nSiamese Networks: learns what makes two inputs the same\n\nHere are a few applications of siamese networks:\n\n\n\n\n\n\n\nFigure 2: NLP applications of Siamese Networks include, comparing two signatures, comparing questions or search engine queries,\n\n\n\n\n\n\n\n\nVideo Transcript\n\n\n\n\n\n\nIn this video you’re going to learn about a special type of neural network known as the Siamese network. It is a neural network made up of two identical neural networks which are merged at the end. This type of architecture has many applications and NLP. And in this video, you’ll see the different examples where you can use it.\n\n\nConsider the following question. How old are you? And what is your age? You can see that these questions don’t have any words in common. However, they both mean the same thing. On the other hand, if you were to look at the following questions, where are you from? And where are you going? You can see that the first three words are the same. However, the last word completely changes the meaning of each question. This example shows that comparing meaning is not as simple as just comparing words. Coming up, you’re going to see how you can use Siamese networks to compare the meaning of word sequences, and identify question duplicates, which is a very important NLP application at the core of platforms like Stack Overflow or Quora.\n\n\nBefore these platforms allow you to post a new question, they want to be sure that your question hasn’t already been posted by somebody else. Now take this sentence, I’m happy because I’m learning, and consider it in the context of sentiment analysis and binary classification. Now in training a classification algorithm, you discover what features give the statement a positive or negative sentiment.\n\n\nWith Siamese networks you’ll be aiming to identify what’s makes two input similar, and what makes them different. Take a look at these two questions. What is your age? And how old are you? When you build a Siamese model, you’re trying to identify the difference or the similarity between these two questions. You do this by computing a single similarity score, representing the relationship between the two questions. And based on that score when compared against a threshold, you can predict whether these two are the same or different.\n\n\nSiamese networks have many applications in NLP, you can use them to authenticate handwritten checks by determining whether two signatures are the same or not. You can use them to identify question duplicates on platforms like Quora or Stack Overflow.\n\n\nAnd you can use them in search engine queries to predict whether a new query is similar to the one that was already executed. These are just a few examples, but there are many more applications of Siamese networks in NLP.\n\nYou can use Siamese networks in many types of NLP applications. In the next video, I’ll walk you through the architecture that is used in this type of model. And I’ll show you how you can use it in a text. I’ll see you in the next video.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Siamese Networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w4/index.html#sec-architecture",
    "href": "notes/c3w4/index.html#sec-architecture",
    "title": "Siamese Networks",
    "section": "Architecture",
    "text": "Architecture\nThe model architecture of a typical siamese network could look as follows:\n\n\n\n\n\n\n\nFigure 3: The architecture of a typical siamese network has two sub-networks consisting of embedding LSTMs and a cosine similarity function that evaluates their outputs.\n\n\nThese two sub-networks are sister-networks which come together to produce a similarity score. Not all Siamese networks will be designed to contain LSTMs. One thing to remember is that sub-networks share identical parameters. This means that we only need to train one set of weights and not two.\nThe output of each sub-network is a vector. We can then run the output through a cosine similarity function to get the similarity score. In the next video, we will talk about the cost function for such a network.\n\n\n\n\n\n\nVideo Transcript\n\n\n\n\n\nSiamese networks have a special type of architecture. They have two identical sub-networks which are merged together through a dense layer to produce a final output or its similarity score. I like to think of these two sub-networks as sister-networks which come together to produce a similarity score.\nIn Figure 3 we can see the model architecture for a Siamese network. Note that the architecture presented here is just an example. Not all Siamese networks will be designed to contain LSTMs.\nOn the left, you have two inputs which represents Question 1 and Question 2. You will take each question, transform it into an embedding and then you’ll run the embedding through an LSTM layer to model the questions meaning. Each LSTM outputs a vector.\nIn this architecture, you have two identical sub-networks.\nOne for Question 1 and the second for Question 2. An important note here is that the sub-networks share identical parameters. That is the learned parameters of each sub-network are exactly the same. So you actually only need to train one sets of weights, not two.\nThen given the two outputs vectors, one corresponding to each question, find their cosine similarity.\n\nWhat is cosine similarity?\nRecall that the cosine similarity is a measure of similarity between two vectors. When two vectors point generally in the same direction, the cosine of the angle between them is near one. For vectors that point in opposite directions, the cosine of the angle between them is minus one. If that sounds unfamiliar don’t worry.\nRight now you just need to know that the cosine similarity tells you how similar two vectors are. In this case, it tells you how similar the two questions are.\nThe cosine similarity gives the Siamese networks prediction, denoted here by the variable y-hat, which will be a value between minus one and positive one.\n\n\nHow do we interpret y-hat and tau?\nIf y-hat is less than or equal to some threshold, tau, then you will say that the input questions are different. If y-hat is greater than tau, then you will say that they are the same.\nThe threshold tau is a parameter that you will choose based on how often you want to interpret cosine similarity to indicate that two questions are similar or not. A higher threshold means that only very similar sentences will be considered similar.\n\n\nHow would we walking through the architecture\nIf you think of this process as a series of steps you take to get from your inputs to your outputs, it would go something like this; you start with a model architecture for a Siamese network made up of two identical sub-networks. In this case, your inputs are questions that you feed into each sub-network and each question gets transformed into an embedding and pass through an LSTM layer. Then you take the outputs of each of the sub-networks and compare them using cosine similarity to get your y-hat.\nAfter seeing the model architecture, I’ll start talking about different cost functions you can use for this type of architecture.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Siamese Networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w4/index.html#sec-lab-siamese",
    "href": "notes/c3w4/index.html#sec-lab-siamese",
    "title": "Siamese Networks",
    "section": "Lab: Creating a Siamese Model using Trax",
    "text": "Lab: Creating a Siamese Model using Trax\nCreating a Siamese Model using Trax",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Siamese Networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w4/index.html#sec-cost-function",
    "href": "notes/c3w4/index.html#sec-cost-function",
    "title": "Siamese Networks",
    "section": "Cost Function",
    "text": "Cost Function\nLet us take a close look at the following slide:\n\n\n\n\n\n\n\nFigure 4: Understanding the triplet loss cost function\n\n\nNote that when trying to compute the cost for a siamese network we use the triplet loss. The triplet loss usually consists of an Anchor and a Positive example. Note that the anchor and the positive example have a cosine similarity score that is very close to one. On the other hand, the anchor and the negative example have a cosine similarity score close to -1. Now we are ideally trying to optimize the following equation: −cos(A,P)+cos(A,N)≤0\nNote that if cos(A,P)=1 is 1 and cos(A,N)=−1, then the equation is definitely less than 0. However, as cos(A,P) deviates from 1 and cos(A,N) deviates from -1, then we can end up getting a cost that is &gt; 0. Here is a visualization that would help we understand what is going on. Feel free to play with different numbers.\n\n\n\n\n\n\n\nFigure 5: A worked example of triplet loss\n\n\n\n\n\n\n\n\nFigure 6: Chart for the loss function\n\n\n\n\n\n\n\n\n\nVideo Transcript\n\n\n\n\n\n\nI’ll now show you a simple loss function you can use in your Siamese network.\nJust as a recap, this is the overall structure of the Siamese network, which enables you to predict whether two questions are similar or different, or the outputs of the network, you are able to calculate y-hat, which is the similarity between the two questions.\nNow, I’ll show you a loss function for a Siamese network.\n\n\nWhat are positive and negative questions?\n\nI’ll starts by looking at this first question, which is, “How old are you?” I’ll call this first question the anchor, which I’m going to use to compare against two other questions relative to the anchor.\n\n\nOther questions that have the same meaning as the anchor are called positive questions. Whereas questions that do not have the same meaning as the anchor are called negative questions.\n\n\nNote that the meaning of positive and negative in the context of finding question duplicates is referring to whether a question is similar to the anchor or not, and not whether it has a positive or negative sentiment.\n\n\n\nWhat is a positive question?\n\nThe question, “What is your age?” is considered a positive question relative to the anchor, because “How old are you?” and “What is your age?” mean the same thing.\n\n\n\nWhat is a negative question?\n\nThis other question, “Where are you from?” is considered a negative question because it does not have the same meaning as the anchor question.\n\n\n\nWhat is cosine similarity?\n\nHere’s a definition of cosine similarity between two vectors. Figure 4 That will be the similarity of function s. To train your model, you’ll be comparing the vectors that are outputs by each sub-network using similarity.\nSo for this example, you’re going to take the similarity between A and P, where A refers to the anchor question, and P refers to the positive question.\nSimilarity is bounded between negative one and one. So for vectors that are completely different, the similarity is near negative one, and for vectors that are nearly identical, there similarity is close to positive one.\nFor a well-trained model, you would like to see a similarity close to one when comparing the anchor and the positive example. Similarly, when comparing the anchor to the negative example, a successful model should yield a similarity close to negative one.\n\n\n\nHow do you compute the loss?\n\nTo begin building a loss function, you start with the similarity of A and N and subtract the similarity of A and P to calculate the difference.\nWhat you have here Figure 6 is a loss function that allows you to determine whether your model is roughly doing what you hope it will do.\nNamely, finding that the anchor and the positive example are similar, and that the anchor and the negative example are different.\nAs the difference gets bigger or smaller along the x-axis, the loss gets bigger or smaller along the y-axis.\nWhen minimizing the loss in training, you are in effect minimizing this difference. You’ve started seeing a difference approach which will allow you to build a different cost function.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Siamese Networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w4/index.html#sec-triplets",
    "href": "notes/c3w4/index.html#sec-triplets",
    "title": "Siamese Networks",
    "section": "Triplets",
    "text": "Triplets\nWe will now build on top of our previous cost function. To get the full cost function we will add a margin.\n\n\n\n\n\n\n\nFigure 7: Adding a margin to the triplet loss\n\n\nNote that we added an α in the equation above. This allows we to have a margin of “safety”.\nWhen computing the full cost, we take the max of that the outcome of −cos(A,P)+cos(A,N)+α and 0. Note, we do not want to take a negative number as a cost.\nHere is a quick summary:\n\n𝜶: controls how far cos(A,P) is from cos(A,N)\nEasy negative triplet: cos(A,N) &lt; cos(A,P)\nSemi-hard negative triplet: $cos(A,N) &lt; cos(A,P) &lt; cos(A,N) + 𝜶 $\nHard negative triplet: cos(A,P) &lt; cos(A,N)\n\n\n\n\n\n\n\nVideo Transcript\n\n\n\n\n\n\nYou will now explore triplets. You’ll see how you can build pairs of inputs. Rather than just classifying what’s an input is, you’re going to build something that will allow you to identify the difference between two inputs. Let’s see how this works.\nHere are three questions where the first one, how old are you, is the anchor. The second one is a positive example, what is your age? The third one, where are you from? is a negative example.\nHaving the three components here is what gives rise to the name triplets, which is to say, an anchor being used in conjunction with a positive and negative pairing. Accordingly, triplet loss is the name for a loss function that uses three components. The intuition behind this simple function is to minimize the difference between the similarity of A and N, and the similarity of A and P. You already know that, as the difference gets bigger or smaller along the x axis, the loss gets bigger or smaller along the y axis.\n\n\nDo we want the loss to be less than zero?\n\nBut notice, when the difference is less than zero, do you also want the loss to be less than zero? Let’s think about this for a moment.\nIf you gave the model a positive loss value, the model uses this to update its weight to improve.\nIf you gave the model a negative loss value, this is like telling the model, “Good job. Please update your weight to the worst next time.”\nSo you don’t actually want to give the model a loss value that’s less than zero. In other words, when the model is doing a good job, you don’t want it to undo a its update. To make sure that the model doesn’t update itself to do worse, you can modify the loss so that whenever the diff is less than zero, the loss should just be zero. When the loss is zero, we’re effectively not asking the model to update it’s weights, because it is performing as expected for that training example. The loss-function now cannot take on negative values. If the difference is less than or equal to 0, the loss is 0. If the difference is greater than 0, then the loss is equal to the difference.\nNotice the non-linearity happens at the origin of this line chart.\nBut you might also wonder what’s happens when the model is correct but only by a tiny bits? The model is still correct if the difference is a tiny number, that is less than zero.\nWhat if you want the model to still learn from this example, and ask it to predict a wider difference for this training example?\nYou can think of shifting this loss function a little to the left, by a margin that we’ll refer to as Alpha. Let’s say we chose Alpha to be 0.2, if the difference between similarities is very small, like negative 0.1, then if you add it to the Alpha of 0.2, the result is still greater than 0. The sum of the diff plus Alpha can be considered a positive loss that tells the model to learn from this example.\nYou can see this visually in the line chart.  The loss function is shifted to the left by the amount Alpha. The diff is along the horizontal axis. When the difference is less than zero but small in magnitude, the loss is greater than zero. So if the difference is smaller in magnitude than Alpha, then there is still a loss. This loss tells the model that it can still improve and learn from this training example. Triplet loss, as the difference with a margin Alpha, is what you will implement in the assignments which you will code like this, which is the triplet loss function for A, P and N. A small detail worth noting.\nIn these explanations, I’ve been using similarity because that’s what will be used in the programming assignments, so similarity of v_1, v_2.\nBut if you were to read the literature, you might find d of v_1, v_2 used also, where this d could be any function that calculates the distance between two vectors. A distance metric is the mirror image of a similarity metric, and a similarity metric can be derived from a distance metric.\nOne example of a distance metric is Euclidean distance.\n\n\n\nHow do we pick good triplets?\n\nSelecting triplets A, P, and N for training involves two steps; first, select a pair of questions that are known to be duplicates to serve as the anchor and positive, and you’ll do this from the training set; second, select a question that is known to be difference in meaning from the anchor, to form the anchor and the negative pair.\n\n\n\nWhy not use random triplets?\n\nIf you were to select triplets at random, you’d be likely to select non-duplicative pairs A and N, where the loss is 0.\nThe loss is zero whenever the model correctly predicts that A and P are more similar relative to A and N.\nWhen the loss is 0, the network has nothing more to learn from the triplets example. So we can train more efficiently if we choose triplets that show the model when it’s incorrect, so that’s just going to adjust it’s weight and improve.\n\n\n\nWhat are hard triplets?\n\nInstead of selecting random triplets, you’ll specifically select so-called hard triplets. That is, triplets that are more difficult to train on. Hard triplets are those where the similarity between anchor and negative is very close to, but still smaller than the similarity between anchor and positive. When the model encounters a hard triplet, the learning algorithm needs to adjust its weight, so that’s it’s going to yield similarities that line up with the real-world labels. So by selecting hard triplets, focusing the training on doing better, on the difficult cases, that it’s predicting incorrectly.\n\n\nI spoke about easy and hard triplets. I also spoke about a margin. In the next video, you’ll see how all these concepts come together to help us create a cost function.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Siamese Networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w4/index.html#sec-computing-cost-1",
    "href": "notes/c3w4/index.html#sec-computing-cost-1",
    "title": "Siamese Networks",
    "section": "Computing the Cost I",
    "text": "Computing the Cost I\nTo compute the cost, we will prepare the batches as follows:\n\n\n\n\n\n\n\nFigure 8: An example batch of question pairs\n\n\nNote that each example, has a similar example to its right, but no other example means the same thing. We will now introduce hard negative mining.\n\n\n\n\n\n\n\nFigure 9: Hard negative mining\n\n\nEach horizontal vector corresponds to the encoding of the corresponding question. Now when we multiply the two matrices and compute the cosine, we get the following:\n\n\n\n\n\n\n\nFigure 10: Understanding Cost matrix for a batch of question pairs\n\n\nThe diagonal line corresponds to scores of similar sentences, (normally they should be positive). The off-diagonals correspond to cosine scores between the anchor and the negative examples.\n\n\n\n\n\n\nVideo Transcript\n\n\n\n\n\nWelcome back. As promised, you’ll see how everything fits together now. You will start by building a cost function, and then you will use gradient descent to optimize this cost function. Let’s take a look at how this works. To compute the cost, begin by preparing the data in batches. Here you have the questions, what is your age and how old are you? You can see these are duplicates, because they mean the same thing. Can you see me and are you seeing me? Are also duplicates. Where are thou and where are you? Are duplicates too. As are, when is the game and what time is the game? So with four pairs, you have batch size of four. Here we will use the letter b to stand for batch size. Something that’s very important to note is that each question has its corresponding duplicate to the left or right of it. That is, in each row all of the sentences in the columns are duplicates. But you will notice that each question has no duplicates above or below it. That is, for any column, none of the rows in those column contain a sentence that is a duplicate of another sentence in those column. So this is how you prepare the batches. Now, let me show you how you will want to organize the data in this way. Given the first batch, you’re going to run it through this model to get a vector v_1 with dimensions one row by five columns. The number of columns shown in this matrix is equal to the dimension of your embedding layer, which in this case is five. I’ll refer to this dimension of the embedding layer as d model for each question in the batch. I haven’t talked about the dimension of the embedding layer yet, but don’t worry, it will become more clear once you’re working with the code. The important takeaway is that the dimension of the embedding, the model, is a parameter that determines the dimensions of the weights through each layer in the model, and thus determines the size of the outputs vector. The model is running a batch size that is greater than one. So the v_1 outputs is actually a matrix of stacked vectors like this. In this visual example, there are four rows in this matrix to indicate that there are four observations in this batch. The batch size is four. Our subscript to observations in the batch as v_1_1, v_1_2, and so on corresponding to the vector outputs for each question in the batch. You’ll do the same thing for the batch of v_2 vectors. Each question in the batch 1 is a duplicate of its corresponding question in batch 2. But none of the questions in batch 1 are duplicates of each other. The same applies to batch 2. Here, for example, v_1_1 is a duplicate of v_2_1, as are the rest of the respective row pairs. But v_1_1 is not a duplicates of any other rows in v_1. The last step is to combine the two branches of the Siamese network by calculating the similarity between all vector pair combinations of v_1 with v_2. For this example with a batch size of four, you might get a matrix of similarities that looks like this. The diagonal is a key feature here. These values are the similarities for all your positive examples, the question duplicates. Notice that all the values are generally greater than the numbers in the off diagonals. So the model is performing as you would expect for duplicates questions, because you would expect that the question duplicates to have higher similarity compared to the non-duplicates. In the upper right and lower left, you have the similarities for all the negative examples. These are the results for the non-duplicates pairs. Notice that most of these numbers are lower than the similarities that’s are along the diagonal. Also notice that you can have negative example question pairs that still have a similarity greater than zero. The range of similarity ranges from negative 1 to positive 1, but there isn’t any special requirements that a similarity greater than zero indicates duplicates or that’s a similarity less than zero indicates non-duplicates. What’s matters for a properly functioning model is that it generally finds that duplicates have a higher similarity relative to non-duplicates. Creating non-duplicates pairs like this removes the need for additional non-duplicate examples and the input data, which turns out to be a big deal. Instead of needing to sets up specific batches with negative examples, your model can learn from them in the existing question duplicates batches. Now, you can just stop here and use these similarities with the triplet loss function you already know shown here. Then the overall costs for your Siamese network will be the sum of these individual losses over the training sets. Here you can see that superscripts i refers to a specific training example and there are m observations, but there are more techniques available that’s can vastly improve upon model performance. I’ll show you those next.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Siamese Networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w4/index.html#sec-computing-cost-2",
    "href": "notes/c3w4/index.html#sec-computing-cost-2",
    "title": "Siamese Networks",
    "section": "Computing the Cost II",
    "text": "Computing the Cost II\nNow that we have the matrix with cosine similarity scores, which is the product of two matrices, we go ahead and compute the cost.\n\n\n\n\n\n\n\nFigure 11\n\n\nWe now introduce two concepts, the mean_neg, which is the mean negative of all the other off diagonals in the row, and the closest_neg, which corresponds to the highest number in the off diagonals.\n\nCost = \\max(−\\cos(A,P)+\\cos(A,N)+α,0)\n\nSo we will have two costs now:\n\nCost_1 = \\max(−\\cos(A,P)+ mean_n eg + α,0)\n\n\nCost_2 = \\max(−\\cos(A,P)+ closest_n eg + α,0)\n ⁡\nThe full cost is defined as: Cost1 + Cost2.\n\n\n\n\n\n\nVideo Transcript\n\n\n\n\n\n\nPreviously, you set up the training data into two specific batches, each batch containing no duplicate questions within it. You ran those batches through one sub network each. And that’s produced a vector of supports per question. Which has dimension 1 by d_model, where d_model is the embedding dimension. And is equal to the number of columns in the matrix, which is five, at least in this example. The v_1 vectors for a single batch are stuck together. And in this case, the batch size is the number of rows shown in this matrix, which is four. You can see a similar batch of v_2 vectors as well. The last step was to combine the two branches of the Siamese network. By calculating the similarity between all vector pair combinations of the v_1 vectors and v_2 vectors. For this example with a batch size of four, that last step would produce a matrix of similarities that looks something like this.\n\n\nWhat are the attributes of this matrix?\n\nThis matrix has some important attributes. The similarities along the green diagonal contain similarities for the duplicate questions. For a well trained model, these values should be greater than similarities for the off-diagonals. Reflecting the fact that the network produces similar vector outputs for duplicate questions. The orange values in the upper right and lower left are similarities for the non duplicate questions.\nNow this is where things get really interesting. You can use this off diagonal information to make some modifications to the loss function and really improve your models performance. To do so, I’m going to make use of two concepts.\n\n\n\nWhat is the mean negative?\n\nThe first concept is the mean negative, which is just the mean or average of all the off-diagonal values in each row. Notice that off-diagonal elements can still be positive numbers. So when I say mean negative, I’m referring to the mean of the similarity for negative examples, not the mean of negative numbers in a row.\nFor example, the mean negative of the first row is just the mean of all the off-diagonal values in that row.\nIn this case, -0.8, 0.3 and -0.5, excluding the value 0.9, which is on the diagonal. You can use the mean negative to help speed up training by modifying the loss function, which I’ll show you soon.\n\n\n\nWhat is the closest negative?\n\nThe next concept is what’s called the closest negative. As mentioned earlier, because of the way you define the triplet loss function, you’ll need to choose so called hard triplets to train on. What this means is that for training, you want to choose triplets where the cosine similarity of the negative example is close to the similarity of the positive example.\nThis forces your model to learn what differentiates these examples and ultimately drive those similarity values further apart through training. To do this, you’ll search each row in your output matrix for the closest negative. Which is to say the off diagonal value which is closest to, but still less than the value on the on diagonal for that row. So in this first row, the value on the diagonal is 0.9. So the closest off-diagonal elements in this case is 0.3. What this means is that this negative example with a similarity of 0.3 has the most to offer your model in terms of learning opportunity.\n\n\n\nHow do we use these new concepts ?\n\nTo make use of these new concepts, recall that the triplet loss was defined as the max of the similarity of A and N minus the similarity of A and B plus the margin alpha and 0. Also recall that we refer to the difference between the two similarities with the variable named diff.\nHere, we’re just writing out the definition of diff. So in order to minimize the loss you want this diff plus the margin alpha to be less than or equal to 0. I’ll introduce loss 1 to be the max of the mean negative minus the similarity of A and P plus alpha and 0. The change between the formulas for triplet loss and loss 1 is the replacement of similarity of A and N. With the mean negative, this helps the model converge faster during training by reducing noise. It reduces noise by training on just the average of several observations, rather than training the model on each of these off-diagonal examples.\nSo why does taking the average of several observations usually reduce noise? Well, we define noise to be a small value that comes from a distribution that is centered around 0. So in other words, the average of several noise values is usually 0. So if we took the average of several examples, this has the effect of cancelling out the individual noise from those observations. Then loss 2 will be the max of the closest negative minus the similarity of A and B plus alpha and 0.\nThe difference between the formulas this time is the replacement of the cosine of A and N. With the closest negative, this helps create a slightly larger penalty by diminishing the effects of the otherwise more negative similarity of A and N that it replaces.\nYou can think of the closest negative as finding the negative example that results in the smallest difference between the two cosine similarities. If you had that small difference to alpha, then you’re able to generate the largest loss among all of the other examples in that row.\nBy focusing the training on the examples that produce higher loss values, you make the model update its weights more.\nTo learn from these more difficult examples, then you can define the full loss as loss 1 + loss 2. And you will use this new full loss as an improved triplet loss in the assignments. The overall costs for your Siamese network will be the sum of these individual losses over the training sets.\n\n\nIn the next video, you will use this cost function in one shot learning. One shot learning is a very effective technique that can save you a lot of time when comparing the authenticity of checks or of any other type of inputs",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Siamese Networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w4/index.html#sec-lab-modified",
    "href": "notes/c3w4/index.html#sec-lab-modified",
    "title": "Siamese Networks",
    "section": "Lab: Lecture Notebook: Modified Triplet Loss",
    "text": "Lab: Lecture Notebook: Modified Triplet Loss\nModified Triplet Loss",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Siamese Networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w4/index.html#sec-one-shot",
    "href": "notes/c3w4/index.html#sec-one-shot",
    "title": "Siamese Networks",
    "section": "One Shot Learning",
    "text": "One Shot Learning\nImagine we are working in a bank and we need to verify the signature of a check. We can either build a classifier with K possible signatures as an output or we can build a classifier that tells we whether two signatures are the same.\n\n\n\n\n\n\n\nFigure 12: Classification vs one shot learning\n\n\nHence, we resort to one shot learning. Instead of retraining your model for every signature, we can just learn a similarity score as follows:\n\n\n\n\n\n\n\nFigure 13\n\n\n\n\n\n\n\n\nVideo Transcript\n\n\n\n\n\n\nLet’s say that you’re trying to identify whether the author for a certain poem is Lucas or not. You can either take all of Lucas’ poems and put them into datasets, and instead of predicting K classes, you will now predict K plus 1 classes. All the previous poems of other authors plus Lucas’, so that’s why it’s k plus 1. Or you can compare one of Lucas’ poems to another poem, and that is where one-shot learning comes in. In this video, I’ll show you how you can do that. To understand the difference between classification and one-shot learning, first consider identifying or classifying signatures based on one through K possible classes. You might use some classification model trained on the K classes, probably with a softmax function at the end to find the maximum probability. Then at recognition time, classify the input signature to one of those corresponding classes. That’s great if you have a signature list that’s rarely changes. But what if you get a new signature to classify? It would be expensive to retrain the model every time this happens, and besides, unless you have a great many examples of that new signature, model training won’t work very well. In one-shot learning, you need to be able to recognize a signature repeatedly from just one example. You can do this with a learned similarity function. Then you can test a similarity score against some threshold to see if two signatures are the same. So the problem changes to determining which class to instead measuring similarity between two classes. This is very useful, especially in banks, for example. Every time there’s a new signature, you can’t retrain your entire system to classify the signatures into K possible outputs. So instead, you just learn a similarity function that can be used to calculate a similarity score. That can in turn be used to identify whether two signatures are the same. You already did this using cosine similarity as the similarity function. If the result was greater than some threshold Tau, you determine the inputs to be the same. In the case of comparing signatures, if the similarity is less than or equal to Tau, then the signatures are different. In this video, I spoke about one-shot learning and I told you why it is a very effective technique. One-shot learning makes use of Siamese networks. In the next video, I’ll show you how you can train and test your Siamese network.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Siamese Networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w4/index.html#sec-training-testing",
    "href": "notes/c3w4/index.html#sec-training-testing",
    "title": "Siamese Networks",
    "section": "Training and Testing",
    "text": "Training and Testing\nAfter preparing the batches of vectors, we can proceed to multiplying the two matrices.\nHere is a quick recap of the first step:\n\n\n\n\n\n\n\nFigure 14: Preparing batches of questions\n\n\nThe next step is to implement the siamese model as follows:\n\n\n\n\n\n\n\nFigure 15: Reviewing the architecture of the siamese networks\n\n\nFinally when testing:\n\nConvert two inputs into an array of numbers\nFeed it into your model\nCompare 𝒗_1,𝒗_2 using cosine similarity\nTest against a threshold \\tau\n\n\n\n\n\n\n\nVideo Transcript\n\n\n\n\n\n\nIn this video, you’re goingto see what the dataset would look like for a Siamese network. I’ll show you how you can train your model and then you can use that model to test your Siamese network. Let’s take a look at how you can do this. You’ll be using the Quora question duplicates datasets for this week’s programming assignment. It looks like this. It consists of a collection of question pairs within its duplicates Boolean for each question. For example, for Question 1 and Question 2, “What is your age?” and “How old are you?” Its duplicate equals “true” because these two questions are duplicates. “Where are you from?” and “Where are you going?” are not duplicates, so it’s false and so on. This dataset gives your model plenty of examples to learn from. First, you will process the dataset so that it looks like this. You will pre-process the data into batches of size b. The corresponding questions from each batch are duplicates. For example, the first question in Batch 1, “What is your age?” is a duplicate of the first question in Batch 2, “How old are you?” The second question in Batch 1 is a duplicate of the second question in Batch 2 and so on.\nNote however, that there are no duplicates within an individual batch. If I call this q1_a, this q2_a, then q1_a and q2_a are duplicates. If this was q1_b and this was q2_b, then q1_b and q2_b are duplicates. However, q1_a and q1_b are not duplicates. Similarly, q2_a and q2_b are not duplicates. I’ll show you how to prepare the batches in such a way that no question within the same batch is duplicated. Finally, you’ll use these inputs to get outputs vectors for each batch. Then, you can calculate the cosine similarity between each pair of output vectors. This is the Siamese model that you’ll be implementing in the assignment. You’ll create a subnetwork, which is then duplicated and drawn in parallel. In each subnetwork, you got the embedding, run it through the LSTM, take your vector output, and then use them to find the cosine similarity.\n\n\nAn important note here, is that the learned parameters of the subnetworks are exactly the same between the two subnetworks. So you are actually only training one sets of weights, not two. When testing the model, you will perform one-shot learning. The goal is to find a similarity score between two inputs questions. First, convert each input into an array of numbers. Feed these into your model. Compare the subnetwork outputs v_1 and v_2 using cosine similarity for a similarity score. Then, test the score against some threshold Tau, and if the cosine similarity is greater than Tau, then the questions are classified as duplicates.\n\n\nNote that both $tau$ and the margin \\alpha from the last function are tunable hyperparameters.\n\n\nCongratulations, you now know how to train your Siamese network and you know how to test it. In this week’s programming exercise, you’ll be using a Siamese network to identify whether a question is a duplicate or not. Specifically, you’ll be using the Quora question duplicate data sets, and using that, you’ll be able to get a very good accuracy.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Siamese Networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w4/index.html#sec-lab-evaluate",
    "href": "notes/c3w4/index.html#sec-lab-evaluate",
    "title": "Siamese Networks",
    "section": "Lab: Evaluate a Siamese Model",
    "text": "Lab: Evaluate a Siamese Model\nEvaluate a Siamese Model",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Siamese Networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w4/index.html#sec-reflections",
    "href": "notes/c3w4/index.html#sec-reflections",
    "title": "Siamese Networks",
    "section": "Reflections",
    "text": "Reflections\n\nCould we improve the model by Can we give each LSTM it own loss function?\nWhat are some typical use cases for using Siamese networks?\n\nFace recognition. The faces are very different from each other but we don’t want to retrain the model for every new face - we typically want to check if a face is the same as one of the faces on file.\nSignature verification is a good example of a use case for Siamese networks. We want to check if a signatures we get is sufficiently similar to the few samples we have on file.\nOne shot learning is another area where Siamese networks could be useful.\n\nIn which NLP tasks are Siamese networks utilized?\n\nSearch engine queries are often challenging since they are very brief compared to the documents they are searching for, and so they tend to miss the best variation for some query. Also the distribution has many similar queries and a long tail of unique queries.\nQuestion-Answering sites like Stack overflow or Quora is another place where Siamese networks could be useful. The crowd sourcing works better if the questions are not repeated so that all the answers are in one place.\nParaphrase detection is another area where Siamese networks could be useful. The paraphrase could be a completely different sentence but the meaning remains the same.\nSpam detection is another area where Siamese networks could be useful. The spammer could change the words in the spam message but the meaning remains the same.\n\nHow can we improve the model by using a different similarity function?\nAre there any benefits to use Location sensitive hashing with the cosine similarity function ?",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Siamese Networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w4/index.html#sec-acknowledgments",
    "href": "notes/c3w4/index.html#sec-acknowledgments",
    "title": "Siamese Networks",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\n(Chadha 2020) Aman Chadha’s Notes\nIbrahim Jelliti’s Notes",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Siamese Networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w2/lab03.html",
    "href": "notes/c3w2/lab03.html",
    "title": "Vanilla RNNs, GRUs and the scan function",
    "section": "",
    "text": "course banner\nIn this notebook, you will learn how to define the forward method for vanilla RNNs and GRUs. Additionally, you will see how to define and use the function scan to compute forward propagation for RNNs.\nBy completing this notebook, you will:\nimport numpy as np\nfrom numpy import random\nfrom time import perf_counter\nAn implementation of the sigmoid function is provided below so you can use it in this notebook.\ndef sigmoid(x): # Sigmoid function\n    return 1.0 / (1.0 + np.exp(-x))",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "L3 - Vanilla RNNs, GRUs and the scan function"
    ]
  },
  {
    "objectID": "notes/c3w2/lab03.html#forward-method-for-vanilla-rnns",
    "href": "notes/c3w2/lab03.html#forward-method-for-vanilla-rnns",
    "title": "Vanilla RNNs, GRUs and the scan function",
    "section": "1.1 Forward method for vanilla RNNs",
    "text": "1.1 Forward method for vanilla RNNs\nThe vanilla RNN cell is quite straight forward. Its most general structure is presented in the next figure:\n\nAs you saw in the lecture videos, the computations made in a vanilla RNN cell are equivalent to the following equations:\n\\begin{equation}\nh^{&lt;t&gt;}=g(W_{h}[h^{&lt;t-1&gt;},x^{&lt;t&gt;}] + b_h)\n\\label{eq: htRNN}\n\\end{equation}\n\\begin{equation}\n\\hat{y}^{&lt;t&gt;}=g(W_{yh}h^{&lt;t&gt;} + b_y)\n\\label{eq: ytRNN}\n\\end{equation}\nwhere [h^{&lt;t-1&gt;},x^{&lt;t&gt;}] means that h^{&lt;t-1&gt;} and x^{&lt;t&gt;} are concatenated together. In the next cell we provide the implementation of the forward method for a vanilla RNN.\n\ndef forward_V_RNN(inputs, weights): # Forward propagation for a a single vanilla RNN cell\n    x, h_t = inputs\n\n    # weights.\n    wh, _, _, bh, _, _ = weights\n\n    # new hidden state\n    h_t = np.dot(wh, np.concatenate([h_t, x])) + bh\n    h_t = sigmoid(h_t)\n\n    return h_t, h_t\n\nAs you can see, we omitted the computation of \\hat{y}^{&lt;t&gt;}. This was done for the sake of simplicity, so you can focus on the way that hidden states are updated here and in the GRU cell.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "L3 - Vanilla RNNs, GRUs and the scan function"
    ]
  },
  {
    "objectID": "notes/c3w2/lab03.html#forward-method-for-grus",
    "href": "notes/c3w2/lab03.html#forward-method-for-grus",
    "title": "Vanilla RNNs, GRUs and the scan function",
    "section": "1.2 Forward method for GRUs",
    "text": "1.2 Forward method for GRUs\nA GRU cell have more computations than the ones that vanilla RNNs have. You can see this visually in the following diagram:\n\nAs you saw in the lecture videos, GRUs have relevance \\Gamma_r and update \\Gamma_u gates that control how the hidden state h^{&lt;t&gt;} is updated on every time step. With these gates, GRUs are capable of keeping relevant information in the hidden state even for long sequences. The equations needed for the forward method in GRUs are provided below:\n\\begin{equation}\n\\Gamma_r=\\sigma{(W_r[h^{&lt;t-1&gt;}, x^{&lt;t&gt;}]+b_r)}\n\\end{equation}\n\\begin{equation}\n\\Gamma_u=\\sigma{(W_u[h^{&lt;t-1&gt;}, x^{&lt;t&gt;}]+b_u)}\n\\end{equation}\n\\begin{equation}\nc^{&lt;t&gt;}=\\tanh{(W_h[\\Gamma_r*h^{&lt;t-1&gt;},x^{&lt;t&gt;}]+b_h)}\n\\end{equation}\n\\begin{equation}\nh^{&lt;t&gt;}=\\Gamma_u*c^{&lt;t&gt;}+(1-\\Gamma_u)*h^{&lt;t-1&gt;}\n\\end{equation}\nIn the next cell, please implement the forward method for a GRU cell by computing the update u and relevance r gates, and the candidate hidden state c.\n\ndef forward_GRU(inputs, weights): # Forward propagation for a single GRU cell\n    x, h_t = inputs\n\n    # weights.\n    wu, wr, wc, bu, br, bc = weights\n\n    # Update gate\n    ### START CODE HERE (1-2 lINES) ###\n    u = np.dot(wu, np.concatenate([h_t, x])) + bu\n    u = sigmoid(u)\n    ### END CODE HERE ###\n    \n    # Relevance gate\n    ### START CODE HERE (1-2 lINES) ###\n    r = np.dot(wr, np.concatenate([h_t, x])) + br\n    r = sigmoid(u)\n    ### END CODE HERE ###\n    \n    # Candidate hidden state \n    ### START CODE HERE (1-2 lINES) ###\n    c = np.dot(wc, np.concatenate([r * h_t, x])) + bc\n    c = np.tanh(c)\n    ### END CODE HERE ###\n    \n    # New Hidden state h_t\n    h_t = u* c + (1 - u)* h_t\n    return h_t, h_t\n\nRun the following cell to check your implementation.\n\nforward_GRU([X[1],h_0], weights)[0]\n\narray([[ 9.77779014e-01],\n       [-9.97986240e-01],\n       [-5.19958083e-01],\n       [-9.99999886e-01],\n       [-9.99707004e-01],\n       [-3.02197037e-04],\n       [-9.58733503e-01],\n       [ 2.10804828e-02],\n       [ 9.77365398e-05],\n       [ 9.99833090e-01],\n       [ 1.63200940e-08],\n       [ 8.51874303e-01],\n       [ 5.21399924e-02],\n       [ 2.15495959e-02],\n       [ 9.99878828e-01],\n       [ 9.77165472e-01]])\n\n\nExpected output:\narray([[ 9.77779014e-01],\n       [-9.97986240e-01],\n       [-5.19958083e-01],\n       [-9.99999886e-01],\n       [-9.99707004e-01],\n       [-3.02197037e-04],\n       [-9.58733503e-01],\n       [ 2.10804828e-02],\n       [ 9.77365398e-05],\n       [ 9.99833090e-01],\n       [ 1.63200940e-08],\n       [ 8.51874303e-01],\n       [ 5.21399924e-02],\n       [ 2.15495959e-02],\n       [ 9.99878828e-01],\n       [ 9.77165472e-01]])",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "L3 - Vanilla RNNs, GRUs and the scan function"
    ]
  },
  {
    "objectID": "notes/c3w2/lab02.html",
    "href": "notes/c3w2/lab02.html",
    "title": "Working with JAX numpy and calculating perplexity: Ungraded Lecture Notebook",
    "section": "",
    "text": "course banner\nNormally you would import numpy and rename it as np.\nHowever in this week’s assignment you will notice that this convention has been changed.\nNow standard numpy is not renamed and trax.fastmath.numpy is renamed as np.\nThe rationale behind this change is that you will be using Trax’s numpy (which is compatible with JAX) far more often. Trax’s numpy supports most of the same functions as the regular numpy so the change won’t be noticeable in most cases.\nimport jax\nprint(jax.__version__)\n\n0.5.0\nimport numpy\nimport trax\nimport trax.fastmath.numpy as np\n\n# Setting random seeds\nfrom trax import fastmath\nseed=32\nrng = fastmath.random.get_prng(seed)\n#trax.supervised.trainer_lib.init_random_number_generators(32)\nnumpy.random.seed(32)\n\n2025-02-10 16:58:32.288612: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1739199512.303899  125772 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1739199512.308311  125772 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nOne important change to take into consideration is that the types of the resulting objects will be different depending on the version of numpy. With regular numpy you get numpy.ndarray but with Trax’s numpy you will get jax.interpreters.xla.DeviceArray. These two types map to each other. So if you find some error logs mentioning DeviceArray type, don’t worry about it, treat it like you would treat an ndarray and march ahead.\nYou can get a randomized numpy array by using the numpy.random.random() function.\nThis is one of the functionalities that Trax’s numpy does not currently support in the same way as the regular numpy.\nnumpy_array = numpy.random.random((5,10))\nprint(f\"The regular numpy array looks like this:\\n\\n {numpy_array}\\n\")\nprint(f\"It is of type: {type(numpy_array)}\")\n\nThe regular numpy array looks like this:\n\n [[0.85888927 0.37271115 0.55512878 0.95565655 0.7366696  0.81620514\n  0.10108656 0.92848807 0.60910917 0.59655344]\n [0.09178413 0.34518624 0.66275252 0.44171349 0.55148779 0.70371249\n  0.58940123 0.04993276 0.56179184 0.76635847]\n [0.91090833 0.09290995 0.90252139 0.46096041 0.45201847 0.99942549\n  0.16242374 0.70937058 0.16062408 0.81077677]\n [0.03514717 0.53488673 0.16650012 0.30841038 0.04506241 0.23857613\n  0.67483453 0.78238275 0.69520163 0.32895445]\n [0.49403187 0.52412136 0.29854125 0.46310814 0.98478429 0.50113492\n  0.39807245 0.72790532 0.86333097 0.02616954]]\n\nIt is of type: &lt;class 'numpy.ndarray'&gt;\nYou can easily cast regular numpy arrays or lists into trax numpy arrays using the trax.fastmath.numpy.array() function:\ntrax_numpy_array = np.array(numpy_array)\nprint(f\"The trax numpy array looks like this:\\n\\n {trax_numpy_array}\\n\")\nprint(f\"It is of type: {type(trax_numpy_array)}\")\n\nThe trax numpy array looks like this:\n\n [[0.8588893  0.37271115 0.55512875 0.9556565  0.7366696  0.81620514\n  0.10108656 0.9284881  0.60910916 0.59655344]\n [0.09178413 0.34518623 0.6627525  0.44171348 0.5514878  0.70371246\n  0.58940125 0.04993276 0.56179184 0.7663585 ]\n [0.91090834 0.09290995 0.9025214  0.46096042 0.45201847 0.9994255\n  0.16242374 0.7093706  0.16062407 0.81077677]\n [0.03514718 0.5348867  0.16650012 0.30841038 0.04506241 0.23857613\n  0.67483455 0.7823827  0.69520164 0.32895446]\n [0.49403188 0.52412134 0.29854125 0.46310815 0.9847843  0.50113493\n  0.39807245 0.72790533 0.86333096 0.02616954]]\n\nIt is of type: &lt;class 'jaxlib.xla_extension.ArrayImpl'&gt;\nHope you now understand the differences (and similarities) between these two versions and numpy. Great!\nThe previous section was a quick look at Trax’s numpy. However this notebook also aims to teach you how you can calculate the perplexity of a trained model.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "L2 - Calculating Perplexity"
    ]
  },
  {
    "objectID": "notes/c3w2/lab02.html#calculating-perplexity",
    "href": "notes/c3w2/lab02.html#calculating-perplexity",
    "title": "Working with JAX numpy and calculating perplexity: Ungraded Lecture Notebook",
    "section": "Calculating Perplexity",
    "text": "Calculating Perplexity\nThe perplexity is a metric that measures how well a probability model predicts a sample and it is commonly used to evaluate language models. It is defined as:\nP(W) = \\sqrt[N]{\\prod_{i=1}^{N} \\frac{1}{P(w_i| w_1,...,w_{n-1})}}\nAs an implementation hack, you would usually take the log of that formula (to enable us to use the log probabilities we get as output of our RNN, convert exponents to products, and products into sums which makes computations less complicated and computationally more efficient). You should also take care of the padding, since you do not want to include the padding when calculating the perplexity (because we do not want to have a perplexity measure artificially good). The algebra behind this process is explained next:\nlog P(W) = {log\\big(\\sqrt[N]{\\prod_{i=1}^{N} \\frac{1}{P(w_i| w_1,...,w_{n-1})}}\\big)}\n = {log\\big({\\prod_{i=1}^{N} \\frac{1}{P(w_i| w_1,...,w_{n-1})}}\\big)^{\\frac{1}{N}}}\n = {log\\big({\\prod_{i=1}^{N}{P(w_i| w_1,...,w_{n-1})}}\\big)^{-\\frac{1}{N}}}   = -\\frac{1}{N}{log\\big({\\prod_{i=1}^{N}{P(w_i| w_1,...,w_{n-1})}}\\big)}   = -\\frac{1}{N}{\\big({\\sum_{i=1}^{N}{logP(w_i| w_1,...,w_{n-1})}}\\big)} \nYou will be working with a real example from this week’s assignment. The example is made up of: - predictions : batch of tensors corresponding to lines of text predicted by the model. - targets : batch of actual tensors corresponding to lines of text.\n\nfrom trax import layers as tl\n\n# Load from .npy files\npredictions = numpy.load('predictions.npy')\ntargets = numpy.load('targets.npy')\n\n# Cast to jax.interpreters.xla.DeviceArray\npredictions = np.array(predictions)\ntargets = np.array(targets)\n\n# Print shapes\nprint(f'predictions has shape: {predictions.shape}')\nprint(f'targets has shape: {targets.shape}')\n\npredictions has shape: (32, 64, 256)\ntargets has shape: (32, 64)\n\n\nNotice that the predictions have an extra dimension with the same length as the size of the vocabulary used.\nBecause of this you will need a way of reshaping targets to match this shape. For this you can use trax.layers.one_hot().\nNotice that predictions.shape[-1] will return the size of the last dimension of predictions.\n\nreshaped_targets = tl.one_hot(targets, predictions.shape[-1]) #trax's one_hot function takes the input as one_hot(x, n_categories, dtype=optional)\nprint(f'reshaped_targets has shape: {reshaped_targets.shape}')\n\nreshaped_targets has shape: (32, 64, 256)\n\n\nBy calculating the product of the predictions and the reshaped targets and summing across the last dimension, the total log perplexity can be computed:\n\ntotal_log_ppx = np.sum(predictions * reshaped_targets, axis= -1)\n\nNow you will need to account for the padding so this metric is not artificially deflated (since a lower perplexity means a better model). For identifying which elements are padding and which are not, you can use np.equal() and get a tensor with 1s in the positions of actual values and 0s where there are paddings.\n\nnon_pad = 1.0 - np.equal(targets, 0)\nprint(f'non_pad has shape: {non_pad.shape}\\n')\nprint(f'non_pad looks like this: \\n\\n {non_pad}')\n\nnon_pad has shape: (32, 64)\n\nnon_pad looks like this: \n\n [[1. 1. 1. ... 0. 0. 0.]\n [1. 1. 1. ... 0. 0. 0.]\n [1. 1. 1. ... 0. 0. 0.]\n ...\n [1. 1. 1. ... 0. 0. 0.]\n [1. 1. 1. ... 0. 0. 0.]\n [1. 1. 1. ... 0. 0. 0.]]\n\n\nBy computing the product of the total log perplexity and the non_pad tensor we remove the effect of padding on the metric:\n\nreal_log_ppx = total_log_ppx * non_pad\nprint(f'real perplexity still has shape: {real_log_ppx.shape}')\n\nreal perplexity still has shape: (32, 64)\n\n\nYou can check the effect of filtering out the padding by looking at the two log perplexity tensors:\n\nprint(f'log perplexity tensor before filtering padding: \\n\\n {total_log_ppx}\\n')\nprint(f'log perplexity tensor after filtering padding: \\n\\n {real_log_ppx}')\n\nlog perplexity tensor before filtering padding: \n\n [[ -5.396545    -1.0311184   -0.66916656 ... -22.37673    -23.18771\n  -21.843483  ]\n [ -4.5857706   -1.1341286   -8.538033   ... -20.15686    -26.837097\n  -23.57502   ]\n [ -5.2223887   -1.2824144   -0.17312431 ... -21.328228   -19.854412\n  -33.88444   ]\n ...\n [ -5.396545   -17.291681    -4.360766   ... -20.825802   -21.065838\n  -22.443115  ]\n [ -5.9313164  -14.247417    -0.2637329  ... -26.743248   -18.38433\n  -22.355278  ]\n [ -5.670536    -0.10595131   0.         ... -23.332523   -28.087376\n  -23.878807  ]]\n\nlog perplexity tensor after filtering padding: \n\n [[ -5.396545    -1.0311184   -0.66916656 ...  -0.          -0.\n   -0.        ]\n [ -4.5857706   -1.1341286   -8.538033   ...  -0.          -0.\n   -0.        ]\n [ -5.2223887   -1.2824144   -0.17312431 ...  -0.          -0.\n   -0.        ]\n ...\n [ -5.396545   -17.291681    -4.360766   ...  -0.          -0.\n   -0.        ]\n [ -5.9313164  -14.247417    -0.2637329  ...  -0.          -0.\n   -0.        ]\n [ -5.670536    -0.10595131   0.         ...  -0.          -0.\n   -0.        ]]\n\n\nTo get a single average log perplexity across all the elements in the batch you can sum across both dimensions and divide by the number of elements. Notice that the result will be the negative of the real log perplexity of the model:\n\nlog_ppx = np.sum(real_log_ppx) / np.sum(non_pad)\nlog_ppx = -log_ppx\nprint(f'The log perplexity and perplexity of the model are respectively: {log_ppx} and {np.exp(log_ppx)}')\n\nThe log perplexity and perplexity of the model are respectively: 2.3281209468841553 and 10.258646965026855\n\n\nCongratulations on finishing this lecture notebook! Now you should have a clear understanding of how to work with Trax’s numpy and how to compute the perplexity to evaluate your language models. Keep it up!",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "L2 - Calculating Perplexity"
    ]
  },
  {
    "objectID": "notes/cs11-737-w11/index.html",
    "href": "notes/cs11-737-w11/index.html",
    "title": "Translation and Translation Data",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\nVideo 1: Lesson Video\n\n\n\n\n\n\n\n\nThis week’s slides\n\n\n\n\nSupplementary Figure 1\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{bochman2022,\n  author = {Bochman, Oren},\n  title = {Translation and {Translation} {Data}},\n  date = {2022-01-24},\n  url = {https://orenbochman.github.io/notes-nlp/notes/cs11-737-w11/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2022. “Translation and Translation Data.”\nJanuary 24, 2022. https://orenbochman.github.io/notes-nlp/notes/cs11-737-w11/."
  },
  {
    "objectID": "notes/c4w1/lab01.html",
    "href": "notes/c4w1/lab01.html",
    "title": "Stack Semantics in Trax: Ungraded Lab",
    "section": "",
    "text": "In this ungraded lab, we will explain the stack semantics in Trax. This will help in understanding how to use layers like Select and Residual which gets . If you’ve taken a computer science class before, you will recall that a stack is a data structure that follows the Last In, First Out (LIFO) principle. That is, whatever is the latest element that is pushed into the stack will also be the first one to be popped out. If you’re not yet familiar with stacks, then you may find this short tutorial useful. In a nutshell, all you really need to remember is it puts elements one on top of the other. You should be aware of what is on top of the stack to know which element you will be popping. You will see this in the discussions below. Let’s get started!",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation",
      "L1 - Stack Semantics"
    ]
  },
  {
    "objectID": "notes/c4w1/lab01.html#imports",
    "href": "notes/c4w1/lab01.html#imports",
    "title": "Stack Semantics in Trax: Ungraded Lab",
    "section": "Imports",
    "text": "Imports\n\nimport numpy as np              # regular ol' numpy\nfrom trax import layers as tl   # core building block\nfrom trax import shapes         # data signatures: dimensionality and type\nfrom trax import fastmath       # uses jax, offers numpy on steroids\n\n2025-02-10 16:59:40.963815: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1739199580.979731  126645 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1739199580.984598  126645 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation",
      "L1 - Stack Semantics"
    ]
  },
  {
    "objectID": "notes/c4w1/lab01.html#the-tl.serial-combinator-is-stack-oriented.",
    "href": "notes/c4w1/lab01.html#the-tl.serial-combinator-is-stack-oriented.",
    "title": "Stack Semantics in Trax: Ungraded Lab",
    "section": "1. The tl.Serial Combinator is Stack Oriented.",
    "text": "1. The tl.Serial Combinator is Stack Oriented.\nTo understand how stack-orientation works in Trax, most times one will be using the Serial layer. We will define two simple Function layers: 1) Addition and 2) Multiplication.\nSuppose we want to make the simple calculation (3 + 4) * 15 + 3. Serial will perform the calculations in the following manner 3 4 add 15 mul 3 add. The steps of the calculation are shown in the table below. The first column shows the operations made on the stack and the second column the output of those operations. Moreover, the rightmost element in the second column represents the top of the stack (e.g. in the second row, Push(3) pushes 3 on top of the stack and 4 is now under it).\n\n\n\nAfter processing all the stack contains 108 which is the answer to our simple computation.\nFrom this, the following can be concluded: a stack-based layer has only one way to handle data, by taking one piece of data from atop the stack, termed popping, and putting data back atop the stack, termed pushing. Any expression that can be written conventionally, can be written in this form and thus be amenable to being interpreted by a stack-oriented layer like Serial.\n\nCoding the example in the table:\nDefining addition\n\ndef Addition():\n    layer_name = \"Addition\"  # don't forget to give your custom layer a name to identify\n\n    # Custom function for the custom layer\n    def func(x, y):\n        return x + y\n\n    return tl.Fn(layer_name, func)\n\n\n# Test it\nadd = Addition()\n\n# Inspect properties\nprint(\"-- Properties --\")\nprint(\"name :\", add.name)\nprint(\"expected inputs :\", add.n_in)\nprint(\"promised outputs :\", add.n_out, \"\\n\")\n\n# Inputs\nx = np.array([3])\ny = np.array([4])\nprint(\"-- Inputs --\")\nprint(\"x :\", x, \"\\n\")\nprint(\"y :\", y, \"\\n\")\n\n# Outputs\nz = add((x, y))\nprint(\"-- Outputs --\")\nprint(\"z :\", z)\n\n-- Properties --\nname : Addition\nexpected inputs : 2\npromised outputs : 1 \n\n-- Inputs --\nx : [3] \n\ny : [4] \n\n-- Outputs --\nz : [7]\n\n\nDefining multiplication\n\ndef Multiplication():\n    layer_name = (\n        \"Multiplication\"  # don't forget to give your custom layer a name to identify\n    )\n\n    # Custom function for the custom layer\n    def func(x, y):\n        return x * y\n\n    return tl.Fn(layer_name, func)\n\n\n# Test it\nmul = Multiplication()\n\n# Inspect properties\nprint(\"-- Properties --\")\nprint(\"name :\", mul.name)\nprint(\"expected inputs :\", mul.n_in)\nprint(\"promised outputs :\", mul.n_out, \"\\n\")\n\n# Inputs\nx = np.array([7])\ny = np.array([15])\nprint(\"-- Inputs --\")\nprint(\"x :\", x, \"\\n\")\nprint(\"y :\", y, \"\\n\")\n\n# Outputs\nz = mul((x, y))\nprint(\"-- Outputs --\")\nprint(\"z :\", z)\n\n-- Properties --\nname : Multiplication\nexpected inputs : 2\npromised outputs : 1 \n\n-- Inputs --\nx : [7] \n\ny : [15] \n\n-- Outputs --\nz : [105]\n\n\nImplementing the computations using Serial combinator.\n\n# Serial combinator\nserial = tl.Serial(\n    Addition(), Multiplication(), Addition()  # add 3 + 4  # multiply result by 15\n)\n\n# Initialization\nx = (np.array([3]), np.array([4]), np.array([15]), np.array([3]))  # input\n\nserial.init(shapes.signature(x))  # initializing serial instance\n\n\nprint(\"-- Serial Model --\")\nprint(serial, \"\\n\")\nprint(\"-- Properties --\")\nprint(\"name :\", serial.name)\nprint(\"sublayers :\", serial.sublayers)\nprint(\"expected inputs :\", serial.n_in)\nprint(\"promised outputs :\", serial.n_out, \"\\n\")\n\n# Inputs\nprint(\"-- Inputs --\")\nprint(\"x :\", x, \"\\n\")\n\n# Outputs\ny = serial(x)\nprint(\"-- Outputs --\")\nprint(\"y :\", y)\n\n(((), (), ()), ((), (), ()))\n\n\n-- Serial Model --\nSerial_in4[\n  Addition_in2\n  Multiplication_in2\n  Addition_in2\n] \n\n-- Properties --\nname : Serial\nsublayers : [Addition_in2, Multiplication_in2, Addition_in2]\nexpected inputs : 4\npromised outputs : 1 \n\n-- Inputs --\nx : (array([3]), array([4]), array([15]), array([3])) \n\n-- Outputs --\ny : [108]\n\n\nThe example with the two simple adition and multiplication functions that where coded together with the serial combinator show how stack semantics work in Trax.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation",
      "L1 - Stack Semantics"
    ]
  },
  {
    "objectID": "notes/c4w1/lab01.html#the-tl.select-combinator-in-the-context-of-the-serial-combinator",
    "href": "notes/c4w1/lab01.html#the-tl.select-combinator-in-the-context-of-the-serial-combinator",
    "title": "Stack Semantics in Trax: Ungraded Lab",
    "section": "2. The tl.Select combinator in the context of the Serial combinator",
    "text": "2. The tl.Select combinator in the context of the Serial combinator\nHaving understood how stack semantics work in Trax, we will demonstrate how the tl.Select combinator works.\n\nFirst example of tl.Select\nSuppose we want to make the simple calculation (3 + 4) * 3 + 4. We can use Select to perform the calculations in the following manner:\n\n4\n3\ntl.Select([0,1,0,1])\nadd\nmul\nadd.\n\nThe tl.Select requires a list or tuple of 0-based indices to select elements relative to the top of the stack. For our example, the top of the stack is 3 (which is at index 0) then 4 (index 1) and we Select to add in an ordered manner to the top of the stack which after the command is 3 4 3 4. The steps of the calculation for our example are shown in the table below. As in the previous table each column shows the contents of the stack and the outputs after the operations are carried out.\n\n\n\nAfter processing all the inputs the stack contains 25 which is the answer we get above.\n\nserial = tl.Serial(tl.Select([0, 1, 0, 1]), Addition(), Multiplication(), Addition())\n\n# Initialization\nx = (np.array([3]), np.array([4]))  # input\n\nserial.init(shapes.signature(x))  # initializing serial instance\n\n\nprint(\"-- Serial Model --\")\nprint(serial, \"\\n\")\nprint(\"-- Properties --\")\nprint(\"name :\", serial.name)\nprint(\"sublayers :\", serial.sublayers)\nprint(\"expected inputs :\", serial.n_in)\nprint(\"promised outputs :\", serial.n_out, \"\\n\")\n\n# Inputs\nprint(\"-- Inputs --\")\nprint(\"x :\", x, \"\\n\")\n\n# Outputs\ny = serial(x)\nprint(\"-- Outputs --\")\nprint(\"y :\", y)\n\n(((), (), (), ()), ((), (), (), ()))\n\n\n-- Serial Model --\nSerial_in2[\n  Select[0,1,0,1]_in2_out4\n  Addition_in2\n  Multiplication_in2\n  Addition_in2\n] \n\n-- Properties --\nname : Serial\nsublayers : [Select[0,1,0,1]_in2_out4, Addition_in2, Multiplication_in2, Addition_in2]\nexpected inputs : 2\npromised outputs : 1 \n\n-- Inputs --\nx : (array([3]), array([4])) \n\n-- Outputs --\ny : [25]\n\n\n\n\nSecond example of tl.Select\nSuppose we want to make the simple calculation (3 + 4) * 4. We can use Select to perform the calculations in the following manner:\n\n4\n3\ntl.Select([0,1,0,1])\nadd\ntl.Select([0], n_in=2)\nmul\n\nThe example is a bit contrived but it demonstrates the flexibility of the command. The second tl.Select pops two elements (specified in n_in) from the stack starting from index 0 (i.e. top of the stack). This means that 7 and 3 will be popped out because n_in = 2) but only 7 is placed back on top because it only selects [0]. As in the previous table each column shows the contents of the stack and the outputs after the operations are carried out.\n\n\n\nAfter processing all the inputs the stack contains 28 which is the answer we get above.\n\nserial = tl.Serial(\n    tl.Select([0, 1, 0, 1]), Addition(), tl.Select([0], n_in=2), Multiplication()\n)\n\n# Initialization\nx = (np.array([3]), np.array([4]))  # input\n\nserial.init(shapes.signature(x))  # initializing serial instance\n\n\nprint(\"-- Serial Model --\")\nprint(serial, \"\\n\")\nprint(\"-- Properties --\")\nprint(\"name :\", serial.name)\nprint(\"sublayers :\", serial.sublayers)\nprint(\"expected inputs :\", serial.n_in)\nprint(\"promised outputs :\", serial.n_out, \"\\n\")\n\n# Inputs\nprint(\"-- Inputs --\")\nprint(\"x :\", x, \"\\n\")\n\n# Outputs\ny = serial(x)\nprint(\"-- Outputs --\")\nprint(\"y :\", y)\n\n(((), (), (), ()), ((), (), (), ()))\n\n\n-- Serial Model --\nSerial_in2[\n  Select[0,1,0,1]_in2_out4\n  Addition_in2\n  Select[0]_in2\n  Multiplication_in2\n] \n\n-- Properties --\nname : Serial\nsublayers : [Select[0,1,0,1]_in2_out4, Addition_in2, Select[0]_in2, Multiplication_in2]\nexpected inputs : 2\npromised outputs : 1 \n\n-- Inputs --\nx : (array([3]), array([4])) \n\n-- Outputs --\ny : [28]\n\n\nIn summary, what Select does in this example is a copy of the inputs in order to be used further along in the stack of operations.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation",
      "L1 - Stack Semantics"
    ]
  },
  {
    "objectID": "notes/c4w1/lab01.html#the-tl.residual-combinator-in-the-context-of-the-serial-combinator",
    "href": "notes/c4w1/lab01.html#the-tl.residual-combinator-in-the-context-of-the-serial-combinator",
    "title": "Stack Semantics in Trax: Ungraded Lab",
    "section": "3. The tl.Residual combinator in the context of the Serial combinator",
    "text": "3. The tl.Residual combinator in the context of the Serial combinator\n\ntl.Residual\nResidual networks are frequently used to make deep models easier to train and you will be using it in the assignment as well. Trax already has a built in layer for this. The Residual layer computes the element-wise sum of the stack-top input with the output of the layer series. For example, if we wanted the cumulative sum of the folowing series of computations (3 + 4) * 3 + 4. The result can be obtained with the use of the Residual combinator in the following manner\n\n4\n3\ntl.Select([0,1,0,1])\nadd\nmul\ntl.Residual.\n\nFor our example the top of the stack is 3 4 and we select to add the same to numbers in an ordered manner to the top of the stack which after the command is 3 4 3 4. The steps of the calculation for our example are shown in the table below together with the cumulative sum which is the result of tl.Residual.\n\n\n\nAfter processing all the inputs the stack contains 50 which is the cumulative sum of all the operations.\n\nserial = tl.Serial(\n    tl.Select([0, 1, 0, 1]), Addition(), Multiplication(), Addition(), tl.Residual()\n)\n\n# Initialization\nx = (np.array([3]), np.array([4]))  # input\n\nserial.init(shapes.signature(x))  # initializing serial instance\n\n\nprint(\"-- Serial Model --\")\nprint(serial, \"\\n\")\nprint(\"-- Properties --\")\nprint(\"name :\", serial.name)\nprint(\"sublayers :\", serial.sublayers)\nprint(\"expected inputs :\", serial.n_in)\nprint(\"promised outputs :\", serial.n_out, \"\\n\")\n\n# Inputs\nprint(\"-- Inputs --\")\nprint(\"x :\", x, \"\\n\")\n\n# Outputs\ny = serial(x)\nprint(\"-- Outputs --\")\nprint(\"y :\", y)\n\n(((), (), (), (), (((), ((), ())), ())),\n ((), (), (), (), (((), ((), ())), ())))\n\n\n-- Serial Model --\nSerial_in2[\n  Select[0,1,0,1]_in2_out4\n  Addition_in2\n  Multiplication_in2\n  Addition_in2\n  Serial[\n    Branch_out2[\n      None\n      Serial\n    ]\n    Add_in2\n  ]\n] \n\n-- Properties --\nname : Serial\nsublayers : [Select[0,1,0,1]_in2_out4, Addition_in2, Multiplication_in2, Addition_in2, Serial[\n  Branch_out2[\n    None\n    Serial\n  ]\n  Add_in2\n]]\nexpected inputs : 2\npromised outputs : 1 \n\n-- Inputs --\nx : (array([3]), array([4])) \n\n-- Outputs --\ny : [50]\n\n\n\n\nA slightly trickier example:\nNormally, the Residual layer will accept a layer as an argument and it will add the output of that layer to the current stack top input. In the example below, you’ll notice that in the last step, we specify tl.Residual(Addition()). If you refer to the same figure above, you’ll notice that the stack at that point has 21 4 where 21 is the top of the stack. The Residual layer remembers this value (i.e. 21) so the result of the Addition() layer nested into it (i.e. 25) is added to this stack top input to arrive at the result: 46.\n\nserial = tl.Serial(\n    tl.Select([0, 1, 0, 1]), Addition(), Multiplication(), tl.Residual(Addition()) \n)\n\n# Initialization\nx = (np.array([3]), np.array([4]))  # input\n\nserial.init(shapes.signature(x))  # initializing serial instance\n\n\nprint(\"-- Serial Model --\")\nprint(serial, \"\\n\")\nprint(\"-- Properties --\")\nprint(\"name :\", serial.name)\nprint(\"sublayers :\", serial.sublayers)\nprint(\"expected inputs :\", serial.n_in)\nprint(\"promised outputs :\", serial.n_out, \"\\n\")\n\n# Inputs\nprint(\"-- Inputs --\")\nprint(\"x :\", x, \"\\n\")\n\n# Outputs\ny = serial(x)\nprint(\"-- Outputs --\")\nprint(\"y :\", y)\n\n(((), (), (), (((), ((), ())), ())), ((), (), (), (((), ((), ())), ())))\n\n\n-- Serial Model --\nSerial_in2[\n  Select[0,1,0,1]_in2_out4\n  Addition_in2\n  Multiplication_in2\n  Serial_in2[\n    Branch_in2_out2[\n      None\n      Addition_in2\n    ]\n    Add_in2\n  ]\n] \n\n-- Properties --\nname : Serial\nsublayers : [Select[0,1,0,1]_in2_out4, Addition_in2, Multiplication_in2, Serial_in2[\n  Branch_in2_out2[\n    None\n    Addition_in2\n  ]\n  Add_in2\n]]\nexpected inputs : 2\npromised outputs : 1 \n\n-- Inputs --\nx : (array([3]), array([4])) \n\n-- Outputs --\ny : [46]",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation",
      "L1 - Stack Semantics"
    ]
  },
  {
    "objectID": "notes/c4w1/index.html",
    "href": "notes/c4w1/index.html",
    "title": "Neural Machine Translation",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\nThis week’s slides\n\n\n\n\nSupplementary Figure 1\nMy notes for Week 1 of the Natural Language Processing with Attention Labels Course in the Natural Language Processing Specialization Offered by DeepLearning.AI on Coursera",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w1/index.html#sec-sec-video1-intro",
    "href": "notes/c4w1/index.html#sec-sec-video1-intro",
    "title": "Neural Machine Translation",
    "section": "Intro",
    "text": "Intro\nThis course covers most modern practical NLP methods. We’ll use a powerful technique called attention to build several different models. Some of the things we build using the attention mechanism, include a powerful language translation model, an algorithm capable of summarizing texts, a model that can actually answer questions about the piece of text, and a chat bot that we can actually have a conversation with.\nWe also take another look at sentiment analysis.\nWhen it comes to modern deep learning, there’s a sort of new normal, which is to say, most people aren’t actually building and training models from scratch. Instead, it’s more common to download a pre-trained model and then tweak it and find units for your specific use case. In this course, we show we how to build the models from scratch, but we also provide we custom pre-trained models that we created just for you. By training them continuously for weeks on the most powerful TPU clusters that are currently only available to researchers as Google.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w1/index.html#sec-sec-video2-seq2seq",
    "href": "notes/c4w1/index.html#sec-sec-video2-seq2seq",
    "title": "Neural Machine Translation",
    "section": "Seq2Seq",
    "text": "Seq2Seq\n\nOutline:\n\nIntroduction to Neural Machine Translation\nSeq2Seq model and its shortcomings\nSolution for the information bottleneck\n\nThe sequential nature of models we learned in the previous course (RNNs, LSTMs, GRUs) does not allow for speed ups within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. (because we can run different batches or examples in parallel or even different directions)\n\n\n\n\nscreenshot_of_outline_slide\n\nIn other words, if we rely on sequences and we need to know the beginning of a text before being able to compute something about the ending of it, then we can not use parallel computing. We would have to wait until the initial computations are complete. This isn’t good, because if your text is too long, then\n\nIt’ll take a long time for we to process it and\nThere is the information loss mentioned earlier in the text as we approach the end.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w1/index.html#sec-sec-seq2seq-model",
    "href": "notes/c4w1/index.html#sec-sec-seq2seq-model",
    "title": "Neural Machine Translation",
    "section": "Seq2Seq model",
    "text": "Seq2Seq model\n\nIntroduced by Google in 2014\nMaps variable-length sequences to fixed-length memory\nLSTMs and GRUs are typically used to overcome the vanishing gradient problem\n\n\n\n\n\nencoder decoder architecture\n\nTherefore, attention mechanisms have become critical for sequence modeling in various tasks, allowing modeling of dependencies without caring too much about their distance in the input or output sequences.\nin this encoder decoder architecture the yellow block in the middle is the final hidden state produced by the encoder. It’s essentials a compressed representation of the sequence in this case the English sentence. The problem with RNN is they tend to have a bias for representing more recent data.\nOne approach to overcome this issue is to provide the decoder with the attention layer.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w1/index.html#sec-sec-video3-alignment",
    "href": "notes/c4w1/index.html#sec-sec-video3-alignment",
    "title": "Neural Machine Translation",
    "section": "3: Alignment",
    "text": "3: Alignment\nAlignment is an old problem and there are a number of papers on learning to align and translate which helped put attention mechanism into focus.\n\nNEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE 2016\nJointly Learning to Align and Translate with Transformer Models 2019\n\nberliner = citizen of berlin\nberliner = jelly doughnut\nNot all words translate precisely to another word. Adding an attention layers allows the model to give different words more importance when translating another word. This is a good task for an attention layer\nDeveloping intuition about alignment:\nalso check out this page Deep Learning: The Good, the Bad and the Ugly in a 2017 talk, Lukasz Kaiser referred to [K,V] as a memory. We want to manage information better in our model. We keep the information in a memory consisting of keys and values. (It needs to be differentiable so we can use it with back propagation)\nThen we put in the query a sequence and in the keys another sequence (depending on the task they may be the same say for summarization or different for alignment or translation) By combining Q K using a Softmax we get a vector of probabilities each position in the memory is relevant. weight matrix to apply to the values in the memory.\n\nget all of the available hidden states ready for the encoder and do the same for the first hidden states of the decoder. (In the example, there are two encoder hidden states, shown by blue dots, and one decoder hidden states.)\nNext, score each of the encoder hidden states by getting its dot product between each encoder state and decoder hidden states.\n\nA higher score means that the hidden state has greater influence on the output.\nThen we run the scores through a Softmax, squashing them between 0 and 1, and giving the attention distribution.\n\nTake each encoder hidden state, and multiply it by its Softmax score, which is a number between 0 and 1, this results in the alignments vector.\nAdd up everything in the alignments vector to arrive at what’s called the context vector.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w1/index.html#sec-w1v4-attention",
    "href": "notes/c4w1/index.html#sec-w1v4-attention",
    "title": "Neural Machine Translation",
    "section": "Attention",
    "text": "Attention\nThe attention mechanism uses encoded representations of both the input or the encoder hidden states and the outputs or the decoder hidden states. The keys and values are pairs. Both of dimension NN, where NN is the input sequence length and comes from the encoder hidden states. Keys and values have their own respective matrices, but the matrices have the same shape and are often the same. While the queries come from the decoder hidden states. One way we can think of it is as follows. Imagine that we are translating English into German. We can represent the word embeddings in the English language as keys and values. The queries will then be the German equivalent. We can then calculate the dot product between the query and the key. Note that similar vectors have higher dot products and non-similar vectors will have lower dot products. The intuition here is that we want to identify the corresponding words in the queries that are similar to the keys. This would allow your model to “look” or focus on the right place when translating each word.\nWe then run a Softmax:\n\nsoftmax(QK^T )  \n\\tag{1}\nThat gives a distribution of numbers between 0 and 1.\nWe then would multiply the output by V. Remember V in this example was the same as our keys, corresponding to the English word embeddings. Hence the equation becomes\n\nsoftmax(QK^T )V  \n {#sec-softmax-formula-2}\nIn the matrix, the lighter square shows where the model is actually looking when making the translation of that word. This mapping isn’t necessarily be one to one. The lighting just tells we to what extent is each word contributing to the input that’s fed into the decoder. As we can see several words can contribute to translating another word, depending on the weights (output) of the softmax that we use to create the new input. a picture of attention in translation with English to German An important thing to keep in mind is that the model should be flexible enough to connect each English word with its relevant German word, even if they don’t appear in the same position in their respective sentences. In other words, it should be flexible enough to handle differences in grammar and word ordering in different languages.\nIn a situation like the one just mentioned, where the grammar of foreign language requires a difference word order than the other, the attention is so flexible enough to find the connection. The first four tokens, the agreements on the, are pretty straightforward, but then the grammatical structure between French and English changes. Now instead of looking at the corresponding fifth token to translate the French word zone, the attention knows to look further down at the eighth token, which corresponds to the English word area, glorious and necessary. It’s pretty amazing, was a little matrix multiplication can do.\nSo attention is a layer of calculations that let your model focus on the most important parts of the sequence for each step. Queries, values, and keys are representations of the encoder and decoder hidden states. And they’re used to retrieve information inside the attention layer by calculating the similarity between the decoder queries and the encoder key- value pairs.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w1/index.html#sec-evaluation-metrics",
    "href": "notes/c4w1/index.html#sec-evaluation-metrics",
    "title": "Neural Machine Translation",
    "section": "Evaluation metrics for Machine Translation",
    "text": "Evaluation metrics for Machine Translation\n\nBLEU\n\nThe authors of (Papineni et al. 2002) introduced the BLEU score.\nThe closer the BLEU score is to 1, the better a model preforms.\nThe closer to 0, the worse it does.\n\nTo get the BLEU score, the candidates and the references are usually based on an average of unigrams, bigrams, trigrams or even four-gram precision. For example using uni-grams:\n\n\n\n\nscreenshot_of_outline_slide\n\nWe would sum over the unique n-gram counts in the candidate and divide by the total number of words in the candidate. The same concept could apply to unigrams, bigrams, etc. One issue with the BLEU score is that it doesn’t take into account semantics, so it doesn’t take into account the order of the n-grams in the sentence.\n\nBLEU = BP\\Bigl(\\prod_{i=1}^{4}\\text{precision}_i\\Bigr)^{(1/4)}\n\\tag{2}\nwith the Brevity Penalty and precision defined as:\n\nBP = min\\Bigl(1, e^{(1-({ref}/{cand}))}\\Bigr)\n\\tag{3}\n\n\\text{Precision}_i = \\frac {\\sum_{snt \\in{cand}}\\sum_{i\\in{snt}}min\\Bigl(m^{i}_{cand}, m^{i}_{ref}\\Bigr)}{w^{i}_{t}}\n\\tag{4}\nwhere:\n\nm^{i}_{cand}, is the count of i-gram in candidate matching the reference translation.\nm^{i}_{ref}, is the count of i-gram in the reference translation.\nw^{i}_{t}, is the total number of i-grams in candidate translation.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w1/index.html#sec-rouge",
    "href": "notes/c4w1/index.html#sec-rouge",
    "title": "Neural Machine Translation",
    "section": "ROUGE",
    "text": "ROUGE\n(Lin 2004) introduced a similar method for evaluation called the ROUGE score which calculates precision and recall for machine texts by counting the n-gram overlap between the machine texts and a reference text. Here is an example that calculates recall: \n\nRouge_{recall} = \\sum  \\frac{(\\{\\text{prediction n-grams}\\} \\cap \\{ \\text{test n-grams}\\})}{\\vert{ \\text{test n-grams}}\\vert }\n\\tag{5}\nRouge also allows we to compute precision as follows:\n\n\n\n\nprecision in ROUGE\n\n \\text{ROUGE}_{\\text{precision}} = \\sum \\frac{(\\{\\text{prediction n-grams}\\} \\cap \\{ \\text{test ngrams}\\})}{\\vert\\{ \\text{vocab}\\}\\vert}\n\\tag{6}\nThe ROUGE-N refers to the overlap of N-grams between the actual system and the reference summaries. The F-score metric combines Recall and precision into one metric.\n\nF_{score}= 2 \\times \\frac{(\\text{precision} \\times \\text{recall})}{(\\text{precision} + \\text{recall})}\n\\tag{7}",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w1/index.html#sec-random-sampling",
    "href": "notes/c4w1/index.html#sec-random-sampling",
    "title": "Neural Machine Translation",
    "section": "Random sampling",
    "text": "Random sampling\nRandom sampling for decoding involves drawing a word from the softmax distribution. To explore the latent space it is possible to introduce a temperature variable which controls the randomness of the sample.\ndef logsoftmax_sample(log_probs, temperature=1.0):  \n  \"\"\"Returns a sample from a log-softmax output, with temperature.\n  Args:\n    log_probs: Logarithms of probabilities (often coming from LogSofmax)\n    temperature: For scaling before sampling (1.0 = default, 0.0 = pick argmax)\n  \"\"\"\n  # This is equivalent to sampling from a softmax with temperature.\n  u = np.random.uniform(low=1e-6, high=1.0 - 1e-6, size=log_probs.shape)\n  g = -np.log(-np.log(u))\n  return np.argmax(log_probs + g * temperature, axis=-1)",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w1/index.html#sec-beam-search",
    "href": "notes/c4w1/index.html#sec-beam-search",
    "title": "Neural Machine Translation",
    "section": "Beam Search",
    "text": "Beam Search\nThe beam search algorithm is a limited (best-first search). The parameter for the beam width limits the choices considered at each step.\n\n\n\n\nBeam Search",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w1/index.html#sec-minimum-bayes-risk",
    "href": "notes/c4w1/index.html#sec-minimum-bayes-risk",
    "title": "Neural Machine Translation",
    "section": "Minimum Bayes Risk",
    "text": "Minimum Bayes Risk\nMBR (Minimum Bayes Risk) Compares many samples against one another. To implement MBR:\n\nGenerate several random samples.\nCompare each sample against all the others and assign a similarity score using ROUGE.\nSelect the sample with the highest similarity: the golden one.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w1/index.html#sec-summary",
    "href": "notes/c4w1/index.html#sec-summary",
    "title": "Neural Machine Translation",
    "section": "Summary",
    "text": "Summary\n\nMaximal Probability is a baseline - but not a particularly good one when the data is noisy.\nRandom sampling with temperature is better.\nBeam search uses conditional probabilities and the parameter.\nMBR takes several samples and compares them against each other to find the golden one.\n\nnote: although not mentioned in the next week’s notes Beam Search is useful for improving the summarization task. We can extract a golden summary from a number of samples using MBR. ROUGE-N is the preferred metric for evaluating summarization\n\nReferences\n\n-​ (Peters et al. 2018)\n(Alammar 2024)\n\n\n\nAlammar, Jay. 2024. “The Illustrated Transformer.” https://jalammar.github.io/illustrated-transformer.\n\n\nLin, Chin-Yew. 2004. “ROUGE: A Package for Automatic Evaluation of Summaries.” In Text Summarization Branches Out, 74–81. Barcelona, Spain: Association for Computational Linguistics. https://aclanthology.org/W04-1013.\n\n\nPapineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. “Bleu: A Method for Automatic Evaluation of Machine Translation.” In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, 311–18. https://www.aclweb.org/anthology/P02-1040.pdf.\n\n\nPeters, Matthew E., Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. “Deep Contextualized Word Representations.” CoRR abs/1802.05365. http://arxiv.org/abs/1802.05365.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w4/lab04.html",
    "href": "notes/c2w4/lab04.html",
    "title": "Word Embeddings: Ungraded Practice Notebook",
    "section": "",
    "text": "course banner\nIn this ungraded notebook, you’ll try out all the individual techniques that we learned about in the lecture. Practicing on small examples will prepare we for the graded assignment, where we will combine the techniques in more advanced ways to create word embeddings from a real-life corpus.\nThis notebook is made of two main parts:\nTo get started, import and initialize all the libraries we will need.\nimport sys\n!{sys.executable} -m pip install emoji\n\nRequirement already satisfied: emoji in /home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages (1.4.1)\nimport re\nimport nltk\nfrom nltk.tokenize import word_tokenize\nimport emoji\nimport numpy as np\n\nfrom utils2 import get_dict\n\nnltk.download('punkt')  # download pre-trained Punkt tokenizer for English\n\n[nltk_data] Downloading package punkt to /home/oren/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n\n\nTrue"
  },
  {
    "objectID": "notes/c2w4/lab04.html#cleaning-and-tokenization",
    "href": "notes/c2w4/lab04.html#cleaning-and-tokenization",
    "title": "Word Embeddings: Ungraded Practice Notebook",
    "section": "Cleaning and tokenization",
    "text": "Cleaning and tokenization\nTo demonstrate the cleaning and tokenization process, consider a corpus that contains emojis and various punctuation signs.\n\ncorpus = 'Who ❤️ \"word embeddings\" in 2020? I do!!!'\n\nFirst, replace all interrupting punctuation signs — such as commas and exclamation marks — with periods.\n\nprint(f'Corpus:  {corpus}')\ndata = re.sub(r'[,!?;-]+', '.', corpus)\nprint(f'After cleaning punctuation:  {data}')\n\nCorpus:  Who ❤️ \"word embeddings\" in 2020? I do!!!\nAfter cleaning punctuation:  Who ❤️ \"word embeddings\" in 2020. I do.\n\n\nNext, use NLTK’s tokenization engine to split the corpus into individual tokens.\n\nprint(f'Initial string:  {data}')\ndata = nltk.word_tokenize(data)\nprint(f'After tokenization:  {data}')\n\nInitial string:  Who ❤️ \"word embeddings\" in 2020. I do.\nAfter tokenization:  ['Who', '❤️', '``', 'word', 'embeddings', \"''\", 'in', '2020', '.', 'I', 'do', '.']\n\n\nFinally, as we saw in the lecture, get rid of numbers and punctuation other than periods, and convert all the remaining tokens to lowercase.\n\nprint(f'Initial list of tokens:  {data}')\ndata = [ ch.lower() for ch in data\n         if ch.isalpha()\n         or ch == '.'\n         or emoji.get_emoji_regexp().search(ch)\n       ]\nprint(f'After cleaning:  {data}')\n\nInitial list of tokens:  ['Who', '❤️', '``', 'word', 'embeddings', \"''\", 'in', '2020', '.', 'I', 'do', '.']\nAfter cleaning:  ['who', '❤️', 'word', 'embeddings', 'in', '.', 'i', 'do', '.']\n\n\nNote that the heart emoji is considered as a token just like any normal word.\nNow let’s streamline the cleaning and tokenization process by wrapping the previous steps in a function.\n\ndef tokenize(corpus):\n    data = re.sub(r'[,!?;-]+', '.', corpus)\n    data = nltk.word_tokenize(data)  # tokenize string to words\n    data = [ ch.lower() for ch in data\n             if ch.isalpha()\n             or ch == '.'\n             or emoji.get_emoji_regexp().search(ch)\n           ]\n    return data\n\nApply this function to the corpus that you’ll be working on in the rest of this notebook: “I am happy because I am learning”\n\ncorpus = 'I am happy because I am learning'\nprint(f'Corpus:  {corpus}')\nwords = tokenize(corpus)\nprint(f'Words (tokens):  {words}')\n\nCorpus:  I am happy because I am learning\nWords (tokens):  ['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n\n\nNow try it out yourself with your own sentence.\n\ntokenize(\"Now it's your turn: try with your own sentence!\")\n\n['now', 'it', 'your', 'turn', 'try', 'with', 'your', 'own', 'sentence', '.']"
  },
  {
    "objectID": "notes/c2w4/lab04.html#sliding-window-of-words",
    "href": "notes/c2w4/lab04.html#sliding-window-of-words",
    "title": "Word Embeddings: Ungraded Practice Notebook",
    "section": "Sliding window of words",
    "text": "Sliding window of words\nNow that we have transformed the corpus into a list of clean tokens, we can slide a window of words across this list. For each window we can extract a center word and the context words.\nThe get_windows function in the next cell was introduced in the lecture.\n\ndef get_windows(words, C):\n    i = C\n    while i &lt; len(words) - C:\n        center_word = words[i]\n        context_words = words[(i - C):i] + words[(i+1):(i+C+1)]\n        yield context_words, center_word\n        i += 1\n\nThe first argument of this function is a list of words (or tokens). The second argument, C, is the context half-size. Recall that for a given center word, the context words are made of C words to the left and C words to the right of the center word.\nHere is how we can use this function to extract context words and center words from a list of tokens. These context and center words will make up the training set that we will use to train the CBOW model.\n\nfor x, y in get_windows(\n            ['i', 'am', 'happy', 'because', 'i', 'am', 'learning'],\n            2\n        ):\n    print(f'{x}\\t{y}')\n\n['i', 'am', 'because', 'i'] happy\n['am', 'happy', 'i', 'am']  because\n['happy', 'because', 'am', 'learning']  i\n\n\nThe first example of the training set is made of:\n\nthe context words “i”, “am”, “because”, “i”,\nand the center word to be predicted: “happy”.\n\nNow try it out yourself. In the next cell, we can change both the sentence and the context half-size.\n\nfor x, y in get_windows(tokenize(\"Now it's your turn: try with your own sentence!\"), 1):\n    print(f'{x}\\t{y}')\n\n['now', 'your'] it\n['it', 'turn']  your\n['your', 'try'] turn\n['turn', 'with']    try\n['try', 'your'] with\n['with', 'own'] your\n['your', 'sentence']    own\n['own', '.']    sentence"
  },
  {
    "objectID": "notes/c2w4/lab04.html#transforming-words-into-vectors-for-the-training-set",
    "href": "notes/c2w4/lab04.html#transforming-words-into-vectors-for-the-training-set",
    "title": "Word Embeddings: Ungraded Practice Notebook",
    "section": "Transforming words into vectors for the training set",
    "text": "Transforming words into vectors for the training set\nTo finish preparing the training set, we need to transform the context words and center words into vectors.\n\nMapping words to indices and indices to words\nThe center words will be represented as one-hot vectors, and the vectors that represent context words are also based on one-hot vectors.\nTo create one-hot word vectors, we can start by mapping each unique word to a unique integer (or index). We have provided a helper function, get_dict, that creates a Python dictionary that maps words to integers and back.\n\nword2Ind, Ind2word = get_dict(words)\n\nHere’s the dictionary that maps words to numeric indices.\n\nword2Ind\n\n{'am': 0, 'because': 1, 'happy': 2, 'i': 3, 'learning': 4}\n\n\nWe can use this dictionary to get the index of a word.\n\nprint(\"Index of the word 'i':  \",word2Ind['i'])\n\nIndex of the word 'i':   3\n\n\nAnd conversely, here’s the dictionary that maps indices to words.\n\nInd2word\n\n{0: 'am', 1: 'because', 2: 'happy', 3: 'i', 4: 'learning'}\n\n\n\nprint(\"Word which has index 2:  \",Ind2word[2] )\n\nWord which has index 2:   happy\n\n\nFinally, get the length of either of these dictionaries to get the size of the vocabulary of your corpus, in other words the number of different words making up the corpus.\n\nV = len(word2Ind)\nprint(\"Size of vocabulary: \", V)\n\nSize of vocabulary:  5\n\n\n\n\nGetting one-hot word vectors\nRecall from the lecture that we can easily convert an integer, n, into a one-hot vector.\nConsider the word “happy”. First, retrieve its numeric index.\n\nn = word2Ind['happy']\nn\n\n2\n\n\nNow create a vector with the size of the vocabulary, and fill it with zeros.\n\ncenter_word_vector = np.zeros(V)\ncenter_word_vector\n\narray([0., 0., 0., 0., 0.])\n\n\nWe can confirm that the vector has the right size.\n\nlen(center_word_vector) == V\n\nTrue\n\n\nNext, replace the 0 of the n-th element with a 1.\n\ncenter_word_vector[n] = 1\n\nAnd we have your one-hot word vector.\n\ncenter_word_vector\n\narray([0., 0., 1., 0., 0.])\n\n\nWe can now group all of these steps in a convenient function, which takes as parameters: a word to be encoded, a dictionary that maps words to indices, and the size of the vocabulary.\n\ndef word_to_one_hot_vector(word, word2Ind, V):\n    # BEGIN your code here\n    one_hot_vector = np.zeros(V)\n    one_hot_vector[word2Ind[word]] = 1\n    # END your code here\n    return one_hot_vector\n\nCheck that it works as intended.\n\nword_to_one_hot_vector('happy', word2Ind, V)\n\narray([0., 0., 1., 0., 0.])\n\n\nWhat is the word vector for “learning”?\n\n# BEGIN your code here\nword_to_one_hot_vector('learning', word2Ind, V)\n# END your code here\n\narray([0., 0., 0., 0., 1.])\n\n\nExpected output:\narray([0., 0., 0., 0., 1.])\n\n\nGetting context word vectors\nTo create the vectors that represent context words, we will calculate the average of the one-hot vectors representing the individual words.\nLet’s start with a list of context words.\n\ncontext_words = ['i', 'am', 'because', 'i']\n\nUsing Python’s list comprehension construct and the word_to_one_hot_vector function that we created in the previous section, we can create a list of one-hot vectors representing each of the context words.\n\ncontext_words_vectors = [word_to_one_hot_vector(w, word2Ind, V) for w in context_words]\ncontext_words_vectors\n\n[array([0., 0., 0., 1., 0.]),\n array([1., 0., 0., 0., 0.]),\n array([0., 1., 0., 0., 0.]),\n array([0., 0., 0., 1., 0.])]\n\n\nAnd we can now simply get the average of these vectors using numpy’s mean function, to get the vector representation of the context words.\n\nnp.mean(context_words_vectors, axis=0)\n\narray([0.25, 0.25, 0.  , 0.5 , 0.  ])\n\n\nNote the axis=0 parameter that tells mean to calculate the average of the rows (if we had wanted the average of the columns, we would have used axis=1).\nNow create the context_words_to_vector function that takes in a list of context words, a word-to-index dictionary, and a vocabulary size, and outputs the vector representation of the context words.\n\ndef context_words_to_vector(context_words, word2Ind, V):\n    # BEGIN your code here\n    context_words_vectors = [word_to_one_hot_vector(w, word2Ind, V) for w in context_words]\n    context_words_vectors = np.mean(context_words_vectors, axis=0)\n    # END your code here\n    return context_words_vectors\n\nAnd check that we obtain the same output as the manual approach above.\n\ncontext_words_to_vector(['i', 'am', 'because', 'i'], word2Ind, V)\n\narray([0.25, 0.25, 0.  , 0.5 , 0.  ])\n\n\nWhat is the vector representation of the context words “am happy i am”?\n\n# BEGIN your code here\ncontext_words_to_vector(['am', 'happy', 'i', 'am'], word2Ind, V)\n# END your code here\n\narray([0.5 , 0.  , 0.25, 0.25, 0.  ])\n\n\nExpected output:\narray([0.5 , 0.  , 0.25, 0.25, 0.  ])"
  },
  {
    "objectID": "notes/c2w4/lab04.html#building-the-training-set",
    "href": "notes/c2w4/lab04.html#building-the-training-set",
    "title": "Word Embeddings: Ungraded Practice Notebook",
    "section": "Building the training set",
    "text": "Building the training set\nWe can now combine the functions that we created in the previous sections, to build a training set for the CBOW model, starting from the following tokenized corpus.\n\nwords\n\n['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n\n\nTo do this we need to use the sliding window function (get_windows) to extract the context words and center words, and we then convert these sets of words into a basic vector representation using word_to_one_hot_vector and context_words_to_vector.\n\nfor context_words, center_word in get_windows(words, 2):  # reminder: 2 is the context half-size\n    print(f'Context words:  {context_words} -&gt; {context_words_to_vector(context_words, word2Ind, V)}')\n    print(f'Center word:  {center_word} -&gt; {word_to_one_hot_vector(center_word, word2Ind, V)}')\n    print()\n\nContext words:  ['i', 'am', 'because', 'i'] -&gt; [0.25 0.25 0.   0.5  0.  ]\nCenter word:  happy -&gt; [0. 0. 1. 0. 0.]\n\nContext words:  ['am', 'happy', 'i', 'am'] -&gt; [0.5  0.   0.25 0.25 0.  ]\nCenter word:  because -&gt; [0. 1. 0. 0. 0.]\n\nContext words:  ['happy', 'because', 'am', 'learning'] -&gt; [0.25 0.25 0.25 0.   0.25]\nCenter word:  i -&gt; [0. 0. 0. 1. 0.]\n\n\n\nIn this practice notebook you’ll be performing a single iteration of training using a single example, but in this week’s assignment you’ll train the CBOW model using several iterations and batches of example. Here is how we would use a Python generator function (remember the yield keyword from the lecture?) to make it easier to iterate over a set of examples.\n\ndef get_training_example(words, C, word2Ind, V):\n    for context_words, center_word in get_windows(words, C):\n        yield context_words_to_vector(context_words, word2Ind, V), word_to_one_hot_vector(center_word, word2Ind, V)\n\nThe output of this function can be iterated on to get successive context word vectors and center word vectors, as demonstrated in the next cell.\n\nfor context_words_vector, center_word_vector in get_training_example(words, 2, word2Ind, V):\n    print(f'Context words vector:  {context_words_vector}')\n    print(f'Center word vector:  {center_word_vector}')\n    print()\n\nContext words vector:  [0.25 0.25 0.   0.5  0.  ]\nCenter word vector:  [0. 0. 1. 0. 0.]\n\nContext words vector:  [0.5  0.   0.25 0.25 0.  ]\nCenter word vector:  [0. 1. 0. 0. 0.]\n\nContext words vector:  [0.25 0.25 0.25 0.   0.25]\nCenter word vector:  [0. 0. 0. 1. 0.]\n\n\n\nYour training set is ready, we can now move on to the CBOW model itself."
  },
  {
    "objectID": "notes/c2w4/lab04.html#activation-functions",
    "href": "notes/c2w4/lab04.html#activation-functions",
    "title": "Word Embeddings: Ungraded Practice Notebook",
    "section": "Activation functions",
    "text": "Activation functions\nLet’s start by implementing the activation functions, ReLU and softmax.\n\nReLU\nReLU is used to calculate the values of the hidden layer, in the following formulas:\n\\begin{align}\n\\mathbf{z_1} &= \\mathbf{W_1}\\mathbf{x} + \\mathbf{b_1}  \\tag{1} \\\\\n\\mathbf{h} &= \\mathrm{ReLU}(\\mathbf{z_1})  \\tag{2} \\\\\n\\end{align}\nLet’s fix a value for \\mathbf{z_1} as a working example.\n\nnp.random.seed(10)\nz_1 = 10*np.random.rand(5, 1)-5\nz_1\n\narray([[ 2.71320643],\n       [-4.79248051],\n       [ 1.33648235],\n       [ 2.48803883],\n       [-0.01492988]])\n\n\nTo get the ReLU of this vector, we want all the negative values to become zeros.\nFirst create a copy of this vector.\n\nh = z_1.copy()\n\nNow determine which of its values are negative.\n\nh &lt; 0\n\narray([[False],\n       [ True],\n       [False],\n       [False],\n       [ True]])\n\n\nWe can now simply set all of the values which are negative to 0.\n\nh[h &lt; 0] = 0\n\nAnd that’s it: we have the ReLU of \\mathbf{z_1}!\n\nh\n\narray([[2.71320643],\n       [0.        ],\n       [1.33648235],\n       [2.48803883],\n       [0.        ]])\n\n\nNow implement ReLU as a function.\n\ndef relu(z):\n    # BEGIN your code here\n    result = z.copy()\n    result[result &lt; 0] = 0\n    # END your code here\n    \n    return result\n\nAnd check that it’s working.\n\nz = np.array([[-1.25459881], [ 4.50714306], [ 2.31993942], [ 0.98658484], [-3.4398136 ]])\nrelu(z)\n\narray([[0.        ],\n       [4.50714306],\n       [2.31993942],\n       [0.98658484],\n       [0.        ]])\n\n\nExpected output:\narray([[0.        ],\n       [4.50714306],\n       [2.31993942],\n       [0.98658484],\n       [0.        ]])\n\n\nSoftmax\nThe second activation function that we need is softmax. This function is used to calculate the values of the output layer of the neural network, using the following formulas:\n\\begin{align}\n\\mathbf{z_2} &= \\mathbf{W_2}\\mathbf{h} + \\mathbf{b_2}   \\tag{3} \\\\\n\\mathbf{\\hat y} &= \\mathrm{softmax}(\\mathbf{z_2})   \\tag{4} \\\\\n\\end{align}\nTo calculate softmax of a vector \\mathbf{z}, the i-th component of the resulting vector is given by:\n \\textrm{softmax}(\\textbf{z})_i = \\frac{e^{z_i} }{\\sum\\limits_{j=1}^{V} e^{z_j} }  \\tag{5} \nLet’s work through an example.\n\nz = np.array([9, 8, 11, 10, 8.5])\nz\n\narray([ 9. ,  8. , 11. , 10. ,  8.5])\n\n\nYou’ll need to calculate the exponentials of each element, both for the numerator and for the denominator.\n\ne_z = np.exp(z)\ne_z\n\narray([ 8103.08392758,  2980.95798704, 59874.1417152 , 22026.46579481,\n        4914.7688403 ])\n\n\nThe denominator is equal to the sum of these exponentials.\n\nsum_e_z = np.sum(e_z)\nsum_e_z\n\nnp.float64(97899.41826492078)\n\n\nAnd the value of the first element of \\textrm{softmax}(\\textbf{z}) is given by:\n\ne_z[0]/sum_e_z\n\nnp.float64(0.08276947985173956)\n\n\nThis is for one element. We can use numpy’s vectorized operations to calculate the values of all the elements of the \\textrm{softmax}(\\textbf{z}) vector in one go.\nImplement the softmax function.\n\ndef softmax(z):\n    # BEGIN your code here\n    e_z = np.exp(z)\n    sum_e_z = np.sum(e_z)\n    return e_z / sum_e_z\n    # END your code here\n\nNow check that it works.\n\nsoftmax([9, 8, 11, 10, 8.5])\n\narray([0.08276948, 0.03044919, 0.61158833, 0.22499077, 0.05020223])\n\n\nExpected output:\narray([0.08276948, 0.03044919, 0.61158833, 0.22499077, 0.05020223])"
  },
  {
    "objectID": "notes/c2w4/lab04.html#dimensions-1-d-arrays-vs-2-d-column-vectors",
    "href": "notes/c2w4/lab04.html#dimensions-1-d-arrays-vs-2-d-column-vectors",
    "title": "Word Embeddings: Ungraded Practice Notebook",
    "section": "Dimensions: 1-D arrays vs 2-D column vectors",
    "text": "Dimensions: 1-D arrays vs 2-D column vectors\nBefore moving on to implement forward propagation, backpropagation, and gradient descent, let’s have a look at the dimensions of the vectors you’ve been handling until now.\nCreate a vector of length V filled with zeros.\n\nx_array = np.zeros(V)\nx_array\n\narray([0., 0., 0., 0., 0.])\n\n\nThis is a 1-dimensional array, as revealed by the .shape property of the array.\n\nx_array.shape\n\n(5,)\n\n\nTo perform matrix multiplication in the next steps, we actually need your column vectors to be represented as a matrix with one column. In numpy, this matrix is represented as a 2-dimensional array.\nThe easiest way to convert a 1D vector to a 2D column matrix is to set its .shape property to the number of rows and one column, as shown in the next cell.\n\nx_column_vector = x_array.copy()\nx_column_vector.shape = (V, 1)  # alternatively ... = (x_array.shape[0], 1)\nx_column_vector\n\narray([[0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.]])\n\n\nThe shape of the resulting “vector” is:\n\nx_column_vector.shape\n\n(5, 1)\n\n\nSo we now have a 5x1 matrix that we can use to perform standard matrix multiplication."
  },
  {
    "objectID": "notes/c2w4/lab04.html#forward-propagation",
    "href": "notes/c2w4/lab04.html#forward-propagation",
    "title": "Word Embeddings: Ungraded Practice Notebook",
    "section": "Forward propagation",
    "text": "Forward propagation\nLet’s dive into the neural network itself, which is shown below with all the dimensions and formulas you’ll need.\n\n Figure 2\n\nSet N equal to 3. Remember that N is a hyperparameter of the CBOW model that represents the size of the word embedding vectors, as well as the size of the hidden layer.\n\nN = 3\n\n\nInitialization of the weights and biases\nBefore we start training the neural network, we need to initialize the weight matrices and bias vectors with random values.\nIn the assignment we will implement a function to do this yourself using numpy.random.rand. In this notebook, we’ve pre-populated these matrices and vectors for you.\n\nW1 = np.array([[ 0.41687358,  0.08854191, -0.23495225,  0.28320538,  0.41800106],\n               [ 0.32735501,  0.22795148, -0.23951958,  0.4117634 , -0.23924344],\n               [ 0.26637602, -0.23846886, -0.37770863, -0.11399446,  0.34008124]])\n\nW2 = np.array([[-0.22182064, -0.43008631,  0.13310965],\n               [ 0.08476603,  0.08123194,  0.1772054 ],\n               [ 0.1871551 , -0.06107263, -0.1790735 ],\n               [ 0.07055222, -0.02015138,  0.36107434],\n               [ 0.33480474, -0.39423389, -0.43959196]])\n\nb1 = np.array([[ 0.09688219],\n               [ 0.29239497],\n               [-0.27364426]])\n\nb2 = np.array([[ 0.0352008 ],\n               [-0.36393384],\n               [-0.12775555],\n               [-0.34802326],\n               [-0.07017815]])\n\nCheck that the dimensions of these matrices match those shown in the figure above.\n\n# BEGIN your code here\nprint(f'V (vocabulary size): {V}')\nprint(f'N (embedding size / size of the hidden layer): {N}')\nprint(f'size of W1: {W1.shape} (NxV)')\nprint(f'size of b1: {b1.shape} (Nx1)')\nprint(f'size of W2: {W1.shape} (VxN)')\nprint(f'size of b2: {b2.shape} (Vx1)')\n# END your code here\n\nV (vocabulary size): 5\nN (embedding size / size of the hidden layer): 3\nsize of W1: (3, 5) (NxV)\nsize of b1: (3, 1) (Nx1)\nsize of W2: (3, 5) (VxN)\nsize of b2: (5, 1) (Vx1)\n\n\n\n\nTraining example\nRun the next cells to get the first training example, made of the vector representing the context words “i am because i”, and the target which is the one-hot vector representing the center word “happy”.\n\nWe don’t need to worry about the Python syntax, but there are some explanations below if we want to know what’s happening behind the scenes.\n\n\ntraining_examples = get_training_example(words, 2, word2Ind, V)\n\n\nget_training_examples, which uses the yield keyword, is known as a generator. When run, it builds an iterator, which is a special type of object that… we can iterate on (using a for loop for instance), to retrieve the successive values that the function generates.\nIn this case get_training_examples yields training examples, and iterating on training_examples will return the successive training examples.\n\n\nx_array, y_array = next(training_examples)\n\n\nnext is another special keyword, which gets the next available value from an iterator. Here, you’ll get the very first value, which is the first training example. If we run this cell again, you’ll get the next value, and so on until the iterator runs out of values to return.\nIn this notebook next is used because we will only be performing one iteration of training. In this week’s assignment with the full training over several iterations you’ll use regular for loops with the iterator that supplies the training examples.\n\nThe vector representing the context words, which will be fed into the neural network, is:\n\nx_array\n\narray([0.25, 0.25, 0.  , 0.5 , 0.  ])\n\n\nThe one-hot vector representing the center word to be predicted is:\n\ny_array\n\narray([0., 0., 1., 0., 0.])\n\n\nNow convert these vectors into matrices (or 2D arrays) to be able to perform matrix multiplication on the right types of objects, as explained above.\n\nx = x_array.copy()\nx.shape = (V, 1)\nprint('x')\nprint(x)\nprint()\n\ny = y_array.copy()\ny.shape = (V, 1)\nprint('y')\nprint(y)\n\nx\n[[0.25]\n [0.25]\n [0.  ]\n [0.5 ]\n [0.  ]]\n\ny\n[[0.]\n [0.]\n [1.]\n [0.]\n [0.]]\n\n\n\n\nValues of the hidden layer\nNow that we have initialized all the variables that we need for forward propagation, we can calculate the values of the hidden layer using the following formulas:\n\\begin{align}\n\\mathbf{z_1} = \\mathbf{W_1}\\mathbf{x} + \\mathbf{b_1}  \\tag{1} \\\\\n\\mathbf{h} = \\mathrm{ReLU}(\\mathbf{z_1})  \\tag{2} \\\\\n\\end{align}\nFirst, we can calculate the value of \\mathbf{z_1}.\n\nz1 = np.dot(W1, x) + b1\n\n\n np.dot is numpy’s function for matrix multiplication.\n\nAs expected we get an N by 1 matrix, or column vector with N elements, where N is equal to the embedding size, which is 3 in this example.\n\nz1\n\narray([[ 0.36483875],\n       [ 0.63710329],\n       [-0.3236647 ]])\n\n\nWe can now take the ReLU of \\mathbf{z_1} to get \\mathbf{h}, the vector with the values of the hidden layer.\n\nh = relu(z1)\nh\n\narray([[0.36483875],\n       [0.63710329],\n       [0.        ]])\n\n\nApplying ReLU means that the negative element of \\mathbf{z_1} has been replaced with a zero.\n\n\nValues of the output layer\nHere are the formulas we need to calculate the values of the output layer, represented by the vector \\mathbf{\\hat y}:\n\\begin{align}\n\\mathbf{z_2} &= \\mathbf{W_2}\\mathbf{h} + \\mathbf{b_2}   \\tag{3} \\\\\n\\mathbf{\\hat y} &= \\mathrm{softmax}(\\mathbf{z_2})   \\tag{4} \\\\\n\\end{align}\nFirst, calculate \\mathbf{z_2}.\n\n# BEGIN your code here\nz2 = np.dot(W2, h) + b2\n# END your code here\n\nz2\n\narray([[-0.31973737],\n       [-0.28125477],\n       [-0.09838369],\n       [-0.33512159],\n       [-0.19919612]])\n\n\nExpected output:\narray([[-0.31973737],\n       [-0.28125477],\n       [-0.09838369],\n       [-0.33512159],\n       [-0.19919612]])\nThis is a V by 1 matrix, where V is the size of the vocabulary, which is 5 in this example.\nNow calculate the value of \\mathbf{\\hat y}.\n\n# BEGIN your code here\ny_hat = softmax(z2)\n# END your code here\n\ny_hat\n\narray([[0.18519074],\n       [0.19245626],\n       [0.23107446],\n       [0.18236353],\n       [0.20891502]])\n\n\nExpected output:\narray([[0.18519074],\n       [0.19245626],\n       [0.23107446],\n       [0.18236353],\n       [0.20891502]])\nAs you’ve performed the calculations with random matrices and vectors (apart from the input vector), the output of the neural network is essentially random at this point. The learning process will adjust the weights and biases to match the actual targets better.\nThat being said, what word did the neural network predict?\n\n\nSolution\n\n\nThe neural network predicted the word “happy”: the largest element of \\mathbf{\\hat y} is the third one, and the third word of the vocabulary is “happy”.\n\n\nHere’s how we could implement this in Python:\n\n\nprint(Ind2word[np.argmax(y_hat)])\n\n\nWell done, you’ve completed the forward propagation phase!"
  },
  {
    "objectID": "notes/c2w4/lab04.html#cross-entropy-loss",
    "href": "notes/c2w4/lab04.html#cross-entropy-loss",
    "title": "Word Embeddings: Ungraded Practice Notebook",
    "section": "Cross-entropy loss",
    "text": "Cross-entropy loss\nNow that we have the network’s prediction, we can calculate the cross-entropy loss to determine how accurate the prediction was compared to the actual target.\n\nRemember that we are working on a single training example, not on a batch of examples, which is why we are using loss and not cost, which is the generalized form of loss.\n\nFirst let’s recall what the prediction was.\n\ny_hat\n\narray([[0.18519074],\n       [0.19245626],\n       [0.23107446],\n       [0.18236353],\n       [0.20891502]])\n\n\nAnd the actual target value is:\n\ny\n\narray([[0.],\n       [0.],\n       [1.],\n       [0.],\n       [0.]])\n\n\nThe formula for cross-entropy loss is:\n J=-\\sum\\limits_{k=1}^{V}y_k\\log{\\hat{y}_k} \\tag{6}\nImplement the cross-entropy loss function.\nHere are a some hints if you’re stuck.\n\n\nHint 1\n\n&lt;p&gt;To multiply two numpy matrices (such as &lt;code&gt;y&lt;/code&gt; and &lt;code&gt;y_hat&lt;/code&gt;) element-wise, we can simply use the &lt;code&gt;*&lt;/code&gt; operator.&lt;/p&gt;\n\n\n\nHint 2\n\n\nOnce we have a vector equal to the element-wise multiplication of y and y_hat, we can use np.sum to calculate the sum of the elements of this vector.\n\n\n\ndef cross_entropy_loss(y_predicted, y_actual):\n    # BEGIN your code here\n    loss = np.sum(-np.log(y_hat)*y)\n    # END your code here\n    return loss\n\nNow use this function to calculate the loss with the actual values of \\mathbf{y} and \\mathbf{\\hat y}.\n\ncross_entropy_loss(y_hat, y)\n\nnp.float64(1.4650152923611108)\n\n\nExpected output:\n1.4650152923611106\nThis value is neither good nor bad, which is expected as the neural network hasn’t learned anything yet.\nThe actual learning will start during the next phase: backpropagation."
  },
  {
    "objectID": "notes/c2w4/lab04.html#backpropagation",
    "href": "notes/c2w4/lab04.html#backpropagation",
    "title": "Word Embeddings: Ungraded Practice Notebook",
    "section": "Backpropagation",
    "text": "Backpropagation\nThe formulas that we will implement for backpropagation are the following.\n\\begin{align}\n\\frac{\\partial J}{\\partial \\mathbf{W_1}} &= \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right )\\mathbf{x}^\\top \\tag{7}\\\\\n\\frac{\\partial J}{\\partial \\mathbf{W_2}} &= (\\mathbf{\\hat{y}} - \\mathbf{y})\\mathbf{h^\\top} \\tag{8}\\\\\n\\frac{\\partial J}{\\partial \\mathbf{b_1}} &= \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right ) \\tag{9}\\\\\n\\frac{\\partial J}{\\partial \\mathbf{b_2}} &= \\mathbf{\\hat{y}} - \\mathbf{y} \\tag{10}\n\\end{align}\n\nNote: these formulas are slightly simplified compared to the ones in the lecture as you’re working on a single training example, whereas the lecture provided the formulas for a batch of examples. In the assignment you’ll be implementing the latter.\n\nLet’s start with an easy one.\nCalculate the partial derivative of the loss function with respect to \\mathbf{b_2}, and store the result in grad_b2.\n\\frac{\\partial J}{\\partial \\mathbf{b_2}} = \\mathbf{\\hat{y}} - \\mathbf{y} \\tag{10}\n\n# BEGIN your code here\ngrad_b2 = y_hat - y\n# END your code here\n\ngrad_b2\n\narray([[ 0.18519074],\n       [ 0.19245626],\n       [-0.76892554],\n       [ 0.18236353],\n       [ 0.20891502]])\n\n\nExpected output:\narray([[ 0.18519074],\n       [ 0.19245626],\n       [-0.76892554],\n       [ 0.18236353],\n       [ 0.20891502]])\nNext, calculate the partial derivative of the loss function with respect to \\mathbf{W_2}, and store the result in grad_W2.\n\\frac{\\partial J}{\\partial \\mathbf{W_2}} = (\\mathbf{\\hat{y}} - \\mathbf{y})\\mathbf{h^\\top} \\tag{8}\n\nHint: use .T to get a transposed matrix, e.g. h.T returns \\mathbf{h^\\top}.\n\n\n# BEGIN your code here\ngrad_W2 = np.dot(y_hat - y, h.T)\n# END your code here\n\ngrad_W2\n\narray([[ 0.06756476,  0.11798563,  0.        ],\n       [ 0.0702155 ,  0.12261452,  0.        ],\n       [-0.28053384, -0.48988499,  0.        ],\n       [ 0.06653328,  0.1161844 ,  0.        ],\n       [ 0.07622029,  0.13310045,  0.        ]])\n\n\nExpected output:\narray([[ 0.06756476,  0.11798563,  0.        ],\n       [ 0.0702155 ,  0.12261452,  0.        ],\n       [-0.28053384, -0.48988499,  0.        ],\n       [ 0.06653328,  0.1161844 ,  0.        ],\n       [ 0.07622029,  0.13310045,  0.        ]])\nNow calculate the partial derivative with respect to \\mathbf{b_1} and store the result in grad_b1.\n\\frac{\\partial J}{\\partial \\mathbf{b_1}} = \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right ) \\tag{9}\n\n# BEGIN your code here\ngrad_b1 = relu(np.dot(W2.T, y_hat - y))\n# END your code here\n\ngrad_b1\n\narray([[0.        ],\n       [0.        ],\n       [0.17045858]])\n\n\nExpected output:\narray([[0.        ],\n       [0.        ],\n       [0.17045858]])\nFinally, calculate the partial derivative of the loss with respect to \\mathbf{W_1}, and store it in grad_W1.\n\\frac{\\partial J}{\\partial \\mathbf{W_1}} = \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right )\\mathbf{x}^\\top \\tag{7}\n\n# BEGIN your code here\ngrad_W1 = np.dot(relu(np.dot(W2.T, y_hat - y)), x.T)\n# END your code here\n\ngrad_W1\n\narray([[0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.04261464, 0.04261464, 0.        , 0.08522929, 0.        ]])\n\n\nExpected output:\narray([[0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.04261464, 0.04261464, 0.        , 0.08522929, 0.        ]])\nBefore moving on to gradient descent, double-check that all the matrices have the expected dimensions.\n\n# BEGIN your code here\nprint(f'V (vocabulary size): {V}')\nprint(f'N (embedding size / size of the hidden layer): {N}')\nprint(f'size of grad_W1: {grad_W1.shape} (NxV)')\nprint(f'size of grad_b1: {grad_b1.shape} (Nx1)')\nprint(f'size of grad_W2: {grad_W1.shape} (VxN)')\nprint(f'size of grad_b2: {grad_b2.shape} (Vx1)')\n# END your code here\n\nV (vocabulary size): 5\nN (embedding size / size of the hidden layer): 3\nsize of grad_W1: (3, 5) (NxV)\nsize of grad_b1: (3, 1) (Nx1)\nsize of grad_W2: (3, 5) (VxN)\nsize of grad_b2: (5, 1) (Vx1)"
  },
  {
    "objectID": "notes/c2w4/lab04.html#gradient-descent",
    "href": "notes/c2w4/lab04.html#gradient-descent",
    "title": "Word Embeddings: Ungraded Practice Notebook",
    "section": "Gradient descent",
    "text": "Gradient descent\nDuring the gradient descent phase, we will update the weights and biases by subtracting \\alpha times the gradient from the original matrices and vectors, using the following formulas.\n\\begin{align}\n\\mathbf{W_1} &:= \\mathbf{W_1} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{W_1}} \\tag{11}\\\\\n\\mathbf{W_2} &:= \\mathbf{W_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{W_2}} \\tag{12}\\\\\n\\mathbf{b_1} &:= \\mathbf{b_1} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_1}} \\tag{13}\\\\\n\\mathbf{b_2} &:= \\mathbf{b_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_2}} \\tag{14}\\\\\n\\end{align}\nFirst, let set a value for \\alpha.\n\nalpha = 0.03\n\nThe updated weight matrix \\mathbf{W_1} will be:\n\nW1_new = W1 - alpha * grad_W1\n\nLet’s compare the previous and new values of \\mathbf{W_1}:\n\nprint('old value of W1:')\nprint(W1)\nprint()\nprint('new value of W1:')\nprint(W1_new)\n\nold value of W1:\n[[ 0.41687358  0.08854191 -0.23495225  0.28320538  0.41800106]\n [ 0.32735501  0.22795148 -0.23951958  0.4117634  -0.23924344]\n [ 0.26637602 -0.23846886 -0.37770863 -0.11399446  0.34008124]]\n\nnew value of W1:\n[[ 0.41687358  0.08854191 -0.23495225  0.28320538  0.41800106]\n [ 0.32735501  0.22795148 -0.23951958  0.4117634  -0.23924344]\n [ 0.26509758 -0.2397473  -0.37770863 -0.11655134  0.34008124]]\n\n\nThe difference is very subtle (hint: take a closer look at the last row), which is why it takes a fair amount of iterations to train the neural network until it reaches optimal weights and biases starting from random values.\nNow calculate the new values of \\mathbf{W_2} (to be stored in W2_new), \\mathbf{b_1} (in b1_new), and \\mathbf{b_2} (in b2_new).\n\\begin{align}\n\\mathbf{W_2} &:= \\mathbf{W_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{W_2}} \\tag{12}\\\\\n\\mathbf{b_1} &:= \\mathbf{b_1} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_1}} \\tag{13}\\\\\n\\mathbf{b_2} &:= \\mathbf{b_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_2}} \\tag{14}\\\\\n\\end{align}\n\n# BEGIN your code here\nW2_new = W2 - alpha * grad_W2\nb1_new = b1 - alpha * grad_b1\nb2_new = b2 - alpha * grad_b2\n# END your code here\n\nprint('W2_new')\nprint(W2_new)\nprint()\nprint('b1_new')\nprint(b1_new)\nprint()\nprint('b2_new')\nprint(b2_new)\n\nW2_new\n[[-0.22384758 -0.43362588  0.13310965]\n [ 0.08265956  0.0775535   0.1772054 ]\n [ 0.19557112 -0.04637608 -0.1790735 ]\n [ 0.06855622 -0.02363691  0.36107434]\n [ 0.33251813 -0.3982269  -0.43959196]]\n\nb1_new\n[[ 0.09688219]\n [ 0.29239497]\n [-0.27875802]]\n\nb2_new\n[[ 0.02964508]\n [-0.36970753]\n [-0.10468778]\n [-0.35349417]\n [-0.0764456 ]]\n\n\nExpected output:\nW2_new\n[[-0.22384758 -0.43362588  0.13310965]\n [ 0.08265956  0.0775535   0.1772054 ]\n [ 0.19557112 -0.04637608 -0.1790735 ]\n [ 0.06855622 -0.02363691  0.36107434]\n [ 0.33251813 -0.3982269  -0.43959196]]\n\nb1_new\n[[ 0.09688219]\n [ 0.29239497]\n [-0.27875802]]\n\nb2_new\n[[ 0.02964508]\n [-0.36970753]\n [-0.10468778]\n [-0.35349417]\n [-0.0764456 ]]\nCongratulations, we have completed one iteration of training using one training example!\nYou’ll need many more iterations to fully train the neural network, and we can optimize the learning process by training on batches of examples, as described in the lecture. We will get to do this during this week’s assignment."
  },
  {
    "objectID": "notes/c2w4/lab04.html#extracting-word-embedding-vectors",
    "href": "notes/c2w4/lab04.html#extracting-word-embedding-vectors",
    "title": "Word Embeddings: Ungraded Practice Notebook",
    "section": "Extracting word embedding vectors",
    "text": "Extracting word embedding vectors\nOnce we have finished training the neural network, we have three options to get word embedding vectors for the words of your vocabulary, based on the weight matrices \\mathbf{W_1} and/or \\mathbf{W_2}.\n\nOption 1: extract embedding vectors from \\mathbf{W_1}\nThe first option is to take the columns of \\mathbf{W_1} as the embedding vectors of the words of the vocabulary, using the same order of the words as for the input and output vectors.\n\nNote: in this practice notebook the values of the word embedding vectors are meaningless after a single iteration with just one training example, but here’s how we would proceed after the training process is complete.\n\nFor example \\mathbf{W_1} is this matrix:\n\nW1\n\narray([[ 0.41687358,  0.08854191, -0.23495225,  0.28320538,  0.41800106],\n       [ 0.32735501,  0.22795148, -0.23951958,  0.4117634 , -0.23924344],\n       [ 0.26637602, -0.23846886, -0.37770863, -0.11399446,  0.34008124]])\n\n\nThe first column, which is a 3-element vector, is the embedding vector of the first word of your vocabulary. The second column is the word embedding vector for the second word, and so on.\nThe first, second, etc. words are ordered as follows.\n\nfor i in range(V):\n    print(Ind2word[i])\n\nam\nbecause\nhappy\ni\nlearning\n\n\nSo the word embedding vectors corresponding to each word are:\n\n# loop through each word of the vocabulary\nfor word in word2Ind:\n    # extract the column corresponding to the index of the word in the vocabulary\n    word_embedding_vector = W1[:, word2Ind[word]]\n    \n    print(f'{word}: {word_embedding_vector}')\n\nam: [0.41687358 0.32735501 0.26637602]\nbecause: [ 0.08854191  0.22795148 -0.23846886]\nhappy: [-0.23495225 -0.23951958 -0.37770863]\ni: [ 0.28320538  0.4117634  -0.11399446]\nlearning: [ 0.41800106 -0.23924344  0.34008124]\n\n\n\n\nOption 2: extract embedding vectors from \\mathbf{W_2}\nThe second option is to take \\mathbf{W_2} transposed, and take its columns as the word embedding vectors just like we did for \\mathbf{W_1}.\n\nW2.T\n\narray([[-0.22182064,  0.08476603,  0.1871551 ,  0.07055222,  0.33480474],\n       [-0.43008631,  0.08123194, -0.06107263, -0.02015138, -0.39423389],\n       [ 0.13310965,  0.1772054 , -0.1790735 ,  0.36107434, -0.43959196]])\n\n\n\n# loop through each word of the vocabulary\nfor word in word2Ind:\n    # extract the column corresponding to the index of the word in the vocabulary\n    word_embedding_vector = W2.T[:, word2Ind[word]]\n    \n    print(f'{word}: {word_embedding_vector}')\n\nam: [-0.22182064 -0.43008631  0.13310965]\nbecause: [0.08476603 0.08123194 0.1772054 ]\nhappy: [ 0.1871551  -0.06107263 -0.1790735 ]\ni: [ 0.07055222 -0.02015138  0.36107434]\nlearning: [ 0.33480474 -0.39423389 -0.43959196]\n\n\n\n\nOption 3: extract embedding vectors from \\mathbf{W_1} and \\mathbf{W_2}\nThe third option, which is the one we will use in this week’s assignment, uses the average of \\mathbf{W_1} and \\mathbf{W_2^\\top}.\nCalculate the average of \\mathbf{W_1} and \\mathbf{W_2^\\top}, and store the result in W3.\n\n# BEGIN your code here\nW3 = (W1+W2.T)/2\n# END your code here\n\nW3\n\narray([[ 0.09752647,  0.08665397, -0.02389858,  0.1768788 ,  0.3764029 ],\n       [-0.05136565,  0.15459171, -0.15029611,  0.19580601, -0.31673866],\n       [ 0.19974284, -0.03063173, -0.27839106,  0.12353994, -0.04975536]])\n\n\nExpected output:\narray([[ 0.09752647,  0.08665397, -0.02389858,  0.1768788 ,  0.3764029 ],\n       [-0.05136565,  0.15459171, -0.15029611,  0.19580601, -0.31673866],\n       [ 0.19974284, -0.03063173, -0.27839106,  0.12353994, -0.04975536]])\nExtracting the word embedding vectors works just like the two previous options, by taking the columns of the matrix you’ve just created.\n\n# loop through each word of the vocabulary\nfor word in word2Ind:\n    # extract the column corresponding to the index of the word in the vocabulary\n    word_embedding_vector = W3[:, word2Ind[word]]\n    \n    print(f'{word}: {word_embedding_vector}')\n\nam: [ 0.09752647 -0.05136565  0.19974284]\nbecause: [ 0.08665397  0.15459171 -0.03063173]\nhappy: [-0.02389858 -0.15029611 -0.27839106]\ni: [0.1768788  0.19580601 0.12353994]\nlearning: [ 0.3764029  -0.31673866 -0.04975536]\n\n\nYou’re now ready to take on this week’s assignment!\n\n\nHow this practice relates to and differs from the upcoming graded assignment\n\nIn the assignment, for each iteration of training we will use batches of examples instead of a single example. The formulas for forward propagation and backpropagation will be modified accordingly, and we will use cross-entropy cost instead of cross-entropy loss.\nWe will also complete several iterations of training, until we reach an acceptably low cross-entropy cost, at which point we can extract good word embeddings from the weight matrices.\nAfter extracting the word embedding vectors, we will use principal component analysis (PCA) to visualize the vectors, which will enable we to perform an intrinsic evaluation of the quality of the vectors, as explained in the lecture."
  },
  {
    "objectID": "notes/c2w4/lab01.html",
    "href": "notes/c2w4/lab01.html",
    "title": "Word Embeddings First Steps: Data Preparation",
    "section": "",
    "text": "course banner\nIn this series of ungraded notebooks, you’ll try out all the individual techniques that we learned about in the lectures. Practicing on small examples will prepare we for the graded assignment, where we will combine the techniques in more advanced ways to create word embeddings from a real-life corpus.\nThis notebook focuses on data preparation, which is the first step of any machine learning algorithm. It is a very important step because models are only as good as the data they are trained on and the models used require the data to have a particular structure to process it properly.\nTo get started, import and initialize all the libraries we will need.\nimport re\nimport nltk\nimport emoji\nimport numpy as np\nfrom nltk.tokenize import word_tokenize\nfrom utils2 import get_dict",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "L1 - Data preparation"
    ]
  },
  {
    "objectID": "notes/c2w4/lab01.html#cleaning-and-tokenization",
    "href": "notes/c2w4/lab01.html#cleaning-and-tokenization",
    "title": "Word Embeddings First Steps: Data Preparation",
    "section": "Cleaning and tokenization",
    "text": "Cleaning and tokenization\nTo demonstrate the cleaning and tokenization process, consider a corpus that contains emojis and various punctuation signs.\n\n# Define a corpus\ncorpus = 'Who ❤️ \"word embeddings\" in 2020? I do!!!'\n\nFirst, replace all interrupting punctuation signs — such as commas and exclamation marks — with periods.\n\n# Print original corpus\nprint(f'Corpus:  {corpus}')\n\n# Do the substitution\ndata = re.sub(r'[,!?;-]+', '.', corpus)\n\n# Print cleaned corpus\nprint(f'After cleaning punctuation:  {data}')\n\nCorpus:  Who ❤️ \"word embeddings\" in 2020? I do!!!\nAfter cleaning punctuation:  Who ❤️ \"word embeddings\" in 2020. I do.\n\n\nNext, use NLTK’s tokenization engine to split the corpus into individual tokens.\n\n# Print cleaned corpus\nprint(f'Initial string:  {data}')\n\n# Tokenize the cleaned corpus\ndata = nltk.word_tokenize(data)\n\n# Print the tokenized version of the corpus\nprint(f'After tokenization:  {data}')\n\nInitial string:  Who ❤️ \"word embeddings\" in 2020. I do.\nAfter tokenization:  ['Who', '❤️', '``', 'word', 'embeddings', \"''\", 'in', '2020', '.', 'I', 'do', '.']\n\n\nFinally, as we saw in the lecture, get rid of numbers and punctuation other than periods, and convert all the remaining tokens to lowercase.\n\n# Print the tokenized version of the corpus\nprint(f'Initial list of tokens:  {data}')\n\n# Filter tokenized corpus using list comprehension\ndata = [ ch.lower() for ch in data\n         if ch.isalpha()\n         or ch == '.'\n         or emoji.get_emoji_regexp().search(ch)\n       ]\n\n# Print the tokenized and filtered version of the corpus\nprint(f'After cleaning:  {data}')\n\nInitial list of tokens:  ['Who', '❤️', '``', 'word', 'embeddings', \"''\", 'in', '2020', '.', 'I', 'do', '.']\nAfter cleaning:  ['who', '❤️', 'word', 'embeddings', 'in', '.', 'i', 'do', '.']\n\n\nNote that the heart emoji is considered as a token just like any normal word.\nNow let’s streamline the cleaning and tokenization process by wrapping the previous steps in a function.\n\n# Define the 'tokenize' function that will include the steps previously seen\ndef tokenize(corpus):\n    data = re.sub(r'[,!?;-]+', '.', corpus)\n    data = nltk.word_tokenize(data)  # tokenize string to words\n    data = [ ch.lower() for ch in data\n             if ch.isalpha()\n             or ch == '.'\n             or emoji.get_emoji_regexp().search(ch)\n           ]\n    return data\n\nApply this function to the corpus that you’ll be working on in the rest of this notebook: “I am happy because I am learning”\n\n# Define new corpus\ncorpus = 'I am happy because I am learning'\n\n# Print new corpus\nprint(f'Corpus:  {corpus}')\n\n# Save tokenized version of corpus into 'words' variable\nwords = tokenize(corpus)\n\n# Print the tokenized version of the corpus\nprint(f'Words (tokens):  {words}')\n\nCorpus:  I am happy because I am learning\nWords (tokens):  ['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n\n\nNow try it out yourself with your own sentence.\n\n# Run this with any sentence\ntokenize(\"Now it's your turn: try with your own sentence!\")\n\n['now', 'it', 'your', 'turn', 'try', 'with', 'your', 'own', 'sentence', '.']",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "L1 - Data preparation"
    ]
  },
  {
    "objectID": "notes/c2w4/lab01.html#sliding-window-of-words",
    "href": "notes/c2w4/lab01.html#sliding-window-of-words",
    "title": "Word Embeddings First Steps: Data Preparation",
    "section": "Sliding window of words",
    "text": "Sliding window of words\nNow that we have transformed the corpus into a list of clean tokens, we can slide a window of words across this list. For each window we can extract a center word and the context words.\nThe get_windows function in the next cell was introduced in the lecture.\n\n# Define the 'get_windows' function\ndef get_windows(words, C):\n    i = C\n    while i &lt; len(words) - C:\n        center_word = words[i]\n        context_words = words[(i - C):i] + words[(i+1):(i+C+1)]\n        yield context_words, center_word\n        i += 1\n\nThe first argument of this function is a list of words (or tokens). The second argument, C, is the context half-size. Recall that for a given center word, the context words are made of C words to the left and C words to the right of the center word.\nHere is how we can use this function to extract context words and center words from a list of tokens. These context and center words will make up the training set that we will use to train the CBOW model.\n\n# Print 'context_words' and 'center_word' for the new corpus with a 'context half-size' of 2\nfor x, y in get_windows(['i', 'am', 'happy', 'because', 'i', 'am', 'learning'], 2):\n    print(f'{x}\\t{y}')\n\n['i', 'am', 'because', 'i'] happy\n['am', 'happy', 'i', 'am']  because\n['happy', 'because', 'am', 'learning']  i\n\n\nThe first example of the training set is made of:\n\nthe context words “i”, “am”, “because”, “i”,\nand the center word to be predicted: “happy”.\n\nNow try it out yourself. In the next cell, we can change both the sentence and the context half-size.\n\n# Print 'context_words' and 'center_word' for any sentence with a 'context half-size' of 1\nfor x, y in get_windows(tokenize(\"Now it's your turn: try with your own sentence!\"), 1):\n    print(f'{x}\\t{y}')\n\n['now', 'your'] it\n['it', 'turn']  your\n['your', 'try'] turn\n['turn', 'with']    try\n['try', 'your'] with\n['with', 'own'] your\n['your', 'sentence']    own\n['own', '.']    sentence",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "L1 - Data preparation"
    ]
  },
  {
    "objectID": "notes/c2w4/lab01.html#transforming-words-into-vectors-for-the-training-set",
    "href": "notes/c2w4/lab01.html#transforming-words-into-vectors-for-the-training-set",
    "title": "Word Embeddings First Steps: Data Preparation",
    "section": "Transforming words into vectors for the training set",
    "text": "Transforming words into vectors for the training set\nTo finish preparing the training set, we need to transform the context words and center words into vectors.\n\nMapping words to indices and indices to words\nThe center words will be represented as one-hot vectors, and the vectors that represent context words are also based on one-hot vectors.\nTo create one-hot word vectors, we can start by mapping each unique word to a unique integer (or index). We have provided a helper function, get_dict, that creates a Python dictionary that maps words to integers and back.\n\n# Get 'word2Ind' and 'Ind2word' dictionaries for the tokenized corpus\nword2Ind, Ind2word = get_dict(words)\n\nHere’s the dictionary that maps words to numeric indices.\n\n# Print 'word2Ind' dictionary\nword2Ind\n\n{'am': 0, 'because': 1, 'happy': 2, 'i': 3, 'learning': 4}\n\n\nWe can use this dictionary to get the index of a word.\n\n# Print value for the key 'i' within word2Ind dictionary\nprint(\"Index of the word 'i':  \",word2Ind['i'])\n\nIndex of the word 'i':   3\n\n\nAnd conversely, here’s the dictionary that maps indices to words.\n\n# Print 'Ind2word' dictionary\nInd2word\n\n{0: 'am', 1: 'because', 2: 'happy', 3: 'i', 4: 'learning'}\n\n\n\n# Print value for the key '2' within Ind2word dictionary\nprint(\"Word which has index 2:  \",Ind2word[2] )\n\nWord which has index 2:   happy\n\n\nFinally, get the length of either of these dictionaries to get the size of the vocabulary of your corpus, in other words the number of different words making up the corpus.\n\n# Save length of word2Ind dictionary into the 'V' variable\nV = len(word2Ind)\n\n# Print length of word2Ind dictionary\nprint(\"Size of vocabulary: \", V)\n\nSize of vocabulary:  5\n\n\n\n\nGetting one-hot word vectors\nRecall from the lecture that we can easily convert an integer, n, into a one-hot vector.\nConsider the word “happy”. First, retrieve its numeric index.\n\n# Save index of word 'happy' into the 'n' variable\nn = word2Ind['happy']\n\n# Print index of word 'happy'\nn\n\n2\n\n\nNow create a vector with the size of the vocabulary, and fill it with zeros.\n\n# Create vector with the same length as the vocabulary, filled with zeros\ncenter_word_vector = np.zeros(V)\n\n# Print vector\ncenter_word_vector\n\narray([0., 0., 0., 0., 0.])\n\n\nWe can confirm that the vector has the right size.\n\n# Assert that the length of the vector is the same as the size of the vocabulary\nlen(center_word_vector) == V\n\nTrue\n\n\nNext, replace the 0 of the n-th element with a 1.\n\n# Replace element number 'n' with a 1\ncenter_word_vector[n] = 1\n\nAnd we have your one-hot word vector.\n\n# Print vector\ncenter_word_vector\n\narray([0., 0., 1., 0., 0.])\n\n\nWe can now group all of these steps in a convenient function, which takes as parameters: a word to be encoded, a dictionary that maps words to indices, and the size of the vocabulary.\n\n# Define the 'word_to_one_hot_vector' function that will include the steps previously seen\ndef word_to_one_hot_vector(word, word2Ind, V):\n    one_hot_vector = np.zeros(V)\n    one_hot_vector[word2Ind[word]] = 1\n    return one_hot_vector\n\nCheck that it works as intended.\n\n# Print output of 'word_to_one_hot_vector' function for word 'happy'\nword_to_one_hot_vector('happy', word2Ind, V)\n\narray([0., 0., 1., 0., 0.])\n\n\nWhat is the word vector for “learning”?\n\n# Print output of 'word_to_one_hot_vector' function for word 'learning'\nword_to_one_hot_vector('learning', word2Ind, V)\n\narray([0., 0., 0., 0., 1.])\n\n\nExpected output:\narray([0., 0., 0., 0., 1.])\n\n\nGetting context word vectors\nTo create the vectors that represent context words, we will calculate the average of the one-hot vectors representing the individual words.\nLet’s start with a list of context words.\n\n# Define list containing context words\ncontext_words = ['i', 'am', 'because', 'i']\n\nUsing Python’s list comprehension construct and the word_to_one_hot_vector function that we created in the previous section, we can create a list of one-hot vectors representing each of the context words.\n\n# Create one-hot vectors for each context word using list comprehension\ncontext_words_vectors = [word_to_one_hot_vector(w, word2Ind, V) for w in context_words]\n\n# Print one-hot vectors for each context word\ncontext_words_vectors\n\n[array([0., 0., 0., 1., 0.]),\n array([1., 0., 0., 0., 0.]),\n array([0., 1., 0., 0., 0.]),\n array([0., 0., 0., 1., 0.])]\n\n\nAnd we can now simply get the average of these vectors using numpy’s mean function, to get the vector representation of the context words.\n\n# Compute mean of the vectors using numpy\nnp.mean(context_words_vectors, axis=0)\n\narray([0.25, 0.25, 0.  , 0.5 , 0.  ])\n\n\nNote the axis=0 parameter that tells mean to calculate the average of the rows (if we had wanted the average of the columns, we would have used axis=1).\nNow create the context_words_to_vector function that takes in a list of context words, a word-to-index dictionary, and a vocabulary size, and outputs the vector representation of the context words.\n\n# Define the 'context_words_to_vector' function that will include the steps previously seen\ndef context_words_to_vector(context_words, word2Ind, V):\n    context_words_vectors = [word_to_one_hot_vector(w, word2Ind, V) for w in context_words]\n    context_words_vectors = np.mean(context_words_vectors, axis=0)\n    return context_words_vectors\n\nAnd check that we obtain the same output as the manual approach above.\n\n# Print output of 'context_words_to_vector' function for context words: 'i', 'am', 'because', 'i'\ncontext_words_to_vector(['i', 'am', 'because', 'i'], word2Ind, V)\n\narray([0.25, 0.25, 0.  , 0.5 , 0.  ])\n\n\nWhat is the vector representation of the context words “am happy i am”?\n\n# Print output of 'context_words_to_vector' function for context words: 'am', 'happy', 'i', 'am'\ncontext_words_to_vector(['am', 'happy', 'i', 'am'], word2Ind, V)\n\narray([0.5 , 0.  , 0.25, 0.25, 0.  ])\n\n\nExpected output:\narray([0.5 , 0.  , 0.25, 0.25, 0.  ])",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "L1 - Data preparation"
    ]
  },
  {
    "objectID": "notes/c2w4/lab01.html#building-the-training-set",
    "href": "notes/c2w4/lab01.html#building-the-training-set",
    "title": "Word Embeddings First Steps: Data Preparation",
    "section": "Building the training set",
    "text": "Building the training set\nWe can now combine the functions that we created in the previous sections, to build a training set for the CBOW model, starting from the following tokenized corpus.\n\n# Print corpus\nwords\n\n['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n\n\nTo do this we need to use the sliding window function (get_windows) to extract the context words and center words, and we then convert these sets of words into a basic vector representation using word_to_one_hot_vector and context_words_to_vector.\n\n# Print vectors associated to center and context words for corpus\nfor context_words, center_word in get_windows(words, 2):  # reminder: 2 is the context half-size\n    print(f'Context words:  {context_words} -&gt; {context_words_to_vector(context_words, word2Ind, V)}')\n    print(f'Center word:  {center_word} -&gt; {word_to_one_hot_vector(center_word, word2Ind, V)}')\n    print()\n\nContext words:  ['i', 'am', 'because', 'i'] -&gt; [0.25 0.25 0.   0.5  0.  ]\nCenter word:  happy -&gt; [0. 0. 1. 0. 0.]\n\nContext words:  ['am', 'happy', 'i', 'am'] -&gt; [0.5  0.   0.25 0.25 0.  ]\nCenter word:  because -&gt; [0. 1. 0. 0. 0.]\n\nContext words:  ['happy', 'because', 'am', 'learning'] -&gt; [0.25 0.25 0.25 0.   0.25]\nCenter word:  i -&gt; [0. 0. 0. 1. 0.]\n\n\n\nIn this practice notebook you’ll be performing a single iteration of training using a single example, but in this week’s assignment you’ll train the CBOW model using several iterations and batches of example. Here is how we would use a Python generator function (remember the yield keyword from the lecture?) to make it easier to iterate over a set of examples.\n\n# Define the generator function 'get_training_example'\ndef get_training_example(words, C, word2Ind, V):\n    for context_words, center_word in get_windows(words, C):\n        yield context_words_to_vector(context_words, word2Ind, V), word_to_one_hot_vector(center_word, word2Ind, V)\n\nThe output of this function can be iterated on to get successive context word vectors and center word vectors, as demonstrated in the next cell.\n\n# Print vectors associated to center and context words for corpus using the generator function\nfor context_words_vector, center_word_vector in get_training_example(words, 2, word2Ind, V):\n    print(f'Context words vector:  {context_words_vector}')\n    print(f'Center word vector:  {center_word_vector}')\n    print()\n\nContext words vector:  [0.25 0.25 0.   0.5  0.  ]\nCenter word vector:  [0. 0. 1. 0. 0.]\n\nContext words vector:  [0.5  0.   0.25 0.25 0.  ]\nCenter word vector:  [0. 1. 0. 0. 0.]\n\nContext words vector:  [0.25 0.25 0.25 0.   0.25]\nCenter word vector:  [0. 0. 0. 1. 0.]\n\n\n\nYour training set is ready, we can now move on to the CBOW model itself which will be covered in the next lecture notebook.\nCongratulations on finishing this lecture notebook! Hopefully we now have a better understanding of how to prepare your data before feeding it to a continuous bag-of-words model.\nKeep it up!",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "L1 - Data preparation"
    ]
  },
  {
    "objectID": "notes/c2w4/index.html",
    "href": "notes/c2w4/index.html",
    "title": "Word embeddings with neural networks",
    "section": "",
    "text": "course banner\n\n\n\n\nFigure 1\n\n\n\n\n\n\n\n\nFigure 2\nThese are my notes for Week 4 notes for the Natural Language Processing with Probabilistic Models Course in the Natural Language Processing Specialization Offered by DeepLearning.AI on Coursera.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w4/index.html#overview",
    "href": "notes/c2w4/index.html#overview",
    "title": "Word embeddings with neural networks",
    "section": "Overview",
    "text": "Overview\nWord embeddings are used in most NLP applications. Whenever we are dealing with text, we first have to find a way to encode the words as numbers. Word embedding are a very common technique that allows we to do so. Here are a few applications of word embeddings that we should be able to implement by the time we complete the specialization.\n\n\n\n\n\n\n\napplications\n\n\n\n\nFigure 3: Basic Applications of word embeddings\n\n\n\n\n\n\n\n\napplications\n\n\n\n\nFigure 4: Advanced applications of word embeddings\n\n\n\nBy the end of this week we will be able to:\n\nIdentify the key concepts of word representations\nGenerate word embeddings\nPrepare text for machine learning\nImplement the continuous bag-of-words model",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w4/index.html#basic-word-representations",
    "href": "notes/c2w4/index.html#basic-word-representations",
    "title": "Word embeddings with neural networks",
    "section": "Basic Word Representations",
    "text": "Basic Word Representations\n\n\n\n\n\n\n\nFigure 5: One-hot vectors\n\n\n\n\n\n\n\n\nFigure 6: One-hot vectors\n\n\n\nBasic word representations could be classified into the following:\n\nIntegers\nOne-hot vectors\nWord embeddings\n\nTo the left, we have an example where we use integers to represent a word. The issue there is that there is no reason why one word corresponds to a bigger number than another. To fix this problem we introduce one hot vectors (diagram on the right). To implement one hot vectors, we have to initialize a vector of zeros of dimension V and then put a 1 in the index corresponding to the word we are representing.\nThe pros of one-hot vectors: simple and require no implied ordering. The cons of one-hot vectors: huge and encode no meaning.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w4/index.html#word-embeddings",
    "href": "notes/c2w4/index.html#word-embeddings",
    "title": "Word embeddings with neural networks",
    "section": "Word Embeddings",
    "text": "Word Embeddings\n\n\n\n\n\n\n\nFigure 7: Meaning as vectors in 1D\n\n\n\n\n\n\n\n\nFigure 8: Meaning as vectors in 2D\n\n\n\nFrom the plot above, we can see that when encoding a word in 2D, similar words tend to be found next to each other. Perhaps the first coordinate represents whether a word is positive or negative. The second coordinate tell we whether the word is abstract or concrete. This is just an example, in the real world we will find embeddings with hundreds of dimensions. We can think of each coordinate as a number telling we something about the word.\nThe pros:\n\nLow dimensions (less than V)\nAllow we to encode meaning",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w4/index.html#how-to-create-word-embeddings",
    "href": "notes/c2w4/index.html#how-to-create-word-embeddings",
    "title": "Word embeddings with neural networks",
    "section": "How to Create Word Embeddings?",
    "text": "How to Create Word Embeddings?\n\n\n\n\n\n\n\nFigure 9: Meaning as vectors in 2D\n\n\nTo create word embeddings we always need a corpus of text, and an embedding method.\nThe context of a word tells we what type of words tend to occur near that specific word. The context is important as this is what will give meaning to each word embedding.\nEmbeddings There are many types of possible methods that allow we to learn the word embeddings. The machine learning model performs a learning task, and the main by-products of this task are the word embeddings. The task could be to learn to predict a word based on the surrounding words in a sentence of the corpus, as in the case of the continuous bag-of-words.\nThe task is self-supervised: it is both unsupervised in the sense that the input data — the corpus — is unlabelled, and supervised in the sense that the data itself provides the necessary context which would ordinarily make up the labels.\nWhen training word vectors, there are some parameters we need to tune. (i.e. the dimension of the word vector)",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w4/index.html#word-embedding-methods",
    "href": "notes/c2w4/index.html#word-embedding-methods",
    "title": "Word embeddings with neural networks",
    "section": "Word Embedding Methods",
    "text": "Word Embedding Methods\nClassical Methods\n\nword2vec (Google, 2013)\nContinuous bag-of-words (CBOW): the model learns to predict the center word given some context words.\nContinuous skip-gram / Skip-gram with negative sampling (SGNS): the model learns to predict the words surrounding a given input word.\nGlobal Vectors (GloVe) (Stanford, 2014): factorizes the logarithm of the corpus’s word co-occurrence matrix, similar to the count matrix you’ve used before.\nfastText (Facebook, 2016): based on the skip-gram model and takes into account the structure of words by representing words as an n-gram of characters. It supports out-of-vocabulary (OOV) words.\n\nDeep learning, contextual embeddings\nIn these more advanced models, words have different embeddings depending on their context. We can download pre-trained embeddings for the following models.\n\nBERT (Google, 2018):\nELMo (Allen Institute for AI, 2018)\nGPT-2 (OpenAI, 2018)",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w4/index.html#continuous-bag-of-words-model",
    "href": "notes/c2w4/index.html#continuous-bag-of-words-model",
    "title": "Word embeddings with neural networks",
    "section": "Continuous Bag-of-Words Model",
    "text": "Continuous Bag-of-Words Model\n\n\n\n\n\n\n\nFigure 10: Meaning as vectors in 2D\n\n\n\n\n\n\n\n\nFigure 11: Meaning as vectors in 2D\n\n\n\n\n\n\n\n\nFigure 12: Meaning as vectors in 2D\n\n\n\n\nContinuous Bag of Words Model To create word embeddings, we need a corpus and a learning algorithm. The by-product of this task would be a set of word embeddings. In the case of the continuous bag-of-words model, the objective of the task is to predict a missing word based on the surrounding words.\nHere is a visualization that shows we how the models works.\nAs we can see, the window size in the image above is 5. The context size, C, is 2. C usually tells we how many words before or after the center word the model will use to make the prediction. Here is another visualization that shows an overview of the model.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w4/index.html#cleaning-and-tokenization",
    "href": "notes/c2w4/index.html#cleaning-and-tokenization",
    "title": "Word embeddings with neural networks",
    "section": "Cleaning and Tokenization",
    "text": "Cleaning and Tokenization\n\n\n\n\n\n\n\nFigure 13: Meaning as vectors in 2D\n\n\n\n\n\n\n\n\nFigure 14: Meaning as vectors in 2D\n\n\n\nBefore implementing any natural language processing algorithm, we might want to clean the data and tokenize it. Here are a few things to keep track of when handling your data.\nWe can clean data using python as follows:\nWe can add as many conditions as we want in the lines corresponding to the green rectangle above.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w4/index.html#sliding-window-of-words-in-python",
    "href": "notes/c2w4/index.html#sliding-window-of-words-in-python",
    "title": "Word embeddings with neural networks",
    "section": "Sliding Window of words in Python",
    "text": "Sliding Window of words in Python\n\n\n\n\n\n\n\nFigure 15: Sliding Window of words in Python\n\n\nThe code above shows we a function which takes in two parameters.\nWords: a list of words.\nC: the context size.\nWe first start by setting i to C. Then we single out the center_word, and the context_words. We then yield those and increment i.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w4/index.html#transforming-words-into-vectors",
    "href": "notes/c2w4/index.html#transforming-words-into-vectors",
    "title": "Word embeddings with neural networks",
    "section": "Transforming Words into Vectors",
    "text": "Transforming Words into Vectors\n\n\n\n\n\n\n\nFigure 16: Transforming Words into Vectors\n\n\nAs we can see, we started with one-hot vectors for the context words and and we transform them into a single vector by taking an average. As a result we end up having the following vectors that we can use for your training.\n\n\n\n\n\n\n\nFigure 17: Sliding Window of words in Python",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w4/index.html#lecture-notebook---data-preparation",
    "href": "notes/c2w4/index.html#lecture-notebook---data-preparation",
    "title": "Word embeddings with neural networks",
    "section": "Lecture Notebook - Data Preparation",
    "text": "Lecture Notebook - Data Preparation",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w4/index.html#architecture-of-the-cbow-model",
    "href": "notes/c2w4/index.html#architecture-of-the-cbow-model",
    "title": "Word embeddings with neural networks",
    "section": "Architecture of the CBOW Model",
    "text": "Architecture of the CBOW Model\nThe architecture for the CBOW model could be described as follows\n\n\n\n\n\n\n\nFigure 18: Architecture for the CBOW Model\n\n\nWe have an input, X, which is the average of all context vectors. We then multiply it by W_1 and add b1. The result goes through a ReLU function to give we your hidden layer. That layer is then multiplied by W_2 and we add b_2. The result goes through a softmax which gives we a distribution over V, vocabulary words. We pick the vocabulary word that corresponds to the arg-max of the output.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w4/index.html#architecture-of-the-cbow-model-dimensions",
    "href": "notes/c2w4/index.html#architecture-of-the-cbow-model-dimensions",
    "title": "Word embeddings with neural networks",
    "section": "Architecture of the CBOW Model: Dimensions",
    "text": "Architecture of the CBOW Model: Dimensions\nThe equations for the previous model are:\n\nz_1 = W_1 x + b_1\n\n\nh = ReLU(z_1)\n\n\nz_2 = W_2 h + b_2\n\n\n\\hat{y} = softmax(z_2)\n\nHere, we can see the dimensions:\n\n\n\n\n\n\n\nFigure 19: Architecture for the CBOW Model\n\n\nMake sure we go through the matrix multiplications and understand why the dimensions make sense.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w4/index.html#architecture-of-the-cbow-model-dimensions-2",
    "href": "notes/c2w4/index.html#architecture-of-the-cbow-model-dimensions-2",
    "title": "Word embeddings with neural networks",
    "section": "Architecture of the CBOW Model: Dimensions 2",
    "text": "Architecture of the CBOW Model: Dimensions 2\nWhen dealing with batch input, we can stack the examples as columns. We can then proceed to multiply the matrices as follows:\n\n\n\n\n\n\n\nFigure 20: Dimensions Batch Input\n\n\nIn the diagram above, we can see the dimensions of each matrix. Note that your \\hat{Y} is of dimension V by m. Each column is the prediction of the column corresponding to the context words. So the first column in \\hat{Y} is the prediction corresponding to the first column of X.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w4/index.html#architecture-of-the-cbow-model-activation-functions",
    "href": "notes/c2w4/index.html#architecture-of-the-cbow-model-activation-functions",
    "title": "Word embeddings with neural networks",
    "section": "Architecture of the CBOW Model: Activation Functions",
    "text": "Architecture of the CBOW Model: Activation Functions\n\nReLU funciton\nThe rectified linear unit (ReLU), is one of the most popular activation functions. When we feed a vector, namely x, into a ReLU function. We end up taking x=max(0,x). This is a drawing that shows ReLU.\n\n\n\n\n\n\n\nFigure 21: Dimensions Batch Input\n\n\n\n\nSoftmax function\nThe softmax function takes a vector and transforms it into a probability distribution. For example, given the following vector z, we can transform it into a probability distribution as follows.\n\n\n\n\n\n\n\nFigure 22: Dimensions Batch Input\n\n\nAs we can see, we can compute:\n\n\\hat{y} = \\frac{e^{z_i}}{\\sum_{j=1}^V e^{z_j}}",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w4/index.html#lecture-notebook---intro-to-cbow-model",
    "href": "notes/c2w4/index.html#lecture-notebook---intro-to-cbow-model",
    "title": "Word embeddings with neural networks",
    "section": "Lecture Notebook - Intro to CBOW model",
    "text": "Lecture Notebook - Intro to CBOW model\nlab 2 the CBOW model",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w4/index.html#training-a-cbow-model-cost-function",
    "href": "notes/c2w4/index.html#training-a-cbow-model-cost-function",
    "title": "Word embeddings with neural networks",
    "section": "Training a CBOW Model: Cost Function",
    "text": "Training a CBOW Model: Cost Function\nThe cost function for the CBOW model is a cross-entropy loss defined as:\n\nJ = -\\sum_{k=1}^V y_k log(\\hat{y}_k)\n\\tag{1}\nHere is an example where we use the equation above.\n\n\n\n\n\n\n\nFigure 23: Dimensions Batch Input\n\n\nWhy is the cost 4.61 in the example above?",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w4/index.html#training-a-cbow-model-forward-propagation",
    "href": "notes/c2w4/index.html#training-a-cbow-model-forward-propagation",
    "title": "Word embeddings with neural networks",
    "section": "Training a CBOW Model: Forward Propagation",
    "text": "Training a CBOW Model: Forward Propagation\nTraining a CBOW Model: Forward Propagation Forward propagation is defined as:\n\nZ_1 = W_1 X + B_1\n\n\nH = ReLU(Z_1)\n\n\nZ_2 = W_2 H + B_2\n\n\n\\hat{Y} = softmax(Z_2)\n\nIn the image below we start from the left and we forward propagate all the way to the right.\n\n\n\n\n\n\n\nFigure 24: Dimensions Batch Input\n\n\nTo calculate the loss of a batch, we have to compute the following:\n\nJ_{batch} = -\\frac{1}{m} \\sum_{i=1}^m \\sum_{j=1}^V y_j(i) log(\\hat{y}^j(i))\n\nGiven, your predicted center word matrix, and actual center word matrix, we can compute the loss.\n\n\n\n\n\n\n\nFigure 25: Dimensions Batch Input",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w4/index.html#training-a-cbow-model-backpropagation-and-gradient-descent",
    "href": "notes/c2w4/index.html#training-a-cbow-model-backpropagation-and-gradient-descent",
    "title": "Word embeddings with neural networks",
    "section": "Training a CBOW Model: Backpropagation and Gradient Descent",
    "text": "Training a CBOW Model: Backpropagation and Gradient Descent\n\n\n\n\n\n\n\nFigure 26: Dimensions Batch Input\n\n\n\n\n\n\n\n\nFigure 27: Dimensions Batch Input\n\n\n\nTraining a CBOW Model: Backpropagation and Gradient Descent Backpropagation: calculate partial derivatives of cost with respect to weights and biases.\nWhen computing the back-prop in this model, we need to compute the following:\n\n\\frac{\\partial J_{batch}}{\\partial W_1}, \\frac{\\partial J_{batch}}{\\partial W_2}, \\frac{\\partial J_{batch}}{\\partial B_1}, \\frac{\\partial J_{batch}}{\\partial B_2}\n\nGradient descent: update weights and biases\nNow to update the weights we can iterate as follows:\n\nW_1 := W_1 - \\alpha \\frac{\\partial J_{batch}}{\\partial W_1}\n\n\nW_2 := W_2 - \\alpha \\frac{\\partial J_{batch}}{\\partial W_2}\n\n\nB_1 := B_1 - \\alpha \\frac{\\partial J_{batch}}{\\partial B_1}\n\n\nB_2 := B_2 - \\alpha \\frac{\\partial J_{batch}}{\\partial B_2}\n\nA smaller alpha allows for more gradual updates to the weights and biases, whereas a larger number allows for a faster update of the weights. If α is too large, we might not learn anything, if it is too small, your model will take forever to train.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w4/index.html#lecture-notebook---training-the-cbow-model",
    "href": "notes/c2w4/index.html#lecture-notebook---training-the-cbow-model",
    "title": "Word embeddings with neural networks",
    "section": "Lecture Notebook - Training the CBOW model",
    "text": "Lecture Notebook - Training the CBOW model",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w4/index.html#extracting-word-embedding-vectors",
    "href": "notes/c2w4/index.html#extracting-word-embedding-vectors",
    "title": "Word embeddings with neural networks",
    "section": "Extracting Word Embedding Vectors",
    "text": "Extracting Word Embedding Vectors\nThere are two options to extract word embeddings after training the continuous bag of words model. We can use W_1 as follows:\n\n\n\n\n\n\n\nFigure 28: Dimensions Batch Input\n\n\nIf we were to use W_1, each column will correspond to the embeddings of a specific word. We can also use W_2 as follows:\n\n\n\n\n\n\n\nFigure 29: Dimensions Batch Input\n\n\nThe final option is to take an average of both matrices as follows:\n\n\n\n\n\n\n\nFigure 30: Dimensions Batch Input",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w4/index.html#lecture-notebook---word-embeddings",
    "href": "notes/c2w4/index.html#lecture-notebook---word-embeddings",
    "title": "Word embeddings with neural networks",
    "section": "Lecture Notebook - Word Embeddings",
    "text": "Lecture Notebook - Word Embeddings\nlab 4 - Word Embeddings",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w4/index.html#lecture-notebook-word-embeddings-step-by-step",
    "href": "notes/c2w4/index.html#lecture-notebook-word-embeddings-step-by-step",
    "title": "Word embeddings with neural networks",
    "section": "Lecture notebook: Word embeddings step by step",
    "text": "Lecture notebook: Word embeddings step by step\nLab 5 - Word embeddings step by step",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w4/index.html#evaluating-word-embeddings-intrinsic-evaluation",
    "href": "notes/c2w4/index.html#evaluating-word-embeddings-intrinsic-evaluation",
    "title": "Word embeddings with neural networks",
    "section": "Evaluating Word Embeddings: Intrinsic Evaluation",
    "text": "Evaluating Word Embeddings: Intrinsic Evaluation\nIntrinsic evaluation allows we to test relationships between words. It allows we to capture semantic analogies as, “France” is to “Paris” as “Italy” is to &lt;?&gt; and also syntactic analogies as “seen” is to “saw” as “been” is to &lt;?&gt;.\nAmbiguous cases could be much harder to track:\n\n\n\n\n\n\n\nFigure 31: Dimensions Batch Input\n\n\nHere are a few ways that allow to use intrinsic evaluation.\n\n\n\n\n\n\n\nFigure 32: Dimensions Batch Input\n\n\n\n\n\n\n\n\nFigure 33: Dimensions Batch Input",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w4/index.html#evaluating-word-embeddings-extrinsic-evaluation",
    "href": "notes/c2w4/index.html#evaluating-word-embeddings-extrinsic-evaluation",
    "title": "Word embeddings with neural networks",
    "section": "Evaluating Word Embeddings: Extrinsic Evaluation",
    "text": "Evaluating Word Embeddings: Extrinsic Evaluation\nExtrinsic evaluation tests word embeddings on external tasks like named entity recognition, parts-of-speech tagging, etc.\n\nEvaluates actual usefulness of embeddings\nTime Consuming\nMore difficult to trouble shoot\n\nSo now we know both intrinsic and extrinsic evaluation.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w4/index.html#conclusion",
    "href": "notes/c2w4/index.html#conclusion",
    "title": "Word embeddings with neural networks",
    "section": "Conclusion",
    "text": "Conclusion\nThis week we learned the following concepts.\n\nData preparation\nWord representations\nContinuous bag-of-words model\nEvaluation\n\nWe have all the foundations now. From now on, we will start using some advanced AI libraries in the next courses. Congratulations and good luck with the assignment",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/cs11-737-w02-sequence-labeling/index.html",
    "href": "notes/cs11-737-w02-sequence-labeling/index.html",
    "title": "Sequence Labeling",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\nVideo 1: Lesson Video\n\n\n\n\n\n\n\n\nThis week’s slides\n\n\n\n\nSupplementary Figure 1"
  },
  {
    "objectID": "notes/cs11-737-w02-sequence-labeling/index.html#text-classification-and-sequence-labeling",
    "href": "notes/cs11-737-w02-sequence-labeling/index.html#text-classification-and-sequence-labeling",
    "title": "Sequence Labeling",
    "section": "Text classification and sequence labeling",
    "text": "Text classification and sequence labeling\n\nSo text classification and sequence labeling are both very broad. I like to call these task categories. They’re not tasks of themselves but they’re categories of tasks that’ll look very similar and thus can be solved in similar ways. Text classification: given, input text X, predicted output: some categorical label Y. This can be all kinds of different things like topic classification where we take i like peaches and pears and that gives us the topic food and i like peaches and herb and that gives you the topic music because peaches and herb is a old band basically we have language identification which is a particularly important task in multilingual learning and basically this is Taking in language and outputting labels. Taking in text and now putting the language that it’s written in some you know obviously the first is english and the second one is japanese here this becomes very interesting and difficult as i’ll be elaborating later in the lecture portion. Another example that’s very widely known is sentiment analysis and this can be done at the sentence level the document level or you can even do some sentiment analysis with respect to you know individual entities or things like that but from a text classification point of view it would be like sentences or documents so if we have i like peaches and pairs i’d be positive i hate peaches in paris i’d be negative obviously and you know there’s many many other many many other tasks that fall into this sequence labeling on the other hand is given an input text text predict an output label sequence y usually of equal length so this can be formulated as taking in a sequence of words and outputting one tag for each sequence of words so we have part of speech tagging is one example of this so we take in words and output parts of speech another one is lemmatization so what this is doing is it’s basically taking in words and outputting their base form this is kind of this is used sometimes in in english but it’s actually particularly useful or important in languages that have richer morphology than english because essentially if they have lots and lots of conjugation or other things like this the words themselves can become very sparse so identifying the underlying base form helps you understand like what the word is referring to better another variety is morphological tagging in morphological tagging again we’re going to be talking about morphology in about two classes but basically it’s predicting various features of the word based on like for example here we have he saw becomes the past tense and a finite verb form two it’s a number and the type is a cardinal number this is plural for example so this can become more complicated but more languages have more complicated morphologicals and there’s other ones as well there’s also span labeling tasks and sometimes these fan labeling tasks are treated in sequence labeling tasks but they’re actually a little bit different and basically the idea is given in input text x predict output spans and labels y and these include things like named entity recognition where you want to identify the spans and labels on the spans for named entities like person people or things so here we have graham newbig as a person in carnegie mellon university as an organization another example is something called syntactic chunking or shallow syntactic parsing where basically you want to split up the sentence into like noun phrases and verb phrases like this and that’s another example and there’s also semantic role labeling where semantic role labeling basically identifies fans and tries to identify like this is an actor this is a predicate and this is a location so what role each of these arguments is playing with respect to the predicate but as you can see all of these basically have to do with like identifying spans and labeling them in some way so span labeling can also be treated as a sequence labeling task so you predict the beginning in and out tags bio tags for each word in the span so if we take our span labeling task like this where we want to identify a person or organization we convert this into a thing where we basically have one task or one tag for each word so we have beginning of person inside a person out out out which means no no span is identified beginning organization in organization in an organization like this so the good news is then you know if you want to do this span identification task you can just solve it with a sequence labeling model so sequence labeling is nice in that way there might be better ways to handle span identification but this is this is one way to do it so another another task that is slightly different but also can be handled as sequence labeling is text segmentation so given in input text x split it into segmented text y so a very common example of this in many languages is tokenization where you want to take something like a well a well-conceived thought exercise that has like lots of punctuation and intervening hyphens and stuff like this and split it up into things that look a little bit more like just kind of natural word boundaries like like this adding spaces between each of the punctuation etc another variety of this which is necessary for some languages not not so many languages in the world but but some languages is word segmentation and this is a example from japanese because japanese is written with no spaces between words so you can’t just split on white space using your python like split function you have to actually find the locations of the word boundaries and this is a non-trivial task so this is one segmentation that you could have here this is the kind of like quote-unquote correct segmentation but you could also make a mistake and segment like this and this phrase itself if you split it this way means foreign people voting rights or like suffrage for whereas this means the foreign government so basically if you split the correct way you get one meaning if you split the wrong way you get another meeting and this can you know mess up information retrieval systems any translation systems anything that you would think of and then there’s also morphological segmentation which again we’re going to talk about a bit more which is for example we might have something that looks a little bit like this which can be split into different ways i i believe this is turkish i think someone can correct me if i’m wrong here i i actually forgot where i got the example but i believe it’s turkish and so then we have dog and plural or dog paddle and attempts so basically whether you split it one way or the other also affects whether this is like a plural noun or a verb and thank you for confirming that this is a determination so this is another another issue that you could have cool are there any questions about this so far oh is the word segmentation solved with additional context in japanese yeah that’s a really good question and morphological analysis also is similar you know having context so basically yes having context is very important because both of these are both of these are reasonable morphological segmentations in different contexts you know they’re both things that could happen sure this might be a lot more frequent but there certainly are examples where this would occur it’s also the case in japanese although that’s a little bit more rare like one one example is that i can put in the zoom chat this means if you split off the first character that means american nuclear reactor if you split off the second character it means a train that’s departing from maihara which is a place in japan and so depending on whether you’re whether you’re in a news newspaper article or a train schedule either of those would be correct so it’s yeah it is based on context then i had another question explaining the distinction between b and i here so b basically is the first tag in a sequence in a span so it would be applied to the first word in the span i would be applied to every subsequent word at the span so you you if you had a single word span it would just have a b but if you have the multi word span it will be b i i i tell the span finishes and having these two different tags is necessary to distinguish cases where you have two spans in a row so if you have like person and then another person later how do we constrain the model so that b is predicted before i for a particular entity that’s a another really good question so if you i maybe maybe i’ll talk about how we actually do this prediction first and then get back to that question great okay so i’d like to talk a little bit about modeling for sequence labeling and classification so the first question is how do we make predictions so given an input x input text x extract features h and predict labels y and the way this works is basically we have text classification and sequence labeling so we have like i like peaches we have some sort of feature extractor that extracts a whole bunch of features from the whole sequence into like a single vector here and then given that single vector we make a prediction we have for sequence labeling we have a feature extractor that extracts one feature or one vector of features for each word in the input and then we make a prediction from this like that and so either way we need a feature extractor it’s just a matter of whether we are extracting a single vector of features for the whole sequence or one vector for each word so a very simple feature extractor that we might use for text classification for example is bag of words where what we do is we look up a a single feature for each of the words and then we add them together to get a vector a vector representing the number of times a particular word occurred in the sentence and then we feed this into a predictor maybe we have a a matrix multiply that turns this into label label values and then we use this to calculate label probabilities so just to clarify when i say bag of words this we have a one hot vector which means a single value in this vector here is one and all the other values are zeros so this is essentially a a vector that represents the identity of the word and nothing else so as our simple predictor we can have something like a linear transform and a softmax function so what that looks like is we take we take the extracted features we multiply them by a weight matrix we add a bias here which tells you essentially how how likely each of the labels is a priori and then the softmax converts arbitrary scores into probabilities so we exponentiate the score at each of the elements in the resulting vector divided by a normalizer to make sure that all of them add up to one add up to one and then that gives us a probability of our final output so it might look a little bit like this after the after the linear transform and bias addition we might have a score vector that looks like this and we turn it into a probability so i think this should be pretty familiar to a lot of people [Music] no questions about this any questions if not i have i have a little bit of a quiz so like we talked about text classification sorry every time i touch my mouse it moves my my slides it’s a little bit annoying but so we talked about text classification where we extract features for the whole sequence and then we use that to predict the the label probabilities for the whole sequence by adding all of these together what do you think of a bag of words of a similar feature extractor used for sequence labeling does that make any sense whatsoever so we extract one one vector using this kind of bag of words lookup function and then make predictions based on this vector would that would that do anything would there be a reasonable way of solving this problem maybe not so reasonable because it’s neglecting the word order yeah maybe not so reasonable because it’s neglecting the word order yeah that’s a a pretty good idea but it’s actually maybe not that bad right what it what it would end up doing is it would basically look up i and then it would make a prediction of what part of speech tag i would be based on what the most frequent part of speech tag for i is and it would make the prediction of like and it would make the prediction of the most frequent thing for likes so yeah it’s a frequency-based model it would be looking up the majority class for each word and actually for part of speech tagging in english and even more so for other languages this actually isn’t that bad at part of speech tagging it gets like high 80s accuracy just because there’s relatively little like peaches is always a plural noun i i can’t i can’t think of any case where it wouldn’t be a plural so even that would be you know a moderately okay feature extractor both for classification and and sequence labeling however you know it does neglect the word order so if you want to do like even better than just majority class you have to come up with something that way so another issue language is not just a bag of words for classification or labeling so to have some examples we have i don’t love pairs there’s nothing i don’t love about pears if you just look at the words in the sentence like love is the positive word and then you have a negation but negation in itself is not you know very indicative of positive or negative sentiment so both of these would be relatively hard to tackle with a bag of words model so what we want to do is we want to come up with a better futurizer to better pull out features for our sequences for each word in our sequences so one example of a better futurizer might be a bag of engrams model so basically instead of looking up each word we would look up each engram so here for example then don’t love would also become a feature in our model and if don’t love is a feature in our model that’s kind of like a negative feature right it would maybe overpower the the love feature and make move that more negative you could also come up with syntax-based features like subject object pairs or neural networks like recurrent neural networks convolutional neural networks self-attention and so until you know 20 or so it was very common to use these kind of handcrafted features or at least handcrafted feature templates and throw them into a support vector machine model or something like this to do text classification now it’s much more common to use neural networks and they also have some really nice properties like allowing transfer across languages which is a very big you know part of this class so we’re going to mainly be focusing on these models in this class this time i’ll talk about recurrent neural networks just because they’re conceptually easy but we’re also going to be talking about things like self-attention so a neural network which a lot of people know is basically a computation graph that is parameterized in a certain way in order to allow it to make predictions the name neural networks comes from neurons in the brain where basically you know they take an information across synapses and then they fire and they give output over the the outputs and the absolutes but the current conception of how we use them it’s basically a mathematical object that allows us to calculate something take in an input generate an output and in particular in this case it’s going to be our featurizer and our predictor so it’s going to take in an output input output some features for each word or each class or it’s going to take in an input and output a score for each for each class so if we define an expression the way we represent it as a computation graph is through nodes and edges between the nodes and so here we have a single variable maybe a vector and that’s represented as a single node the node can be like a scalar value of vector value matrix value tensor value and we can also have operations over all of these values so like let’s say we transpose the vector that operation would be demonstrated as an edge to another node that implements the transpose so an edge represents a function argument and a node with an incoming edge is a function of that edge’s tail node and a node knows how to compute its value and the value of its derivative with respect to each argument and then functions can be nullary like input values here or unary unity or binary so here’s a something representing a matrix multiply so basically now we have x transpose times times a here and these are directed in the cyclic graphs that allow us to calculate more and more complicated expressions so they allow us to you know do things like calculate features make predictions et cetera and we can also name individual parts of the graph and this allows us to you know calculate values that we would like to be calculating like the score of the the probability of prediction and so some algorithms are graph construction and forward propagation that allow us to calculate values so forward propagation basically we we start out with the input values of the graph and we gradually move through the graph to calculate the final result here and we also have back propagation and back propagation basically what it does is it processes examples in reverse topological order calculating the derivatives of the parameters with respect to the final value and this is usually the loss function the value we want to minimize so in many cases this is the negative log likelihood of the predictions over the true values and by minimizing this negative log likelihood that allows us to maximize the probability of getting the correct answer and then we take the derivatives calculated through back propagation to update the parameters of the model so back propagation basically works like this we start from the end of the graph and gradually move back until we back propagate into parameters so let’s say a was a parameter a b was a parameter and c was a parameter we would back propagate into these values and once we have the derivatives that we calculate through this process of back propagation we can use them to update the parameters to kind of improve the likelihood of getting the correct answer so this is a kind of five-minute intro to neural networks if you’re not very familiar with them there’s lots of good tutorials online in particular we’re going to be using a neural network framework pytorch for examples in this class that i think you know a lot of people are familiar with already and our first assignment this time is aimed to kind of have the dual purpose of allowing you to get familiar with you know building models in high torch and stuff like this and also learn more about kind of the interesting difficulties that you have to deal with when you apply these multilingually so if you’ve already taken a class or already used neural networks pretty widely in your in your work then you know all of this will be old to you and you’ll more or less know this already if you haven’t then definitely take advantage of the ta office hours look at the examples ask lots of questions we can forward you some tutorials online to help you out as well so this will be your chance to catch up before we get into the more involved assignments that happen later so yeah actually maybe i’ll skip that part so to give an example of a type of neural network that can be used for featurizing a text classifier or a sequence labeler we are going to talk briefly about recurrent neural networks and recurrent neural networks are models that allow us to do precisely what was mentioned before as being the [Music] issue with a bag of words model which is handling either short or long distance dependencies in language handling word order handling other things like this and so in language there’s many dependencies that span across whole sentences so for example agreement is one example there’s not a whole lot of agreement in english like for example there’s gender agreement and there’s a plural there’s like number agreement between subjects and verbs so here we can see he and himself need to agree she and herself need to agree also he does needs to agree here so if this were i would be i do he does so that’s an example also word order in general we talked about that last class so we need to have some sort of model that’s able to handle these things these are syntactic characteristics that we need to be able to handle and there’s also semantic characteristics so for example we need to have semantic congruency between rain and queen and rain and clouds here you know they’re they’re just things that make sense make sense semantically and don’t make sense mentally based on our knowledge of the world and we also need to handle these as well so recurrent neural networks basically are are one of the tools that we can use to encode sequences either to get representations for each word or representations for other words so basically what recurrent neural networks do is they look up they look up the context or the input at the current time step do a transformation of this into features and then they feed in the features from the previous time step for the next time steps so to give an example if we have i we would feed it through an rnn and get the next vector here like we would feed this through another rnn function we would calculate a vector corresponding to light this is a parameter of the model and then we feed in the result of running i through the rnn to get the and this input here to get the representation for i like and then we have these and we calculate the representation of these i like these and then we have pairs and we calculate the representation of i like these pairs and basically this is a recursive function so each time you’re using the result of the previous function to calculate the result of the next function so when we represent the sentence for text classification basically it would look a bit like this so we would take the last vector in the sequence to make a prediction and that would be useful for things like text classification condition generation retrieval we’ll be talking about the latter queue later in the class but text classification is the one we’re talking about this time and it can also be used to represent words so if we wanted to predict a part of speech label for i and like and these and pairs we could use the immediate output after inputting that word to try to make that prediction as well and this would allow us to do things like pull in contacts from the left side to make this part of speech prediction for things like sequence labeling language modeling calculating representations for for parsing etc so the way we train the rnns is like let’s say we’re training one for sequence labeling we have the predictions here from these we could calculate a negative log likelihood or a loss function something like this we have the label we use the true label of the output to calculate the loss function and we add them together to get the sequence level total loss so this is one big computation graph for the whole sentence and then we take the total loss and we do back propagation from this total loss into the whole the representations for the whole sentence so the parameters of the model are all tied across time the derivatives are aggregated across all time steps and this gives us something called back propagation through time so you can back propagate through the whole sentence and basically optimize the probability of making the correct predictions for the whole science so what did i mean by parameter tying basically the parameters are shared between this rnn function over the entire sentence and because of this this allows you to apply this to sentences of arbitrary length so if you have a sentence of length 50 or a sentence of length 20 or something like this then this would essentially allow you to represent all of them within the same recurrent neural network by just applying this rnn function 50 times or 20 times or three times for a three word sentence and when doing when doing representation for things like sequence labeling it’s very common to use bi-directional rnns and what i mean by this is basically you take the left side and you run a recurrent neural network that steps from time to step zero to time step one to two to three to four from left to right and then you have another recurrent neural network that steps from right to left in this way and aggregates information from both of the directions congratulates that together and makes a prediction and the reason why this is useful is you never know whether the context to disambiguate a particular word would be available on the left side or the right side so this allows you to pull in information from both sides okay so that’s basically the overview of you know a simple method for doing calculation of either representations for the whole sentence so for example if we’re representing the whole sentence we might take this vector we might concatenate the right side of the left rnn and the left side of the right the right side of the back forward rnn and the left side of the backward arm or if we want to represent individual words we might concatenate together the things in each time step to make predictions so this would allow us to do text classification or sequence labels okay are there any questions about this before i jump into the multilingual part okay i guess not so we can jump into the multi-uh multilingual thing that is part of the name of the class of course so i’m gonna be talking about some text some text classification and sequence labeling tasks most of these are tasks that are applicable to any language so they’re explored quite widely on english as well but some of them are inherently multilingual so for example language identification is one that’s inherently multiple so language identification as i mentioned before is the task of identifying the language that a particular text comes in and this is really important for a very broad number of reasons the first reason might be if you want to create like let’s say you want to show people content in only a language that they speak so you know people when they’re doing search online they’ll probably more appreciate results in the language they speak than in another language another example could be for creating data sets for something like machine translation or language modeling or something like this where you only want data in a particular language and and other things like this so actually one of the largest language identification corporal was created by ralph brown here at at lti it’s a benchmark on 1152 languages from a variety of free sources so this is kind of a widely known data set here if you want off the shelf tools for doing language identification one example of a relatively easy one to use is slang id.pai so you can just download this use it for 90 plus languages another example is there’s the chrome language identifier from the chrome browser i actually don’t have a link here but that’s also pretty widely used by people if you want an off-the-shelf method that you can use there’s also a nice survey it’s a little bit old by now but automatic language identification and texts which i can recommend you can take a look at if you’re interested in this and oh missing a slide that i thought i had added here weird okay so i i will just discuss i’ll just discuss this paper so this is a recent paper from 2020 by people at google working on kind of low resource languages it’s quite interesting it’s called language id in the wild unexpected challenges on the path to a thousand language web text corpus and this is not the only paper that has it has pointed out this problem that language identification doesn’t work well but they have some very interesting insights and they also have a very nice kind of example of a of the issues that you encounter when trying to do language identification on web text and so here here are some mind sentences from the web that were supposedly in one language according to google’s text like language identification model so like a whole bunch of people raising their hand and emojis got classified as amani beri the in this was in twee which is why you lie in why you always lie in written in kind of like strange characters a misrendered pdf was for hadi the non-unicode font i’m not sure what this was\n\nyeah it’s a non-non-unicode font i guess was written in this way here this was as balinese it was just like boilerplate this was also english but it was written in like the cherokee script for stabilization he just wrote me ow that became cooler so you can see basically here when people write in like slightly non-standard language it gets identified as other things like even this is like clearly standard english but there were hints of words that often occur in remote so it got recognized as a remote so this kind of just demonstrates how difficult this this task is when you start applying it to web text and i actually have had a similar experience when i was trying to do twitter there was a certain like face like not emoji but like the the faces written with regular characters it was written in canada characters and so many many things were recognized as commonly just because they use that like popular face so there are lots of large corpora like while i’m at it here i can also introduce the oscar purpose this is a very large corpus huge and multilingual obtained by language classification and filtering of the common crawl corpus it’s gotten a bit better since they first released it in terms of the noisiness but when it was first released it was like extremely noisy just because language id didn’t didn’t work so well so this is actually like a really big problem that you need to be aware of if you’re if you’re starting out are there any questions about language id before i move on to the next okay so also kind of standard text classification like yes i said text classifications were like a class class of tasks that make tasks in itself here are some representative ones that people have used these are mostly used for benchmarking multilingual models as opposed to like actually building anything useful so but still you know sometimes you want to know how good your multilingual representations are so they could be good test beds one example is ml doc corpus which is a corpus of a multilingual document classification there’s also the pos x corpus so the this is a paraphrase detection between languages it’s sentence pair classification where you feed in two sentences also cross-lingual natural language inference so this is textual entailment prediction or natural language inference which is also a sentence pair classification task there’s also cross-lingual sentiment classification in chinese in english so this could be used for facing another thing is part of speech and morphological tagging i’m not going to go into a whole lot of about this because i know it’s going to be covered more when we talk about like words parts of speech and morphology but basically there’s the universal dependencies treebank and the universal dependencies treebank basically it contains syntactic parses like dependency forces but it also contains parts of speech and morphological features for 90 languages and it has a standardized universal part of speech set in universal morphology headset to make things consistent across the languages so this is one of the highest quality like multilingual corpora that i’m aware of it’s you know well controlled well conceived and there are some pre-trained models that can use that can do like syntactic analysis on many languages trained on these datasets like unify and stanza if you’re interested in doing multilingual like syntactic analysis named entity recognition this is going to be what we’re going to be doing for the assignment and there’s different types of named entity recognition data sets there’s a gold standard data set from connell 2002 2003 on language independent named entity recognition this is an english german spanish and dutch with human annotated data i actually forgot to add one that i just remembered now that just came out i actually i helped out with this a little bit but it’s a an identity recognition data set for african languages i called masakonner and this is this is nice because it’s also manually labeled but it’s in african languages that have like a lot fewer resources than english german spanish and dutch so it gives kind of a better idea of [Music] like you know how well we’ll be doing a lower resource languages there’s also this wiki and data set for entity recognition and linking in 282 languages this was extracted from wikipedia using inter page links so in wikipedia of course if we go to carnegie mellon university on wikipedia there are many there are many links so there’s no link here but here’s pittsburgh pennsylvania the melon institute of industrial research andrew carnegie so all of these links link to other pages and then if you look up the type of the page according to some annotations that come on wikipedia you can tell that pittsburgh is a city mellon institute of industrial researchers and organization and andrew carnegie is a is a person and then of course you know this is available in lots of languages so we can go to chinese and find the chinese equivalent of andrew carnegie or the chinese equivalent of pittsburgh and and do the same thing so basically this data creates that in many different languages there’s also several composite benchmarks for multilingual learning so they aggregate many different sequence labeling or classification tasks for testing multilingual models one popular one is extreme it’s a massively multilingual benchmark for 10 different tasks 40 different languages another one that came out at a similar time is exclu with 11 tasks over 19 languages so there’s also a new version of extreme cold extreme r that just came out i had been a little bit involved in both of these and extreme r is they swapped out some easy tasks added some harder tasks and added better analysis so you might also consider looking at that for your class project i would warn you that these these benchmarks are very popular and there’s people with like lots of compute that are competing on these benchmarks so you might not it might be a bit of a challenge to keep up with the state of the art there but i think you could work on individual tasks and still do a very good job like some of the tasks where kind of generic models are not working as well so you can definitely take a look to get inspiration for ideas okay great so that’s all i have are there any questions before we move on to the the like discussion period which in this case we’re not doing discussion we’re having a presentation of the assignment but any question about data sets or tasks or oh sorry the homework was on ner i said it beyond that in er but the top part of speech tagging i apologize cool yeah but we’ll have the description of the the task for assignment one before i go into that i just like to point out that starting next time we will indeed be having discussion and reading assignments so the reading assignment for next time is this modeling language variation in universals a survey on typological linguistics for natural language processing the reading is actually it’s only required for suggested that you do sections one through three but the whole survey is good so if you don’t mind reading 30 pages or so it would be worth taking a look at that as well so required is one through three and then based on what you learned in that reading you can try to think of what are some unique typological features of a language that you know regarding phonology morphologies and dex pragmatics doesn’t you don’t need to cover all of them but you can cover like one or two of these and we’ll have a discussion where everybody will share what they came up with cool and today is assignment one introduction tr vijay or who’s going to be presenting this thing\nare you speaking if you’re speaking around me do i need to unmute me i don’t think he’s on mute okay there we go now we can take gray i think you are talking right can you hear me now yep yes great okay so okay hi hi i’m think gray and ti is going to give some introduction yeah all right so this first assignment is to give you a practical introduction to multilingual parts of speech tagging and i think briefly was mentioned so part of speech is just lexical categories or word classes or tags so in the example sentence he saw two words we assigned the part of speech pronoun to he verb to saw and so on so you’ll get a data set of a sentence sentences in different languages and you want to output the pos tags for each of the words in the sentence yeah so as mentioned previously aside from giving you guys a practical introduction to multilingual pause tagging yeah we want to give you a sort of like an experimental approach to multilingual problems such as investigating the challenges to languages which are low resource meaning that there’s a limited availability of label data and of course just be familiarized with deep learning frameworks aws and multilingual data sets so to do this assignment you would need a machine with a gpu so you can use aws or you can use your own computer if you have a gpu and you’ll have to install some python packages for this assignment so the tricky part here i guess would be like the aws setup so shortly i’ll be posting instructions on piazza on how to request aws credit and i think the assignment will have further instructions on how to set it up and all students should have an aws account using your andrew email and just follow the instructions on how to set it up we tried doing the assignment without a gpu but it’s strongly recommended because i think the next assignments are not really doable without a gpu and i trained it on a very old macbook air and it took me around three to four hours to complete the training and you need to retrain it you know when you’re changing the parameters and so on so yeah do it with the gpu and i think one more thing to take note is that make sure that you stop the aws instance when you’re not doing it because you will be continuously billed so i think the aws setup should be fairly straightforward i’m not sure myself tingerie has done it before without instructions but the instruction instructors and the tas of the intro to deep learning course have provided a very comprehensive aws fundamentals playlist so it will be linked in the assignment handout page as well and you can follow the steps in case you have any difficulties yeah so for this assignment we are going to give you a deep file and this this this file will contain all the code and the data that is to use for this assignment and we will post a link later on psl so in the date file you can find the training data it is the training data for six languages and the right hand side is an example what what the format looks like for this training data so this is an example of one sentence so you can see there are the words and the pls text of that word separated by some new lines and each line contains a word and it’s text separated by a tab but actually you don’t have to worry too much about this format because our code will handle this for you and in this homework we are going to use this simple baseline model by rcm model it is a model with an embedding layer and also a bi-directional stem there and so the input of this model will be a sequence of words and the outputs will be a sequence of pos tags so let’s work through the files in the the file so the first file is the config.json file it is the file that contains the hyper parameters that will be used to trend the model you may have to change this ma this file in order to write a report when you are doing some analysis and this udprs.pipe file is the place where we implement how to read a data set but actually i don’t think you will have to modify this so i won’t go into detail here and this model.pi file is the place where we implement this bi-directional stm model and if you are familiar with pytorch then you can see it’s a very simple model it just simply applies and it’s embedding layer and then lcn there and then finally predict the text using a fully connected layer yeah but if you want to make a stronger model then you may have to modify this file and most of the complex jobs are done in this main.pi file it handles the loading of data and it also do some pre-process and do the thing that have simple into batch and also the part that trends the model so let’s go through this content so the first thing it does is that a load the data set with the function we define and then it build a vocabulary for the input text and also the the output pos text the reason that we want to build this vocabulary is that in modern deep learning framework we are when we are using a embedding layer what the embedding layer does is that a map the index of sound balls into some batters so that’s to say before we can use this embedding layer we need to first build a mapping they can assign each word with an index so we have to iterate through the changing data to see what to see the words that occurs in the training data and for similar reason we have to iterate through the data set to see all the possible possible pos tags in this data so these are the purpose of this device and once we have the mapping that map the tokens to the indices and the mapping then map the pos tag to the index we can define these two functions that converge they convert the train data into indices and with these two functions we can define this collet batch function the purpose of this function is that it pack a bunch of text label pairs into a single batch and then this batch will be used to train the model so yeah so what this function does is that it first converts the words inside the the samples to a tensor by coding the function with the file above and you also can convert the tag into indexes by code by cleaning the function and then the most important thing it does is that it paid those sequence of tokens into the sentence so those tensors can be stacked into a single tensor and that single tensor can be later used to train the model and once we have this correct batch function we can define a data loader a data network is an iterator we can get some batches from it by iterate through this data loader so here we define three data loaders for the training set the validation set and testing set respectively and then we can use the data loader to change our model the way we train our model is that we repeat this process for a certain number of times defined in this hyper-parameter mesh epoch and the process is that we first train the model by using the training data and then evaluate the model using the validation set and if after this epoch the model get a better validation loss then this grip will set the model to some place so at the end of this training process you will have the model that has the minimum validation loss as for what this trend function does it is also very simple it just iterates through the data order the chain data loader and then makes prediction over the text and then compute the laws by comparing its prediction and the quantum tags and then use these those to do backward propagation and then code atomizer to update the parameters in the model yeah so it’s basically what our codes do so what you need to submit for this assignment is code and a write-up so part of the you know how to obtain points for this assignment is you need to run the code you need to make notifications to the model and so on but it’s also equally important to give a detailed explanation on the results that you see or what you do and why do you think your changes have made an effect on the results and you can submit this on campus and i think this is the part that everyone really wants to see is how do i get a good grade in this assignment so there’s a lot of ways there’s a lot of tiers as well so if you just want to get you can get a b by just running the code on the existing english model and just running that running the model on the test set you’ll get a b but if you train the model on the different multilingual data sets and you evaluate them using their respective test sets you’ll get a b plus to get anywhere of an a you need to write a report with detailed analysis so there’s a lot of ways you can comment on the results you can see how the performance varies across different languages you can also see you know which tags are most often misplaced for other tags so you know our pronouns and nouns more easily mistaken for each other and so on so that’s what we want to see and if you have a report you know detailing all these explanation you’ll probably get an a minus to get an a or above you will need to get to create a non-trivial extension to improve the existing scores and there’s really a lot of ways you can do this you can add like cnn input layer to capture character level features you can use pre-trained embeddings and so on so there’s really it’s kind of like an experiment that you need to run on your own and we’re excited to see your results."
  },
  {
    "objectID": "notes/cs11-737-w02-sequence-labeling/index.html#resources",
    "href": "notes/cs11-737-w02-sequence-labeling/index.html#resources",
    "title": "Sequence Labeling",
    "section": "Resources",
    "text": "Resources\n\nlangid.py\nUdify\nStanza\nLTI LangID Corpus\nMLDoc\nPAWS-X\nXNLI\nCross-lingual Sentence Classification\nUniversal Dependencies/POS Tags\nCoNLL NER Tasks\nXTREME\nXGLUE"
  },
  {
    "objectID": "notes/c1w2/lab01.html",
    "href": "notes/c1w2/lab01.html",
    "title": "Visualizing Naive Bayes",
    "section": "",
    "text": "Figure 1: course banner\n\n\nIn this lab, we will cover an essential part of data analysis that has not been included in the lecture videos. As we stated in the previous module, data visualization gives insight into the expected performance of any model.\nIn the following exercise, we are going to make a visual inspection of the tweets dataset using the Naïve Bayes features. We will see how we can understand the log-likelihood ratio explained in the videos as a pair of numerical features that can be fed in a machine learning algorithm.\nAt the end of this lab, we will introduce the concept of confidence ellipse as a tool for representing the Naïve Bayes model visually.\n\nimport numpy as np # Library for linear algebra and math utils\nimport pandas as pd # Dataframe library\n\nimport matplotlib.pyplot as plt # Library for plots\nfrom utils import confidence_ellipse # Function to add confidence ellipses to charts\n\n## Calculate the likelihoods for each tweet\nFor each tweet, we have calculated the likelihood of the tweet to be positive and the likelihood to be negative. We have calculated in different columns the numerator and denominator of the likelihood ratio introduced previously.\n\nlog \\frac{P(tweet|pos)}{P(tweet|neg)} = log(P(tweet|pos)) - log(P(tweet|neg))\n\n\npositive = log(P(tweet|pos)) = \\sum_{i=0}^{n}{log P(W_i|pos)}\n\n\nnegative = log(P(tweet|neg)) = \\sum_{i=0}^{n}{log P(W_i|neg)}\n\nWe did not include the code because this is part of this week’s assignment. The ‘bayes_features.csv’ file contains the final result of this process.\nThe cell below loads the table in a dataframe. Dataframes are data structures that simplify the manipulation of data, allowing filtering, slicing, joining, and summarization.\n\ndata = pd.read_csv('bayes_features.csv'); # Load the data from the csv file\n\ndata.head(5) # Print the first 5 tweets features. Each row represents a tweet\n\n\n\n\n\n\n\n\npositive\nnegative\nsentiment\n\n\n\n\n0\n-45.763393\n-63.351354\n1.0\n\n\n1\n-105.491568\n-114.204862\n1.0\n\n\n2\n-57.028078\n-67.216467\n1.0\n\n\n3\n-10.055885\n-18.589057\n1.0\n\n\n4\n-125.749270\n-138.334845\n1.0\n\n\n\n\n\n\n\n\n# Plot the samples using columns 1 and 2 of the matrix\n\nfig, ax = plt.subplots(figsize = (8, 8)) #Create a new figure with a custom size\n\ncolors = ['red', 'green'] # Define a color palete\n\n# Color base on sentiment\nax.scatter(data.positive, data.negative, \n    c=[colors[int(k)] for k in data.sentiment], s = 0.1, marker='*')  # Plot a dot for each tweet\n\n# Custom limits for this chart\nplt.xlim(-250,0)\nplt.ylim(-250,0)\n\nplt.xlabel(\"Positive\") # x-axis label\nplt.ylabel(\"Negative\") # y-axis label\n\nText(0.5, 0, 'Positive')\n\n\nText(0, 0.5, 'Negative')\n\n\n\n\n\n\n\n\n\n\nUsing Confidence Ellipses to interpret Naïve Bayes\nIn this section, we will use the confidence ellipse to give us an idea of what the Naïve Bayes model see.\nA confidence ellipse is a way to visualize a 2D random variable. It is a better way than plotting the points over a cartesian plane because, with big datasets, the points can overlap badly and hide the real distribution of the data. Confidence ellipses summarize the information of the dataset with only four parameters:\n\nCenter: It is the numerical mean of the attributes\nHeight and width: Related with the variance of each attribute. The user must specify the desired amount of standard deviations used to plot the ellipse.\nAngle: Related with the covariance among attributes.\n\nThe parameter n_std stands for the number of standard deviations bounded by the ellipse. Remember that for normal random distributions:\n\nAbout 68% of the area under the curve falls within 1 standard deviation around the mean.\nAbout 95% of the area under the curve falls within 2 standard deviations around the mean.\nAbout 99.7% of the area under the curve falls within 3 standard deviations around the mean.\n\nstandard normal\nIn the next chart, we will plot the data and its corresponding confidence ellipses using 2 std and 3 std.\n\n# Plot the samples using columns 1 and 2 of the matrix\nfig, ax = plt.subplots(figsize = (8, 8))\n\ncolors = ['red', 'green'] # Define a color palete\n\n# Color base on sentiment\n\nax.scatter(data.positive, data.negative, c=[colors[int(k)] for k in data.sentiment], s = 0.1, marker='*')  # Plot a dot for tweet\n\n# Custom limits for this chart\nplt.xlim(-200,40)  \nplt.ylim(-200,40)\n\nplt.xlabel(\"Positive\") # x-axis label\nplt.ylabel(\"Negative\") # y-axis label\n\ndata_pos = data[data.sentiment == 1] # Filter only the positive samples\ndata_neg = data[data.sentiment == 0] # Filter only the negative samples\n\n# Print confidence ellipses of 2 std\nconfidence_ellipse(data_pos.positive, data_pos.negative, ax, n_std=2, edgecolor='black', label=r'$2\\sigma$' )\nconfidence_ellipse(data_neg.positive, data_neg.negative, ax, n_std=2, edgecolor='orange')\n\n# Print confidence ellipses of 3 std\nconfidence_ellipse(data_pos.positive, data_pos.negative, ax, n_std=3, edgecolor='black', linestyle=':', label=r'$3\\sigma$')\nconfidence_ellipse(data_neg.positive, data_neg.negative, ax, n_std=3, edgecolor='orange', linestyle=':')\nax.legend()\n\nplt.show()\n\nText(0.5, 0, 'Positive')\n\n\nText(0, 0.5, 'Negative')\n\n\n\n\n\n\n\n\n\nIn the next cell, we will modify the features of the samples with positive sentiment (1), in a way that the two distributions overlap. In this case, the Naïve Bayes method will produce a lower accuracy than with the original data.\n\ndata2 = data.copy() # Copy the whole data frame\n\n# The following 2 lines only modify the entries in the data frame where sentiment == 1\ndata2.negative[data.sentiment == 1] =  data2.negative * 1.5 + 50 # Modify the negative attribute\ndata2.positive[data.sentiment == 1] =  data2.positive / 1.5 - 50 # Modify the positive attribute \n\n/tmp/ipykernel_128077/2253601370.py:4: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  data2.negative[data.sentiment == 1] =  data2.negative * 1.5 + 50 # Modify the negative attribute\n/tmp/ipykernel_128077/2253601370.py:5: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  data2.positive[data.sentiment == 1] =  data2.positive / 1.5 - 50 # Modify the positive attribute\n\n\nNow let us plot the two distributions and the confidence ellipses\n\n# Plot the samples using columns 1 and 2 of the matrix\nfig, ax = plt.subplots(figsize = (8, 8))\n\ncolors = ['red', 'green'] # Define a color palete\n\n# Color base on sentiment\n\n#data.negative[data.sentiment == 1] =  data.negative * 2\n\nax.scatter(data2.positive, data2.negative, c=[colors[int(k)] for k in data2.sentiment], s = 0.1, marker='*')  # Plot a dot for tweet\n# Custom limits for this chart\nplt.xlim(-200,40)  \nplt.ylim(-200,40)\n\nplt.xlabel(\"Positive\") # x-axis label\nplt.ylabel(\"Negative\") # y-axis label\n\ndata_pos = data2[data2.sentiment == 1] # Filter only the positive samples\ndata_neg = data[data2.sentiment == 0] # Filter only the negative samples\n\n# Print confidence ellipses of 2 std\nconfidence_ellipse(data_pos.positive, data_pos.negative, ax, n_std=2, edgecolor='black', label=r'$2\\sigma$' )\nconfidence_ellipse(data_neg.positive, data_neg.negative, ax, n_std=2, edgecolor='orange')\n\n# Print confidence ellipses of 3 std\nconfidence_ellipse(data_pos.positive, data_pos.negative, ax, n_std=3, edgecolor='black', linestyle=':', label=r'$3\\sigma$')\nconfidence_ellipse(data_neg.positive, data_neg.negative, ax, n_std=3, edgecolor='orange', linestyle=':')\nax.legend()\n\nplt.show()\n\nText(0.5, 0, 'Positive')\n\n\nText(0, 0.5, 'Negative')\n\n\n\n\n\n\n\n\n\nTo give away: Understanding the data allows us to predict if the method will perform well or not. Alternatively, it will allow us to understand why it worked well or bad.\n\n\n\n\nCitationBibTeX citation:@online{bochman2020,\n  author = {Bochman, Oren},\n  title = {Visualizing {Naive} {Bayes}},\n  date = {2020-10-06},\n  url = {https://orenbochman.github.io/notes-nlp/notes/c1w2/lab01.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2020. “Visualizing Naive Bayes.” October 6,\n2020. https://orenbochman.github.io/notes-nlp/notes/c1w2/lab01.html.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Probability & Bayes Rule",
      "L1 - Visualizing Naive Bayes"
    ]
  },
  {
    "objectID": "notes/c1w2/index.html",
    "href": "notes/c1w2/index.html",
    "title": "Probability and Bayes Rule",
    "section": "",
    "text": "Figure 1: course banner\n\n\n\n\n\n\n\n\nFigure 2\nMy notes for Week 2 of the Natural Language Processing with Probabilistic Models Course in the Natural Language Processing Specialization Offered by DeepLearning.AI on Coursera\nThe following two results are due to (Ng and Jordan 2001) by way of Naive_Bayes_classifier wikipedia article.\nAnother detail that can help we make sense of this lesson is the following result relating Naïve Bayes to Logistic Regression which we covered last week. In the case of discrete inputs like indicator or frequency features for discrete events, naive Bayes classifiers form a generative-discriminative pair with multinomial logistic regression classifiers: each naive Bayes classifier can be considered a way of fitting a probability model that optimizes the joint likelihood p (C,x), while logistic regression fits the same probability model to optimize the conditional p(C ∣ x).",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Probability & Bayes Rule",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w2/index.html#probability-of-a-randomly-selected-tweets-sentiment",
    "href": "notes/c1w2/index.html#probability-of-a-randomly-selected-tweets-sentiment",
    "title": "Probability and Bayes Rule",
    "section": "Probability of a randomly selected tweet’s sentiment",
    "text": "Probability of a randomly selected tweet’s sentiment\n\nTo calculate a probability of a certain event happening, we take the count of that specific event and divide it by the sum of all events.\nFurthermore, the sum of all probabilities has to equal 1. If we pick a tweet at random, what is the probability of it being +? We define an event A: “A tweet is positive” and calculate its probability\n\n\nP(A) = P(+) = \\frac{N_{+}}{N}=\\frac{13}{20}=0.65\n\nAnd since probabilities add up to one:\n\nP(-) = 1- P(+)=0.35",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Probability & Bayes Rule",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w2/index.html#probability-for-a-specific-words-sentiment",
    "href": "notes/c1w2/index.html#probability-for-a-specific-words-sentiment",
    "title": "Probability and Bayes Rule",
    "section": "Probability for a specific word’s sentiment",
    "text": "Probability for a specific word’s sentiment\nWithin that corpus, the word happy is sometimes labeled + and in other cases, -. This indicates that some negative tweets contain the word happy. Shown below is a graphical representation of this “overlap”. Let’s explore how we may represent this graphically using a venn diagram and then derive a probability-based representation.\n\n\n\n\nTweets with “Happy”\n\nFirst, we need to estimate the probability of the event B: “tweets containing the word happy”\n\nP(B) = P(\\text{happy})=\\frac{N_\\text{happy}}{N}=\\frac{4}{20}=0.2\n\n\n\n\n\nVenn diagram for defining probabilities from events\n\nTo compute the probability of 2 events happening like happy and + in the picture we would be looking at the intersection, or overlap of the two events, In this case, the red and the blue boxes overlap in three boxes, So the answer is: \nP(A \\cap B) = P(A,B) = \\frac{2}{20}\n\nThe Event “A is labeled +”, - The probability of events A shown as P(A) is calculated as the ratio between the count of positive tweets and the corpus divided by the total number of tweets in the corpus.\n\n\n\n\n\n\n\n specific tweets color coded per the Venn diagram\n\n\n\n\n\n\n\n\nDefinition of conditional probability\n\n\n\nConditional probability is the probability of an outcome B when we already know for certain that an event A has already happened. Notation: \nP(B|A)\n\n\n\n\nand there more + than - more specifically our prior knowledge is that : \n  \\frac{P(+)}{P(−)}=\\frac{13}{7}\n\nthe likelihood of a tweet with happy being + is\nthe challenge arises from some words being in both + and - tweets Conditional probabilities help us reduce the sample search space by restricting it to a specific event which is a given. We should understand the difference between P(A|B) and P(B|A)",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Probability & Bayes Rule",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w2/index.html#what-is-phappy",
    "href": "notes/c1w2/index.html#what-is-phappy",
    "title": "Probability and Bayes Rule",
    "section": "what is P(+|happy)",
    "text": "what is P(+|happy)\n\nWe start with the Venn diagram for the P(A|B). \nWhere we restricted the diagram to just A the subset of happy tweets.\nAnd we just want those tweets that are also + i.e. (B).\nall we need is to plug in the counts from our count chart. \nwhich we now estimate \nP(A \\mid B) = P(Positive \\mid happy) = \\frac{3}{4} = 0.75",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Probability & Bayes Rule",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w2/index.html#what-is-phappy-1",
    "href": "notes/c1w2/index.html#what-is-phappy-1",
    "title": "Probability and Bayes Rule",
    "section": "what is P(happy|+)",
    "text": "what is P(happy|+)\n\nWe start with the Venn diagram for the P(B|A)\nwhere we have restricted the diagram to just B the subset of + tweets. \nand we just want from those the tweets that are also happy i.e. (A).\nand the counts for P(B|A) \nwhich we now estimate \nP(B \\mid A) = P(happy \\mid Positive) = \\frac{3}{13} = 0.231",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Probability & Bayes Rule",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w2/index.html#naïve-bayes-introduction",
    "href": "notes/c1w2/index.html#naïve-bayes-introduction",
    "title": "Probability and Bayes Rule",
    "section": "Naïve Bayes Introduction",
    "text": "Naïve Bayes Introduction\nHere is a sample corpus\n\n\n\n\nTable 1: And these are the class frequencies and probabilities\n\n\n\n\n\n\n\n\n\n+ tweets\n- tweets\n\n\n\n\nI am happy because I am learning NLP\nI am sad, I am not learning NLP\n\n\nI am happy\nI am sad\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport string \nraw_tweets=[\n  \"I am happy because I am learning NLP\",\n  \"I am sad, I am not learning NLP\",\n  \"I am happy, not sad\",\n  \"I am sad, not happy\",\n]\ndef clean(tweet:str):\n  return  tweet.translate(str.maketrans('', '', string.punctuation)).lower()\ntweets = [clean(tweet) for tweet in raw_tweets]\nlabels=['+','-','+','-']\ndf = pd.DataFrame({'tweets': tweets, 'labels': labels})\ndf\n\n\n\n\n\n\n\n\ntweets\nlabels\n\n\n\n\n0\ni am happy because i am learning nlp\n+\n\n\n1\ni am sad i am not learning nlp\n-\n\n\n2\ni am happy not sad\n+\n\n\n3\ni am sad not happy\n-\n\n\n\n\n\n\n\n\nimport numpy as np\nfrom collections import Counter\np_freq,n_freq = Counter(), Counter()\n#print( df[df.labels == '+']['tweets'].to_list())\n[p_freq.update(tweet.split()) for tweet in df[df.labels == '+']['tweets'].to_list()]\n[n_freq.update(tweet.split()) for tweet in df[df.labels == '-']['tweets'].to_list()]\nprint(p_freq)\nprint(n_freq)\nvocab = list(set(p_freq.keys()).union(set(n_freq.keys())))\npos_freq = [p_freq[word] for word in vocab ]\nneg_freq = [n_freq[word] for word in vocab ]\nvocab_df=pd.DataFrame({'vocab':vocab,'pos_freq':pos_freq,'neg_freq':neg_freq})\nvocab_df['p_pos']=vocab_df.pos_freq/vocab_df.pos_freq.sum()\nvocab_df['p_neg']=vocab_df.neg_freq/vocab_df.neg_freq.sum()\nvocab_df['p_pos_sm']=(vocab_df.pos_freq+1)/(vocab_df.pos_freq.sum()+vocab_df.shape[1])\nvocab_df['p_neg_sm']=(vocab_df.neg_freq+1)/(vocab_df.neg_freq.sum()+vocab_df.shape[1])\nvocab_df['ratio']= vocab_df.p_pos_sm/vocab_df.p_neg_sm\nvocab_df['lambda']= np.log(vocab_df.p_pos_sm/vocab_df.p_neg_sm)\npd.set_option('display.float_format', '{:.2f}'.format)\nvocab_df\nprint(vocab_df.shape)\n\n[None, None]\n\n\n[None, None]\n\n\nCounter({'i': 3, 'am': 3, 'happy': 2, 'because': 1, 'learning': 1, 'nlp': 1, 'not': 1, 'sad': 1})\nCounter({'i': 3, 'am': 3, 'sad': 2, 'not': 2, 'learning': 1, 'nlp': 1, 'happy': 1})\n\n\n\n\n\n\n\n\n\nvocab\npos_freq\nneg_freq\np_pos\np_neg\np_pos_sm\np_neg_sm\nratio\nlambda\n\n\n\n\n0\nam\n3\n3\n0.23\n0.23\n0.22\n0.21\n1.06\n0.05\n\n\n1\nnlp\n1\n1\n0.08\n0.08\n0.11\n0.11\n1.06\n0.05\n\n\n2\nbecause\n1\n0\n0.08\n0.00\n0.11\n0.05\n2.11\n0.75\n\n\n3\nlearning\n1\n1\n0.08\n0.08\n0.11\n0.11\n1.06\n0.05\n\n\n4\nhappy\n2\n1\n0.15\n0.08\n0.17\n0.11\n1.58\n0.46\n\n\n5\nsad\n1\n2\n0.08\n0.15\n0.11\n0.16\n0.70\n-0.35\n\n\n6\ni\n3\n3\n0.23\n0.23\n0.22\n0.21\n1.06\n0.05\n\n\n7\nnot\n1\n2\n0.08\n0.15\n0.11\n0.16\n0.70\n-0.35\n\n\n\n\n\n\n\n(8, 9)\n\n\n\nfrom IPython.display import Markdown\nfrom tabulate import tabulate\ntable = [[\"Sun\",696000,1989100000],\n         [\"Earth\",6371,5973.6],\n         [\"Moon\",1737,73.5],\n         [\"Mars\",3390,641.85]]\nMarkdown(tabulate(\n  table, \n  headers=[\"Planet\",\"R (km)\", \"mass (x 10^29 kg)\"]\n))\n\n\n\nTable 2: Planets\n\n\n\n\n\n\nPlanet\nR (km)\nmass (x 10^29 kg)\n\n\n\n\nSun\n696000\n1.9891e+09\n\n\nEarth\n6371\n5973.6\n\n\nMoon\n1737\n73.5\n\n\nMars\n3390\n641.85\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 3: Frequency Table\n\n\n\n\n\nword\n+\n-\n\n\n\n\nI\n3\n3\n\n\nam\n3\n3\n\n\nhappy\n2\n1\n\n\nbecause\n1\n0\n\n\nlearning\n1\n1\n\n\nNLP\n1\n1\n\n\nsad\n1\n2\n\n\nnot\n1\n2\n\n\n\n\n\n\n\n\n\n\n\nTable 4: Probabilities Table\n\n\n\n\n\nword\n+\n-\n\n\n\n\nI\n0.24\n0.25\n\n\nam\n0.24\n0.25\n\n\nhappy\n0.15\n0.08\n\n\nbecause\n0.08\n0.00\n\n\nlearning\n0.08\n0.08\n\n\nNLP\n0.08\n0.08\n\n\nsad\n0.08\n0.17\n\n\nnot\n0.08\n0.17\n\n\n\n\n\n\n\n\n\nLet’s motivate the Naïve Bayes inference condition rule for binary classification:\nTo build a classifier, we will first start by creating conditional probabilities given the table;\n\n\n\n\nNaïve Bayes\n\n\nWe want to find if given our prior knowledge of P(+) and P(-) if a new tweet has + or - sentiment.\nTo do that we will estimate p(+|T) and p(-|T) and then decide based on which is greater than 0.5.\n\n\n\n\n\nTable of probabilities\n\nWe can use the Bayes rule:\n\np(+|T) = \\frac{ p(T|+) \\times p(+) }{ p(T) }\n\nand\n\np(-|T) = \\frac{ p(T|-) \\times p(-) }{ p(T) }\n\nwhere:\n\np(+|T) is the posterior probability of a label + given tweet T\np(+) is our prior knowledge\np(T|+) is the likelihood of tweet T being +.\n{p(T)}\n\nThe term p(T) is in both terms and can be eliminated. However, it will cancel out when we use the ratio for the inference. This lets us compute the following table of probabilities; word am learning NLP Pos 0.24 0.08 0.08 Neg 0.25 0.08 0.08 .17 Naïve Bayes is the simplest probabilistic graphical model which comes with an independence assumption for the features.\n\np(T|+) = \\prod^m_{i=1}P(w_i|+) \\implies p(+|T)=\\frac{P(+)}{P(T)} \\prod^m_{i=1}P(w_i|+)\n\nand\n\np(T|−) = \\prod^m_{i=1}P(w_i|−) \\implies p(−|T) =  \\frac{P(−)}{P(T)} \\prod^m_{i=1} P(w_i|−)\n\nOnce we have the probabilities, we can compute the likelihood score as follows:\nTweet: I am happy today: I am learning. - Since there is no entry for today in our conditional probabilities table, this implies that this word is not in your vocabulary. So we’ll ignore its contribution to the overall score. - All the neutral words in the tweet such as I and am cancel out in the expression, as shown in the figure below.\n\n   \\prod^m_{i=1} \\frac{P(w_i|+)}{P(w_i|-)}= \\frac {0.14}{0.10} =1.4 &gt; 1\n\n\nA score greater than 1 indicates that the class is positive, otherwise, it is negative.\n\n\nP(+|T) &gt; P(−|T)\n\nthen we infer that the T has + sentiment. dividing by the right term we get the inference rule:\n\n\\frac{P(+|T)}{P(−|T)} &gt; 1\n which expands to : \n  \\frac {P(+|T)}{P(−|T)} = \\frac {P(+)}{P(-)}\\prod^m_{i=1} \\frac {P(w_i|+)}{P(w_i|−)} &gt; 1\n\nThis is the inference rule for naïve Bayes.\nNote: Naïve Bayes is a model which assumes all features are independent, so the basic component here is:\n\n\\frac{P(w_i|+)}{P(w_i|-)} &gt; 1\n the ratio of the probability that a word appears in a positive tweet and that it appears in a negative tweet",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Probability & Bayes Rule",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w2/index.html#additive-smoothing",
    "href": "notes/c1w2/index.html#additive-smoothing",
    "title": "Probability and Bayes Rule",
    "section": "Additive smoothing:",
    "text": "Additive smoothing:\n\np_{addative}(w_i|class)=\\frac{ freq(w,class)+\\delta}{ N_{class} + \\delta \\times V}",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Probability & Bayes Rule",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w2/index.html#more-alternatives-to-laplacian-smoothing",
    "href": "notes/c1w2/index.html#more-alternatives-to-laplacian-smoothing",
    "title": "Probability and Bayes Rule",
    "section": "More alternatives to Laplacian smoothing",
    "text": "More alternatives to Laplacian smoothing\n\n\n\n\nGood Turing smoothing\n\n\nKneser-Ney smoothing c.f. (Ney, Essen, and Kneser 1994) which corrects better for smaller data sets. \nGood-Turing smoothing c.f. (Good 1953) which uses order statistics to give even better estimates.\nwith a survey of the subject here: (Chen and Goodman 1996)",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Probability & Bayes Rule",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w2/index.html#sources-of-errors-in-naïve-bayes",
    "href": "notes/c1w2/index.html#sources-of-errors-in-naïve-bayes",
    "title": "Probability and Bayes Rule",
    "section": "Sources of Errors in Naïve Bayes",
    "text": "Sources of Errors in Naïve Bayes\n\nError Analysis\nBad sentiment classifications are due to:\n\npreprocessing dropping punctuation that encodes emotion like a sad smiley.\nWord order can contribute to meaning - breaking the independence assumption of our model\nPronouns removed as stop words - may encode emotion\nSarcasm can confound the model\nEuphemisms are also a challenge",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Probability & Bayes Rule",
      "Notes"
    ]
  },
  {
    "objectID": "notes/cs11-737-w04/index.html",
    "href": "notes/cs11-737-w04/index.html",
    "title": "Words, Parts of Speech, Morphology",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\nVideo 1: Lesson Video\n\n\n\n\n\n\n\n\nThis week’s slides\n\n\n\n\nSupplementary Figure 1"
  },
  {
    "objectID": "notes/cs11-737-w04/index.html#words",
    "href": "notes/cs11-737-w04/index.html#words",
    "title": "Words, Parts of Speech, Morphology",
    "section": "Words",
    "text": "Words\n\nSo now um we’re going to move on to the main part where we’re um going to talk about um words which sort of seems really fairly appropriate for any um any nlp class because words are pretty important okay and this is going to give you a little bit more definition of what we mean by words and how that’s not always as well defined um as um it might be in languages like English so let’s try and count the words in this sentence bob’s handyman is a do-it-yourself kind of guy isn’t he okay so that’s a fairly standard sentence although i have selected it to be particularly interesting for this case but the question is where are the words okay so what i’ve done is i’ve tried to highlight what people might think the words actually are now if you grew up in um europe and maybe other places as well you probably think that white space separated tokens is a good approximation for words and it is a good approximation for words but from a linguistic point of view it’s actually a little bit more complex than that and that’s what we’re actually going to talk about and when we actually look at other languages that’s the thing we want to highlight that the notion of what a word boundary actually is might not be as trivial as what you hope it’s going to be okay"
  },
  {
    "objectID": "notes/cs11-737-w04/index.html#whitespace",
    "href": "notes/cs11-737-w04/index.html#whitespace",
    "title": "Words, Parts of Speech, Morphology",
    "section": "Whitespace",
    "text": "Whitespace\n\nLet’s have a look at the first word of the white speak white space um uh identified token is bob’s the apostrophe s is what’s called a clitic it’s not isolated word on its own it can’t stand on its own it only appears when it’s actually bound to some other word but in some sense it’s independent of the word that it’s bound to so what you can have is um actually it can go into whole noun phrases rather than just single words so we talk about jack and jill’s bucket we’re not talking about jack and jill’s bucket we’re talking about jack and jill in the bucket belonging to jack and jill so that apostrophe s isn’t just going on the immediate token before but a bunch of tokens before that okay so be aware that that apostrophe s is a somewhat special thing in English and sometimes it’s a good idea to actually separate that apostrophe off and treat it as an independent token than actually just putting it together as bob’s let’s look at the next one handyman is really a now known compound okay we put a space in it because we’re not german um but uh um but some compounds in English don’t get a space between them and it’s sort of really quite complex to know which ones are which and it’s really sort of up to the speaker and to decide how to do that but sorry the writer from the speaking point of view it’s even harder to distinguish between these but handyman really here is being used as a single word and it would be useful to keep them together even though they have a ascii white space between them is is a word as a word now let’s have a look at do it yourself it’s hyphenated um and it really is a sort of set phrase it can be shortened to diy um and we’d like to treat that as a single word but sometimes hyphenated forms should be separated and we have to make um interesting decisions about that now we’ve got words like kinda and isn’t it that are fairly standard contractions um in English we could write them out as kind space off and we could write isn’t it as is not but often when people are speaking they don’t do that and they actually do reduce form and sometimes when writing they may actually do that simplified form as well and do use these contractions and we have to make some decision about how these actually might appear and whether we want to separate them out many of these are sort of closed class they can only be applied to a number of words and they’re not general to anything but some of them are not apostrophe s can clearly go into anything and apostrophe ll can sort of go into any noun as well um so that’s a hard it’s a decision that we actually have to make and it will affect all of our downstream tasks once we decide how to token these tokenize these into what we’re going to term words how we’re going to have word embeddings how we’re actually going to do parsing or whatever our next task is going to be."
  },
  {
    "objectID": "notes/cs11-737-w04/index.html#other-languages",
    "href": "notes/cs11-737-w04/index.html#other-languages",
    "title": "Words, Parts of Speech, Morphology",
    "section": "Other Languages",
    "text": "Other Languages\n\nIn other languages it can be way more complex and sometimes way more simple. It could be that we actually have no spaces whatsoever and everything is joined together agglutinated um and there has to be some process to try to separate these individual parts out if we don’t separate these things out what you’re going to have in a language is an awful lot of words so if you look at turkish for example which is an agglutinate language there’s an awful lot of compounding in it an awful lot of interesting morphology in it what you end up with is the number of words number of white space separated tokens and in the language is much bigger than English for because what we maybe think about as being phrases in English are actually whole single words with no white space between them there also can be ambiguity in those particular words about how we actually decompose them and on the right here we actually have a hebrew example where i’m sorry i can’t read that this um where it can mean depending on how you separate that out into individual morphemes and her saturday and that in t and that her daughter all of those are potential meanings given the rest of the context but they’re all written as the same single word"
  },
  {
    "objectID": "notes/cs11-737-w04/index.html#linguistics",
    "href": "notes/cs11-737-w04/index.html#linguistics",
    "title": "Words, Parts of Speech, Morphology",
    "section": "Linguistics",
    "text": "Linguistics\n\nNow let’s go back to our knowledge of linguistics and see how we might be able to use our knowledge of linguistics to be able to answer these questions about what words actually are.\nBecause we’re going to look at each of those and see how they we might be able to use that as a definition of a useful way of splitting things into little bits words so there could be phonetics so there could be something about um the way things are being said that could actually tell us something about that we could say it’s got to have a of uh every word has to have a vowel in it although I don’t know where apostle vs which sometimes is a violent and sometimes doesn’t sometimes and it’s sometimes just phonology and it could be something about there’s some structure that’s actually required in syllables and so everything has to be at least one syllable if it has to be a word it could be morphology if we’re looking at the actual atoms that are in the morphemes that are in the word we could talk about the individual morphemes in there and make some definition based on that we could talk about syntax we could talk about whether we could exchange that for another word in the same class and treat those as being words if they have classes over them we could talk about semantics of whether it changes meaning and we could talk about pragmatics about realistically new york is used as a single word even though we happen to put a space in it but it’s treated basically as a fundamental um a word because although it maybe historically has some relationship to the word new and the word york it really has nothing to do with that anymore um let’s have a look at the orthographic"
  },
  {
    "objectID": "notes/cs11-737-w04/index.html#orthography",
    "href": "notes/cs11-737-w04/index.html#orthography",
    "title": "Words, Parts of Speech, Morphology",
    "section": "Orthography",
    "text": "Orthography\n\nDefinition so this is where white space comes into mind and be aware that even in English white space is quite complex So you know we’ve got spaces we’ve got tabs we’ve got new lines we’ve got carriage returns we’ve got vertical tabs are all within the standard ascii set but beyond that we actually have a lot more if we look at unicode and we want to make definitions of that and we also have things like non-breakable space and a non-breakable space can be used and may appear on on on the web in html and we may or may not want to decide that as being a word boundary or not depending on our definitions okay and remember we’re talking here about orthography we are pretending that the whole world writes everything down and that when they write everything down it’s the same as what speech is most languages are not written we write an awful lot and most people on the planet are actually literate but they may speak languages that they’re not literate in and when you speak you don’t put any spaces between words okay you don’t say a space between each word and we don’t do that at all okay and so when you’re wondering how chinese people can understand chinese um text when there’s no species in it think about how well you can understand speech which has got no species in it and yet you can still deal with it and the notion of a word has been around for a lot longer than the notion of writing so in other words we’ve had that notion and we need a definition of words that are actually independent of the written form"
  },
  {
    "objectID": "notes/cs11-737-w04/index.html#prosodic",
    "href": "notes/cs11-737-w04/index.html#prosodic",
    "title": "Words, Parts of Speech, Morphology",
    "section": "Prosodic",
    "text": "Prosodic\n\nThere can be a prosodic definition a prosodic definition is something to do with international phrases they can be hard to define but they’ll still have some reasonable definition and things which are actually grouped together in the single international phrase are often written as a single word in other languages so for example in the park in English the way i just said it is as a single international phrase in the park and many other languages would have the same concepts of location and determiners um and park all in the single word that would have no spaces between it so in the park could be treated as a single word um depending on what your definitions are going to be"
  },
  {
    "objectID": "notes/cs11-737-w04/index.html#semantic",
    "href": "notes/cs11-737-w04/index.html#semantic",
    "title": "Words, Parts of Speech, Morphology",
    "section": "Semantic",
    "text": "Semantic\n\nSemantic definitions these are word units that got some something like the same idea and so it might be useful to be able to treat these um in a reasonable way um we can think about colors that might have multiple names we can think about uh navy blue is um is a color and um but when written it would have probably have a space in it um but we may want to treat that as a color which is the same as blue or red or yellow or any other single word color because navy blue is still a color okay and although technically maybe has something to do with glue and something to do with navy um it might not be a reasonable thing to actually separate it out we can have a syntactic definition where we’re looking at blocks in these sentences so again new york is a good example here where um it really is being used in the same sense as the word pittsburgh is it’s referring to a particular city in north america and just because it’s got a space in it doesn’t mean that we want to treat it as two different cities a new um city and a york city which is elsewhere in there’s almost certainly places called york in north america but i can’t well there’s um a one just outside toronto now this comes to the notion of how can we identify classes of words and i’m currently really talking about syntactic classes of words rather than semantic classes so so colors are sort of a semantic class but i’m interested in what words have got the same class that i could exchange them without what it might change the meaning but you know they get used in the same way now we’ve all been talking talk about these standard open classes which appear in most languages not all languages but most languages nouns verbs and adjectives and adverbs there’s almost an infinite number of those um new ones can come along that didn’t exist before um but they all fall into these particular classes this is in contrast with closed classes where there’s a finite set in the language they’re sometimes called function words it might be a difficult class to list absolutely everything but you rarely very rarely find new things moving into that class so things like prepositions so in above uh behind there’s a sort of finite set of those determiners the and ah this and that and pronouns um i uh um are somewhat finite in English conjunctions and or but um not an exclusive or if you’re a computer scientist and other auxiliary verbs like is was have etc and these are sort of closed class they’re very common in the language they’re often short in a language and in most languages have something like them they might be something to do with morphemes but they don’t have random new ones appearing every day so if we think about what happens with the open class think about words which you know exist today that didn’t exist five years ago and think about some can you type them in the chat words which we have today that we didn’t have five years ago and you’ll find out that they’re all nouns verbs adjectives and adverbs and not closed classes because can anybody think of any words which we have now that we didn’t have um five years ago covid um excellent example um i’m sorry to disappoint you but if you have a look at wikipedia in 2015 there’s an article on coveted okay um but it was dull and boring and uninteresting and the covered um uh self-help group decided that they wanted to do something and become more popular but you’re right that covert was incredibly rare okay and probably was only used in occasional circumstances doomsco yes doom scrolling excellent quarantine yeah wfh all of these things are very very common now and sort of didn’t exist at all before and language is like that but all of these words for the most part are coming in to be nouns and verbs okay occasionally they’re going to be adjectives and maybe adverbs but for the most part these do change over time so you can’t list them all but for the close class ones you can sort of do it i mean there’s some really rare conjunctions that people don’t use nowadays like not withstanding okay um apart from used as examples of really rare conjunctions that’s about the only time i use that word"
  },
  {
    "objectID": "notes/cs11-737-w04/index.html#tag-sets",
    "href": "notes/cs11-737-w04/index.html#tag-sets",
    "title": "Words, Parts of Speech, Morphology",
    "section": "Tag Sets",
    "text": "Tag Sets\nthere’s a bunch of classes that define what part of speech tag sets the tags that are actually there and in English and many other languages they’re often based on the tag set that was used in um the pen tree bank which was one of the first large um data sets that was labeled consistently um with nouns verbs etc and there’s 43 in total i don’t know if there’s 43 there but i know from other things and there’s some that are relatively rare and maybe questionable and there’s some compound ones where sometimes things get joined together it does make distinctions between different types of nouns singular plural and and proper nouns um and um it it’s quite useful in many tag sets that are now used in in nlp are derived from this tag set because that was decided um mostly by the group at u pen at the time people were doing a part of speech tagging before that i mean even before computers they were doing it but coming up with a finite computational tag set was something that really started in the 80s and depending on the language that you’re dealing with you may want to have different tags because there are some tags that are really only relevant in some languages and not in others okay importantly um there actually is a definition of a small number of tags which is in some sense a reduction of the number of tags that were in the pen tree bank that has been used in the universal dependency m sets that originally came out of google and it’s now an independent project but it’s still quite google influenced um because of the original data sets and this covers somebody’s going to tell me the number of languages but i think it’s about 40 or 50 languages where they’re all labeled with the same tag set and produces a universal dependency grammar which is also very use useful from a syntactic point of view that over a bunch of fairly major important languages okay so um getting this tag set is useful and you might say why should i care about getting a tag set i can train from um words and the answer is yes if you have lots of data but as usual in trying to do machine learning if you can give more information in a structured way or in a reduced standardized way you can typically get by with less data and try to train better and training should happen faster so often being able to get um a part of speech for a language would be quite useful and of course this is going to be hard in the low resource language because you sort of need labeled data to start off with but there are unsupervised ways to try to find out what these tags actually are now i’m naively talking about words here and words having part of speech tag because i’m one of these English speakers where actually for the most part that’s pretty easy in English for the most part you know white space separated tokens or words and for the most part each token has got one um a tag one proper tag and the context mostly defines what it is but in most languages most languages and we really have to introduce the notion of morpheme which is sort of the single smallest atomic part of a word"
  },
  {
    "objectID": "notes/cs11-737-w04/index.html#morphology",
    "href": "notes/cs11-737-w04/index.html#morphology",
    "title": "Words, Parts of Speech, Morphology",
    "section": "Morphology",
    "text": "Morphology\n\nHere for example in English we do have interesting morphology especially in what’s called derivational morphology which is not like the ing eds plurals where we’re changing some syntactic property the tense or the the number of a language but we’re actually changing the meaning and we’re often changing the class the part of the speech class so if we take a verb like establish we can have another verb that’s disestablished we can then make that into a noun by say disestablishment.\nWe can make it another noun putting anti- in front of it and saying anti-disestablishment. We can make it into an adjective by saying anti-disestablishmentary. We can make it a noun into anti-disestablishmentarian. We can then make that in a further noun by saying anti-disestablishmentarianism Now these things are a little bit extended, but actually we do this all the time. So it’s actually quite hard to really list all of the words that are in English because although some of these don’t appear very often. There will be new and novel words, and you’ll see a number per day of new words that you’ll understand. Where they’re actually morphologically variants of something that you can work out what the meaning actually is.\n\n\nNow so it would be useful if we could get these words and decompose them into their roots and morphemes so that we can actually work out what the important classes are. So we’d like to be able to get some notion of these decomposed forms in from a word if we can do it. Now some of these forms are what we call stems or roots they’re often words on their own. And we’ll have prefixes and suffixes.\nIn English we rarely have anything that inserts in the middle of a word we’re usually putting things at the beginning and end. In some languages you actually get sort of bracketed things that you have to put them at the beginning and the end some of the gaelics have got that. There are some things where you can actually put infixes and so there are some plural things that actually happen in interesting languages in southeast asia where they’re basically plural things where syllables or partial syllables will get duplicated and and therefore you have to deal with that.\n\n\nIn English the only example of being able to do that is um the infix form of putting swear words in the middle of a word so for example if you have the words pittsburgh and you want to put a swear word in the middle. I can only get away with this because I’m British i’m going to use the word bloody is a as a swear word although actually usually in linguistics we use the f-word but we can use the word bloody and if i want to put the word bloody in pittsburgh it’s going to be pit’s— bloody—berg and i could do that and it could be compounded and possibly but it’s a little bit of a stretch and maybe you would put species in in there to do that in other languages. Infects will happen in some languages you’ll actually even change things in templatic morphology in things like the Hebrew and Arabic and Tagalog is a an example. This is one of the examples from the philippines where we’ve got interesting morphology going on and we’ve got much more interesting morphology going on than what’s in English and we’d like to be able to decompose these things so that we’ve got finite sets of morphemes when we’re doing um processing so when we’re doing tokenization when before you give it to your word embedding system you’d like to have a standardized tokenizer that’s going to give you meaningful the most meaningful atomic parts when you actually do it."
  },
  {
    "objectID": "notes/cs11-737-w04/index.html#arabic",
    "href": "notes/cs11-737-w04/index.html#arabic",
    "title": "Words, Parts of Speech, Morphology",
    "section": "Arabic",
    "text": "Arabic\n\nArabic’s very interesting, because things actually are done within the consonants so you have a backbone of consonants and the vowels will change before and after after them. This changes meaning both semantically and inflectional so syntactic information about tense etc there’s a number of semitic languages that actually do templatic morphology and it always breaks a lot of our systems from doing it but we are not allowed to define what natural language actually is we still need to be able to deal with it chinese has a relatively um small amount of morphology but it still has derivational morphology so you can take words and join them together in interesting compounds which are not necessarily directly to do with the meaning of the individual character that you’re joining together so a number of things of um for example um if you take fire and wheel and put it together it doesn’t mean a wheel that’s on fire it actually means a steam train um and so or maybe that’s only in japanese i always get that one wrong um which varies between the different languages but um there’s often a relationship but the compound might be different and so sometimes you want to be able to decompose it and sometimes you don’t so there’s two types of morphology which are identified as what’s called derivational morphology and derivational morphology you’re mostly changing the part of speech class when you’re doing things English is a rich derivational morphology um and we can write it out and it’s mostly productive by productively means we’re allowed to construct new words without explaining to people what the meaning is um inflectional morphology is usually syntactic class changes or some classic feature changing so this is things like changing the tense changing the plurality and the number and other languages it may do things like used in agreement um used in tense and aspect and verbs um and these are usually treated as different classes and they’re usually quite different they don’t overlap they’re sometimes maybe a little bit confusing and for the most part inflectional morphology happens after derivational morphology in almost all languages we can talk about morphosyntax about how these phonemes join together and which ones are allowed to join to each other and we can talk about a lower level thing that we call it morphofunnymix or morphographymix where things are changing at the boundaries when we join them together so an example of morphophony thin morphography mix because it happens in the graphene form but the pronunciation is affected as well when you take apostrophe s okay or the plural s when we add it onto things sometimes we insert an e when we add e d to the end of things if it already ends in an e we don’t double the e and so this is i’ve got the e d and i’m going to join it on if the previous one is an e i’m just going to do it so move plus e d this is m-o-v-e-d okay while walked plus e-d is walk plus e-d joined together um there’s different classes of Types of Morphology morphology and typography and typology of these so that we can put these into different groups and to be able to identify what they’re all about um we have isolating or analytic m1s where there’s very little morphological changes vietnamese many of the chinese dialects English are good examples of that we have synthetic ones where things are being created all the time they’re sometimes called fusional or flectional so german greek russian and templatic where we’ve got some um form of often consonants and the vowels are changed and interspersed between those and we have a glutenatif where there’s lots of things joining together japanese finnish turkish are really good examples of that and we have polysynthetic which are really complexly joining things together where almost every phrase is actually ends up in a single word many of the north american um native american languages are that the reason the word snow is there is because you can often have lots of variations of the word snow in inuit that’s actually not true well it is sort of true um but um it’s notable that in scottish English we have lots of words for rain and for some reason i have no idea why that would be Morphology Analyzers and people have worked on morphology for a long time so often when you’re working a new language there already is a morphological analyzer is there and if you look at the project unimorph and they’ve actually collected together these in a fairly standardized way so you might just be able to use it from python and all of a sudden you get morphological decomposition for your language or maybe you could use a nearby language and it would almost work and that might make your life a lot easier often when we’re doing um novel languages especially when we’re caring about things under time we will look for one of those or we might even spend a couple of hours writing something because we’ll get something better than trying to do it fully automatically there are fully automatic ways but it might be better if there’s already something that allows us to do that okay there’s actually a competition Morphology Competition every year sigmar phone has been running for at least um 10 years and it gets harder and harder every year as they find harder and harder tasks from both supervised and unsupervised m techniques so it’s worthwhile looking at these and using that as a resource okay Shared Tasks um these shared tasks allow you to compete and people have done them from this class before actually and done interesting novel techniques to be able to work out to do it sharing information across language when you’ve got not enough data to train etc in order to be able to learn how to do that Finite State Morphology um finite state morphology is often used for morphemes actually um morphology is never very complex there’s probably something to do with a human brain it can’t really deal with something as complex as say syntax within um morphology and therefore it’s often quite localized and therefore finite state machines are quite good at being able to cover everything and there’s good toolkits out there that help you to be able to write these things there’s also completely unsupervised techniques there’s more fessor just a python thing you give it examples and it will try to find out the prefixes and suffixes that actually might um allow you to be able to do um analysis it does assume a certain segmental and view of phonology and therefore it can get confused sometimes and it might treat the different types of ed or just d um in different forms in English and separate them out and maybe you want to join them together or maybe you don’t um there’s a sort of related thing called stemming which is often quite useful especially when you’re doing things like information retrieval where you’d like to say look i i just i just want the root of the word and i don’t want all these other variations especially when you’re in a limited domain or when you’ve got limited amount of data and so maybe if you removed all of the morphological variants the plurals the eds it might be easier to do comparisons later especially if you don’t have good data in order to be able to do good word embedding there’s also purely completely automatic techniques um and bpe is a good example of that byte pair encoding you can’t really work it out from the name where what we do is we look at the string of the actual letters that are there and try to find and optimize the sequence of letters together and the overall predictability of the group of letters that we actually find and this originally came out of work in machine translation to try and find the best um segmentation for doing um translation um but we end up using it for lots of lots of things it’s often worth trying if you don’t have anything else because it does sort of work but you really want to know about for your particular language is it likely to work before you actually do all of your bpe you get a tokenization representation you build all your um word embedding that you learn from it and then learn oh no you could have downloaded the uh morphological analyzer that would have given a better result and a more consistent result and therefore you would have been able to learn back Tokenization okay um as i say tokenization is also something that we um get this often gets called where you’re actually just trying to split these things into words and you’ve got to care about what the tokenization actually is because if you have a different tokenization that won’t be the same lemmetization or stemming is somewhat similar but limit hydration is really talking about the linguistic root of the word which may or may not be well well defined and it’s usually after um morphological and decomposition you find the root of the word um a [Music] we can also do this across um languages you may want to care about characters rather than words and looking inside characters that can actually help and caring about things that are happening over long boundaries um somewhat related to this is um word segmentation in languages like japanese and um chinese and in fact they end up using something similar to bpe to be able to segment things there’s a couple of related things text normalization where you actually are trying to replace Text normalization everything as words we know that there’s an infinite number of numbers um and would be nice if we could change them into words maybe or maybe just change them into the word number or maybe classes of numbers and this is something that’s been studied in um a in text-to-speech and there’s various machine learning techniques to try to do well on this you might want to also care about spelling correction um and do be aware that tokenization this mismatch can really break everything so if you’re using bert you sort of have to use their tokenization because they’ve assumed that and it can be quite hard if you do something else okay that’s everything about works and morphology we will be talking about morphology again later on in more detail but um now we’re going to care about splitting out up out into groups and what we want you to do today is we want you to take one of these um languages language families um often morphology and or aspect of writing and orthography are similar within language families and i’d like you to identify something that you would need to care about if you were trying to do some form of tokenization"
  },
  {
    "objectID": "notes/cs11-737-w04/index.html#discussion-prompt",
    "href": "notes/cs11-737-w04/index.html#discussion-prompt",
    "title": "Words, Parts of Speech, Morphology",
    "section": "Discussion Prompt",
    "text": "Discussion Prompt\n\nPick a language in one of the following branches of language families: Bantu, Dravidian, Finno-Ugric, Japonic, Papuan, Semitic, Slavic, Turkic. Tell us about some interesting aspects of morphology of that language, following examples from the assigned reading. Cite your sources.\nIf you would need to implement a tokenizer for that language, what language specific knowledge would need to be incorporated into the tokenizer?"
  },
  {
    "objectID": "notes/cs11-737-w05/index.html",
    "href": "notes/cs11-737-w05/index.html",
    "title": "Translation and Translation Data",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\nVideo 1: Lesson Video\n\n\n\n\n\n\n\n\nThis week’s slides\n\n\n\n\nSupplementary Figure 1"
  },
  {
    "objectID": "notes/cs11-737-w05/index.html#intro",
    "href": "notes/cs11-737-w05/index.html#intro",
    "title": "Translation and Translation Data",
    "section": "Intro",
    "text": "Intro\n\nThis time we’re going to be talking less about models and more about the actual phenomenon of translation itself and so translation i mean i barely need to introduce what translation is i’m sure everybody knows but you know basically it’s the conversion of content in one language into content in another language uh for the purpose of making that content understandable to somebody who doesn’t know the original language of course and that can be anything from you know translating novels like harry potter um to you know any other variety of translation that happens and this time i’m going to be talking about how this translation happens a little bit about machine translation and then also about machine translation data sources and empty evaluation"
  },
  {
    "objectID": "notes/cs11-737-w05/index.html#translation-vs-interpretation",
    "href": "notes/cs11-737-w05/index.html#translation-vs-interpretation",
    "title": "Translation and Translation Data",
    "section": "Translation vs Interpretation",
    "text": "Translation vs Interpretation\n\nThere’s actually two types of conversion of language between uh different languages there are two main types one is translation another is interpretation and translation is basically the conversion of written word from one language to another and interpretation is conversion of spoken language from one language to another and they’re very different in the requirements for them uh also how people work and the type of type of people who work uh in these areas so uh translation in general is usually less time constrained you may still have a deadline uh for when your translation results are required but usually it’s on the order of you know like a day somebody gives you a translated document and they uh they say i want this back in a day whereas an interpreter may interpret while the content is being produced you know while i’m talking an interpreter may be interpreting my speech into another language that’s simultaneous interpretation uh you can also do consecutive interpretation which is basically like i speak for a while then the interpreter speaks for a while i speak for a while interpreter speaks for a while um translators a high degree of accuracy and fluent very fluent natural output is necessary and partially because of this uh translators in may translate all kinds of things but very often they specialize in a single area like i’m a medical translator or i’m a patent translator or something like that on the other hand interpreters may specialize in an area but more commonly they’re generalists who can interpret lots of different things um i actually worked as an interpreter and translator for about a year and a half and i did both of them because i was kind of in a position um that was a little bit less specialized i worked at a local government in japan they additionally had a single uh translator in a single interpreter um and it’s very interesting because uh personalities are also very different translators are are often somewhat introverted uh you know they like working on their own they really like being precise whereas interpreters have to be really good at talking because they spend their whole day talking they tend to be very extroverted um you know uh like talking to people in general not just interpreting between languages so um both of these jobs are very hard i found interpretation harder because um of the time pressure in the constraints and it’s not uh it’s you know you can’t get 100 accuracy in a situation where you’re expected to interpret things so you just need to do as well as you can"
  },
  {
    "objectID": "notes/cs11-737-w05/index.html#translation-industry",
    "href": "notes/cs11-737-w05/index.html#translation-industry",
    "title": "Translation and Translation Data",
    "section": "Translation Industry",
    "text": "Translation Industry\n\nHow much translation is done the language services market is 56.1 billion dollars that’s a lot of dollars and there’s 640 000 translators worldwide about 75 of them are working freelance and the other ones are employed by a specific uh organization interestingly europe owns 49 of the market share so 49 of the people about half the people are working in europe um and one interesting thing is you might think that as machine translation technology is getting better that was you know threatening translators in their jobs and you know reducing the value of the industry actually the con the contrary is true um you know more translation is being done than ever both machine translation and human translation and the value of the industry is doing nothing but improving so in a way you know more language is being translated now than ever another thing is that the translation industry is becoming more technological so 88 of full-time translators use some variety of computer-aided translation which means that they’re using machine translation they’re using something like translation memory that allows you to look up uh other related translations uh the most common tool is something called Tranos uh and uh basically it has translation memory it has integrated empty uh it has terminology e management software other things like this and a lot of translation agencies require uh that you use Tranos in order to be able to work with them there’s also other ones that i’m less familiar with but basically if you have this idea of like human translators versus machine translators in reality it’s not the case anymore it’s now a hybrid of humans and machines working together um some people like this some people don’t like this some people would prefer not to be told what to translate by their translation memory but you know in order to improve efficiency people now have to do this as part of their life\n\n\nWhy people don’t like translation\n\nThe reason why some people don’t like translation computer aided translation is i think they feel it stifles their creativity or it um because they can do it more efficiently now the requirement is that they do do it more efficiently so they have less time to sit and think about the perfect translation and just have to crank out content basically so i think those are the main reasons why people are like resistant to technology but nonetheless 88 of people are using it so even if they don’t like it they’re using it because they you know uh have to in order to keep up with the amount they’re expected to produce a lot of people do like it so it’s not everybody yeah so from the modeling type of view the biggest difference is whether speech is your input or text is your input um another big difference is interpretation sometimes you’re expected to do it in real time so that’s called simultaneous translation so you need to create the output before you’ve like read the whole document essentially that’s a very interesting topic it might be a topic that some people in this class want to work on if you like the speech you like the translation it’s a very hot topic right now cool so now uh what about difficulty in translation why is this hard um so this is an example i i cannot read this example i inherited it with uh from someone else uh but it’s basically a um an old uh chinese poem and uh the reason why it’s difficult to translate in general is because there’s divergences in uh lexical uh information so words in structure so this is an alignment of the glosses in chinese with the words in english if you don’t know what a gloss is it’s basically a word by word translation in the same order as the original sentence so um this is uh what it looked like in chinese unfortunately i don’t have the chinese characters above for all the chinese speakers but you can kind of see what it looks like and basically if you look at the chinese it’s like daiyu alone on bed top think uh baochai which gets translated into as she lay there alone daiu’s thoughts turned about uh and um you can see that the ordering is different between the two also um you know some words exist in the chinese but don’t exist in the english even more so in the next sentence so um you need to get the words uh right you need to get the words in the right order and this is that non-trivial when you know the translations are different between the two languages\n\n\n\nTranslation Ease\n\nHere’s another example from german which might be a little bit easier to parse if you’re not a chinese speaker so if you have um uh here you can see the gray gloss on top which is in the in city exploded uh car bomb and the uh the kind of canonical english translation that’s listed here is a car bomb exploded downtown so you can see that the word order changed also in uh in german some things like uh car bomb is a single word here it’s multiple words here um there’s also a phenomenon called translation ease and what translation eases is it’s not exactly natural in the language you’re translating into but it’s a translation that is direct and kind of maintains the original characteristics of the original language um there’s actually a fair amount of computational study of translations seeing how like which language you’re translating from affects the output and you can even take translation ease cluster it together and you get a very nice reproduction of the language family tree uh that the languages came from so basically the effects on translationese very strongly inherit the effects of of the original language and even you know are similar between similar languages etc so you can see there’s a very clear effect and here um in the inner city there exploded a car bomb would be a very like literal translation that you can understand in english but it’s also not natural english it’s not like what an english speaker would produce"
  },
  {
    "objectID": "notes/cs11-737-w05/index.html#lexical-ambiguity",
    "href": "notes/cs11-737-w05/index.html#lexical-ambiguity",
    "title": "Translation and Translation Data",
    "section": "Lexical Ambiguity",
    "text": "Lexical Ambiguity\n\nAnother issue is not just structure but also lexical ambiguities so this is an example from jarefsky and martin speech and language processing where basically you have leg and foot and paw and how they are how they are translated in different ways based on whether it’s a an animal leg a leg of a journey a leg of a human a leg of a chair a bird foot or a human foot etc etc"
  },
  {
    "objectID": "notes/cs11-737-w05/index.html#literary-translation",
    "href": "notes/cs11-737-w05/index.html#literary-translation",
    "title": "Translation and Translation Data",
    "section": "Literary Translation",
    "text": "Literary Translation\n\nI also thought i had another example but in order to handle all these things you’ll have to handle um you know syntactic differences lexical differences um how idiomatic the output is so um you know there’s lots of issues here and then if you start talking about literary literary translation it becomes even harder right you know you want to translate a poem or something like this as we had in the assignments uh for the discussion today um and suddenly you need to think about rhyming you need to think about like beauty of the expressions that you’re using so um lots of depth in uh doing translation um any questions there before i talk about uh mt quickly yeah is there any criteria so um the criteria that i would use to define translations there might be a different formal definition but i think this is basically right is it is language that only occurs because you’re translating from another language and would not normally be you know how you would how a native speaker would express the same content in that language so it’s kind of like structural or lexical influences of the original language of the produced language it doesn’t need to be like the effect of translation of the output could be very subtle um and often for a good translator it is very subtle but it’s still like there i have no confidence um i’m a native speaker of english i’m very good at japanese but i have no confidence that i can produce english that sounds like my natural english when i’m transmitting to japanese for example um any other things okay cool um under the machine translation so machine translation um a checked is a three billion dollar market um they’re uh oh actually i should mention that some of the statistics i got here are from this very very nice blog of the translation industry in 2021 um if you uh didn’t look at this on the page uh i would definitely take a look it summarizes a whole bunch of statistics and was insightful to me as well um these statistics are also uh from there which is machine translation is a three billion dollar market now so it has about five percent of the market share of the language services industry overall um the top providers of it are google um amazon and l uh so google uh i think a lot of people know uh amazon and aws web services uh provide translation for a lot of businesses for example and uh deepel is a startup that many people might have heard of but it has actually very good translation accuracy um they haven’t revealed all of their secrets but one of the things is that they um use uh like cleaner training data they have good training data cleaning strategies and they also consider context uh in a better way than other things like google do another thing about machine translation is these are the markets that machine translation is used in you can see that the most common ones are uh healthcare automotive and military and defense markets but it’s kind of spread out pretty widely including e-commerce and other things like that um there’s a very interesting uh paper that examined the effect of translation on e-commerce that i don’t have in the references but i can share uh which demonstrated that when ebay i believe introduced automatic translation between spanish and english the number of sales from uh latin america to the us basically started increasing immediately after that so uh you can see also that you know mt has real world consequences impact uh etc so these are the lists this is maybe a slightly old list of languages uh supported by google translate uh it’s pretty impressive you know at least 100 languages maybe it’s uh nearing 200 now um one thing that i like to mention to people uh whenever you look at this list is just because there’s a hundred languages on this list doesn’t mean that mt is equally good for all of the languages on this list um you know it may be obvious if you think about it a little bit but sometimes you think well you know it’s on this list google released a product for it it must work um that’s definitely not the case uh and uh you know if you try to use it to understand articles you’ll see a very big uh very big difference"
  },
  {
    "objectID": "notes/cs11-737-w05/index.html#human-translators",
    "href": "notes/cs11-737-w05/index.html#human-translators",
    "title": "Translation and Translation Data",
    "section": "Human Translators",
    "text": "Human Translators\n\nThere are some reports that mt is now at the level of human professionals um in some areas like for example uh news translation between very high resource languages like chinese and english. I didn’t believe this at first when microsoft first put out an article that said this essentially um and i went in and like analyzed their data looked at their data and they actually their outputs are quite good and human translator outputs are not perfect either so for example um let’s say you hire a human translator on a freelancing site and tell them i want you to translate these new news articles uh because i would like to create training data for a machine translation system um if you do that the translator will say sure i want your money um i want your money i will i’ll be happy to do that but they’re not going to be super motivated and if you say instead to the translator say i’m going to be asking you to translate these news articles for cnn and uh or the new york times and a hundred thousand people are going to read your article you’re pretty sure they’re going to do a good job right they’re not going to make a mistake so which human translator you’re trans comparing people to also makes a big difference in these uh these outputs and not even just which human translator but how motivated that human translator is so i have a feeling that um mt systems and high resource languages are almost as good as moderately motivated good professional human translators but they’re not as good as somebody who’s translating for the new york times for like 100 000 people for example"
  },
  {
    "objectID": "notes/cs11-737-w05/index.html#translation",
    "href": "notes/cs11-737-w05/index.html#translation",
    "title": "Translation and Translation Data",
    "section": "Translation",
    "text": "Translation\noh yeah so so sorry here are my other examples uh this is from google translate um i basically put in the first uh sorry i had animation on this until i had to switch computers but i put in the first um sentence from the wikipedia article on translation and i’ll put it through google translate which said translation refers to the act of replacing what is expressed in the form of a in a language uh with the form of b that corresponds to that meaning uh specifically in natural language it refers to the act of converting a sentence in a in the source language into a sentence in another target language um i did a few small edits here the the red stuff are my edits that i think would make it a little bit better but it’s pretty good um here’s another example of machine translation i entered a lexically ambiguous word um kodo in japanese which can be either code chord or chord depending on you know the context and it does a pretty good job at disambiguating like electrical cord uh code for the program chord for the on the score um but if i wrote as a musician i am good at reading chords um it made a mistake with that in java i wrote a chord that displays the chord of a guitar um that should have been code up here so you can see that it you know is not perfect for doing this as well um so you know translation is is hard even good things like google translate are not perfect but they’re pretty good in high resource languages anyway um so why do these work i’m gonna be talking more about translation models next class um but basically i’d like to go through a little bit of uh you know history into how um these were conceptualized and um from 1968 there’s this famous thing called the vaqua triangle and basically what it is saying is there’s multiple ways to do translation um you can go from words directly to words so you can basically replace words by words you can go up to the syntactic structure of languages and then generate from the syntactic structure you can go up to semantics in the language convert the semantics between the languages and go down and you can go up to uh something called an interlingua with lingua which is like universal semantics for all languages"
  },
  {
    "objectID": "notes/cs11-737-w05/index.html#direct-transfer",
    "href": "notes/cs11-737-w05/index.html#direct-transfer",
    "title": "Translation and Translation Data",
    "section": "Direct Transfer",
    "text": "Direct Transfer\num so what does this look like direct transfer looks like a word by word translation um so you would just be translating directly from word words to words syntactic transfer would be like analyzing the syntax of the sentence and then using that to translate you could also generate a syntax of the target sentence or translate from syntax of the source sentence to the target sentence and generate the output"
  },
  {
    "objectID": "notes/cs11-737-w05/index.html#semantics",
    "href": "notes/cs11-737-w05/index.html#semantics",
    "title": "Translation and Translation Data",
    "section": "Semantics",
    "text": "Semantics\nYou could also have something like semantics which is a logical form which basically says well something was detonated what was detonated it was a bomb\nlike a car bomb uh it was that mediated downtown and that was in the past tense and then you generate the output based on that"
  },
  {
    "objectID": "notes/cs11-737-w05/index.html#interlingua",
    "href": "notes/cs11-737-w05/index.html#interlingua",
    "title": "Translation and Translation Data",
    "section": "Interlingua",
    "text": "Interlingua\nYou could do other things and then there’s also an interlingua which basically says nothing about the uh you know individual languages and instead is completely in kind of a form a logical form um so each of these methods has their own advantages and disadvantages um the advantage of going directly is like let’s say we have a language like spanish and italian which are very very similar in words and structure and other things like this there you could basically do a word by word translation and do a pretty good job um however if you have very different languages you know you’re going to have a lot of trouble you need to have very basically a very powerful model to allow you to do this and uh you know before neural networks we didn’t have any model that really did this very convincingly well"
  },
  {
    "objectID": "notes/cs11-737-w05/index.html#neural-models",
    "href": "notes/cs11-737-w05/index.html#neural-models",
    "title": "Translation and Translation Data",
    "section": "Neural Models",
    "text": "Neural Models\n\nNeural models now you can kind of view them as models that map word by word they take in a whole bunch of words they generate a whole bunch of words but you can also view them maybe as a interlingua based model where you know they’re taking in words and they’re generating hidden vectors they correspond to the meaning of all of those words and then they’re generating from that you know like interlingua between the the languages so where exactly we lie now on the spot triangle is kind of you know unclear but uh you know it’s kind of an interesting question uh as well and you know maybe considering syntax or other things like that would help us generalize better in the resource languages for example"
  },
  {
    "objectID": "notes/cs11-737-w05/index.html#data",
    "href": "notes/cs11-737-w05/index.html#data",
    "title": "Translation and Translation Data",
    "section": "Data",
    "text": "Data\n\nI realize i have a lot of slides left and i don’t want to take all of the time. Maybe i’ll just go through the data part and leave the evaluation part till next time um but are there any questions uh so far okay cool so um i’d like to talk a little bit about data because data is very important for training our um mt models and um so basically all models including you know like google translate amazon dpel any of the things that people are using are using machine learning based methods and basically the way they work is they’re trained from parallel data sources um where you have one language and then another language one language and then another language um this is an example that you can actually do yourself if you’re uh like interested in trying a puzzle which is basically like take this parallel corpus and then try to translate this sentence at the bottom yourself and uh you’ll see that you need to like form associations between words you need to understand about what the syntax of the language looks like but it’s definitely possible from this uh this small parallel corpus"
  },
  {
    "objectID": "notes/cs11-737-w05/index.html#parallelcorpora",
    "href": "notes/cs11-737-w05/index.html#parallelcorpora",
    "title": "Translation and Translation Data",
    "section": "Parallelcorpora",
    "text": "Parallelcorpora\n\nNo i think these are made up yeah i’m pretty sure um so where can we get parallel corpora basically it’s anywhere that translators are doing lots of translation um this is an example from the united nations uh from a few days ago uh and you can see that this is translated into you know three languages here it’s actually six languages uh i believe it’s the official languages of the um um"
  },
  {
    "objectID": "notes/cs11-737-w05/index.html#languages",
    "href": "notes/cs11-737-w05/index.html#languages",
    "title": "Translation and Translation Data",
    "section": "Languages",
    "text": "Languages\n\nanother good source that we love using for um like very low resource languages is the bible because the bible is translated into more languages than any other text as far as i know"
  },
  {
    "objectID": "notes/cs11-737-w05/index.html#books",
    "href": "notes/cs11-737-w05/index.html#books",
    "title": "Translation and Translation Data",
    "section": "Books",
    "text": "Books\n\nYou can also get uh books of course or other other things this is harry potter and english and chinese Restaurants when you go to a restaurant if you’re not chinese you can go to your favorite chinese restaurant if you’re uh chinese you can go to your favorite indian restaurant i don’t know something else um and uh get the menu and that’s a very good parallel purpose for you to try your your own learning skills on"
  },
  {
    "objectID": "notes/cs11-737-w05/index.html#web-data",
    "href": "notes/cs11-737-w05/index.html#web-data",
    "title": "Translation and Translation Data",
    "section": "Web Data",
    "text": "Web Data\n\nyou can also harvest data from the web like you can harvest data from micro uh micro blogs twitter uh social media other things like this so um this can give you you know more informal language and that’s why you know google doesn’t completely fall over when it tries to translate twitter uh for example"
  },
  {
    "objectID": "notes/cs11-737-w05/index.html#opus",
    "href": "notes/cs11-737-w05/index.html#opus",
    "title": "Translation and Translation Data",
    "section": "Opus",
    "text": "Opus\n\nand the canonical place to get data for any of these models now is this place called opus and what opus does is it collects a whole bunch of open parallel corpora in many many different languages language pairs and across many domains so if you want to train models this is your best place to get it um so to leave some time for the discussion i think i’ll uh move the evaluation part to tomorrow but are there any questions about stuff we talked about so far"
  },
  {
    "objectID": "notes/cs11-737-w05/index.html#discussion-question",
    "href": "notes/cs11-737-w05/index.html#discussion-question",
    "title": "Translation and Translation Data",
    "section": "Discussion Question:",
    "text": "Discussion Question:\n\nUse Google translate to back-translate the text via a pivot language, e.g., “English → Spanish → English” or “English → L1 → L2 → English”, where L1 and L2 are typologically different from English and from each other.Compare the original text and its English back-translation, and share your observations. For example, (1) what information got lost in the process of translation? (2) are there translation errors associated with linguistic properties of pivot languages and with linguistic divergences across languages?\nTry different pivot languages: can you provide insights about the quality of MT for those language pairs?"
  },
  {
    "objectID": "notes/cs11-737-w05/index.html#resources",
    "href": "notes/cs11-737-w05/index.html#resources",
    "title": "Translation and Translation Data",
    "section": "Resources:",
    "text": "Resources:\n\nhttps://redokun.com/blog/translation-statistics"
  },
  {
    "objectID": "notes/cs11-737-w09/index.html",
    "href": "notes/cs11-737-w09/index.html",
    "title": "Translation and Translation Data",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\nVideo 1: Lesson Video\n\n\n\n\n\n\n\n\nThis week’s slides\n\n\n\n\nSupplementary Figure 1\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{bochman2022,\n  author = {Bochman, Oren},\n  title = {Translation and {Translation} {Data}},\n  date = {2022-01-24},\n  url = {https://orenbochman.github.io/notes-nlp/notes/cs11-737-w09/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2022. “Translation and Translation Data.”\nJanuary 24, 2022. https://orenbochman.github.io/notes-nlp/notes/cs11-737-w09/."
  },
  {
    "objectID": "notes/c1w3/lab02.html",
    "href": "notes/c1w3/lab02.html",
    "title": "Manipulating word embeddings",
    "section": "",
    "text": "Figure 1: course banner\nIn this week’s assignment, we are going to use a pre-trained word embedding for finding word analogies and equivalence. This exercise can be used as an Intrinsic Evaluation for the word embedding performance. In this notebook, we will apply linear algebra operations using NumPy to find analogies between words manually. This will help we to prepare for this week’s assignment.\nimport pandas as pd # Library for Dataframes \nimport numpy as np # Library for math functions\nimport pickle # Python object serialization library. Not secure\n\nword_embeddings = pickle.load( open( \"./data/word_embeddings_subset.p\", \"rb\" ) )\nlen(word_embeddings) # there should be 243 words that will be used in this assignment\n\n243\nNow that the model is loaded, we can take a look at the word representations. First, note that word_embeddings is a dictionary. Each word is the key to the entry, and the value is its corresponding vector presentation. Remember that square brackets allow access to any entry if the key exists.\ncountryVector = word_embeddings['country'] # Get the vector representation for the word 'country'\nprint(type(countryVector)) # Print the type of the vector. Note it is a numpy array\nprint(countryVector) # Print the values of the vector.  \n\n&lt;class 'numpy.ndarray'&gt;\n[-0.08007812  0.13378906  0.14355469  0.09472656 -0.04736328 -0.02355957\n -0.00854492 -0.18652344  0.04589844 -0.08154297 -0.03442383 -0.11621094\n  0.21777344 -0.10351562 -0.06689453  0.15332031 -0.19335938  0.26367188\n -0.13671875 -0.05566406  0.07470703 -0.00070953  0.09375    -0.14453125\n  0.04296875 -0.01916504 -0.22558594 -0.12695312 -0.0168457   0.05224609\n  0.0625     -0.1484375  -0.01965332  0.17578125  0.10644531 -0.04760742\n -0.10253906 -0.28515625  0.10351562  0.20800781 -0.07617188 -0.04345703\n  0.08642578  0.08740234  0.11767578  0.20996094 -0.07275391  0.1640625\n -0.01135254  0.0025177   0.05810547 -0.03222656  0.06884766  0.046875\n  0.10107422  0.02148438 -0.16210938  0.07128906 -0.16210938  0.05981445\n  0.05102539 -0.05566406  0.06787109 -0.03759766  0.04345703 -0.03173828\n -0.03417969 -0.01116943  0.06201172 -0.08007812 -0.14941406  0.11914062\n  0.02575684  0.00302124  0.04711914 -0.17773438  0.04101562  0.05541992\n  0.00598145  0.03027344 -0.07666016 -0.109375    0.02832031 -0.10498047\n  0.0100708  -0.03149414 -0.22363281 -0.03125    -0.01147461  0.17285156\n  0.08056641 -0.10888672 -0.09570312 -0.21777344 -0.07910156 -0.10009766\n  0.06396484 -0.11962891  0.18652344 -0.02062988 -0.02172852  0.29296875\n -0.00793457  0.0324707  -0.15136719  0.00227356 -0.03540039 -0.13378906\n  0.0546875  -0.03271484 -0.01855469 -0.10302734 -0.13378906  0.11425781\n  0.16699219  0.01361084 -0.02722168 -0.2109375   0.07177734  0.08691406\n -0.09960938  0.01422119 -0.18261719  0.00741577  0.01965332  0.00738525\n -0.03271484 -0.15234375 -0.26367188 -0.14746094  0.03320312 -0.03344727\n -0.01000977  0.01855469  0.00183868 -0.10498047  0.09667969  0.07910156\n  0.11181641  0.13085938 -0.08740234 -0.1328125   0.05004883  0.19824219\n  0.0612793   0.16210938  0.06933594  0.01281738  0.01550293  0.01531982\n  0.11474609  0.02758789  0.13769531 -0.08349609  0.01123047 -0.20507812\n -0.12988281 -0.16699219  0.20410156 -0.03588867 -0.10888672  0.0534668\n  0.15820312 -0.20410156  0.14648438 -0.11572266  0.01855469 -0.13574219\n  0.24121094  0.12304688 -0.14550781  0.17578125  0.11816406 -0.30859375\n  0.10888672 -0.22363281  0.19335938 -0.15722656 -0.07666016 -0.09082031\n -0.19628906 -0.23144531 -0.09130859 -0.14160156  0.06347656  0.03344727\n -0.03369141  0.06591797  0.06201172  0.3046875   0.16796875 -0.11035156\n -0.03833008 -0.02563477 -0.09765625  0.04467773 -0.0534668   0.11621094\n -0.15039062 -0.16308594 -0.15527344  0.04638672  0.11572266 -0.06640625\n -0.04516602  0.02331543 -0.08105469 -0.0255127  -0.07714844  0.0016861\n  0.15820312  0.00994873 -0.06445312  0.15722656 -0.03112793  0.10644531\n -0.140625    0.23535156 -0.11279297  0.16015625  0.00061798 -0.1484375\n  0.02307129 -0.109375    0.05444336 -0.14160156  0.11621094  0.03710938\n  0.14746094 -0.04199219 -0.01391602 -0.03881836  0.02783203  0.10205078\n  0.07470703  0.20898438 -0.04223633 -0.04150391 -0.00588989 -0.14941406\n -0.04296875 -0.10107422 -0.06176758  0.09472656  0.22265625 -0.02307129\n  0.04858398 -0.15527344 -0.02282715 -0.04174805  0.16699219 -0.09423828\n  0.14453125  0.11132812  0.04223633 -0.16699219  0.10253906  0.16796875\n  0.12597656 -0.11865234 -0.0213623  -0.08056641  0.24316406  0.15527344\n  0.16503906  0.00854492 -0.12255859  0.08691406 -0.11914062 -0.02941895\n  0.08349609 -0.03100586  0.13964844 -0.05151367  0.00765991 -0.04443359\n -0.04980469 -0.03222656 -0.00952148 -0.10888672 -0.10302734 -0.15722656\n  0.19335938  0.04858398  0.015625   -0.08105469 -0.11621094 -0.01989746\n  0.05737305  0.06103516 -0.14550781  0.06738281 -0.24414062 -0.07714844\n  0.04760742 -0.07519531 -0.14941406 -0.04418945  0.09716797  0.06738281]\nIt is important to note that we store each vector as a NumPy array. It allows us to use the linear algebra operations on it.\nThe vectors have a size of 300, while the vocabulary size of Google News is around 3 million words!\n#Get the vector for a given word:\ndef vec(w):\n    return word_embeddings[w]",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "L2 - Manipulating word embeddings"
    ]
  },
  {
    "objectID": "notes/c1w3/lab02.html#operating-on-word-embeddings",
    "href": "notes/c1w3/lab02.html#operating-on-word-embeddings",
    "title": "Manipulating word embeddings",
    "section": "Operating on word embeddings",
    "text": "Operating on word embeddings\nRemember that understanding the data is one of the most critical steps in Data Science. Word embeddings are the result of machine learning processes and will be part of the input for further processes. These word embedding needs to be validated or at least understood because the performance of the derived model will strongly depend on its quality.\nWord embeddings are multidimensional arrays, usually with hundreds of attributes that pose a challenge for its interpretation.\nIn this notebook, we will visually inspect the word embedding of some words using a pair of attributes. Raw attributes are not the best option for the creation of such charts but will allow us to illustrate the mechanical part in Python.\nIn the next cell, we make a beautiful plot for the word embeddings of some words. Even if plotting the dots gives an idea of the words, the arrow representations help to visualize the vector’s alignment as well.\n\nimport matplotlib.pyplot as plt # Import matplotlib\n%matplotlib inline\n\nwords = ['oil', 'gas', 'happy', 'sad', 'city', 'town', 'village', 'country', 'continent', 'petroleum', 'joyful']\n\nbag2d = np.array([vec(word) for word in words]) # Convert each word to its vector representation\n\nfig, ax = plt.subplots(figsize = (10, 10)) # Create custom size image\n\ncol1 = 3 # Select the column for the x axis\ncol2 = 2 # Select the column for the y axis\n\n# Print an arrow for each word\nfor word in bag2d:\n    ax.arrow(0, 0, word[col1], word[col2], head_width=0.005, head_length=0.005, fc='r', ec='r', width = 1e-5)\n\n    \nax.scatter(bag2d[:, col1], bag2d[:, col2]); # Plot a dot for each word\n\n# Add the word label over each dot in the scatter plot\nfor i in range(0, len(words)):\n    ax.annotate(words[i], (bag2d[i, col1], bag2d[i, col2]))\n\n\nplt.show()\n\nText(0.06396484375, -0.279296875, 'oil')\n\n\nText(0.01080322265625, -0.138671875, 'gas')\n\n\nText(0.025390625, 0.00160980224609375, 'happy')\n\n\nText(-0.044677734375, 0.06689453125, 'sad')\n\n\nText(-0.0400390625, 0.18359375, 'city')\n\n\nText(-0.1611328125, 0.030029296875, 'town')\n\n\nText(-0.2158203125, 0.09033203125, 'village')\n\n\nText(0.0947265625, 0.1435546875, 'country')\n\n\nText(0.1279296875, 0.2392578125, 'continent')\n\n\nText(0.10888671875, -0.22265625, 'petroleum')\n\n\nText(0.083984375, 0.1298828125, 'joyful')\n\n\n\n\n\n\n\n\n\nNote that similar words like ‘village’ and ‘town’ or ‘petroleum’, ‘oil’, and ‘gas’ tend to point in the same direction. Also, note that ‘sad’ and ‘happy’ looks close to each other; however, the vectors point in opposite directions.\nIn this chart, one can figure out the angles and distances between the words. Some words are close in both kinds of distance metrics.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "L2 - Manipulating word embeddings"
    ]
  },
  {
    "objectID": "notes/c1w3/lab02.html#word-distance",
    "href": "notes/c1w3/lab02.html#word-distance",
    "title": "Manipulating word embeddings",
    "section": "Word distance",
    "text": "Word distance\nNow plot the words ‘sad’, ‘happy’, ‘town’, and ‘village’. In this same chart, display the vector from ‘village’ to ‘town’ and the vector from ‘sad’ to ‘happy’. Let us use NumPy for these linear algebra operations.\n\nwords = ['sad', 'happy', 'town', 'village']\n\nbag2d = np.array([vec(word) for word in words]) # Convert each word to its vector representation\n\nfig, ax = plt.subplots(figsize = (10, 10)) # Create custom size image\n\ncol1 = 3 # Select the column for the x axe\ncol2 = 2 # Select the column for the y axe\n\n# Print an arrow for each word\nfor word in bag2d:\n    ax.arrow(0, 0, word[col1], word[col2], head_width=0.0005, head_length=0.0005, fc='r', ec='r', width = 1e-5)\n    \n# print the vector difference between village and town\nvillage = vec('village')\ntown = vec('town')\ndiff = town - village\nax.arrow(village[col1], village[col2], diff[col1], diff[col2], fc='b', ec='b', width = 1e-5)\n\n# print the vector difference between village and town\nsad = vec('sad')\nhappy = vec('happy')\ndiff = happy - sad\nax.arrow(sad[col1], sad[col2], diff[col1], diff[col2], fc='b', ec='b', width = 1e-5)\n\n\nax.scatter(bag2d[:, col1], bag2d[:, col2]); # Plot a dot for each word\n\n# Add the word label over each dot in the scatter plot\nfor i in range(0, len(words)):\n    ax.annotate(words[i], (bag2d[i, col1], bag2d[i, col2]))\n\n\nplt.show()\n\nText(-0.044677734375, 0.06689453125, 'sad')\n\n\nText(0.025390625, 0.00160980224609375, 'happy')\n\n\nText(-0.1611328125, 0.030029296875, 'town')\n\n\nText(-0.2158203125, 0.09033203125, 'village')",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "L2 - Manipulating word embeddings"
    ]
  },
  {
    "objectID": "notes/c1w3/lab02.html#linear-algebra-on-word-embeddings",
    "href": "notes/c1w3/lab02.html#linear-algebra-on-word-embeddings",
    "title": "Manipulating word embeddings",
    "section": "Linear algebra on word embeddings",
    "text": "Linear algebra on word embeddings\nIn the lectures, we saw the analogies between words using algebra on word embeddings. Let us see how to do it in Python with Numpy.\nTo start, get the norm of a word in the word embedding.\n\nprint(np.linalg.norm(vec('town'))) # Print the norm of the word town\nprint(np.linalg.norm(vec('sad'))) # Print the norm of the word sad\n\n2.3858097\n2.9004838",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "L2 - Manipulating word embeddings"
    ]
  },
  {
    "objectID": "notes/c1w3/lab02.html#predicting-capitals",
    "href": "notes/c1w3/lab02.html#predicting-capitals",
    "title": "Manipulating word embeddings",
    "section": "Predicting capitals",
    "text": "Predicting capitals\nNow, applying vector difference and addition, one can create a vector representation for a new word. For example, we can say that the vector difference between ‘France’ and ‘Paris’ represents the concept of Capital.\nOne can move from the city of Madrid in the direction of the concept of Capital, and obtain something close to the corresponding country to which Madrid is the Capital.\n\ncapital = vec('France') - vec('Paris')\ncountry = vec('Madrid') + capital\n\nprint(country[0:5]) # Print the first 5 values of the vector\n\n[-0.02905273 -0.2475586   0.53952026  0.20581055 -0.14862823]\n\n\nWe can observe that the vector ‘country’ that we expected to be the same as the vector for Spain is not exactly it.\n\ndiff = country - vec('Spain')\nprint(diff[0:10])\n\n[-0.06054688 -0.06494141  0.37643433  0.08129883 -0.13007355 -0.00952148\n -0.03417969 -0.00708008  0.09790039 -0.01867676]\n\n\nSo, we have to look for the closest words in the embedding that matches the candidate country. If the word embedding works as expected, the most similar word must be ‘Spain’. Let us define a function that helps us to do it. We will store our word embedding as a DataFrame, which facilitate the lookup operations based on the numerical vectors.\n\n# Create a dataframe out of the dictionary embedding. This facilitate the algebraic operations\nkeys = word_embeddings.keys()\ndata = []\nfor key in keys:\n    data.append(word_embeddings[key])\n\nembedding = pd.DataFrame(data=data, index=keys)\n# Define a function to find the closest word to a vector:\ndef find_closest_word(v, k = 1):\n    # Calculate the vector difference from each word to the input vector\n    diff = embedding.values - v \n    # Get the norm of each difference vector. \n    # It means the squared euclidean distance from each word to the input vector\n    delta = np.sum(diff * diff, axis=1)\n    # Find the index of the minimun distance in the array\n    i = np.argmin(delta)\n    # Return the row name for this item\n    return embedding.iloc[i].name\n\n\n# Print some rows of the embedding as a Dataframe\nembedding.head(10)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n\n\n\n\ncountry\n-0.080078\n0.133789\n0.143555\n0.094727\n-0.047363\n-0.023560\n-0.008545\n-0.186523\n0.045898\n-0.081543\n...\n-0.145508\n0.067383\n-0.244141\n-0.077148\n0.047607\n-0.075195\n-0.149414\n-0.044189\n0.097168\n0.067383\n\n\ncity\n-0.010071\n0.057373\n0.183594\n-0.040039\n-0.029785\n-0.079102\n0.071777\n0.013306\n-0.143555\n0.011292\n...\n0.024292\n-0.168945\n-0.062988\n0.117188\n-0.020508\n0.030273\n-0.247070\n-0.122559\n0.076172\n-0.234375\n\n\nChina\n-0.073242\n0.135742\n0.108887\n0.083008\n-0.127930\n-0.227539\n0.151367\n-0.045654\n-0.065430\n0.034424\n...\n0.140625\n0.087402\n0.152344\n0.079590\n0.006348\n-0.037842\n-0.183594\n0.137695\n0.093750\n-0.079590\n\n\nIraq\n0.191406\n0.125000\n-0.065430\n0.060059\n-0.285156\n-0.102539\n0.117188\n-0.351562\n-0.095215\n0.200195\n...\n-0.100586\n-0.077148\n-0.123047\n0.193359\n-0.153320\n0.089355\n-0.173828\n-0.054688\n0.302734\n0.105957\n\n\noil\n-0.139648\n0.062256\n-0.279297\n0.063965\n0.044434\n-0.154297\n-0.184570\n-0.498047\n0.047363\n0.110840\n...\n-0.195312\n-0.345703\n0.217773\n-0.091797\n0.051025\n0.061279\n0.194336\n0.204102\n0.235352\n-0.051025\n\n\ntown\n0.123535\n0.159180\n0.030029\n-0.161133\n0.015625\n0.111816\n0.039795\n-0.196289\n-0.039307\n0.067871\n...\n-0.007935\n-0.091797\n-0.265625\n0.029297\n0.089844\n-0.049805\n-0.202148\n-0.079590\n0.068848\n-0.164062\n\n\nCanada\n-0.136719\n-0.154297\n0.269531\n0.273438\n0.086914\n-0.076172\n-0.018677\n0.006256\n0.077637\n-0.211914\n...\n0.105469\n0.030762\n-0.039307\n0.183594\n-0.117676\n0.191406\n0.074219\n0.020996\n0.285156\n-0.257812\n\n\nLondon\n-0.267578\n0.092773\n-0.238281\n0.115234\n-0.006836\n0.221680\n-0.251953\n-0.055420\n0.020020\n0.149414\n...\n-0.008667\n-0.008484\n-0.053223\n0.197266\n-0.296875\n0.064453\n0.091797\n0.058350\n0.022583\n-0.101074\n\n\nEngland\n-0.198242\n0.115234\n0.062500\n-0.058350\n0.226562\n0.045898\n-0.062256\n-0.202148\n0.080566\n0.021606\n...\n0.135742\n0.109375\n-0.121582\n0.008545\n-0.171875\n0.086914\n0.070312\n0.003281\n0.069336\n0.056152\n\n\nAustralia\n0.048828\n-0.194336\n-0.041504\n0.084473\n-0.114258\n-0.208008\n-0.164062\n-0.269531\n0.079102\n0.275391\n...\n0.021118\n0.171875\n0.042236\n0.221680\n-0.239258\n-0.106934\n0.030884\n0.006622\n0.051270\n-0.135742\n\n\n\n\n10 rows × 300 columns\n\n\n\nNow let us find the name that corresponds to our numerical country:\n\nfind_closest_word(country)\n\n'Spain'",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "L2 - Manipulating word embeddings"
    ]
  },
  {
    "objectID": "notes/c1w3/lab02.html#predicting-other-countries",
    "href": "notes/c1w3/lab02.html#predicting-other-countries",
    "title": "Manipulating word embeddings",
    "section": "Predicting other Countries",
    "text": "Predicting other Countries\n\nfind_closest_word(vec('Italy') - vec('Rome') + vec('Madrid'))\n\n'Spain'\n\n\n\nprint(find_closest_word(vec('Berlin') + capital))\nprint(find_closest_word(vec('Beijing') + capital))\n\nGermany\nChina\n\n\nHowever, it does not always work.\n\nprint(find_closest_word(vec('Lisbon') + capital))\n\nLisbon",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "L2 - Manipulating word embeddings"
    ]
  },
  {
    "objectID": "notes/c1w3/lab02.html#represent-a-sentence-as-a-vector",
    "href": "notes/c1w3/lab02.html#represent-a-sentence-as-a-vector",
    "title": "Manipulating word embeddings",
    "section": "Represent a sentence as a vector",
    "text": "Represent a sentence as a vector\nA whole sentence can be represented as a vector by summing all the word vectors that conform to the sentence. Let us see.\n\ndoc = \"Spain petroleum city king\"\nvdoc = [vec(x) for x in doc.split(\" \")]\ndoc2vec = np.sum(vdoc, axis = 0)\ndoc2vec\n\narray([ 2.87475586e-02,  1.03759766e-01,  1.32629395e-01,  3.33007812e-01,\n       -2.61230469e-02, -5.95703125e-01, -1.25976562e-01, -1.01306152e+00,\n       -2.18544006e-01,  6.60705566e-01, -2.58300781e-01, -2.09960938e-02,\n       -7.71484375e-02, -3.07128906e-01, -5.94726562e-01,  2.00561523e-01,\n       -1.04980469e-02, -1.10748291e-01,  4.82177734e-02,  6.38977051e-01,\n        2.36083984e-01, -2.69775391e-01,  3.90625000e-02,  4.16503906e-01,\n        2.83416748e-01, -7.25097656e-02, -3.12988281e-01,  1.05712891e-01,\n        3.22265625e-02,  2.38403320e-01,  3.88183594e-01, -7.51953125e-02,\n       -1.26281738e-01,  6.60644531e-01, -7.89794922e-01, -7.04345703e-02,\n       -1.14379883e-01, -4.78515625e-02,  4.76318359e-01,  5.31127930e-01,\n        8.10546875e-02, -1.17553711e-01,  1.02050781e+00,  5.59814453e-01,\n       -1.17187500e-01,  1.21826172e-01, -5.51574707e-01,  1.44531250e-01,\n       -7.66113281e-01,  5.36102295e-01, -2.80029297e-01,  3.85986328e-01,\n       -2.39135742e-01, -2.86865234e-02, -5.10498047e-01,  2.59658813e-01,\n       -7.52929688e-01,  4.32128906e-02, -7.17773438e-02, -1.26708984e-01,\n        4.40673828e-02,  5.12939453e-01, -5.15808105e-01,  1.20117188e-01,\n       -5.52978516e-02, -3.92089844e-01, -3.15917969e-01,  1.57226562e-01,\n       -3.19702148e-01,  1.75170898e-01, -3.81835938e-01, -2.07031250e-01,\n       -4.72717285e-02, -2.79296875e-01, -3.29040527e-01, -1.69067383e-01,\n        1.61132812e-02,  1.71569824e-01,  5.73730469e-02, -2.44140625e-03,\n        8.34960938e-02, -1.58203125e-01, -3.10119629e-01,  5.28564453e-02,\n        8.60595703e-02,  5.12695312e-02, -7.22900391e-01,  4.97924805e-01,\n       -5.85937500e-03,  4.49951172e-01,  3.82446289e-01, -2.80029297e-01,\n       -3.28125000e-01, -6.27441406e-02, -4.81933594e-01,  1.93176270e-02,\n       -1.69326782e-01, -4.28649902e-01,  5.39062500e-01, -1.28417969e-01,\n       -8.83789062e-02,  5.13916016e-01,  9.13085938e-02, -1.60156250e-01,\n        6.86035156e-02, -9.74121094e-02, -3.70712280e-01, -3.27270508e-01,\n        1.77978516e-01, -4.65332031e-01,  1.70410156e-01,  9.08203125e-02,\n        2.76857376e-01, -1.69677734e-01,  3.27728271e-01, -3.12500000e-02,\n       -2.20809937e-01, -3.46679688e-01,  4.67407227e-01,  5.31860352e-01,\n       -1.30615234e-01, -2.36816406e-02, -6.56250000e-01, -5.79589844e-01,\n       -2.05810547e-01, -3.03222656e-01,  1.94259644e-01, -7.28515625e-01,\n       -4.92522240e-01, -5.37109375e-01, -3.47656250e-01,  1.08642578e-01,\n       -1.41601562e-01, -2.07031250e-01,  2.52441406e-01, -7.78808594e-02,\n       -5.02441406e-01,  1.53808594e-02,  8.64257812e-02,  2.59765625e-01,\n        6.64062500e-02, -7.12890625e-01, -1.45751953e-01,  7.56835938e-03,\n        4.87792969e-01,  1.39160156e-01,  1.15722656e-01,  1.28662109e-01,\n       -4.75585938e-01,  2.21191406e-01,  3.25317383e-01,  1.06323242e-01,\n       -6.11083984e-01, -3.59619141e-01,  6.54296875e-02, -2.41699219e-01,\n       -6.29882812e-02, -1.62109375e-01,  4.26269531e-01, -4.38354492e-01,\n        1.93725586e-01,  4.89562988e-01,  5.31494141e-01, -7.29370117e-02,\n        1.77246094e-01,  9.39941406e-02,  2.92236328e-01, -2.74047852e-01,\n        2.63366699e-02,  4.36035156e-01, -3.76953125e-01,  3.10546875e-01,\n        4.87304688e-01, -2.43041992e-01,  1.21612549e-02, -3.80371094e-01,\n        3.80493164e-01, -6.22436523e-01, -3.98071289e-01,  1.24206543e-01,\n       -8.20312500e-01, -2.72583008e-01, -6.21582031e-01, -4.87060547e-01,\n        3.06671143e-01, -2.61230469e-01,  5.12451172e-01,  5.55694580e-01,\n        5.66894531e-01,  7.33886719e-01, -1.75781250e-01,  4.13574219e-01,\n       -2.54272461e-01,  1.32507324e-01, -4.78515625e-01,  4.63256836e-01,\n       -6.21948242e-02, -1.80664062e-01, -5.46386719e-01, -6.31103516e-01,\n       -1.47949219e-01, -3.15185547e-01, -7.12890625e-02, -7.67578125e-01,\n        3.92272949e-01, -1.97753906e-01,  2.23144531e-01, -5.07324219e-01,\n        8.39843750e-02, -4.98657227e-02,  1.01074219e-01,  2.07885742e-01,\n       -2.77343750e-01,  1.03027344e-01, -1.38671875e-01,  2.87353516e-01,\n       -4.81895447e-01, -1.66748047e-01, -1.47277832e-01,  3.61633301e-01,\n        6.38504028e-02, -6.69189453e-01,  1.95312500e-03, -7.34375000e-01,\n       -1.28158569e-01,  9.76562500e-04, -7.08007812e-02,  3.72558594e-01,\n        8.31176758e-01,  5.94482422e-01,  5.37109375e-02, -3.00140381e-01,\n       -4.53857422e-01,  1.11511230e-01, -1.32812500e-01,  1.25732422e-01,\n        3.39843750e-01, -2.48352051e-01, -1.62353516e-02, -2.84667969e-01,\n        4.70703125e-01, -4.48242188e-01,  8.50753784e-02,  2.69042969e-01,\n        3.98254395e-03, -3.53759766e-01, -3.90625000e-02, -3.22753906e-01,\n       -6.90917969e-02, -4.13818359e-02,  1.35314941e-01, -8.50396156e-02,\n        1.28417969e-01,  6.15966797e-01,  3.55957031e-01, -6.05468750e-02,\n       -2.25463867e-01, -2.62207031e-01, -2.72949219e-01, -5.16113281e-01,\n        1.59179688e-01,  2.74902344e-01, -7.61718750e-02, -3.41796875e-03,\n        4.37500000e-01,  2.98583984e-01, -4.40795898e-01, -3.43261719e-01,\n        1.73583984e-01,  3.32092285e-01, -2.12646484e-01,  5.76171875e-01,\n        2.06787109e-01, -7.91015625e-02,  5.79695702e-02, -1.01806641e-01,\n       -7.06787109e-01, -3.40576172e-02, -4.11865234e-01,  9.82666016e-02,\n       -1.70410156e-01, -4.18212891e-01,  8.39233398e-01, -1.15722656e-01,\n        1.28173828e-01, -2.07763672e-01, -4.08203125e-01, -1.77612305e-01,\n        1.01196289e-01,  4.24072266e-01, -5.26428223e-02, -5.58593750e-01,\n        1.12304688e-02, -1.12060547e-01, -9.42382812e-02,  2.35595703e-02,\n       -3.92578125e-01, -7.12890625e-02,  5.69824219e-01,  9.81445312e-02],\n      dtype=float32)\n\n\n\nfind_closest_word(doc2vec)\n\n'petroleum'\n\n\nCongratulations! We have finished the introduction to word embeddings manipulation!",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "L2 - Manipulating word embeddings"
    ]
  },
  {
    "objectID": "notes/c2w2/lab02.html",
    "href": "notes/c2w2/lab02.html",
    "title": "Parts-of-Speech Tagging - Working with tags and Numpy",
    "section": "",
    "text": "course banner\n\n\n\n\nFigure 1\n\n\nIn this lecture notebook we will create a matrix using some tag information and then modify it using different approaches. This will serve as hands-on experience working with Numpy and as an introduction to some elements used for POS tagging.\n\nimport numpy as np\nimport pandas as pd\n\n\nSome information on tags\nFor this notebook we will be using a toy example including only three tags (or states). In a real world application there are many more tags which can be found here.\n\n# Define tags for Adverb, Noun and To (the preposition) , respectively\ntags = ['RB', 'NN', 'TO']\n\nIn this week’s assignment we will construct some dictionaries that provide useful information of the tags and words we will be working with.\nOne of these dictionaries is the transition_counts which counts the number of times a particular tag happened next to another. The keys of this dictionary have the form (previous_tag, tag) and the values are the frequency of occurrences.\nAnother one is the emission_counts dictionary which will count the number of times a particular pair of (tag, word) appeared in the training dataset.\nIn general think of transition when working with tags only and of emission when working with tags and words.\nIn this notebook we will be looking at the first one:\n\n# Define 'transition_counts' dictionary\n# Note: values are the same as the ones in the assignment\ntransition_counts = {\n    ('NN', 'NN'): 16241,\n    ('RB', 'RB'): 2263,\n    ('TO', 'TO'): 2,\n    ('NN', 'TO'): 5256,\n    ('RB', 'TO'): 855,\n    ('TO', 'NN'): 734,\n    ('NN', 'RB'): 2431,\n    ('RB', 'NN'): 358,\n    ('TO', 'RB'): 200\n}\n\nNotice that there are 9 combinations of the 3 tags used. Each tag can appear after the same tag so we should include those as well.\n\n\nUsing Numpy for matrix creation\nNow we will create a matrix that includes these frequencies using Numpy arrays:\n\n# Store the number of tags in the 'num_tags' variable\nnum_tags = len(tags)\n\n# Initialize a 3X3 numpy array with zeros\ntransition_matrix = np.zeros((num_tags, num_tags))\n\n# Print matrix\ntransition_matrix\n\narray([[0., 0., 0.],\n       [0., 0., 0.],\n       [0., 0., 0.]])\n\n\nVisually we can see the matrix has the correct dimensions. Don’t forget we can check this too using the shape attribute:\n\n# Print shape of the matrix\ntransition_matrix.shape\n\n(3, 3)\n\n\nBefore filling this matrix with the values of the transition_counts dictionary we should sort the tags so that their placement in the matrix is consistent:\n\n# Create sorted version of the tag's list\nsorted_tags = sorted(tags)\n\n# Print sorted list\nsorted_tags\n\n['NN', 'RB', 'TO']\n\n\nTo fill this matrix with the correct values we can use a double for loop. We could also use itertools.product to one line this double loop:\n\n# Loop rows\nfor i in range(num_tags):\n    # Loop columns\n    for j in range(num_tags):\n        # Define tag pair\n        tag_tuple = (sorted_tags[i], sorted_tags[j])\n        # Get frequency from transition_counts dict and assign to (i, j) position in the matrix\n        transition_matrix[i, j] = transition_counts.get(tag_tuple)\n\n# Print matrix\ntransition_matrix\n\narray([[1.6241e+04, 2.4310e+03, 5.2560e+03],\n       [3.5800e+02, 2.2630e+03, 8.5500e+02],\n       [7.3400e+02, 2.0000e+02, 2.0000e+00]])\n\n\nLooks like this worked fine. However the matrix can be hard to read as Numpy is more about efficiency, rather than presenting values in a pretty format.\nFor this we can use a Pandas DataFrame. In particular, a function that takes the matrix as input and prints out a pretty version of it will be very useful:\n\n# Define 'print_matrix' function\ndef print_matrix(matrix):\n    print(pd.DataFrame(matrix, index=sorted_tags, columns=sorted_tags))\n\nNotice that the tags are not a parameter of the function. This is because the sorted_tags list will not change in the rest of the notebook so it is safe to use the variable previously declared. To test this function simply run:\n\n# Print the 'transition_matrix' by calling the 'print_matrix' function\nprint_matrix(transition_matrix)\n\n         NN      RB      TO\nNN  16241.0  2431.0  5256.0\nRB    358.0  2263.0   855.0\nTO    734.0   200.0     2.0\n\n\nThat is a lot better, isn’t it?\nAs we may have already deducted this matrix is not symmetrical.\n\n\nWorking with Numpy for matrix manipulation\nNow that we got the matrix set up it is time to see how a matrix can be manipulated after being created.\nNumpy allows vectorized operations which means that operations that would normally include looping over the matrix can be done in a simpler manner. This is consistent with treating numpy arrays as matrices since we get support for common matrix operations. We can do matrix multiplication, scalar multiplication, vector addition and many more!\nFor instance try scaling each value in the matrix by a factor of \\frac{1}{10}. Normally we would loop over each value in the matrix, updating them accordingly. But in Numpy this is as easy as dividing the whole matrix by 10:\n\n# Scale transition matrix\ntransition_matrix = transition_matrix/10\n\n# Print scaled matrix\nprint_matrix(transition_matrix)\n\n        NN     RB     TO\nNN  1624.1  243.1  525.6\nRB    35.8  226.3   85.5\nTO    73.4   20.0    0.2\n\n\nAnother trickier example is to normalize each row so that each value is equal to \\frac{value}{sum \\,of \\,row}.\nThis can be easily done with vectorization. First we will compute the sum of each row:\n\n# Compute sum of row for each row\nrows_sum = transition_matrix.sum(axis=1, keepdims=True)\n\n# Print sum of rows\nrows_sum\n\narray([[2392.8],\n       [ 347.6],\n       [  93.6]])\n\n\nNotice that the sum() method was used. This method does exactly what its name implies. Since the sum of the rows was desired the axis was set to 1. In Numpy axis=1 refers to the columns so the sum is done by summing each column of a particular row, for each row.\nAlso the keepdims parameter was set to True so the resulting array had shape (3, 1) rather than (3,). This was done so that the axes were consistent with the desired operation.\nWhen working with Numpy, always remember to check the shape of the arrays we are working with, many unexpected errors happen because of axes not being consistent. The shape attribute is your friend for these cases.\n\n# Normalize transition matrix\ntransition_matrix = transition_matrix / rows_sum\n\n# Print normalized matrix\nprint_matrix(transition_matrix)\n\n          NN        RB        TO\nNN  0.678745  0.101596  0.219659\nRB  0.102992  0.651036  0.245972\nTO  0.784188  0.213675  0.002137\n\n\nNotice that the normalization that was carried out forces the sum of each row to be equal to 1. We can easily check this by running the sum method on the resulting matrix:\n\ntransition_matrix.sum(axis=1, keepdims=True)\n\narray([[1.],\n       [1.],\n       [1.]])\n\n\nFor a final example we are asked to modify each value of the diagonal of the matrix so that they are equal to the log of the sum of the current row plus the current value. When doing mathematical operations like this one don’t forget to import the math module.\nThis can be done using a standard for loop or vectorization. You’ll see both in action:\n\nimport math\n\n# Copy transition matrix for for-loop example\nt_matrix_for = np.copy(transition_matrix)\n\n# Copy transition matrix for numpy functions example\nt_matrix_np = np.copy(transition_matrix)\n\n\nUsing a for-loop\n\n# Loop values in the diagonal\nfor i in range(num_tags):\n    t_matrix_for[i, i] =  t_matrix_for[i, i] + math.log(rows_sum[i])\n\n# Print matrix\nprint_matrix(t_matrix_for)\n\n          NN        RB        TO\nNN  8.458964  0.101596  0.219659\nRB  0.102992  6.502088  0.245972\nTO  0.784188  0.213675  4.541167\n\n\n/tmp/ipykernel_128864/84584535.py:3: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  t_matrix_for[i, i] =  t_matrix_for[i, i] + math.log(rows_sum[i])\n\n\n\n\nUsing vectorization\n\n# Save diagonal in a numpy array\nd = np.diag(t_matrix_np)\n\n# Print shape of diagonal\nd.shape\n\n(3,)\n\n\nWe can save the diagonal in a numpy array using Numpy’s diag() function. Notice that this array has shape (3,) so it is inconsistent with the dimensions of the rows_sum array which are (3, 1). You’ll have to reshape before moving forward. For this we can use Numpy’s reshape() function, specifying the desired shape in a tuple:\n\n# Reshape diagonal numpy array\nd = np.reshape(d, (3,1))\n\n# Print shape of diagonal\nd.shape\n\n(3, 1)\n\n\nNow that the diagonal has the correct shape we can do the vectorized operation by applying the math.log() function to the rows_sum array and adding the diagonal.\nTo apply a function to each element of a numpy array use Numpy’s vectorize() function providing the desired function as a parameter. This function returns a vectorized function that accepts a numpy array as a parameter.\nTo update the original matrix we can use Numpy’s fill_diagonal() function.\n\n# Perform the vectorized operation\nd = d + np.vectorize(math.log)(rows_sum)\n\n# Use numpy's 'fill_diagonal' function to update the diagonal\nnp.fill_diagonal(t_matrix_np, d)\n\n# Print the matrix\nprint_matrix(t_matrix_np)\n\n          NN        RB        TO\nNN  8.458964  0.101596  0.219659\nRB  0.102992  6.502088  0.245972\nTO  0.784188  0.213675  4.541167\n\n\nTo perform a sanity check that both methods yield the same result we can compare both matrices. Notice that this operation is also vectorized so we will get the equality check for each element in both matrices:\n\n# Check for equality\nt_matrix_for == t_matrix_np\n\narray([[ True,  True,  True],\n       [ True,  True,  True],\n       [ True,  True,  True]])\n\n\nCongratulations on finishing this lecture notebook! Now we should be more familiar with some elements used by a POS tagger such as the transition_counts dictionary and with working with Numpy.\nKeep it up!\n\n\n\n\n\nCitationBibTeX citation:@online{bochman2020,\n  author = {Bochman, Oren},\n  title = {Parts-of-Speech {Tagging} - {Working} with Tags and {Numpy}},\n  date = {2020-10-22},\n  url = {https://orenbochman.github.io/notes-nlp/notes/c2w2/lab02.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2020. “Parts-of-Speech Tagging - Working with Tags\nand Numpy.” October 22, 2020. https://orenbochman.github.io/notes-nlp/notes/c2w2/lab02.html.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "L2 - Working with tags and Numpy"
    ]
  },
  {
    "objectID": "notes/c1w1/lab01.html",
    "href": "notes/c1w1/lab01.html",
    "title": "Lab: Preprocessing",
    "section": "",
    "text": "course banner\n\n\n\n\nFigure 1\nIn this lab, we will be exploring how to preprocess tweets for sentiment analysis. We will provide a function for preprocessing tweets during this week’s assignment, but it is still good to know what is going on under the hood. By the end of this lecture, we will see how to use the NLTK package to perform a preprocessing pipeline for Twitter datasets.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "L1 - Preprocessing"
    ]
  },
  {
    "objectID": "notes/c1w1/lab01.html#setup",
    "href": "notes/c1w1/lab01.html#setup",
    "title": "Lab: Preprocessing",
    "section": "Setup",
    "text": "Setup\nWe will be doing sentiment analysis on tweets in the first two weeks of this course. To help with that, we will be using the Natural Language Toolkit (NLTK) package, an open-source Python library for natural language processing. It has modules for collecting, handling, and processing Twitter data, and we will be acquainted with them as we move along the course.\nFor this exercise, we will use a Twitter dataset that comes with NLTK. This dataset has been manually annotated and serves to establish baselines for models quickly. Let us import them now as well as a few other libraries we will be using.\n\nimport nltk                                # Python library for NLP\nfrom nltk.corpus import twitter_samples    # sample Twitter dataset from NLTK\nimport matplotlib.pyplot as plt            # library for visualization\nimport random                              # pseudo-random number generator",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "L1 - Preprocessing"
    ]
  },
  {
    "objectID": "notes/c1w1/lab01.html#about-the-twitter-dataset",
    "href": "notes/c1w1/lab01.html#about-the-twitter-dataset",
    "title": "Lab: Preprocessing",
    "section": "About the Twitter dataset",
    "text": "About the Twitter dataset\nThe sample dataset from NLTK is separated into positive and negative tweets. It contains 5000 positive tweets and 5000 negative tweets exactly. The exact match between these classes is not a coincidence. The intention is to have a balanced dataset. That does not reflect the real distributions of positive and negative classes in live Twitter streams. It is just because balanced datasets simplify the design of most computational methods that are required for sentiment analysis. However, it is better to be aware that this balance of classes is artificial.\nThe dataset is already downloaded in the Coursera workspace. In a local computer however, we can download the data by doing:\n\n# downloads sample twitter dataset. uncomment the line below if running on a local machine.\n# nltk.download('twitter_samples')\n\nWe can load the text fields of the positive and negative tweets by using the module’s strings() method like this:\n\n# select the set of positive and negative tweets\nall_positive_tweets = twitter_samples.strings('positive_tweets.json')\nall_negative_tweets = twitter_samples.strings('negative_tweets.json')\n\nNext, we’ll print a report with the number of positive and negative tweets. It is also essential to know the data structure of the datasets\n\nprint('Number of positive tweets: ', len(all_positive_tweets))\nprint('Number of negative tweets: ', len(all_negative_tweets))\n\nprint('\\nThe type of all_positive_tweets is: ', type(all_positive_tweets))\nprint('The type of a tweet entry is: ', type(all_negative_tweets[0]))\n\nNumber of positive tweets:  5000\nNumber of negative tweets:  5000\n\nThe type of all_positive_tweets is:  &lt;class 'list'&gt;\nThe type of a tweet entry is:  &lt;class 'str'&gt;\n\n\nWe can see that the data is stored in a list and as we might expect, individual tweets are stored as strings.\nWe can make a more visually appealing report by using Matplotlib’s pyplot library. Let us see how to create a pie chart to show the same information as above. This simple snippet will serve we in future visualizations of this kind of data.\n\n# Declare a figure with a custom size\nfig = plt.figure(figsize=(5, 5))\n\n# labels for the two classes\nlabels = 'Positives', 'Negative'\n\n# Sizes for each slide\nsizes = [len(all_positive_tweets), len(all_negative_tweets)] \n\n# Declare pie chart, where the slices will be ordered and plotted counter-clockwise:\nplt.pie(sizes, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90)\n\n# Equal aspect ratio ensures that pie is drawn as a circle.\nplt.axis('equal')  \n\n# Display the chart\nplt.show()\n\n([&lt;matplotlib.patches.Wedge at 0x7f97dfca62f0&gt;,\n  &lt;matplotlib.patches.Wedge at 0x7f97dfca6230&gt;],\n [Text(-1.0999999999999959, -9.616505800409723e-08, 'Positives'),\n  Text(1.0999999999999832, 1.9233011600819372e-07, 'Negative')],\n [Text(-0.5999999999999978, -5.2453668002234845e-08, '50.0%'),\n  Text(0.5999999999999908, 1.0490733600446929e-07, '50.0%')])\n\n\n(np.float64(-1.100000000000005),\n np.float64(1.100000000000106),\n np.float64(-1.1),\n np.float64(1.1))",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "L1 - Preprocessing"
    ]
  },
  {
    "objectID": "notes/c1w1/lab01.html#looking-at-raw-texts",
    "href": "notes/c1w1/lab01.html#looking-at-raw-texts",
    "title": "Lab: Preprocessing",
    "section": "Looking at raw texts",
    "text": "Looking at raw texts\nBefore anything else, we can print a couple of tweets from the dataset to see how they look. Understanding the data is responsible for 80% of the success or failure in data science projects. We can use this time to observe aspects we’d like to consider when preprocessing our data.\nBelow, we will print one random positive and one random negative tweet. We have added a color mark at the beginning of the string to further distinguish the two. (Warning: This is taken from a public dataset of real tweets and a very small portion has explicit content.)\n\n# print positive in greeen\nprint('\\033[92m' + all_positive_tweets[random.randint(0,5000)])\n\n# print negative in red\nprint('\\033[91m' + all_negative_tweets[random.randint(0,5000)])\n\n@AshrafUzma @RTAluvedAfridi \nWe are not true Pakistanis :p\n@MaayanGean absolute world to me I was so close to seeing him but he did not show up in manila :( i cried so fucking hard that time\n\n\nOne observation we may have is the presence of emoticons and URLs in many of the tweets. This info will come in handy in the next steps.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "L1 - Preprocessing"
    ]
  },
  {
    "objectID": "notes/c1w1/lab01.html#preprocess-raw-text-for-sentiment-analysis",
    "href": "notes/c1w1/lab01.html#preprocess-raw-text-for-sentiment-analysis",
    "title": "Lab: Preprocessing",
    "section": "Preprocess raw text for Sentiment analysis",
    "text": "Preprocess raw text for Sentiment analysis\nData preprocessing is one of the critical steps in any machine learning project. It includes cleaning and formatting the data before feeding into a machine learning algorithm. For NLP, the preprocessing steps are comprised of the following tasks:\n\nTokenizing the string\nLowercasing\nRemoving stop words and punctuation\nStemming\n\nThe videos explained each of these steps and why they are important. Let’s see how we can do these to a given tweet. We will choose just one and see how this is transformed by each preprocessing step.\n\n# Our selected sample. Complex enough to exemplify each step\ntweet = all_positive_tweets[2277]\nprint(tweet)\n\nMy beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off… https://t.co/3tfYom0N1i\n\n\nLet’s import a few more libraries for this purpose.\n\n# download the stopwords from NLTK\nnltk.download('stopwords')\n\n[nltk_data] Downloading package stopwords to /home/oren/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\nTrue\n\n\n\nimport re                                  # library for regular expression operations\nimport string                              # for string operations\n\nfrom nltk.corpus import stopwords          # module for stop words that come with NLTK\nfrom nltk.stem import PorterStemmer        # module for stemming\nfrom nltk.tokenize import TweetTokenizer   # module for tokenizing strings\n\n\nRemove hyperlinks, Twitter marks and styles\nSince we have a Twitter dataset, we’d like to remove some substrings commonly used on the platform like the hashtag, retweet marks, and hyperlinks. We’ll use the re library to perform regular expression operations on our tweet. We’ll define our search pattern and use the sub() method to remove matches by substituting with an empty character (i.e. '')\n\nprint('\\033[92m' + tweet)\nprint('\\033[94m')\n\n# remove old style retweet text \"RT\"\ntweet2 = re.sub(r'^RT[\\s]+', '', tweet)\n\n# remove hyperlinks\ntweet2 = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet2)\n\n# remove hashtags\n# only removing the hash # sign from the word\ntweet2 = re.sub(r'#', '', tweet2)\n\nprint(tweet2)\n\nMy beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off… https://t.co/3tfYom0N1i\n\nMy beautiful sunflowers on a sunny Friday morning off :) sunflowers favourites happy Friday off… \n\n\n\n\nTokenize the string\nTo tokenize means to split the strings into individual words without blanks or tabs. In this same step, we will also convert each word in the string to lower case. The tokenize module from NLTK allows us to do these easily:\n\nprint()\nprint('\\033[92m' + tweet2)\nprint('\\033[94m')\n\n# instantiate tokenizer class\ntokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n                               reduce_len=True)\n\n# tokenize tweets\ntweet_tokens = tokenizer.tokenize(tweet2)\n\nprint()\nprint('Tokenized string:')\nprint(tweet_tokens)\n\n\nMy beautiful sunflowers on a sunny Friday morning off :) sunflowers favourites happy Friday off… \n\n\nTokenized string:\n['my', 'beautiful', 'sunflowers', 'on', 'a', 'sunny', 'friday', 'morning', 'off', ':)', 'sunflowers', 'favourites', 'happy', 'friday', 'off', '…']\n\n\n\n\nRemove stop words and punctuations\nThe next step is to remove stop words and punctuation. Stop words are words that don’t add significant meaning to the text. You’ll see the list provided by NLTK when we run the cells below.\n\n#Import the english stop words list from NLTK\nstopwords_english = stopwords.words('english') \n\nprint('Stop words\\n')\nprint(stopwords_english)\n\nprint('\\nPunctuation\\n')\nprint(string.punctuation)\n\nStop words\n\n['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n\nPunctuation\n\n!\"#$%&'()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~\n\n\nWe can see that the stop words list above contains some words that could be important in some contexts. These could be words like i, not, between, because, won, against. We might need to customize the stop words list for some applications. For our exercise, we will use the entire list.\nFor the punctuation, we saw earlier that certain groupings like ‘:)’ and ‘…’ should be retained when dealing with tweets because they are used to express emotions. In other contexts, like medical analysis, these should also be removed.\nTime to clean up our tokenized tweet!\n\nprint()\nprint('\\033[92m')\nprint(tweet_tokens)\nprint('\\033[94m')\n\ntweets_clean = []\n\nfor word in tweet_tokens: # Go through every word in your tokens list\n    if (word not in stopwords_english and  # remove stopwords\n        word not in string.punctuation):  # remove punctuation\n        tweets_clean.append(word)\n\nprint('removed stop words and punctuation:')\nprint(tweets_clean)\n\n\n\n['my', 'beautiful', 'sunflowers', 'on', 'a', 'sunny', 'friday', 'morning', 'off', ':)', 'sunflowers', 'favourites', 'happy', 'friday', 'off', '…']\n\nremoved stop words and punctuation:\n['beautiful', 'sunflowers', 'sunny', 'friday', 'morning', ':)', 'sunflowers', 'favourites', 'happy', 'friday', '…']\n\n\nPlease note that the words happy and sunny in this list are correctly spelled.\n\n\nStemming\nStemming is the process of converting a word to its most general form, or stem. This helps in reducing the size of our vocabulary.\nConsider the words: * learn * learning * learned * learnt\nAll these words are stemmed from its common root learn. However, in some cases, the stemming process produces words that are not correct spellings of the root word. For example, happi and sunni. That’s because it chooses the most common stem for related words. For example, we can look at the set of words that comprises the different forms of happy:\n\nhappy\nhappiness\nhappier\n\nWe can see that the prefix happi is more commonly used. We cannot choose happ because it is the stem of unrelated words like happen.\nNLTK has different modules for stemming and we will be using the PorterStemmer module which uses the Porter Stemming Algorithm. Let’s see how we can use it in the cell below.\n\nprint()\nprint('\\033[92m')\nprint(tweets_clean)\nprint('\\033[94m')\n\n# Instantiate stemming class\nstemmer = PorterStemmer() \n\n# Create an empty list to store the stems\ntweets_stem = [] \n\nfor word in tweets_clean:\n    stem_word = stemmer.stem(word)  # stemming word\n    tweets_stem.append(stem_word)  # append to the list\n\nprint('stemmed words:')\nprint(tweets_stem)\n\n\n\n['beautiful', 'sunflowers', 'sunny', 'friday', 'morning', ':)', 'sunflowers', 'favourites', 'happy', 'friday', '…']\n\nstemmed words:\n['beauti', 'sunflow', 'sunni', 'friday', 'morn', ':)', 'sunflow', 'favourit', 'happi', 'friday', '…']\n\n\nThat’s it! Now we have a set of words we can feed into to the next stage of our machine learning project.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "L1 - Preprocessing"
    ]
  },
  {
    "objectID": "notes/c1w1/lab01.html#process_tweet",
    "href": "notes/c1w1/lab01.html#process_tweet",
    "title": "Lab: Preprocessing",
    "section": "process_tweet()",
    "text": "process_tweet()\nAs shown above, preprocessing consists of multiple steps before we arrive at the final list of words. We will not ask we to replicate these however. In the week’s assignment, we will use the function process_tweet(tweet) available in utils.py. We encourage we to open the file and you’ll see that this function’s implementation is very similar to the steps above.\nTo obtain the same result as in the previous code cells, we will only need to call the function process_tweet(). Let’s do that in the next cell.\n\nfrom utils import process_tweet # Import the process_tweet function\n\n# choose the same tweet\ntweet = all_positive_tweets[2277]\n\nprint()\nprint('\\033[92m')\nprint(tweet)\nprint('\\033[94m')\n\n# call the imported function\ntweets_stem = process_tweet(tweet); # Preprocess a given tweet\n\nprint('preprocessed tweet:')\nprint(tweets_stem) # Print the result\n\n\n\nMy beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off… https://t.co/3tfYom0N1i\n\npreprocessed tweet:\n['beauti', 'sunflow', 'sunni', 'friday', 'morn', ':)', 'sunflow', 'favourit', 'happi', 'friday', '…']\n\n\nThat’s it for this lab! We now know what is going on when we call the preprocessing helper function in this week’s assignment. Hopefully, this exercise has also given we some insights on how to tweak this for other types of text datasets.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "L1 - Preprocessing"
    ]
  },
  {
    "objectID": "notes/c1w1/index.html",
    "href": "notes/c1w1/index.html",
    "title": "Sentiment Analysis with Logistic Regression",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\nThis week’s slides\n\n\n\n\nSupplementary Figure 1\nMy notes for Week 1 of the Natural Language Processing with Probabilistic Models Course in the Natural Language Processing Specialization Offered by DeepLearning.AI on Coursera",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w1/index.html#sec-supervised-ml-sentiment-analysis",
    "href": "notes/c1w1/index.html#sec-supervised-ml-sentiment-analysis",
    "title": "Sentiment Analysis with Logistic Regression",
    "section": "Supervised ML & Sentiment Analysis",
    "text": "Supervised ML & Sentiment Analysis\n\n\n\n\n\n\n\nFigure 2: classification overview\n\n\n\nIn supervised ML we get a dataframe with input features X and their corresponding ground truth label Y.\nThe goal is to minimize prediction error rates, AKA cost.\nTo do this, one runs the prediction function which takes in parameters and map the features of an input to an output label \\hat{Y}.\nThe optimal mapping from features to labels is when the difference between the expected values Y and the predicted values \\hat{Y} is minimized.\nThe cost function F does this by comparing how closely the output \\hat{Y} is to the label Y.\nUpdate the parameters and repeat the whole process until the cost is minimized.\nWe use the Sigmoid cost function:\n\n\n\n\n\n\n\n\nFigure 3: the Sigmoid cost function",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w1/index.html#sec-sentiment-analysis",
    "href": "notes/c1w1/index.html#sec-sentiment-analysis",
    "title": "Sentiment Analysis with Logistic Regression",
    "section": "Sentiment Analysis",
    "text": "Sentiment Analysis\n\n\n\n\n\n\n\nFigure 4: Sentiment Analysis\n\n\n\n\n\n\n\n\nSentiment Analysis - Motivation\n\n\n\n\n\nIf we are passionate about NLP here is a gem to get we started. This is a popular science with many ideas for additional classifiers using pronouns.\n\n\n\n\n\n\n\n\n\n\n\nVideo 1: The Secret Life of Pronouns: James Pennebaker at TEDxAustin\n\n\n\n\n\n\n\n\nVideo 2: Language of Truth and Lies: I-words\n\n\n\n\n\n\n\n\nVideo 3: LIWC-22 2022 Tutorial 1: Getting started with LIWC-22\n\n\n\n\n\n\n\n\nFigure 5: classification overview\n\n\n\n\n\n\nOne example of a Supervised machine learning classification task for sentiment analysis\nThe objective is to predict whether a tweet has a positive or negative sentiment. (If it is positive/optimistic or negative/pessimistic).\n\nTo perform sentiment analysis on a tweet, we need to:\n\nrepresent the text for example “I am happy because I am learning NLP” as features,\ntrain a logistic regression classifier\n\n\n1 for a positive sentiment\n0 for negative sentiment.\n\n\nand then we use it to classify the text.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w1/index.html#sec-vocabulary-feature-extraction",
    "href": "notes/c1w1/index.html#sec-vocabulary-feature-extraction",
    "title": "Sentiment Analysis with Logistic Regression",
    "section": "Vocabulary & Feature Extraction",
    "text": "Vocabulary & Feature Extraction\n\nSparse Representation\n\n\n\n\nProblems with sparse representation\n\nGiven a tweet, or some text, one can represent it as a vector. The vector has a dimension |V|, where V corresponds to the size of the vocabulary size. If we had the tweet “I am happy because I am learning NLP,” then we would put a 1 in the corresponding index for any word in the tweet, and a 0 otherwise.\n\n\n\n\n\n\n\nFigure 6: A sparse representation\n\n\nAs V gets larger, the vector becomes more sparse. Furthermore, we end up having many more features and end up training  \\theta_0 \\ldots \\theta_n  parameters. This results in a larger training time. And the inference time increases as well.\n\n\nFeature Extraction based on class frequencies\n\n\n\nTable 1: Table of tweets\n\n\n\n\n\n\n\n\n\nPositive tweets\nNegative tweets\n\n\n\n\nI am happy because I am learning NLP\nI am sad, I am not learning NLP\n\n\nI am happy\nI am sad\n\n\n\n\n\n\nGiven a corpus with positive and negative tweets we can represent it as follows:\n\n\n\nTable 2: Word class table\n\n\n\n\n\nVocabulary\nPosFreq (1)\nNegFreq (O)\n\n\n\n\nI\n3\n3\n\n\nam\n3\n3\n\n\nhappy\n2\n0\n\n\nbecause\n1\n0\n\n\nlearning\n1\n1\n\n\nNLP\n1\n1\n\n\nsad\n0\n2\n\n\nnot\n0\n1\n\n\n\n\n\n\nfreqs: dictionary mapping from (word, class) to frequency :\n\n\\underbrace{X_m}_{\\textcolor{#7200ac}{\\text{Features of tweet M}}} =\\left[ \\underbrace{1}_{\\textcolor{#126ed5}{\\text{bias}}}\n,\\sum_w \\underbrace{\\textcolor{#da7801}{freqs}(w,\\textcolor{#2db15d}{1})}_{\\textcolor{#931e18}{\\text{Sum Pos.freqs}}} ,\\sum_w \\underbrace{\\textcolor{#da7801}{frequencies}(w,\\textcolor{#931e18}{0})}_{\\textcolor{#2db15d}{\\text{Sum Neg. frequencies}}}\n\\right]\n\\tag{1}\nwe have to encode each tweet as a vector. Previously, this vector was of dimension VV. Now, as we’ll see in the upcoming videos, we’ll represent it with a vector of dimension 33. We create a dictionary to map the word, it class, either positive or negative, to the number of times that word appeared in its corresponding class.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w1/index.html#sec-preprocessing",
    "href": "notes/c1w1/index.html#sec-preprocessing",
    "title": "Sentiment Analysis with Logistic Regression",
    "section": "Preprocessing",
    "text": "Preprocessing\n\n\n\n\n\n\n\nFigure 7: preprocessing - feature extraction\n\n\n\n\n\n\n\n\nFigure 8: preprocessing - stemming\n\n\n\nWhen preprocessing, we have to perform the following:\n\nEliminate handles and URLs\nTokenize the string into words.\nRemove stop words like “and, is, a, on, etc.”\nStemming - converting each word to its stem. For example dancer, dancing, danced, becomes ‘danc’. We can use Porter’s Stemmer to take care of this.\nConvert all our words to lower case.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w1/index.html#sec-logistic-regression-intro",
    "href": "notes/c1w1/index.html#sec-logistic-regression-intro",
    "title": "Sentiment Analysis with Logistic Regression",
    "section": "Intro to Logistic Regression",
    "text": "Intro to Logistic Regression\n\n\n\n\n\n\nFlatten learning curve 📈 with StatQuest\n\n\n\n\n\nIf we want to flatten our learning curve consider the following videos which will help us get more confident with logistic regression by building from the more familiar OLS regression. Here are three videos from StatQuest\n\n\n\n\nVideo 4\nVideo 5\nVideo 6\n\n\n\n\n\n\n\n\n\nVideo 4: StatQuest: Logistic Regression\n\n\n\n\n\n\n\n\nVideo 5: StatQuest: Logistic Regression Details Pt1: Coefficients\n\n\n\n\n\n\n\n\nVideo 6: StatQuest: Logistic Regression Details Pt 2: Maximum Likelihood",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w1/index.html#sec-training-logistic-regression",
    "href": "notes/c1w1/index.html#sec-training-logistic-regression",
    "title": "Sentiment Analysis with Logistic Regression",
    "section": "Training Logistic Regression",
    "text": "Training Logistic Regression\n\n\n\n\n\n\n\nFigure 9: Training algorithm - flow chart\n\n\nTo train our logistic regression function, we’ll do the following: we initialize our parameter  \\theta , that we can use in we Sigmoid, we then compute the gradient that we’ll use to update \\theta, and then calculate the cost. we keep doing so until good enough\n\n\n\n\n\n\n\nFigure 10: Training\n\n\nUsually we keep training until the cost converges. If we were to plot the number of iterations versus the cost, we should see something like this:",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w1/index.html#sec-testing-logistic-regression",
    "href": "notes/c1w1/index.html#sec-testing-logistic-regression",
    "title": "Sentiment Analysis with Logistic Regression",
    "section": "Testing Logistic Regression",
    "text": "Testing Logistic Regression\n\n\n\n\n\n\n\nFigure 11: Testing logistic regression\n\n\nTo test our model, we would run a subset of our data, known as the validation set, on our model to get predictions. The predictions are the outputs of the Sigmoid function.\nIf the output is ≥ 0.5, we would assign it to a positive class, otherwise to the negative class.\nTo compute accuracy, we solve the following equation:\n\n\\text{accuracy} = \\sum_i \\frac{\\hat{y}^{(i)}= y^{(i)}_{val}}{m}\n\\tag{2}\nwhere:\nCross validation note:\n\nIn reality, given your X data we would usually split it into three components. X_{train}, X_{val}, X_{test}.\nThe distribution usually varies depending on the size of our data set. However, a 80%, 10%, 10% split usually works.\n\nIn other words, we go over all our training examples, m of them, and then for every prediction, if it was wright we add a one. we then divide by m.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w1/index.html#sec-logistic-regression-cost-function",
    "href": "notes/c1w1/index.html#sec-logistic-regression-cost-function",
    "title": "Sentiment Analysis with Logistic Regression",
    "section": "Logistic Regression: Cost Function",
    "text": "Logistic Regression: Cost Function\n\n\n\n\n\n\n\nFigure 12: Cost function for logistic regression\n\n\n\n\n\n\n\n\nSigmoid\n\n\n\nWe should start by developing intuition about how the cost function is designed the way it is.\nThis is important because we’ll meet the sigmoid in Neural Networks, job interviews and so best make a friend of it.\nIn (Hinton 2012) there is the full derivation of Sigmoid cost function. Also the sigmoid and Logistic regression are intimately related - we can’t have one without the other.\n\n\nIn plain english: “The cost function is mean log loss across all training examples”\n\nJ(\\theta) = −\\frac{1}{m} \\sum^m_{i=1}[y^{(i)}\\log h(x^{(i)}, \\theta)+(1 −y^{(i)}) \\log (1−h(x^{(i)}, \\theta))]\n\\tag{3}\nwhere:\n\nm is the count of rows of our training set.\ni indexes a single row in the dataset.\nx^{(i)} is the data for row i.\ny^{(i)} is the ground truth AKA label for rows i.\nh(x^{(i)},\\theta) is the model’s prediction for row i.\n\nWe’ll derive the logistic regression cost function to get the gradients.\nwe can see in the figure:\n\nIf y = 1 and our prediction is close to 0, we get a cost close to  ∞.\nThe same applies when y=0 and we predict ion is close to 1.\nOn the other hand if we get a prediction equal to the label, we get a cost of 0.\n\nIn either, case we are trying to minimize  J(\\theta)\n\n\nMathematical Derivation\nTo see why the cost function is designed that way, let’s take a step back and write up a function that compresses the two cases into one case.\nIf\n\nP(y \\mid x(i), \\theta) =h(x^{(i)}, \\theta)^{y^{(i)}}1−h(x^{(i)}, \\theta)^{1−y^{(i)}}\n\\tag{4}\nThen the likelihood of the data set is given by:\nFrom the preceding, we can see that when y = 1, we get h(x^{(i)}, \\theta)^{y^{(i)}} and when y≈0 the term 1 − h(x^{(i)}, \\theta)^{(1−y^{(i)})}, which makes sense, since the two probabilities equal to 1.\nIn either case, we want to maximize the function h(x^{(i)}, \\theta)^{y(i)} by making it as close to 1 as possible.\nWhen y ≈ 0 , we want the term 1-h(x^{(i)}, \\theta)^{1−y^{(i)}} ≈ 0 which then \\implies  h(x^{(i)}, \\theta)^{y^{(i)}} ≈ 1\nWhen y=1, we want h(x^{(i)}, \\theta)^{y^{(i)}} = 1\nNow we want to find a way to model the entire data set and not just one example. To do so, we’ll define the likelihood as follows:\n\nL(\\theta) = \\prod^m_{i=1} h(\\theta, x^{(i)})^{y^{(i)}} (1−h(\\theta, x^{(i)}))^{(1−y^{(i)})}\n\\tag{5}\nNote that if we mess up the classification of one example, we end up messing up the overall likelihood score, which is exactly what we intended. We want to fit a model to the entire dataset where all data points are related. to\n\n\\lim_{m \\to \\infty} L(\\theta) = 0\n\\tag{6}\nIt goes close to zero, because both h(\\theta, x^{(i)})^{y^{(i)}} and (1−h(\\theta, x^{(i)}))^{(1−y^{(i)})}  are bounded by [0,1]. Since we are trying to maximize  h(\\theta, x^{(i)}) in L(\\theta), we can introduce the log and just maximize the log of the function.\nIntroducing the log, allows us to write the log of a product as the sum of each log. Here are two identities that will come in handy:\n\n  \\log(a*b*c) = \\log(a) + \\log(b) + \\log(c)  \n\nand\n\n  \\log(a^b) = b \\times \\log(a)\n\nGiven the two preceding identities, we can rewrite the equation as follows:   \n  \\begin{align*}  \n    \\max_{ h(x^{(i)},\\theta)}\\log L(\\theta) &= \\log \\prod^m_{i=1}h(x^{(i)}, \\theta)^{y^{(i)}}(1−h(x^{(i)} ,\\theta))^{1−y^{(i)}} \\\\\n    &= \\sum^m_{i=1} \\log h(x^{(i)}, \\theta)^{y^{(i)}} (1−h(x^{(i)}, \\theta)^{1−y^{(i)}})            \\\\\n    &= \\sum^m_{i=1} \\log h(x^{(i)}, \\theta)^{y^{(i)}} + \\log(1−h(x^{(i)}, \\theta)^{1−y^{(i)}} \\\\\n    &= \\sum^m_{i=1} y^{(i)}\\log h(x^{(i)}, \\theta) + (1−y^{(i)}) \\log(1−h(x^{(i)}, \\theta))\n  \\end{align*}\n Hence, we now divide by m, because we want to see the average cost.   \n  \\begin{align*}  \n    \\frac{1}{m} \\sum^m_{i=1} y^{(i)}\\log h(x^{(i)}, \\theta) + (1−y^{(i)}) \\log(1−h(x^{(i)}, \\theta))  \n  \\end{align*}\n\nRemember that we were maximizing h(\\theta, x(i))  in the preceding equation. It turns out that maximizing an equation is the same as minimizing its negative. Think of x^2, feel free to plot it to see that for we yourself. Hence we add a negative sign and we end up minimizing the cost function as follows.\n  \n  \\begin{align*}  \n    J(\\theta)= − \\frac{1}{m} \\sum^m_{i=1} [y^{(i)} \\log h(x^{(i)}, \\theta) + ( 1 − y^{(i)}) \\log ( 1 − h(x^{(i)}, \\theta))]  \n  \\end{align*}\n\nA vectorized implementation is:\n\n\\begin{align*} & h = g(X\\theta)\\newline & J(\\theta)  = \\frac{1}{m} \\cdot \\left(-y^{T}\\log(h)-(1-y)^{T}\\log(1-h)\\right) \\end{align*}",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w1/index.html#logistic-regression-gradient",
    "href": "notes/c1w1/index.html#logistic-regression-gradient",
    "title": "Sentiment Analysis with Logistic Regression",
    "section": "Logistic Regression: Gradient",
    "text": "Logistic Regression: Gradient\n\n\\mathbf{w}(t+1) = \\mathbf{w}(t) - \\eta\\nabla E_{in}\n\nFor the case of logistic regression, the gradient of the error measure with respect to the weights, is calculated as:\n\n\\nabla E_{in}\\left(\\mathbf{w}\\right) = -\\frac{1}{N}\\sum\\limits_{n=1}^N \\frac{y_n\\mathbf{x_N}}{1 + \\exp\\left(y_n \\mathbf{w^T}(t)\\mathbf{x_n}\\right)}\n\nLet’s look into the gradient descent in more detail, as the gradient update rule is given without an explicitly derivation.\nThe general form Of gradient descent is defined\n\n  \\begin{align*} &\n    Repeat \\; \\lbrace \\newline & \\; \\theta_j := \\theta_j - \\alpha \\dfrac{\\partial}{\\partial \\theta_j}J(\\theta) \\newline & \\rbrace\n  \\end{align*}\n\nWe can work out the derivative part using Calculus to get:\n\n  \\begin{align*} &\n    Repeat \\; \\lbrace  \\theta_j := \\theta_j - \\frac{\\alpha}{m} \\sum_{i=1}^m ( h(x^{(i)}, \\theta) - y^{(i)}) x_j^{(i)} \\rbrace\n  \\end{align*}\n\nA vectorized formulation \n\\theta_j := \\theta_j - \\frac{\\alpha}{m} X^T ( H(X, \\theta) -Y)\n\n\nPartial derivative of J(\\theta)\n\n\\begin{align*}\n  h(x)'&=\\left(\\frac{1}{1+e^{-x}}\\right)'=\\frac{-(1+e^{-x})'}{(1+e^{-x})^2}=\\frac{-1'-(e^{-x})'}{(1+e^{-x})^2}=\\frac{0-(-x)'(e^{-x})}{(1+e^{-x})^2}=\\frac{-(-1)(e^{-x})}{(1+e^{-x})^2}=\\frac{e^{-x}}{(1+e^{-x})^2} \\newline &=\\left(\\frac{1}{1+e^{-x}}\\right)\\left(\\frac{e^{-x}}{1+e^{-x}}\\right)=h(x)\\left(\\frac{+1-1 + e^{-x}}{1+e^{-x}}\\right)=h(x)\\left(\\frac{1 + e^{-x}}{1+e^{-x}} - \\frac{1}{1+e^{-x}}\\right)=h(x)(1 - h(x))\n\\end{align*}\n\n\n\\begin{align*}\n\\frac{\\partial}{\\partial \\theta_j} J(\\theta) &= \\frac{\\partial}{\\partial \\theta_j} \\frac{-1}{m}\\sum_{i=1}^m \\left [ y^{(i)} log ( h(x^{(i)}, \\theta) ) + (1-y^{(i)}) log (1 -  h(x^{(i)}, \\theta)) \\right ] \\newline\n&= - \\frac{1}{m}\\sum_{i=1}^m \\left [     y^{(i)} \\frac{\\partial}{\\partial \\theta_j} log ( h(x^{(i)}, \\theta))   + (1-y^{(i)}) \\frac{\\partial}{\\partial \\theta_j} log (1 -  h(x^{(i)}, \\theta))\\right ] \\newline\n&= - \\frac{1}{m}\\sum_{i=1}^m \\left [ \\frac{y^{(i)} \\frac{\\partial}{\\partial \\theta_j}  h(x^{(i)}, \\theta)}{ h(x^{(i)}, \\theta)} + \\frac{(1-y^{(i)})\\frac{\\partial}{\\partial \\theta_j} (1 -  h(x^{(i)}, \\theta))}{1 -  h(x^{(i)}, \\theta)}\\right ] \\newline\n&= - \\frac{1}{m}\\sum_{i=1}^m \\left [ \\frac{y^{(i)} \\frac{\\partial}{\\partial \\theta_j}  h(x^{(i)}, \\theta)}{ h(x^{(i)}, \\theta)} + \\frac{(1-y^{(i)})\\frac{\\partial}{\\partial \\theta_j} (1 -  h(x^{(i)}, \\theta))}{1 -  h(x^{(i)}, \\theta)}\\right ] \\newline\n&= - \\frac{1}{m}\\sum_{i=1}^m \\left [ \\frac{y^{(i)}  h(x^{(i)}, \\theta) (1 -  h(x^{(i)}, \\theta)) \\frac{\\partial}{\\partial \\theta_j} \\theta^T x^{(i)}}{ h(x^{(i)}, \\theta)}   + \\frac{- (1-y^{(i)})  h(x^{(i)}, \\theta)(1 -  h(x^{(i)}, \\theta)) \\frac{\\partial}{\\partial \\theta_j} \\theta^T x^{(i)}}{1 -  h(x^{(i)}, \\theta)}\\right ] \\newline\n&= - \\frac{1}{m}\\sum_{i=1}^m \\left [ \\frac{y^{(i)}  h(x^{(i)}, \\theta) (1 -  h(x^{(i)}, \\theta)) \\frac{\\partial}{\\partial \\theta_j} \\theta^T x^{(i)}}{ h(x^{(i)}, \\theta)} - \\frac{(1-y^{(i)}) h(x^{(i)}, \\theta) (1 -  h(x^{(i)}, \\theta)) \\frac{\\partial}{\\partial \\theta_j} \\theta^T x^{(i)}}{1 -  h(x^{(i)}, \\theta)}\\right ] \\newline\n&= - \\frac{1}{m}\\sum_{i=1}^m \\left [ y^{(i)} (1 -  h(x^{(i)}, \\theta)) x^{(i)}_j - (1-y^{(i)})  h(x^{(i)}, \\theta) x^{(i)}_j\\right ] \\newline\n&= - \\frac{1}{m}\\sum_{i=1}^m \\left [ y^{(i)} (1 -  h(x^{(i)}, \\theta)) - (1-y^{(i)})  h(x^{(i)}, \\theta) \\right ] x^{(i)}_j \\newline\n&= - \\frac{1}{m}\\sum_{i=1}^m \\left [ y^{(i)} - y^{(i)}  h(x^{(i)}, \\theta) -  h(x^{(i)}, \\theta) + y^{(i)}  h(x^{(i)}, \\theta) \\right ] x^{(i)}_j \\newline\n&= - \\frac{1}{m}\\sum_{i=1}^m \\left [ y^{(i)} -  h(x^{(i)}, \\theta) \\right ] x^{(i)}_j \\newline\n&= \\frac{1}{m}\\sum_{i=1}^m \\left [ h(x^{(i)}, \\theta) - y^{(i)} \\right ] x^{(i)}_j\n\\end{align*}\n\\tag{7}\nFirst calculate derivative Of Sigmoid function (it be useful while finding partial derivative Of Note that we computed the partial derivative Of the Sigmoid function If We Were to derive , 9) with respect to O_j, we would get —\nNote that used the chain rule there. We multiply by the derivative Of with respect to Now we are ready to find out resulting partial derivative\nThe Vectorized Version:\n\n\\nabla J(\\theta) = \\frac{1}{m} X^T \\cdot (H(X,\\theta)-Y)\n\\tag{8}\nCongratulations. we now know the full derivation Of logistic regression.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w1/index.html#resources",
    "href": "notes/c1w1/index.html#resources",
    "title": "Sentiment Analysis with Logistic Regression",
    "section": "Resources:",
    "text": "Resources:\n\nDerivative of cost function for Logistic Regression as explained on Math Stack Exchange\nAn Intuitive Explanation of Bayes’ Theorem on Better Explained\n(Chadha 2020) Aman Chadha’s Notes\nIbrahim Jelliti’s Notes",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "Notes"
    ]
  },
  {
    "objectID": "posts/2021-04-24-summerization/index.html",
    "href": "posts/2021-04-24-summerization/index.html",
    "title": "Automatic Summarization Task",
    "section": "",
    "text": "notes\nThis is one of my blogposts on NLP."
  },
  {
    "objectID": "posts/2021-04-24-summerization/index.html#motivation---building-a-good-summarizer.",
    "href": "posts/2021-04-24-summerization/index.html#motivation---building-a-good-summarizer.",
    "title": "Automatic Summarization Task",
    "section": "Motivation - Building a good summarizer.",
    "text": "Motivation - Building a good summarizer.\nIn the Deeplearning.ai NLP specialization, we implemented both Q&A and Summarization tasks. However, we only looked at these tasks in terms of the attention models. It is now time to dive deeper into the summarization task. In this article I will review a talk covering a body of research on summarization, which has many ideas about features. Consider some of these and see what aspects are relevant to a modern task implementation.\nWhen I looked for more information, I found the following video, which, together with a review paper, can provide a good intro to this subject. I also found links to the papers mentioned and extracted some of their abstracts.\nLooking at all the algorithms critically, I found some new ideas for tackling problems beyond what I had come up with on my own."
  },
  {
    "objectID": "posts/2021-04-24-summerization/index.html#automatic-text-summarization-task",
    "href": "posts/2021-04-24-summerization/index.html#automatic-text-summarization-task",
    "title": "Automatic Summarization Task",
    "section": "Automatic Text Summarization Task",
    "text": "Automatic Text Summarization Task\n\n\n\n\n\n\nTL;DR The summarization Task\n\n\n\n\n\n\nSummarization in a nutshell\n\n\nThis is a review of the Automatic Text Summarization Task by Masa Nekic. The talk provides a starter ontology, a review of algorithms, some evaluation methods, and some tools.\n\n\n\n\n\n\n\n\n\nVideo 1: Masa Nekic’s NDC Conferences talk on the Automatic Text Summarization Task\n\n\nNotes from the following lecture by Masa Nekic given at NDC Conferences.\nThe talk provides:\n\na starter ontology.\na review of algorithms.\nsome evaluation methods\nsome tools."
  },
  {
    "objectID": "posts/2021-04-24-summerization/index.html#ontological-mindmap",
    "href": "posts/2021-04-24-summerization/index.html#ontological-mindmap",
    "title": "Automatic Summarization Task",
    "section": "Ontological Mindmap",
    "text": "Ontological Mindmap\n\n\n\n\n\nmindmap\n  Root((Summarization&lt;br&gt;Task))\n    id1[Input Based]\n        id11(Single document) \n        id12(Multi document)\n    id2[Contextal]\n        id21[Generic]\n        id22(Domain Specific)\n        id23(Query)\n           id231{{from IR}}\n    id3[Output Based]\n        id31(Extractive)\n          id311{{Picks sentences from the text}}\n        id32(Abstractive)\n          id321{{Generates from scratch}}\n\n\n\n\n\n\nNote: the Query based approach intersects with the NLP QA task."
  },
  {
    "objectID": "posts/2021-04-24-summerization/index.html#extractive-vs.-abstractive",
    "href": "posts/2021-04-24-summerization/index.html#extractive-vs.-abstractive",
    "title": "Automatic Summarization Task",
    "section": "Extractive vs. Abstractive",
    "text": "Extractive vs. Abstractive\nThe “Summarizing before exams” meme demonstrates the extractive approach. The “abridged classics” meme demonstrates the abstractive approach.\n\n\nExtractive Summaries Illustrated\nExtractive algorithms locate and rank the content of a document.\n\nGame of Thrones is an American fantasy drama television series created by David Benioff and D. B. Weiss for HBO. It is an adaptation of A Song of Ice and Fire, George R. R. Martin’s series of fantasy novels, the first of which is A Game of Thrones. The show was both produced and filmed in Belfast and elsewhere in the United Kingdom. Filming locations also included Canada, Croatia, Iceland, Malta, Morocco, and Spain. The series premiered on HBO in the United States on April 17, 2011, and concluded on May 19, 2019, with 73 episodes broadcast over eight seasons.\nSet on the fictional continents of Westeros and Essos, Game of Thrones has several plots and a large ensemble cast and follows several story arcs. One arc is about the Iron Throne of the Seven Kingdoms and follows a web of alliances and conflicts among the noble dynasties either vying to claim the throne or fighting for independence from it. Another focuses on the last descendant of the realm’s deposed ruling dynasty, who has been exiled and is plotting a return to the throne, while another story arc follows the Night’s Watch, a brotherhood defending the realm against the fierce peoples and legendary creatures of the North.\n\nThe Extractive Summary:\n\nGame of Thrones is an American fantasy drama television series created by David Benioff and D. B. Weiss for HBO. It is an adaptation of A Song of Ice and Fire, George R. R. Martin’s series of fantasy novels\nSet on the fictional continents of Westeros and Essos, Game of Thrones has several plots and a large ensemble cast and follows several story arcs\n\nExtractive Summaries draw text verbatim from the source.\n\nThis was the more common approach in NLP\nit is closely related to IR and Q&A task.\nTheir main challenges of this approach are:\n\na lack balance, when some parts over represented while others under represented.\na lack of cohesion, as extracted text retains dangling pronouns etc."
  },
  {
    "objectID": "posts/2021-04-24-summerization/index.html#abstractive-summaries-illustrated",
    "href": "posts/2021-04-24-summerization/index.html#abstractive-summaries-illustrated",
    "title": "Automatic Summarization Task",
    "section": "Abstractive Summaries Illustrated",
    "text": "Abstractive Summaries Illustrated\nAbstractive algorithms add generation of the extracted content.\n\nGame of Thrones is an American fantasy drama television series created by David Benioff and D. B. Weiss for HBO. It is an adaptation of A Song of Ice and fire, George R. R. Martin’s series of fantasy novels, the first of which is A Game of Thrones. The show was both produced and filmed in Belfast and elsewhere in the United Kingdom. Filming locations also included Canada, Croatia, Iceland, Malta, Morocco, and Spain, The series premiered on HBO in the United States on April 17, 2011, and concluded on May 19, 2019, with 73 episodes broadcast over eight seasons.\nSet on the fictional continents of Westeros and Essos, Game of Thrones has several plots and a large ensemble cast and follows several story arcs. One arc is about the Iron Throne of the Seven Kingdoms and follows a web of alliances and conflicts among the noble dynasties either vying to claim the throne or fighting for independence from it. Another focuses on the last descendant of the realm’s deposed ruling dynasty, who has been exiled and is plotting a return to the throne, while another story arc follows the Night’s Watch, a brotherhood defending the realm against the fierce peoples and legendary creatures of the North.\n\nSummary (Abstractive):\n\nGame of Thrones is a TV show based on book series A Song of Ice and Fire, written by G. R. R. Martin. All eight seasons were filmed in many beautiful countries across three different continents. Game of Thrones has a very complex story with several plots and story arcs — from conflicts between Westeros nobility to claim the Iron Throne and rule over Seven Kingdoms to fight between brotherhood called Night’s watch and enemies from the North.\n\n\nAbstractive Summaries are not constrained to using text drawn the source. They can draw on common-sense and domain knowledge external to the document.\nThis is the more challenging approach in NLP\nTheir main issues are:\n\ngood coverage.\navoiding repetition.\ncan provide better compression."
  },
  {
    "objectID": "posts/2021-04-24-summerization/index.html#positional-method",
    "href": "posts/2021-04-24-summerization/index.html#positional-method",
    "title": "Automatic Summarization Task",
    "section": "Positional method",
    "text": "Positional method\n\nIntroduced in (Baxendale 1958)\n200 paragraphs\nFirst and last sentence of a paragraph are topic sentences (85% vs 7%)\n\ne.g.\n\nGame of Thrones is an American fantasy drama television series created by David Benioff and D. B. Weiss for HBO. It is an adaptation of A Song of Ice and Fire, George R. R. Martinis series of fantasy novels, the first of which is A Game of Thrones. The show was both produced and filmed in Belfast and elsewhere in the United Kingdom. Filming locations also included Canada, Croatia, Iceland, Malta, Morocco, and Spain. The series premiered on HBO in the United States on April 17, 2011, and concluded on May 19, 2019, with 73 episodes broadcast over eight seasons.\n\n\n\n\n\n\n\nMy insights:\n\n\n\nIf we want to build a summerier, first and last sentences may be useful features. Note that in academic writing the penultimate sentence may often the most important.\nSo a simple extractive method might pick one of the sentences from each paragraph. It could have a prior that like the first last sentence of a paragraph. But it would need more features to break ties.\n\n\n\n\n\n\n\ns8-luhn-method-1958"
  },
  {
    "objectID": "posts/2021-04-24-summerization/index.html#luhns-method",
    "href": "posts/2021-04-24-summerization/index.html#luhns-method",
    "title": "Automatic Summarization Task",
    "section": "Luhn’s method",
    "text": "Luhn’s method\n\nIntroduced in (Luhn 1958)\nFrequency of content terms\nData pre-processing\n\nStop words removal\nStemming (cats cat)\n\n\n\n\n\n\ns9-luhn-method-formula\n\nSelect sentences with highest concentrations of salient content terms\n Score = \\frac{\\text{Salient Words}^2}{  \\text{Terms in chunk} }\n\n\n\n\n\ns10-edmundson-method"
  },
  {
    "objectID": "posts/2021-04-24-summerization/index.html#edmundsons-method",
    "href": "posts/2021-04-24-summerization/index.html#edmundsons-method",
    "title": "Automatic Summarization Task",
    "section": "Edmundson’s method",
    "text": "Edmundson’s method\nIntroduced in (Edmundson 1969)\n\nPosition (P)\nWord frequency (F)\nCue words (C)\n\nBonus words — pointing to the important sentence\nStigma words — negative effect on the sentence importance\nNull words — neutral or irrelevant to the importance of the sentence\n\nDocument structure (S)\n\nLinear combination of these 4 features:\n\nscore = \\alpha_1 P + \\alpha_2 F + \\alpha_3 C + \\alpha_4 S \\qquad\n\n\nThis paper describes new methods of automatically extracting documents for screening purposes, i.e. the computer selection of sentences having the greatest potential for conveying to the reader the substance of the document. While previous work has focused on one component of sentence significance, namely, the presence of high-frequency content words (key words), the methods described here also treat three additional components: pragmatic words (cue words); title and heading words; and structural indicators (sentence location). The research has resulted in an operating system and a research methodology. The extracting system is parameterized to control and vary the influence of the above four components. The research methodology includes procedures for the compilation of the required dictionaries, the setting of the control parameters, and the comparative evaluation of the automatic extracts with manually produced extracts. The results indicate that the three newly proposed components dominate the frequency component in the production of better extracts. – (Edmundson 1969)\n\nIt may not be clear from the abstract that the documents were pre-processed manually. And that the outcome was used for screening purposes. Much of the work was done with punch cards and the system was run on a mainframe.\nEdmundson points out some issues that may be relevant to modern summarization tasks:\n\nIf an extracted sentence is an anaphora, it may not be useful as the reader will be less likely to be understood without its antecedent.\n\n\nIn the composition of maximally coherent and meaningful target extracts, it was noticed that requirements of antecedents, deletions of text due to preediting, minimization of redundancy, and restrictions on the length parameter\n\n\nRanking of sentences is of secondary importance to preprocessing and extraction. Also picking the length of the summary can have a big impact on the quality of the summary. The length parameter decided how many of the top ranked sentences are to be included in the summary.\nThey used a dictionary for the Corpus and a Document dictionary called a glossary. Cues words were drawn from a a Corpus level dictionary of words indicating relevance.\n\n\n\n\n\n\n\nMy insights:\n\n\n\n\nWhile generative summaries have many advantages over extractive ones they may hallucinate for example if the model has not been trained on the vocabulary of the text.\nIt is thus best to ensure that the generation is well grounded in the text.\nOn the other hand, extractive summaries have their short comings too.\nWe may want to add a sentence level feature to classify sentences as anaphoric or not. Since the dependency may be on a sentence of sentences extraction may fail.\n\nEdmundson point out that these anaphoric sentences are often marked by certain words or phrases. We might do better identifying anaphoric sentences by looking at the dependency tree of the sentence.\nWe can consider co-reference resolution as a pre-processing step.\n\nCue words are also an interesting feature but a vector space model may yield a better indication of the importance of a sentence. It seems though that the paper did collect information to estimate td/idf scores for each word.\n\nAnother interesting idea is that once we are able to evaluate the words in the document by weights using them to picking the top ranked sentences is probably a bad idea as many may well be redundant, particularly since all the titles and headings are included in the summary. A better approach could be to pick the sentences that are a solution to a knapsack problem where we want to pick sentences with the greatest value in unique cue words. This should allow for a more balanced summary.\nIf we are not using cue words by TD/IDF or a similar information theoretic weighting scheme based on entropy, we may eavluate the knapsack using mutual information between the sentences and the document. If we have a distributional method we could use the KL divergence between the distribution of words and phrases in the document and the distribution of words in the knapsack. This would allow for a more balanced summary."
  },
  {
    "objectID": "posts/2021-04-24-summerization/index.html#frump---fast-reading-understanding-and-memory-program",
    "href": "posts/2021-04-24-summerization/index.html#frump---fast-reading-understanding-and-memory-program",
    "title": "Automatic Summarization Task",
    "section": "FRUMP - Fast Reading Understanding and Memory Program",
    "text": "FRUMP - Fast Reading Understanding and Memory Program\n\n\n\n\ns12-FRUMP-demo\n\n\nIntroduced in (DeJong 1979)\nknowledge-based summarization system.\nTemplate filling approach based on UPI news stories.\nFirst abstractive method.\n50 sketchy scripts\n\nContain important events that are expected to occur in a specific situation\nSummarizer looks for instances of salient events, filling in as many as possible.\n\nIssues - 50 scripts were not enough.\n\n\nThis paper describes a new approach to natural language processing which results in a very robust and efficient system. The approach taken is to integrate the parser with the rest of the system. This enables the parser to benefit from predictions that the rest of the system makes in the course of its processing. These predictions can be invaluable as guides to the parser in such difficult problem areas as resolving referents and selecting meanings of ambiguous words. A program, called FRUMP for Fast Reading Understanding and Memory Program, employs this approach to parsing. FRUMP skims articles rather than reading them for detail. The program works on the relatively unconstrained domain of news articles. It routinely understands stories it has never before seen. The program’s success is largely due to its radically different approach to parsing.\n\n\n\n\n\n\n\nMy insights:\n\n\n\nThis approach has two interesting ideas.\n\nKR using templates or frames.\nKR using scripts is even more powerful method.\n\n\nA modern take on this might involve using a classifier to identify sentences as\n\nFacts\n\ngeneral knowledge (simple)\ndomain knowledge (complex or technical)\n\nOpinions\n\ngeneral knowledge (similar to many documents)\ndomain expert. (similar to a few)\n\nEvents (narrative structure)\nDeductive (logic, inference, statistical, syllogism)\nOthers\n\nUsing a generative approach would allow a deep model to generate its own KR features and templates. An adversarial approach might split this into two nets one to generate and another to test.\nAnalyzing existing summaries and clustering them might allow one to begin summarize using a preferred template rather than starting from scratch. Clustering, deleting and generalizing from existing summaries may be a means for improving abstractive work.\nPutting a focus on the added value of\n\nout of document facts and vocabulary\nhow humans/abstractive summaries differ from extractive ones."
  },
  {
    "objectID": "posts/2021-04-24-summerization/index.html#naive-bayes-classification",
    "href": "posts/2021-04-24-summerization/index.html#naive-bayes-classification",
    "title": "Automatic Summarization Task",
    "section": "Naive Bayes Classification",
    "text": "Naive Bayes Classification\n\nIntroduced in (Kupiec, Pedersen, and Chen 1995)\nFirst trainable method\nTraining set: original documents and manually created extracts\nUsed Naive Bayes classifier:\n\n P (s \\in S \\vert F_1 ... F_k) = \\frac{P (F_1 ... F_k \\vert s \\in S ) P(s \\in S )} {P (F_1 ... F_k)}  \n\nBy assuming statistical independence of the features it reduces to:\n\n  P (s \\in S \\vert F_1 ... F_k)  = \\frac{ \\displaystyle \\prod_{j \\in J} P (F_j \\vert s \\in S ) P(s \\in S )} { \\displaystyle \\prod_{j \\in J} P (F_i)} \n\nPerformance:\n\nFor 25% extracts - 84% precision\nFor smaller summaries - 74% improvement over lead summaries"
  },
  {
    "objectID": "posts/2021-04-24-summerization/index.html#maximum-entropy-classification",
    "href": "posts/2021-04-24-summerization/index.html#maximum-entropy-classification",
    "title": "Automatic Summarization Task",
    "section": "Maximum Entropy Classification",
    "text": "Maximum Entropy Classification\n\nIntroduced in (Osborne 2002)\n\nMaximum entropy models are performing better than Naive Bayes approach"
  },
  {
    "objectID": "posts/2021-04-24-summerization/index.html#mmr",
    "href": "posts/2021-04-24-summerization/index.html#mmr",
    "title": "Automatic Summarization Task",
    "section": "MMR",
    "text": "MMR\n\nIntroduced in (Carbonell and Goldstein-Stewart 1998)\nMaximal Marginal Relevance\nQuery based summaries.\n\n\n\\text{MMR} = \\arg \\max[\\lambda Sim_1(s_i,Q)-(1-\\lambda) \\max Sim_2(s_i, s_j)]\n\nWhere:\n\nQ - user query\nR - ranked list of sentences\nS - already retrieved sentences\nSim - similarity metrics\n\\lambda - hyper-parameter controlling importance of query or other sentence.\n\n\nThis paper presents a method for combining query-relevance with information-novelty in the context of text retrieval and summarization. The Maximal Marginal Relevance (MMR) criterion strives to reduce redundancy while maintaining query relevance in re-ranking retrieved documents and in selecting appropriate passages for text summarization. Preliminary results indicate some benefits for MMR diversity ranking in document retrieval and in single document summarization… the clearest advantage is demonstrated in constructing non-redundant multi-document summaries, where MMR results are clearly superior to non-MMR passage selection. - The Use of MMR (abstract)\n\n\n\n\n\n\n\n\nMy insights\n\n\n\n\nMMR seems to have a binomial formulation.\nBy avoiding to pin down the metric it is possible to use embedding similarity with this formulation.\nMMR offers a formal metric for measuring added value (utility) For Sentences in a summary.\nIt can work with or without a query.\nIt could be adapted as a regularization term in a summarizer loss function.\nIt could be used on a summary to weigh each sentence’s utility.\nIf one were able to generate multiple candidates for a factum MMR could be used to easily rank them.\n\n\n\n\n\n\n\ns16-Mead-Centroid"
  },
  {
    "objectID": "posts/2021-04-24-summerization/index.html#mead",
    "href": "posts/2021-04-24-summerization/index.html#mead",
    "title": "Automatic Summarization Task",
    "section": "Mead",
    "text": "Mead\n\nIntroduced in (Radev, Jing, and Budzikowska 2000)\nCentroid-based method\nSingle and multi document\n\n\nWe present a multi-document summarizer, called MEAD, which generates summaries using cluster centroids produced by a topic detection and tracking system. We also describe two new techniques, based on sentence utility and subsumption, which we have applied to the evaluation of both single and multiple document summaries. Finally, we describe two user studies that test our models of multi-document summarization. - Centroid-based summarization of multiple documents (abstract)\n\n\n\n\n\n\n\nMy insights:\n\n\n\nClustering has its benefits:\n\nEach centroid corresponds a candidate topic.\nCluster size establishes a natural hierarchy for ranking topics.\nCluster centrality provides the a hierarchy for ranking sentence within topics.\nThe centroids may be used in a generative context, to bootstrap attention to each topic !?\nA query similarity can used with the centroids to rank in response to a query (for Q&A)\n\n\n\n\n\n\n\n\nLexrank"
  },
  {
    "objectID": "posts/2021-04-24-summerization/index.html#lexrank",
    "href": "posts/2021-04-24-summerization/index.html#lexrank",
    "title": "Automatic Summarization Task",
    "section": "LexRank",
    "text": "LexRank\n\nIntroduced in (Erkan and Radev 2004) 1(https://www.cs.cmu.edu/afs/cs/project/jair/pub/volume22/erkan04a-html/erkan04a.html)\nGraph based method.\nLexical centrality.\n\n1 page\n\n\n\n\nlexrank rank\n\n\nWe introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents.\n\n\n\n\n\n\nlexrank graph\n\nIdea:\nsimilar to page rank where pages vote for each other:\n\nCreate an adjacency matrix using cosine similarity.\nRepresenting sentences as nodes in the graph\nConnecting nodes based on inter-sentence cosine similarity matrix\nuses eigenvector centrality from this matrix.\nthe sentence with the highest rank would be linked to many other important sentences. Are they very similar or not ?\na threshold is used to determine how many connected components should are used.\n\n\n\n\n\n\n\nMy insights\n\n\n\n\nAlgorithmically lexrank is a more sophisticated way of clustering like the MEAD algorithm. According to the paper, lexrank performed better.\nGraph algorithms are computationally expensive for large graphs. This could mean that the approach would not scale.\nTo build the matrix they used a cosine similarity - but on using words. Replacing words with their embeddings should yield even better results with lower costs.\nThere are a number of centrality measures on graphs. A high eigenvector score means that a node is connected to many nodes who themselves have high scores. The paper looked at Degree, LexRank with threshold, and continuous LexRank. This is clearly a place where one may be able to do better.\nTfiDf is another way to rank concepts.\na problem is that the underlying assumptions for creating the graphical models are difficult to justify. Building a graph from web pages using links seems natural while constructing a graph using similarity between sentences perhaps in different documents seems contrived. Sentences may capture several concepts and arguments may span several sentences. Similar sentences may have very different meaning and different sentences may have the same meaning.\n\n\n\n\n\n\n\n\nseq2seq"
  },
  {
    "objectID": "posts/2021-04-24-summerization/index.html#what-makes-a-good-summary",
    "href": "posts/2021-04-24-summerization/index.html#what-makes-a-good-summary",
    "title": "Automatic Summarization Task",
    "section": "What makes a good summary?",
    "text": "What makes a good summary?\n\nGoals:\n\nOptimize topic coverage\nOptimize readability\n\nEvaluation criteria:\n\nSalience\nLength\nStructure and coherence\nBalance\nGrammar\nNon-redundancy\n\nTypes of evaluation methods\n\nExtrinsic techniques\n\nTask based\nCan a person make the same decision with summary as with the entire document?\n\nIntrinsic techniques\n\nComparing summaries against gold standards\n\n\n\n\n\n\n\nPrecision & Recall"
  },
  {
    "objectID": "posts/2021-04-24-summerization/index.html#precision-and-recall",
    "href": "posts/2021-04-24-summerization/index.html#precision-and-recall",
    "title": "Automatic Summarization Task",
    "section": "Precision and Recall",
    "text": "Precision and Recall\nstarting with a contingency matrix we can get to:\n\nPrecision =\\frac{True_+}{ False_+ + True_+} \\qquad\n {eq-precision}\n\nRecall = \\frac{True_+}{True_+ + False_-} \\qquad\n {eq-recall}\nthese can also be combined into an f-score is a harmonic mean of precision and recall.\n\n\n\n\n\n\nMy insights\n\n\n\nPrecision and Recall make more sense for IR settings, i.e. when we have a query.\n\n\n\n\n\n\n\ns24-utility"
  },
  {
    "objectID": "posts/2021-04-24-summerization/index.html#utility",
    "href": "posts/2021-04-24-summerization/index.html#utility",
    "title": "Automatic Summarization Task",
    "section": "Utility",
    "text": "Utility\n\nUtility is interesting from economic or game theoretic perspective. It indicates an option of applying RL\nUtility is usually translated as a loss function in ML!\n\n\n\n\n\n\ns25-pyramid"
  },
  {
    "objectID": "posts/2021-04-24-summerization/index.html#pyramid-method",
    "href": "posts/2021-04-24-summerization/index.html#pyramid-method",
    "title": "Automatic Summarization Task",
    "section": "Pyramid method",
    "text": "Pyramid method\n\nBased on semantic content units\nUsed for multi-document summarization\n\n\n\n\n\n\ns25-rougue"
  },
  {
    "objectID": "posts/2021-04-24-summerization/index.html#rouge-n",
    "href": "posts/2021-04-24-summerization/index.html#rouge-n",
    "title": "Automatic Summarization Task",
    "section": "ROUGE-N",
    "text": "ROUGE-N\n\nBased on Bleu (used for MT)\nR stands for Recall (Recall-Oriented Understudy for Gisting Evaluation)\nROUGE-N metric compares an automatic summary with a set of reference summaries using the n-gram overlap between the documents\n\n\nROUGE_N - = \\frac{\\sum_{s\\in S_H} \\sum_{g_n \\in S}C_m(g_n)}\n                      {\\sum_{s\\in S_H} \\sum_{g_n \\in S}C(g_n) } \\qquad\n\\tag{1}\n\nS_H is a set of manual summaries\nS is an individual manual summary\ng_n is a N-gram\nC(g_n) is number of occurrences of gn in reference summaries\nC_m(g_n) is number of co-occurrences of g_n in both reference and automatic summary"
  },
  {
    "objectID": "reviews/paper/2018-BERT/index.html",
    "href": "reviews/paper/2018-BERT/index.html",
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "section": "",
    "text": "Litrature review\n\n\n\n\n\n\n\nPre Training v.s. Fine Tuning\n\n\n\n\nFigure 1: Overall pre-training and fine-tuning procedures for BERT. Apart from output layers, the same architectures are used in both pre-training and fine-tuning. The same pre-trained model parameters are used to initialize models for different down-stream tasks. During fine-tuning, all parameters are ine-tuned. [CLS] is a special symbol added in front of every input example, and [SEP] is a special separator token (e.g. separating questions/answers)\n\n\n\n\n\n\n\n\nPre Training v.s. Fine Tuning\n\n\n\n\nFigure 2: BERT input representation. The input embeddings are the sum of the token embeddings, the segmentation embeddings and the position embeddings.\n\n\n\n\n\n\n\n\nPre Training v.s. Fine Tuning\n\n\n\n\nFigure 3: Differences in pre-training model architectures. BERT uses a bidirectional Transformer. OpenAI GPT uses a left-to-right Transformer. ELMo uses the concatenation of independently trained left-to-right and right-toleft LSTMs to generate features for downstream tasks. Among the three, only BERT representations are jointly conditioned on both left and right context in all layers. In addition to the architecture differences, BERT and OpenAI GPT are fine-tuning approaches, while ELMo is a feature-based approach.\n\n\n\n\n\n\n\n\nPre Training v.s. Fine Tuning\n\n\n\n\nFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\n\n\n\n\n\n\n\n\nPre Training v.s. Fine Tuning\n\n\n\n\nFigure 5: Ablation over number of training steps. This shows the MNLI accuracy after fine-tuning, starting from model parameters that have been pre-trained for k steps. The x-axis is the value of k.\nBERT is one of the Transformer papers we discussed in the NLP with Attention models. In fact it was the main example of how the attention mechanism can be used to build a powerful model for NLP tasks. But here is a deeper look at the paper and at the model than we could do in the course."
  },
  {
    "objectID": "reviews/paper/2018-BERT/index.html#podcast",
    "href": "reviews/paper/2018-BERT/index.html#podcast",
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "section": "Podcast",
    "text": "Podcast"
  },
  {
    "objectID": "reviews/paper/2018-BERT/index.html#abstract",
    "href": "reviews/paper/2018-BERT/index.html#abstract",
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "section": "Abstract",
    "text": "Abstract\n\nWe introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement). – (Devlin et al. 2019)"
  },
  {
    "objectID": "reviews/paper/2018-BERT/index.html#outline",
    "href": "reviews/paper/2018-BERT/index.html#outline",
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "section": "Outline",
    "text": "Outline\n\nIntroduction\n\nIntroduce the concept of language model pre-training and its effectiveness in improving NLP tasks.\nDiscuss the two main approaches to applying pre-trained language representations: feature-based and fine-tuning.\nHighlight the limitations of current techniques, particularly the unidirectionality of standard language models.\nIntroduce BERT as a solution that uses bidirectional representations through a masked language model (MLM) pre-training objective.\nMention the use of a “next sentence prediction” (NSP) task for jointly pre-training text-pair representations.\nState the contributions of the paper:\n\nImportance of bidirectional pre-training.\nReduction in need for task-specific architectures.\nAdvancement of state-of-the-art for eleven NLP tasks.\n\n\nRelated Work\n\nBriefly review the history of pre-training general language representations, including non-neural and neural methods.\nDiscuss pre-trained word embeddings and their improvements over embeddings learned from scratch.\nMention generalization to coarser granularities such as sentence and paragraph embeddings.\nExplain the feature-based approach of ELMo, which extracts context-sensitive features from left-to-right and right-to-left language models.\nDescribe unsupervised fine-tuning approaches that pre-train contextual token representations.\nMention transfer learning from supervised tasks with large datasets.\n\nBERT Model\n\nDescribe the two-step framework of BERT: pre-training and fine-tuning.\nExplain that the model is trained on unlabeled data during pre-training and fine-tuned using labeled data from downstream tasks.\nHighlight the unified architecture of BERT across different tasks.\nDetail BERT’s model architecture as a multi-layer bidirectional Transformer encoder.\nMention the two model sizes: BERTBASE and BERTLARGE, including their number of layers, hidden size, and attention heads.\nExplain that BERT uses bidirectional self-attention, unlike the constrained self-attention of OpenAI GPT.\nDescribe how the input representation can represent both single sentences and sentence pairs.\nDetail the use of WordPiece embeddings and special tokens like [CLS] and [SEP].\nExplain how input representation is constructed by summing token, segment, and position embeddings.\n\nBERT Pre-training\n\nExplain that BERT does not use traditional left-to-right or right-to-left language models for pre-training.\nDescribe the first unsupervised task: Masked LM (MLM), where some input tokens are randomly masked and the model predicts the original word.\nExplain the masking procedure: 80% [MASK], 10% random token, 10% unchanged token.\nDescribe the second unsupervised task: Next Sentence Prediction (NSP), where the model predicts if sentence B is the actual next sentence following A.\nMention the use of BooksCorpus and English Wikipedia as the pre-training corpus.\n\nBERT Fine-tuning\n\nExplain the straightforward fine-tuning process due to the self-attention mechanism.\nDescribe how task-specific inputs and outputs are plugged into BERT for fine-tuning.\nMention that fine-tuning is relatively inexpensive.\n\nExperiments\n\nPresent the results of fine-tuning BERT on 11 NLP tasks.\nDiscuss the results on the GLUE benchmark and the substantial improvements over prior state-of-the-art models.\n\nDetail the fine-tuning procedure including batch size and epochs.\nHighlight the performance of both BERTBASE and BERTLARGE.\n\nPresent results on the SQuAD v1.1 question answering task, showing how the input question and passage are represented.\n\nDiscuss the addition of start and end vectors during fine tuning.\nShow the improvement of BERT on this task compared to other systems.\n\nPresent the SQuAD v2.0 results, including how the model handles unanswerable questions.\nDiscuss the results on the SWAG dataset for grounded commonsense inference.\n\nExplain how the input is structured for this task.\n\n\nAblation Studies\n\nDiscuss the importance of the deep bidirectionality of BERT and evaluate the impact of the pre-training objectives.\n\nPresent results comparing BERT to models trained without NSP or with a left-to-right model.\n\nExplore the effect of model size on fine-tuning accuracy.\n\nShow how larger models lead to accuracy improvements.\n\nCompare fine-tuning with a feature-based approach using the CoNLL-2003 Named Entity Recognition (NER) task.\n\nDetail the use of contextual embeddings as input to a BiLSTM.\nShow the effectiveness of both approaches with BERT.\n\n\nConclusion\n\nSummarize the key findings of the paper, emphasizing the importance of rich, unsupervised pre-training for language understanding.\nHighlight the major contribution of generalizing these findings to deep bidirectional architectures.\nReiterate that the same pre-trained model can tackle a broad set of NLP tasks.\n\n\nThis outline covers the main points of the BERT paper and provides a structure you can use for your paper. Let me know if you’d like any modifications or more details on specific sections!"
  },
  {
    "objectID": "reviews/paper/2018-BERT/index.html#the-paper",
    "href": "reviews/paper/2018-BERT/index.html#the-paper",
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "section": "The Paper",
    "text": "The Paper\n\n\n\npaper"
  },
  {
    "objectID": "reviews/paper/2017-attention-is-all-you-need/index.html#attention-is-all-you-need",
    "href": "reviews/paper/2017-attention-is-all-you-need/index.html#attention-is-all-you-need",
    "title": "Attention Is All You Need",
    "section": "Attention Is All You Need",
    "text": "Attention Is All You Need\nAttention Is All You Need is a paper by Vaswani et al. (2023). It is a seminal paper that introduced the transformer architecture, which has since become the backbone of many state-of-the-art models in natural language processing (NLP). The transformer architecture is known for its parallelism, scalability, and ability to capture long-range dependencies in sequences. The key innovation in the transformer is the self-attention mechanism, which allows the model to weigh the importance of different parts of the input sequence when making predictions. This mechanism has proven to be highly effective in capturing complex patterns in language data and has led to significant improvements in a wide range of NLP tasks.",
    "crumbs": [
      "Home",
      "Papers",
      "Attention is all you need"
    ]
  },
  {
    "objectID": "reviews/paper/2016-coverage-embedding-models-for-NMT/index.html",
    "href": "reviews/paper/2016-coverage-embedding-models-for-NMT/index.html",
    "title": "Coverage Embedding Models for Neural Machine Translation",
    "section": "",
    "text": "Literature review",
    "crumbs": [
      "Home",
      "Papers",
      "Coverage Embedding Models for NMT"
    ]
  },
  {
    "objectID": "reviews/paper/2016-coverage-embedding-models-for-NMT/index.html#motivation",
    "href": "reviews/paper/2016-coverage-embedding-models-for-NMT/index.html#motivation",
    "title": "Coverage Embedding Models for Neural Machine Translation",
    "section": "Motivation",
    "text": "Motivation\n\nis discussed in week 6 the Multilingual NLP course. Neural generative models tend to drop or repeat content. But for NMT we can assume that all the inputs should be represented in the output. For each uncovered word it imposes a penalty on the attention model.\n\n\n\n\n\n\n\nTL;DR - Coverage Embedding\n\n\n\n\n\n\nCoverage Embedding in a Nutshell\n\n\n\nThe paper introduces coverage embedding models to address the issues of repeating and dropping translations in NMT.\nThe coverage embedding vectors are updated at each time step to track the coverage status of source words.\nThe coverage embedding models significantly improve translation quality over a large vocabulary NMT system.\nThe best model uses a combination of updating with a GRU and updating as a subtraction.\nThe coverage embedding models also reduce the number of repeated phrases in the output.",
    "crumbs": [
      "Home",
      "Papers",
      "Coverage Embedding Models for NMT"
    ]
  },
  {
    "objectID": "reviews/paper/2016-coverage-embedding-models-for-NMT/index.html#podcast",
    "href": "reviews/paper/2016-coverage-embedding-models-for-NMT/index.html#podcast",
    "title": "Coverage Embedding Models for Neural Machine Translation",
    "section": "Podcast",
    "text": "Podcast",
    "crumbs": [
      "Home",
      "Papers",
      "Coverage Embedding Models for NMT"
    ]
  },
  {
    "objectID": "reviews/paper/2016-coverage-embedding-models-for-NMT/index.html#abstract",
    "href": "reviews/paper/2016-coverage-embedding-models-for-NMT/index.html#abstract",
    "title": "Coverage Embedding Models for Neural Machine Translation",
    "section": "Abstract",
    "text": "Abstract\n\nIn this paper, we enhance the attention-based neural machine translation (NMT) by adding explicit coverage embedding models to alleviate issues of repeating and dropping translations in NMT. For each source word, our model starts with a full coverage embedding vector to track the coverage status, and then keeps updating it with neural networks as the translation goes. Experiments on the large-scale Chinese-to-English task show that our enhanced model improves the translation quality significantly on various test sets over the strong large vocabulary NMT system. – (Mi et al. 2016)",
    "crumbs": [
      "Home",
      "Papers",
      "Coverage Embedding Models for NMT"
    ]
  },
  {
    "objectID": "reviews/paper/2016-coverage-embedding-models-for-NMT/index.html#glossary",
    "href": "reviews/paper/2016-coverage-embedding-models-for-NMT/index.html#glossary",
    "title": "Coverage Embedding Models for Neural Machine Translation",
    "section": "Glossary",
    "text": "Glossary\nThis paper has a number of big words and concepts that are important to understand. Lets break them down together:\n\nNeural Machine Translation (NMT)\n\nA machine translation approach that uses neural networks to learn the mapping between source and target languages.\n\nAttention Mechanism\n\nIn NMT, a mechanism that allows the model to focus on different parts of the source sentence when generating each word in the target sentence.\n\nCoverage Vector\n\nA vector used in statistical machine translation to explicitly track which source words have been translated.\n\nCoverage Embedding Vector\n\nA vector specific to each source word in this model, used to track the translation status of the word. It is initialized with a full embedding and is updated based on attention scores.\n\nGated Recurrent Unit (GRU)\n\nA type of RNN cell used to model sequential data, including language. Here it is used to update coverage embeddings.\n\nAttention Probability (α)\n\nA set of weights that indicate how much attention the model pays to each source word when predicting a target word.\n\nEncoder-Decoder Network\n\nA neural network architecture commonly used in sequence-to-sequence tasks like NMT. The encoder processes the input sequence, and the decoder generates the output sequence.\n\nBi-directional RNN\n\nA RNN that processes a sequence in both forward and backward directions, capturing contextual information from both sides of a word.\n\nSoft Probability\n\nProbabilities in the attention mechanism aren’t hard (0 or 1), but instead are on a continuum, indicating a degree of attention or importance.\n\nFertility\n\nIn the context of translation, fertility refers to the number of words in the target language that can be translated from a single word in the source language.\n\nOne-to-many Translation\n\nA translation where one source word corresponds to multiple words in the target language.\n\nTER (Translation Error Rate)\n\nA metric used to evaluate the quality of machine translation by calculating the number of edits required to match the system’s translation to a reference translation, with lower scores being better.\n\nBLEU (Bilingual Evaluation Understudy)\n\nA metric to evaluate the quality of machine translation by comparing a candidate translation to one or more reference translations, with higher scores being better.\n\nUNK\n\nAbbreviation for “unknown.” In machine translation, it is used to denote words that are not in the model’s vocabulary.\n\nAdaDelta\n\nAn adaptive learning rate optimization algorithm, that adjusts the learning rate during training for faster convergence.\n\nAlignment\n\nIn the context of machine translation, the process of determining which words in the source sentence correspond to words in the target sentence.\n\nF1 Score\n\nA measure of a test’s accuracy and it considers both the precision and recall of the test to compute the score.\n\n\nWith a solid understanding of this terminology we can now dive into the paper.",
    "crumbs": [
      "Home",
      "Papers",
      "Coverage Embedding Models for NMT"
    ]
  },
  {
    "objectID": "reviews/paper/2016-coverage-embedding-models-for-NMT/index.html#outline",
    "href": "reviews/paper/2016-coverage-embedding-models-for-NMT/index.html#outline",
    "title": "Coverage Embedding Models for Neural Machine Translation",
    "section": "Outline",
    "text": "Outline\n\nIntroduction\n\nNotes that in NMT attention mechanisms focus on source words to predict target words.\nPoint out that these models lack history or coverage information, leading to repetition or dropping of words.\nRecalls how Statistical Machine Translation (SMT) used a binary “coverage vector” to track translated words.\nExplains that SMT coverage vectors use 0s and 1s, whereas attention probabilities are soft. SMT systems handle one-to-many fertilities using phrases or hiero rules, while NMT systems predict one word at a time.\nIntroduces coverage embedding vectors, updated at each step, to address these issues.\nExplains that each source word has its own coverage embedding vector that starts as a full embedding vector(as opposed to 0 in SMT).\nStates that coverage embedding vectors are updated based on attention weights, moving toward an empty vector for words that have been translated.\n\nNeural Machine Translation\n\nRecalls that attention-based NMT uses an encoder-decoder architecture.\n\nThe encoder uses a bi-directional RNN to encode the source sentence into hidden states.\nThe decoder predicts the target translation by maximizing the conditional log-probability of the correct translation.\nThe probability of each target word is determined by the previous word and the hidden state.\nThe hidden state is computed using a weighted sum of the encoder states, with weights derived from a two-layer neural network.\n\nIntroduces coverage embedding models into the NMT by adding an input to the attention model.\n\nCoverage Embedding Models\n\nThe model uses a coverage embedding for each source word that is updated at each time step.\nEach source word has its own coverage embedding vector, and the number of coverage embedding vectors is the same as the source vocabulary size.\nThe coverage embedding matrix is initialized with coverage embedding vectors for all source words.\nCoverage embeddings are updated using neural networks (GRU or subtraction).\nAs the translation progresses, coverage embeddings of translated words should approach zero.\nTwo methods are proposed to update the coverage embedding vectors: GRU and subtraction.\n\n\nUpdating Methods\n\nUpdating with a GRU\n\nThe coverage model is updated using a GRU, incorporating the current target word and attention weights.\nThe GRU uses update and reset gates to control the update of the coverage embedding vector.\n\nUpdating as Subtraction\n\nThe coverage embedding is updated by subtracting the embedding of the target word, weighted by the attention probability.\n\n\nObjectives\n\nCoverage embedding models are integrated into attention NMT by adding coverage embedding vectors to the first layer of the attention model.\nThe goal is to remove partial information from the coverage embedding vectors based on the attention probability.\nThe model minimizes the absolute values of the embedding matrix.\nThe model can also use supervised alignments to know when the coverage embedding should be close to zero.\n\n\nRelated Work\n\nTu et al. (2016) also uses a GRU to model the coverage vector. However, this approach differs from the current paper’s method by initializing the word coverage vector with a scalar and adding an accumulation operation and a fertility function.\nCohn et al. (2016) augments the attention model with features from traditional SMT.\n\nExperiments\n\nData Preperation\n\nExperiments were conducted on a Chinese-to-English translation task.\nTwo training sets were used: one with 5 million sentence pairs and another with 11 million.\nThe development set consisted of 4491 sentences.\nTest sets included NIST MT06, MT08 news, and MT08 web.\nFull vocabulary sizes were 300k and 500k, with coverage embedding vector sizes of 100.\nAdaDelta was used to update model parameters with a mini-batch size of 80.\nThe output vocabulary was a subset of the full vocabulary, including the top 2k most frequent words and the top 10 candidates from word-to-word/phrase translation tables.\nThe maximum length of a source phrase was 4.\nA traditional SMT system, a hybrid syntax-based tree-to-string model, was used as a baseline.\nFour different settings for coverage embedding models were tested: updating with a GRU (UGRU), updating as a subtraction (USub), the combination of both methods (UGRU+USub), and UGRU+USub with an additional objective (+Obj).\n\nTranslation Results\n\nThe coverage embedding models improved translation quality significantly over a large vocabulary NMT (LVNMT) baseline system.\nUGRU+USub performed best, achieving a 2.6 point improvement over LVNMT.\nImprovements of coverage models over LVNMT were statistically significant.\nThe UGRU model also improved performance when using a larger training set of 11 million sentences.\n\nAlignment Results\n\nThe best coverage model (UGRU + USub) improved the F1 score by 2.2 points over the baseline NMT system.\nCoverage embedding models reduce the number of repeated phrases in the output.\n\n\nConclusion\n\nThe paper proposed coverage embedding models for attention-based NMT.\nThe models use a coverage embedding vector for each source word and update these vectors as the translation progresses.\nExperiments showed significant improvements over a strong large vocabulary NMT system.",
    "crumbs": [
      "Home",
      "Papers",
      "Coverage Embedding Models for NMT"
    ]
  },
  {
    "objectID": "reviews/paper/2016-coverage-embedding-models-for-NMT/index.html#reflection",
    "href": "reviews/paper/2016-coverage-embedding-models-for-NMT/index.html#reflection",
    "title": "Coverage Embedding Models for Neural Machine Translation",
    "section": "Reflection",
    "text": "Reflection\nThe idea of tracking coverage is very simple. Lets face it many issues in NLP require simple solutions.\nFor instance in the summarization task we have a big headache with the autoregressive tendency to repeat. But it also requires a kind of coverage too, but one that is more spread out. Also in more extreme cases we want to direct the coverage using very specific information like the narative flow of a story. This seems to be an idea that can be further explored in other tasks.",
    "crumbs": [
      "Home",
      "Papers",
      "Coverage Embedding Models for NMT"
    ]
  },
  {
    "objectID": "reviews/paper/2016-coverage-embedding-models-for-NMT/index.html#the-paper",
    "href": "reviews/paper/2016-coverage-embedding-models-for-NMT/index.html#the-paper",
    "title": "Coverage Embedding Models for Neural Machine Translation",
    "section": "The Paper",
    "text": "The Paper\n\n\n\npaper",
    "crumbs": [
      "Home",
      "Papers",
      "Coverage Embedding Models for NMT"
    ]
  },
  {
    "objectID": "notes/cs11-737-w08-contact/index.html",
    "href": "notes/cs11-737-w08-contact/index.html",
    "title": "Language Contact and Change",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\nVideo 1: Lesson Video\n\n\n\n\n\n\n\n\nThis week’s slides\n\n\n\n\nSupplementary Figure 1"
  },
  {
    "objectID": "notes/cs11-737-w06-translation-models/index.html",
    "href": "notes/cs11-737-w06-translation-models/index.html",
    "title": "Translation Models",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\nVideo 1: Lesson Video\n\n\n\n\n\n\n\n\nThis week’s slides\n\n\n\n\nSupplementary Figure 1"
  },
  {
    "objectID": "notes/cs11-737-w07-data-driven-NMT/index.html",
    "href": "notes/cs11-737-w07-data-driven-NMT/index.html",
    "title": "Data-driven Strategies for NMT",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\nVideo 1: Lesson Video\n\n\n\n\n\n\n\n\nVideo 2: Generalized Data Augmentation for Low-Resource Translation (Xia et al. (2019))\n\n\n\n\n\n\n\n\nVideo 3: Google’s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation (Johnson et al. (2017))\n\n\n\n\n\n\n\n\nVideo 4: Rapid Adaptation of Neural Machine Translation to New Languages (Neubig and Hu (2018))\n\n\n\n\n\n\n\n\nThis week’s slides\n\n\n\n\nSupplementary Figure 1"
  },
  {
    "objectID": "notes/cs11-737-w09-training-transfer/index.html",
    "href": "notes/cs11-737-w09-training-transfer/index.html",
    "title": "Translation and Translation Data",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\nVideo 1: Lesson Video\n\n\n\n\n\n\n\n\nThis week’s slides\n\n\n\n\nSupplementary Figure 1\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{bochman2022,\n  author = {Bochman, Oren},\n  title = {Translation and {Translation} {Data}},\n  date = {2022-02-15},\n  url = {https://orenbochman.github.io/notes-nlp/notes/cs11-737-w09-training-transfer/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2022. “Translation and Translation Data.”\nFebruary 15, 2022. https://orenbochman.github.io/notes-nlp/notes/cs11-737-w09-training-transfer/."
  },
  {
    "objectID": "notes/cs11-737-w04-words/index.html",
    "href": "notes/cs11-737-w04-words/index.html",
    "title": "Words, Parts of Speech, Morphology",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\nVideo 1: Lesson Video\n\n\n\n\n\n\n\n\nThis week’s slides\n\n\n\n\nSupplementary Figure 1"
  },
  {
    "objectID": "notes/cs11-737-w04-words/index.html#words",
    "href": "notes/cs11-737-w04-words/index.html#words",
    "title": "Words, Parts of Speech, Morphology",
    "section": "Words",
    "text": "Words\n\nSo now we’re going to move on to the main part where we’re going to talk about words which sort of seems really fairly appropriate for any any nlp class because words are pretty important okay and this is going to give you a little bit more definition of what we mean by words and how that’s not always as well defined as it might be in languages like English so let’s try and count the words in this sentence bob’s handyman is a do-it-yourself kind of guy isn’t he okay so that’s a fairly standard sentence although i have selected it to be particularly interesting for this case but the question is where are the words okay so what i’ve done is i’ve tried to highlight what people might think the words actually are now if you grew up in europe and maybe other places as well you probably think that white space separated tokens is a good approximation for words and it is a good approximation for words but from a linguistic point of view it’s actually a little bit more complex than that and that’s what we’re actually going to talk about and when we actually look at other languages that’s the thing we want to highlight that the notion of what a word boundary actually is might not be as trivial as what you hope it’s going to be okay"
  },
  {
    "objectID": "notes/cs11-737-w04-words/index.html#whitespace",
    "href": "notes/cs11-737-w04-words/index.html#whitespace",
    "title": "Words, Parts of Speech, Morphology",
    "section": "Whitespace",
    "text": "Whitespace\n\nLet’s have a look at the first word of the white speak white space identified token is Bob’s the apostrophe s is what’s called a clitic it’s not isolated word on its own it can’t stand on its own it only appears when it’s actually bound to some other word but in some sense it’s independent of the word that it’s bound to so what you can have is actually it can go into whole noun phrases rather than just single words so we talk about jack and jill’s bucket we’re not talking about Jack and Jill’s bucket we’re talking about Jack and Jill in the bucket belonging to Jack and Jill so that apostrophe s isn’t just going on the immediate token before but a bunch of tokens before that okay so be aware that that apostrophe s is a somewhat special thing in English and sometimes it’s a good idea to actually separate that apostrophe off and treat it as an independent token than actually just putting it together as bob’s let’s look at the next one handyman is really a now known compound okay we put a space in it because we’re not German but but some compounds in English, don’t get a space between them and it’s sort of really quite complex to know which ones are which and it’s really sort of up to the speaker and to decide how to do that but sorry the writer from the speaking point of view it’s even harder to distinguish between these but handyman really here is being used as a single word and it would be useful to keep them together even though they have a ascii white space between them is is a word as a word now let’s have a look at do it yourself it’s hyphenated and it really is a sort of set phrase it can be shortened to diy and we’d like to treat that as a single word but sometimes hyphenated forms should be separated and we have to make interesting decisions about that now we’ve got words like kinda and isn’t it that are fairly standard contractions in English we could write them out as kind space off and we could write isn’t it as is not but often when people are speaking they don’t do that and they actually do reduce form and sometimes when writing they may actually do that simplified form as well and do use these contractions and we have to make some decision about how these actually might appear and whether we want to separate them out many of these are sort of closed class they can only be applied to a number of words and they’re not general to anything but some of them are not apostrophe s can clearly go into anything and apostrophe ll can sort of go into any noun as well so that’s a hard it’s a decision that we actually have to make and it will affect all of our downstream tasks once we decide how to token these tokenize these into what we’re going to term words how we’re going to have word embeddings how we’re actually going to do parsing or whatever our next task is going to be."
  },
  {
    "objectID": "notes/cs11-737-w04-words/index.html#other-languages",
    "href": "notes/cs11-737-w04-words/index.html#other-languages",
    "title": "Words, Parts of Speech, Morphology",
    "section": "Other Languages",
    "text": "Other Languages\n\nIn other languages it can be way more complex and sometimes way more simple. It could be that we actually have no spaces whatsoever and everything is joined together agglutinated and there has to be some process to try to separate these individual parts out if we don’t separate these things out what you’re going to have in a language is an awful lot of words so if you look at turkish for example which is an agglutinate language there’s an awful lot of compounding in it an awful lot of interesting morphology in it what you end up with is the number of words number of white space separated tokens and in the language is much bigger than English for because what we maybe think about as being phrases in English are actually whole single words with no white space between them there also can be ambiguity in those particular words about how we actually decompose them and on the right here we actually have a hebrew example where I’m sorry I can’t read that this where it can mean depending on how you separate that out into individual morphemes and her saturday and that in t and that her daughter all of those are potential meanings given the rest of the context but they’re all written as the same single word"
  },
  {
    "objectID": "notes/cs11-737-w04-words/index.html#linguistics",
    "href": "notes/cs11-737-w04-words/index.html#linguistics",
    "title": "Words, Parts of Speech, Morphology",
    "section": "Linguistics",
    "text": "Linguistics\n\nNow let’s go back to our knowledge of linguistics and see how we might be able to use our knowledge of linguistics to be able to answer these questions about what words actually are.\nBecause we’re going to look at each of those and see how they we might be able to use that as a definition of a useful way of splitting things into little bits words so there could be phonetics so there could be something about the way things are being said that could actually tell us something about that we could say it’s got to have a of every word has to have a vowel in it although I don’t know where apostle vs which sometimes is a violent and sometimes doesn’t sometimes and it’s sometimes just phonology and it could be something about there’s some structure that’s actually required in syllables and so everything has to be at least one syllable if it has to be a word it could be morphology if we’re looking at the actual atoms that are in the morphemes that are in the word we could talk about the individual morphemes in there and make some definition based on that we could talk about syntax we could talk about whether we could exchange that for another word in the same class and treat those as being words if they have classes over them we could talk about semantics of whether it changes meaning and we could talk about pragmatics about realistically new york is used as a single word even though we happen to put a space in it but it’s treated basically as a fundamental a word because although it maybe historically has some relationship to the word new and the word york it really has nothing to do with that anymore let’s have a look at the orthographic"
  },
  {
    "objectID": "notes/cs11-737-w04-words/index.html#orthography",
    "href": "notes/cs11-737-w04-words/index.html#orthography",
    "title": "Words, Parts of Speech, Morphology",
    "section": "Orthography",
    "text": "Orthography\n\nDefinition so this is where white space comes into mind and be aware that even in English white space is quite complex So you know we’ve got spaces we’ve got tabs we’ve got new lines we’ve got carriage returns we’ve got vertical tabs are all within the standard ascii set but beyond that we actually have a lot more if we look at unicode and we want to make definitions of that and we also have things like non-breakable space and a non-breakable space can be used and may appear on on on the web in html and we may or may not want to decide that as being a word boundary or not depending on our definitions okay and remember we’re talking here about orthography we are pretending that the whole world writes everything down and that when they write everything down it’s the same as what speech is most languages are not written we write an awful lot and most people on the planet are actually literate but they may speak languages that they’re not literate in and when you speak you don’t put any spaces between words okay you don’t say a space between each word and we don’t do that at all okay and so when you’re wondering how chinese people can understand chinese text when there’s no species in it think about how well you can understand speech which has got no species in it and yet you can still deal with it and the notion of a word has been around for a lot longer than the notion of writing so in other words we’ve had that notion and we need a definition of words that are actually independent of the written form"
  },
  {
    "objectID": "notes/cs11-737-w04-words/index.html#prosodic",
    "href": "notes/cs11-737-w04-words/index.html#prosodic",
    "title": "Words, Parts of Speech, Morphology",
    "section": "Prosodic",
    "text": "Prosodic\n\nThere can be a prosodic definition a prosodic definition is something to do with international phrases they can be hard to define but they’ll still have some reasonable definition and things which are actually grouped together in the single international phrase are often written as a single word in other languages so for example in the park in English the way I just said it is as a single international phrase in the park and many other languages would have the same concepts of location and determiners and park all in the single word that would have no spaces between it so in the park could be treated as a single word depending on what your definitions are going to be"
  },
  {
    "objectID": "notes/cs11-737-w04-words/index.html#semantic",
    "href": "notes/cs11-737-w04-words/index.html#semantic",
    "title": "Words, Parts of Speech, Morphology",
    "section": "Semantic",
    "text": "Semantic\n\nSemantic definitions these are word units that got some something like the same idea and so it might be useful to be able to treat these in a reasonable way we can think about colors that might have multiple names we can think about navy blue is is a color and but when written it would have probably have a space in it but we may want to treat that as a color which is the same as blue or red or yellow or any other single word color because navy blue is still a color okay and although technically maybe has something to do with glue and something to do with navy it might not be a reasonable thing to actually separate it out we can have a syntactic definition where we’re looking at blocks in these sentences so again new york is a good example here where it really is being used in the same sense as the word pittsburgh is it’s referring to a particular city in north america and just because it’s got a space in it doesn’t mean that we want to treat it as two different cities a new city and a york city which is elsewhere in there’s almost certainly places called york in north america but I can’t. Well there’s a one just outside toronto now this comes to the notion of how can we identify classes of words and I’m currently really talking about syntactic classes of words rather than semantic classes so so colors are sort of a semantic class but I’m interested in what words have got the same class that I could exchange them without what it might change the meaning but you know they get used in the same way now we’ve all been talking talk about these standard open classes which appear in most languages not all languages but most languages nouns verbs and adjectives and adverbs there’s almost an infinite number of those new ones can come along that didn’t exist before but they all fall into these particular classes. This is in contrast with closed classes where there’s a finite set in the language they’re sometimes called function words. It might be a difficult class to list absolutely everything but you rarely very rarely find new things moving into that class so things like prepositions so in above behind there’s a sort of finite set of those determiners the and ah this and that and pronouns I are somewhat finite in English conjunctions and or but not an exclusive or if you’re a computer scientist and other auxiliary verbs like is was have etc and these are sort of closed class they’re very common in the language they’re often short in a language and in most languages have something like them they might be something to do with morphemes but they don’t have random new ones appearing every day so if we think about what happens with the open class think about words which you know exist today that didn’t exist five years ago and think about some can you type them in the chat words which we have today that we didn’t have five years ago and you’ll find out that they’re all nouns verbs adjectives and adverbs and not closed classes because can anybody think of any words which we have now that we didn’t have five years ago covid excellent example I’m sorry to disappoint you, but if you have a look at wikipedia in 2015 there’s an article on coveted okay but it was dull and boring and uninteresting and the covered self-help group decided that they wanted to do something and become more popular but you’re right that covert was incredibly rare okay and probably was only used in occasional circumstances doom scrolling excellent quarantine yeah wfh all of these things are very very common now and sort of didn’t exist at all before and language is like that but all of these words for the most part are coming in to be nouns and verbs okay occasionally they’re going to be adjectives and maybe adverbs but for the most part these do change over time so you can’t list them all but for the close class ones you can sort of do it I mean there’s some really rare conjunctions that people don’t use nowadays like not withstanding okay apart from used as examples of really rare conjunctions that’s about the only time I use that word"
  },
  {
    "objectID": "notes/cs11-737-w04-words/index.html#tag-sets",
    "href": "notes/cs11-737-w04-words/index.html#tag-sets",
    "title": "Words, Parts of Speech, Morphology",
    "section": "Tag Sets",
    "text": "Tag Sets\n\nThere’s a bunch of classes that define what part of speech tag sets the tags that are actually there and in English and many other languages they’re often based on the tag set that was used in the pen tree bank which was one of the first large data sets that was labeled consistently with nouns verbs etc and there’s 43 in total I don’t know if there’s 43 there but I know from other things and there’s some that are relatively rare and maybe questionable and there’s some compound ones where sometimes things get joined together it does make distinctions between different types of nouns singular plural and and proper nouns and it it’s quite useful in many tag sets that are now used in in NLP are derived from this tag set because that was decided mostly by the group at U-pen at the time people were doing a part of speech tagging before that. I mean even before computers they were doing it but coming up with a finite computational tag set was something that really started in the 80s and depending on the language that you’re dealing with you may want to have different tags because there are some tags that are really only relevant in some languages and not in others okay importantly there actually is a definition of a small number of tags which is in some sense a reduction of the number of tags that were in the pen tree bank that has been used in the universal dependency m sets that originally came out of google and it’s now an independent project but it’s still quite google influenced because of the original data sets and this covers somebody’s going to tell me the number of languages but I think it’s about 40 or 50 languages where they’re all labeled with the same tag set and produces a universal dependency grammar which is also very use useful from a syntactic point of view that over a bunch of fairly major important languages okay so getting this tag set is useful and you might say why should I care about getting a tag set I can train from words and the answer is yes if you have lots of data but as usual in trying to do machine learning if you can give more information in a structured way or in a reduced standardized way you can typically get by with less data and try to train better and training should happen faster so often being able to get a part of speech for a language would be quite useful and of course this is going to be hard in the low resource language because you sort of need labeled data to start off with but there are unsupervised ways to try to find out what these tags actually are now I’m naively talking about words here and words having part of speech tag because I’m one of these English speakers where actually for the most part that’s pretty easy in English for the most part you know white space separated tokens or words and for the most part each token has got one a tag one proper tag and the context mostly defines what it is but in most languages most languages and we really have to introduce the notion of morpheme which is sort of the single smallest atomic part of a word"
  },
  {
    "objectID": "notes/cs11-737-w04-words/index.html#morphology",
    "href": "notes/cs11-737-w04-words/index.html#morphology",
    "title": "Words, Parts of Speech, Morphology",
    "section": "Morphology",
    "text": "Morphology\n\nHere for example in English we do have interesting morphology especially in what’s called derivational morphology which is not like the ing eds plurals where we’re changing some syntactic property the tense or the number of a language but we’re actually changing the meaning and we’re often changing the class the part of the speech class so if we take a verb like establish we can have another verb that’s disestablished we can then make that into a noun by say disestablishment.\nWe can make it another noun putting anti- in front of it and saying anti-disestablishment. We can make it into an adjective by saying anti-disestablishmentary. We can make it a noun into anti-disestablishmentarian. We can then make that in a further noun by saying anti-disestablishmentarianism Now these things are a little bit extended, but actually we do this all the time. So it’s actually quite hard to really list all of the words that are in English because although some of these don’t appear very often. There will be new and novel words, and you’ll see a number per day of new words that you’ll understand. Where they’re actually morphologically variants of something that you can work out what the meaning actually is.\nNow so it would be useful if we could get these words and decompose them into their roots and morphemes so that we can actually work out what the important classes are. So we’d like to be able to get some notion of these decomposed forms in from a word if we can do it. Now some of these forms are what we call stems or roots they’re often words on their own. And we’ll have prefixes and suffixes.\nIn English we rarely have anything that inserts in the middle of a word we’re usually putting things at the beginning and end. In some languages you actually get sort of bracketed things that you have to put them at the beginning and the end some of the gaelics have got that. There are some things where you can actually put infixes and so there are some plural things that actually happen in interesting languages in southeast asia where they’re basically plural things where syllables or partial syllables will get duplicated and and therefore you have to deal with that.\nIn English the only example of being able to do that is the infix form of putting swear words in the middle of a word so for example if you have the words pittsburgh and you want to put a swear word in the middle. I can only get away with this because I’m British. I’m going to use the word bloody as a swear word, although actually, usually in linguistics we use the f-word but we can use the word bloody and if I want to put the word bloody in Pittsburgh, it’s going to be pit’s— bloody—berg and I could do that and it could be compounded and possibly but it’s a little bit of a stretch and maybe you would put species in in there to do that in other languages. Infects will happen in some languages. You’ll actually even change things in Templatic morphology in things like Hebrew and Arabic, and Tagalog is an example.\nThis is one of the examples from the Philippines where we’ve got interesting morphology going on and we’ve got much more interesting morphology going on than what’s in English and we’d like to be able to decompose these things so that we’ve got finite sets of morphemes when we’re doing processing so when we’re doing tokenization when before you give it to your word embedding system you’d like to have a standardized tokenizer that’s going to give you meaningful the most meaningful atomic parts when you actually do it."
  },
  {
    "objectID": "notes/cs11-737-w04-words/index.html#arabic",
    "href": "notes/cs11-737-w04-words/index.html#arabic",
    "title": "Words, Parts of Speech, Morphology",
    "section": "Arabic",
    "text": "Arabic\n\nArabic’s very interesting, because things actually are done within the consonants so you have a backbone of consonants and the vowels will change before and after after them. This changes meaning both semantically and inflectional so syntactic information about tense etc there’s a number of Semitic languages that actually do templatic morphology and it always breaks a lot of our systems from doing it but we are not allowed to define what natural language actually is we still need to be able to deal with it Chinese has a relatively small amount of morphology but it still has derivational morphology so you can take words and join them together in interesting compounds which are not necessarily directly to do with the meaning of the individual character that you’re joining together so a number of things of for example if you take fire and wheel and put it together it doesn’t mean a wheel that’s on fire it actually means a steam train and so or maybe that’s only in Japanese I always get that one wrong which varies between the different languages but there’s often a relationship but the compound might be different and so sometimes you want to be able to decompose it and sometimes you don’t so there’s two types of morphology which are identified as what’s called derivational morphology and derivational morphology you’re mostly changing the part of speech class when you’re doing things English is a rich derivational morphology and we can write it out and it’s mostly productive by productively means we’re allowed to construct new words without explaining to people what the meaning is inflectional morphology is usually syntactic class changes or some classic feature changing so this is things like changing the tense changing the plurality and the number and other languages it may do things like used in agreement used in tense and aspect and verbs and these are usually treated as different classes and they’re usually quite different they don’t overlap they’re sometimes maybe a little bit confusing and for the most part inflectional morphology happens after derivational morphology in almost all languages we can talk about morphosyntax about how these phonemes join together and which ones are allowed to join to each other and we can talk about a lower level thing that we call it morphofunnymix or morphographymix where things are changing at the boundaries when we join them together so an example of morphophony thin morphography mix because it happens in the graphene form but the pronunciation is affected as well when you take apostrophe s okay or the plural s when we add it onto things sometimes we insert an e when we add e d to the end of things if it already ends in an we don’t double the e and so this is I’ve got the e d and I’m going to join it on if the previous one is an e I’m just going to do it so move plus e d this is m-o-v-e-d okay while walked plus e-d is walk plus e-d joined together there’s different classes of"
  },
  {
    "objectID": "notes/cs11-737-w04-words/index.html#discussion-prompt",
    "href": "notes/cs11-737-w04-words/index.html#discussion-prompt",
    "title": "Words, Parts of Speech, Morphology",
    "section": "Discussion Prompt",
    "text": "Discussion Prompt\n\nPick a language in one of the following branches of language families: Bantu, Dravidian, Finno-Ugric, Japonic, Papuan, Semitic, Slavic, Turkic. Tell us about some interesting aspects of morphology of that language, following examples from the assigned reading. Cite your sources.\nIf you would need to implement a tokenizer for that language, what language specific knowledge would need to be incorporated into the tokenizer?"
  },
  {
    "objectID": "notes/cs11-737-w05-translation/index.html",
    "href": "notes/cs11-737-w05-translation/index.html",
    "title": "Translation and Translation Data",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\nVideo 1: Lesson Video\n\n\n\n\n\n\n\n\nThis week’s slides\n\n\n\n\nSupplementary Figure 1"
  },
  {
    "objectID": "notes/cs11-737-w05-translation/index.html#intro",
    "href": "notes/cs11-737-w05-translation/index.html#intro",
    "title": "Translation and Translation Data",
    "section": "Intro",
    "text": "Intro\n\nThis time we’re going to be talking less about models and more about the actual phenomenon of translation itself and so translation i mean i barely need to introduce what translation is i’m sure everybody knows but you know basically it’s the conversion of content in one language into content in another language\n\nfor the purpose of making that content understandable to somebody who doesn’t know the original language of course and that can be anything from you know translating novels like harry potter to you know any other variety of translation that happens and this time i’m going to be talking about how this translation happens a little bit about machine translation and then also about machine translation data sources and empty evaluation"
  },
  {
    "objectID": "notes/cs11-737-w05-translation/index.html#translation-vs-interpretation",
    "href": "notes/cs11-737-w05-translation/index.html#translation-vs-interpretation",
    "title": "Translation and Translation Data",
    "section": "Translation vs Interpretation",
    "text": "Translation vs Interpretation\n\nThere’s actually two types of conversion of language between different languages there are two main types one is translation another is interpretation and translation is basically the conversion of written word from one language to another and interpretation is conversion of spoken language from one language to another and they’re very different in the requirements for them also how people work and the type of type of people who work in these areas so translation in general is usually less time constrained you may still have a deadline for when your translation results are required but usually it’s on the order of you know like a day somebody gives you a translated document and they they say i want this back in a day whereas an interpreter may interpret while the content is being produced you know while i’m talking an interpreter may be interpreting my speech into another language that’s simultaneous interpretation you can also do consecutive interpretation which is basically like i speak for a while then the interpreter speaks for a while i speak for a while interpreter speaks for a while translators a high degree of accuracy and fluent very fluent natural output is necessary and partially because of this translators in may translate all kinds of things but very often they specialize in a single area like i’m a medical translator or i’m a patent translator or something like that on the other hand interpreters may specialize in an area but more commonly they’re generalists who can interpret lots of different things i actually worked as an interpreter and translator for about a year and a half and i did both of them because i was kind of in a position that was a little bit less specialized i worked at a local government in japan they additionally had a single translator in a single interpreter and it’s very interesting because personalities are also very different translators are are often somewhat introverted you know they like working on their own they really like being precise whereas interpreters have to be really good at talking because they spend their whole day talking they tend to be very extroverted you know like talking to people in general not just interpreting between languages so both of these jobs are very hard i found interpretation harder because of the time pressure in the constraints and it’s not it’s you know you can’t get 100 accuracy in a situation where you’re expected to interpret things so you just need to do as well as you can"
  },
  {
    "objectID": "notes/cs11-737-w05-translation/index.html#translation-industry",
    "href": "notes/cs11-737-w05-translation/index.html#translation-industry",
    "title": "Translation and Translation Data",
    "section": "Translation Industry",
    "text": "Translation Industry\n\nHow much translation is done the language services market is 56.1 billion dollars that’s a lot of dollars and there’s 640 000 translators worldwide about 75 of them are working freelance and the other ones are employed by a specific organization interestingly europe owns 49 of the market share so 49 of the people about half the people are working in europe\n\nand one interesting thing is you might think that as machine translation technology is getting better that was you know threatening translators in their jobs and you know reducing the value of the industry actually the con the contrary is true you know more translation is being done than ever both machine translation and human translation and the value of the industry is doing nothing but improving so in a way you know more language is being translated now than ever another thing is that the translation industry is becoming more technological so 88 of full-time translators use some variety of computer-aided translation which means that they’re using machine translation they’re using something like translation memory that allows you to look up other related translations the most common tool is something called Tranos and basically it has translation memory it has integrated empty it has terminology e management software other things like this and a lot of translation agencies require that you use Tranos in order to be able to work with them there’s also other ones that i’m less familiar with but basically if you have this idea of like human translators versus machine translators in reality it’s not the case anymore it’s now a hybrid of humans and machines working together some people like this some people don’t like this some people would prefer not to be told what to translate by their translation memory but you know in order to improve efficiency people now have to do this as part of their life\n\nWhy people don’t like translation\n\nThe reason why some people don’t like translation computer aided translation is i think they feel it stifles their creativity or it because they can do it more efficiently now the requirement is that they do do it more efficiently so they have less time to sit and think about the perfect translation and just have to crank out content basically so i think those are the main reasons why people are like resistant to technology but nonetheless 88 of people are using it so even if they don’t like it they’re using it because they you know have to in order to keep up with the amount they’re expected to produce a lot of people do like it so it’s not everybody yeah so from the modeling type of view the biggest difference is whether speech is your input or text is your input another big difference is interpretation sometimes you’re expected to do it in real time so that’s called simultaneous translation so you need to create the output before you’ve like read the whole document essentially that’s a very interesting topic it might be a topic that some people in this class want to work on if you like the speech you like the translation it’s a very hot topic right now cool so now what about difficulty in translation why is this hard so this is an example i i cannot read this example i inherited it with from someone else but it’s basically a\n\nan old chinese poem and the reason why it’s difficult to translate in general is because there’s divergences in lexical information so words in structure so this is an alignment of the glosses in chinese with the words in english if you don’t know what a gloss is it’s basically a word by word translation in the same order as the original sentence so\nthis is what it looked like in chinese unfortunately i don’t have the chinese characters above for all the chinese speakers but you can kind of see what it looks like and basically if you look at the chinese it’s like daiyu alone on bed top think baochai which gets translated into as she lay there alone daiu’s thoughts turned about and you can see that the ordering is different between the two also you know some words exist in the chinese but don’t exist in the english even more so in the next sentence so\nyou need to get the words right you need to get the words in the right order and this is that non-trivial when you know the translations are different between the two languages\n\n\nTranslation Ease\n\nHere’s another example from german which might be a little bit easier to parse if you’re not a chinese speaker so if you have\n\nhere you can see the gray gloss on top which is in the in city exploded car bomb and the the kind of canonical english translation that’s listed here is a car bomb exploded downtown so you can see that the word order changed also in in german some things like car bomb is a single word here it’s multiple words here there’s also a phenomenon called translation ease and what translation eases is it’s not exactly natural in the language you’re translating into but it’s a translation that is direct and kind of maintains the original characteristics of the original language there’s actually a fair amount of computational study of translations seeing how like which language you’re translating from affects the output and you can even take translation ease cluster it together and you get a very nice reproduction of the language family tree that the languages came from so basically the effects on translationese very strongly inherit the effects of of the original language and even you know are similar between similar languages etc so you can see there’s a very clear effect and here in the inner city there exploded a car bomb would be a very like literal translation that you can understand in english but it’s also not natural english it’s not like what an english speaker would produce"
  },
  {
    "objectID": "notes/cs11-737-w05-translation/index.html#lexical-ambiguity",
    "href": "notes/cs11-737-w05-translation/index.html#lexical-ambiguity",
    "title": "Translation and Translation Data",
    "section": "Lexical Ambiguity",
    "text": "Lexical Ambiguity\n\nAnother issue is not just structure but also lexical ambiguities so this is an example from jarefsky and martin speech and language processing where basically you have leg and foot and paw and how they are how they are translated in different ways based on whether it’s a an animal leg a leg of a journey a leg of a human a leg of a chair a bird foot or a human foot etc etc"
  },
  {
    "objectID": "notes/cs11-737-w05-translation/index.html#literary-translation",
    "href": "notes/cs11-737-w05-translation/index.html#literary-translation",
    "title": "Translation and Translation Data",
    "section": "Literary Translation",
    "text": "Literary Translation\n\nI also thought i had another example but in order to handle all these things you’ll have to handle you know syntactic differences lexical differences\n\nhow idiomatic the output is so you know there’s lots of issues here and then if you start talking about literary literary translation it becomes even harder right you know you want to translate a poem or something like this as we had in the assignments for the discussion today and suddenly you need to think about rhyming you need to think about like beauty of the expressions that you’re using so lots of depth in doing translation any questions there before i talk about mt quickly yeah is there any criteria so the criteria that i would use to define translations there might be a different formal definition but i think this is basically right is it is language that only occurs because you’re translating from another language and would not normally be you know how you would how a native speaker would express the same content in that language so it’s kind of like structural or lexical influences of the original language of the produced language it doesn’t need to be like the effect of translation of the output could be very subtle and often for a good translator it is very subtle but it’s still like there i have no confidence i’m a native speaker of english i’m very good at japanese but i have no confidence that i can produce english that sounds like my natural english when i’m transmitting to japanese for example any other things okay cool under the machine translation so machine translation a checked is a three billion dollar market\nthey’re oh actually i should mention that some of the statistics i got here are from this very very nice blog of the translation industry in 2021 if you didn’t look at this on the page i would definitely take a look it summarizes a whole bunch of statistics and was insightful to me as well these statistics are also from there which is machine translation is a three billion dollar market now so it has about five percent of the market share of the language services industry overall the top providers of it are google amazon and l so google i think a lot of people know amazon and aws web services provide translation for a lot of businesses for example and deepel is a startup that many people might have heard of but it has actually very good translation accuracy\nthey haven’t revealed all of their secrets but one of the things is that they use like cleaner training data they have good training data cleaning strategies and they also consider context in a better way than other things like google do another thing about machine translation is these are the markets that machine translation is used in you can see that the most common ones are healthcare automotive and military and defense markets but it’s kind of spread out pretty widely including e-commerce and other things like that there’s a very interesting paper that examined the effect of translation on e-commerce that i don’t have in the references but i can share which demonstrated that when ebay i believe introduced automatic translation between spanish and english the number of sales from\nlatin america to the us basically started increasing immediately after that so you can see also that you know mt has real world consequences impact etc so these are the lists this is maybe a slightly old list of languages supported by google translate it’s pretty impressive you know at least 100 languages maybe it’s nearing 200 now one thing that i like to mention to people whenever you look at this list is just because there’s a hundred languages on this list doesn’t mean that mt is equally good for all of the languages on this list you know it may be obvious if you think about it a little bit but sometimes you think well you know it’s on this list google released a product for it it must work that’s definitely not the case and you know if you try to use it to understand articles you’ll see a very big very big difference"
  },
  {
    "objectID": "notes/cs11-737-w05-translation/index.html#human-translators",
    "href": "notes/cs11-737-w05-translation/index.html#human-translators",
    "title": "Translation and Translation Data",
    "section": "Human Translators",
    "text": "Human Translators\n\nThere are some reports that mt is now at the level of human professionals in some areas like for example news translation between very high resource languages like chinese and english. I didn’t believe this at first when microsoft first put out an article that said this essentially and i went in and like analyzed their data looked at their data and they actually their outputs are quite good and human translator outputs are not perfect either so for example let’s say you hire a human translator on a freelancing site and tell them i want you to translate these new news articles because i would like to create training data for a machine translation system if you do that the translator will say sure i want your money i want your money i will i’ll be happy to do that but they’re not going to be super motivated and if you say instead to the translator say i’m going to be asking you to translate these news articles for cnn and or the new york times and a hundred thousand people are going to read your article you’re pretty sure they’re going to do a good job right they’re not going to make a mistake so which human translator you’re trans comparing people to also makes a big difference in these these outputs and not even just which human translator but how motivated that human translator is so i have a feeling that mt systems and high resource languages are almost as good as moderately motivated good professional human translators but they’re not as good as somebody who’s translating for the new york times for like 100 000 people for example"
  },
  {
    "objectID": "notes/cs11-737-w05-translation/index.html#translation",
    "href": "notes/cs11-737-w05-translation/index.html#translation",
    "title": "Translation and Translation Data",
    "section": "Translation",
    "text": "Translation\noh yeah so so sorry here are my other examples this is from google translate i basically put in the first sorry i had animation on this until i had to switch computers but i put in the first sentence from the wikipedia article on translation and i’ll put it through google translate which said translation refers to the act of replacing what is expressed in the form of a in a language with the form of b that corresponds to that meaning specifically in natural language it refers to the act of converting a sentence in a in the source language into a sentence in another target language i did a few small edits here the the red stuff are my edits that i think would make it a little bit better but it’s pretty good\nhere’s another example of machine translation i entered a lexically ambiguous word kodo in japanese which can be either code chord or chord depending on you know the context and it does a pretty good job at disambiguating like electrical cord code for the program chord for the on the score but if i wrote as a musician i am good at reading chords it made a mistake with that in java i wrote a chord that displays the chord of a guitar that should have been code up here so you can see that it you know is not perfect for doing this as well\nso you know translation is is hard even good things like google translate are not perfect but they’re pretty good in high resource languages anyway so why do these work i’m gonna be talking more about translation models next class but basically i’d like to go through a little bit of you know history into how\nthese were conceptualized and from 1968 there’s this famous thing called the vaqua triangle and basically what it is saying is there’s multiple ways to do translation you can go from words directly to words so you can basically replace words by words you can go up to the syntactic structure of languages and then generate from the syntactic structure you can go up to semantics in the language convert the semantics between the languages and go down and you can go up to something called an interlingua with lingua which is like universal semantics for all languages"
  },
  {
    "objectID": "notes/cs11-737-w05-translation/index.html#direct-transfer",
    "href": "notes/cs11-737-w05-translation/index.html#direct-transfer",
    "title": "Translation and Translation Data",
    "section": "Direct Transfer",
    "text": "Direct Transfer\nso what does this look like direct transfer looks like a word by word translation so you would just be translating directly from word words to words syntactic transfer would be like analyzing the syntax of the sentence and then using that to translate you could also generate a syntax of the target sentence or translate from syntax of the source sentence to the target sentence and generate the output"
  },
  {
    "objectID": "notes/cs11-737-w05-translation/index.html#semantics",
    "href": "notes/cs11-737-w05-translation/index.html#semantics",
    "title": "Translation and Translation Data",
    "section": "Semantics",
    "text": "Semantics\nYou could also have something like semantics which is a logical form which basically says well something was detonated what was detonated it was a bomb\nlike a car bomb it was that mediated downtown and that was in the past tense and then you generate the output based on that"
  },
  {
    "objectID": "notes/cs11-737-w05-translation/index.html#interlingua",
    "href": "notes/cs11-737-w05-translation/index.html#interlingua",
    "title": "Translation and Translation Data",
    "section": "Interlingua",
    "text": "Interlingua\nYou could do other things and then there’s also an interlingua which basically says nothing about the you know individual languages and instead is completely in kind of a form a logical form\nso each of these methods has their own advantages and disadvantages the advantage of going directly is like let’s say we have a language like spanish and italian which are very very similar in words and structure and other things like this there you could basically do a word by word translation and do a pretty good job however if you have very different languages you know you’re going to have a lot of trouble you need to have very basically a very powerful model to allow you to do this and you know before neural networks we didn’t have any model that really did this very convincingly well"
  },
  {
    "objectID": "notes/cs11-737-w05-translation/index.html#neural-models",
    "href": "notes/cs11-737-w05-translation/index.html#neural-models",
    "title": "Translation and Translation Data",
    "section": "Neural Models",
    "text": "Neural Models\n\nNeural models now you can kind of view them as models that map word by word they take in a whole bunch of words they generate a whole bunch of words but you can also view them maybe as a interlingua based model where you know they’re taking in words and they’re generating hidden vectors they correspond to the meaning of all of those words and then they’re generating from that you know like interlingua between the the languages so where exactly we lie now on the spot triangle is kind of you know unclear but you know it’s kind of an interesting question as well and you know maybe considering syntax or other things like that would help us generalize better in the resource languages for example"
  },
  {
    "objectID": "notes/cs11-737-w05-translation/index.html#data",
    "href": "notes/cs11-737-w05-translation/index.html#data",
    "title": "Translation and Translation Data",
    "section": "Data",
    "text": "Data\n\nI realize i have a lot of slides left and i don’t want to take all of the time. Maybe i’ll just go through the data part and leave the evaluation part till next time but are there any questions so far okay cool so i’d like to talk a little bit about data because data is very important for training our mt models and so basically all models including you know like google translate amazon dpel any of the things that people are using are using machine learning based methods and basically the way they work is they’re trained from parallel data sources where you have one language and then another language one language and then another language this is an example that you can actually do yourself if you’re like interested in trying a puzzle which is basically like take this parallel corpus and then try to translate this sentence at the bottom yourself and you’ll see that you need to like form associations between words you need to understand about what the syntax of the language looks like but it’s definitely possible from this this small parallel corpus"
  },
  {
    "objectID": "notes/cs11-737-w05-translation/index.html#parallelcorpora",
    "href": "notes/cs11-737-w05-translation/index.html#parallelcorpora",
    "title": "Translation and Translation Data",
    "section": "Parallelcorpora",
    "text": "Parallelcorpora\n\nNo i think these are made up yeah i’m pretty sure\n\nso where can we get parallel corpora basically it’s anywhere that translators are doing lots of translation this is an example from the united nations from a few days ago and you can see that this is translated into you know three languages here it’s actually six languages i believe it’s the official languages of the"
  },
  {
    "objectID": "notes/cs11-737-w05-translation/index.html#languages",
    "href": "notes/cs11-737-w05-translation/index.html#languages",
    "title": "Translation and Translation Data",
    "section": "Languages",
    "text": "Languages\n\nanother good source that we love using for like very low resource languages is the bible because the bible is translated into more languages than any other text as far as i know"
  },
  {
    "objectID": "notes/cs11-737-w05-translation/index.html#books",
    "href": "notes/cs11-737-w05-translation/index.html#books",
    "title": "Translation and Translation Data",
    "section": "Books",
    "text": "Books\n\nYou can also get books of course or other other things this is harry potter and english and chinese Restaurants when you go to a restaurant if you’re not chinese you can go to your favorite chinese restaurant if you’re chinese you can go to your favorite indian restaurant i don’t know something else and get the menu and that’s a very good parallel purpose for you to try your your own learning skills on"
  },
  {
    "objectID": "notes/cs11-737-w05-translation/index.html#web-data",
    "href": "notes/cs11-737-w05-translation/index.html#web-data",
    "title": "Translation and Translation Data",
    "section": "Web Data",
    "text": "Web Data\n\nyou can also harvest data from the web like you can harvest data from micro micro blogs twitter social media other things like this so this can give you you know more informal language and that’s why you know google doesn’t completely fall over when it tries to translate twitter for example"
  },
  {
    "objectID": "notes/cs11-737-w05-translation/index.html#opus",
    "href": "notes/cs11-737-w05-translation/index.html#opus",
    "title": "Translation and Translation Data",
    "section": "Opus",
    "text": "Opus\n\nand the canonical place to get data for any of these models now is this place called opus and what opus does is it collects a whole bunch of open parallel corpora in many many different languages language pairs and across many domains so if you want to train models this is your best place to get it so to leave some time for the discussion i think i’ll move the evaluation part to tomorrow but are there any questions about stuff we talked about so far"
  },
  {
    "objectID": "notes/cs11-737-w05-translation/index.html#discussion-question",
    "href": "notes/cs11-737-w05-translation/index.html#discussion-question",
    "title": "Translation and Translation Data",
    "section": "Discussion Question:",
    "text": "Discussion Question:\n\nUse Google translate to back-translate the text via a pivot language, e.g., “English → Spanish → English” or “English → L1 → L2 → English”, where L1 and L2 are typologically different from English and from each other.Compare the original text and its English back-translation, and share your observations. For example, (1) what information got lost in the process of translation? (2) are there translation errors associated with linguistic properties of pivot languages and with linguistic divergences across languages?\nTry different pivot languages: can you provide insights about the quality of MT for those language pairs?"
  },
  {
    "objectID": "notes/cs11-737-w05-translation/index.html#resources",
    "href": "notes/cs11-737-w05-translation/index.html#resources",
    "title": "Translation and Translation Data",
    "section": "Resources:",
    "text": "Resources:\n\nhttps://redokun.com/blog/translation-statistics"
  },
  {
    "objectID": "notes/cs11-737-w10-unsupervised-NMT/index.html",
    "href": "notes/cs11-737-w10-unsupervised-NMT/index.html",
    "title": "Unsupervised Machine Translation",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\nVideo 1: Lesson Video\n\n\n\n\n\n\n\n\nThis week’s slides\n\n\n\n\nSupplementary Figure 1"
  },
  {
    "objectID": "notes/cs11-737-w11-code-switching/index.html",
    "href": "notes/cs11-737-w11-code-switching/index.html",
    "title": "Code Switching, Pidgins, Creoles",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\nVideo 1: Lesson Video\n\n\n\n\n\n\n\n\nThis week’s slides\n\n\n\n\nSupplementary Figure 1"
  },
  {
    "objectID": "notes/cs11-737-w10-unsupervised-NMT/index.html#intro",
    "href": "notes/cs11-737-w10-unsupervised-NMT/index.html#intro",
    "title": "Unsupervised Machine Translation",
    "section": "Intro",
    "text": "Intro\nI’m going to be talking about unsupervised machine translation and this is a very interesting topic overall. If you’ve done the reading, you can see that it’s to a greater or lesser extent practical for some varieties of text-to-text translation, but I think there are a lot of other applications as well, maybe including with speech or other things that we’re going to be talking about in the future. I think the underlying technology is interesting and worth discussing and knowing about both with respect to the techniques and the limitations and other things like this."
  },
  {
    "objectID": "notes/cs11-737-w10-unsupervised-NMT/index.html#conditional-text-generation",
    "href": "notes/cs11-737-w10-unsupervised-NMT/index.html#conditional-text-generation",
    "title": "Unsupervised Machine Translation",
    "section": "Conditional Text Generation",
    "text": "Conditional Text Generation\nConditional text generation, which we’ve talked about. Basically, we’re generating text according to a specification like I talked about before in the seq2seq models class. We have our input x and our output y, where it could be for machine translation image captioning summarization, speech recognition, etc. The way we model this is using some variety of conditional language models like seq2seq model, like the ones you’re training for assignment two with our encoder and our decoder, and the way we traditionally estimate the model parameters here is using maximum likelihood estimation to maximize the likelihood of the output given the input, and generally, this needs supervision in the form of parallel data, usually millions of parallel sentences. ## What if we don’t have parallel data?\n\nWhat we’re going to ask about in this class is what if we don’t have parallel data, so just to give a few examples of this. We have parallel data? Well, what if we don’t have parallel data? For example, let’s say we have a photo of a person’s face or something like that. We automatically want to turn it into a painting, you know to put on your wall and display or something like that or turn it into a cartoon because you want a picture of yourself for your social media profile in a cartoon or something so, unfortunately, we don’t have tons and tons of data for this but we do have tons of photos and tons of paintings so we have lots of input x and lots of output y but very few pairs of input x and output y we could also do other things like transferring images between genders or between ages or something like this I think you’ve seen apps that might do this text from impolite to polite so you know correcting the formality transferring a positive review to a negative review or vice versa or doing something like machine translation and I actually modified this to give a few other examples like some really but the slides disappeared some really interesting examples are what if we had an ancient language or a cipher where we didn’t actually know what it was we didn’t have any text but we wanted to decipher this old text and replicate and like understand what it meant in the modern language so that’s another thing that we could do with unsupervised translation."
  },
  {
    "objectID": "notes/cs11-737-w04-words/index.html#types-of-morphology",
    "href": "notes/cs11-737-w04-words/index.html#types-of-morphology",
    "title": "Words, Parts of Speech, Morphology",
    "section": "Types of Morphology",
    "text": "Types of Morphology\n\nmorphology and typography and typology of these so that we can put these into different groups and to be able to identify what they’re all about we have isolating or analytic m1s where there’s very little morphological changes in Vietnamese many of the Chinese dialects English are good examples of that we have synthetic ones where things are being created all the time they’re sometimes called fusional or flectional so german greek russian and templatic where we’ve got some form of often consonants and the vowels are changed and interspersed between those and we have a glutenatif where there’s lots of things joining together japanese finnish turkish are really good examples of that and we have polysynthetic which are really complexly joining things together where almost every phrase is actually ends up in a single word many of the north american native american languages are that the reason the word snow is there is because you can often have lots of variations of the word snow in inuit that’s actually not true well it is sort of true but it’s notable that in Scottish English we have lots of words for rain and for some reason I have no idea why that would be"
  },
  {
    "objectID": "notes/cs11-737-w04-words/index.html#morphology-analyzers",
    "href": "notes/cs11-737-w04-words/index.html#morphology-analyzers",
    "title": "Words, Parts of Speech, Morphology",
    "section": "Morphology Analyzers",
    "text": "Morphology Analyzers\n\nPeople have worked on morphology for a long time so often when you’re working a new language there already is a morphological analyzer is there and if you look at the project unimorph and they’ve actually collected together these in a fairly standardized way so you might just be able to use it from python and all of a sudden you get morphological decomposition for your language or maybe you could use a nearby language and it would almost work and that might make your life a lot easier often when we’re doing novel languages especially when we’re caring about things under time we will look for one of those or we might even spend a couple of hours writing something because we’ll get something better than trying to do it fully automatically there are fully automatic ways but it might be better if there’s already something that allows us to do that okay there’s actually a competition"
  },
  {
    "objectID": "notes/cs11-737-w04-words/index.html#morphology-competition",
    "href": "notes/cs11-737-w04-words/index.html#morphology-competition",
    "title": "Words, Parts of Speech, Morphology",
    "section": "Morphology Competition",
    "text": "Morphology Competition\n\nevery year sigmar phone has been running for at least 10 years and it gets harder and harder every year as they find harder and harder tasks from both supervised and unsupervised m techniques so it’s worthwhile looking at these and using that as a resource"
  },
  {
    "objectID": "notes/cs11-737-w04-words/index.html#shared-tasks",
    "href": "notes/cs11-737-w04-words/index.html#shared-tasks",
    "title": "Words, Parts of Speech, Morphology",
    "section": "Shared Tasks",
    "text": "Shared Tasks\n\nThese shared tasks allow you to compete and people have done them from this class before actually and done interesting novel techniques to be able to work out to do it sharing information across language when you’ve got not enough data to train etc in order to be able to learn how to do that"
  },
  {
    "objectID": "notes/cs11-737-w04-words/index.html#finite-state-morphology",
    "href": "notes/cs11-737-w04-words/index.html#finite-state-morphology",
    "title": "Words, Parts of Speech, Morphology",
    "section": "Finite State Morphology",
    "text": "Finite State Morphology\n\nFinite state morphology is often used for morphemes actually morphology is never very complex there’s probably something to do with a human brain it can’t really deal with something as complex as say syntax within morphology and therefore it’s often quite localized and therefore finite state machines are quite good at being able to cover everything and there’s good toolkits out there that help you to be able to write these things there’s also completely unsupervised techniques there’s more fessor just a python thing you give it examples and it will try to find out the prefixes and suffixes that actually might allow you to be able to do analysis it does assume a certain segmental and view of phonology and therefore it can get confused sometimes and it might treat the different types of ed or just d in different forms in English and separate them out and maybe you want to join them together or maybe you don’t there’s a sort of related thing called stemming which is often quite useful especially when you’re doing things like information retrieval where you’d like to say look I i just I just want the root of the word and I don’t want all these other variations especially when you’re in a limited domain or when you’ve got limited amount of data and so maybe if you removed all of the morphological variants the plurals the eds it might be easier to do comparisons later especially if you don’t have good data in order to be able to do good word embedding there’s also purely completely automatic techniques and bpe is a good example of that byte pair encoding you can’t really work it out from the name where what we do is we look at the string of the actual letters that are there and try to find and optimize the sequence of letters together and the overall predictability of the group of letters that we actually find and this originally came out of work in machine translation to try and find the best segmentation for doing translation but we end up using it for lots of lots of things it’s often worth trying if you don’t have anything else because it does sort of work but you really want to know about for your particular language is it likely to work before you actually do all of your bpe you get a tokenization representation you build all your word embedding that you learn from it and then learn oh no you could have downloaded the morphological analyzer that would have given a better result and a more consistent result and therefore you would have been able to learn back"
  },
  {
    "objectID": "notes/cs11-737-w04-words/index.html#tokenization",
    "href": "notes/cs11-737-w04-words/index.html#tokenization",
    "title": "Words, Parts of Speech, Morphology",
    "section": "Tokenization",
    "text": "Tokenization\n\nTokenization is also something that we get this often gets called where you’re actually just trying to split these things into words and you’ve got to care about what the tokenization actually is because if you have a different tokenization that won’t be the same lemmetization or stemming is somewhat similar but limit hydration is really talking about the linguistic root of the word which may or may not be well well defined and it’s usually after morphological and decomposition you find the root of the word a we can also do this across languages you may want to care about characters rather than words and looking inside characters that can actually help and caring about things that are happening over long boundaries somewhat related to this is word segmentation in languages like japanese and chinese and in fact they end up using something similar to bpe to be able to segment things there’s a couple of related things"
  },
  {
    "objectID": "notes/cs11-737-w04-words/index.html#text-normalization",
    "href": "notes/cs11-737-w04-words/index.html#text-normalization",
    "title": "Words, Parts of Speech, Morphology",
    "section": "Text normalization",
    "text": "Text normalization\n\nText normalization where you actually are trying to replace everything as words we know that there’s an infinite number of numbers and would be nice if we could change them into words maybe or maybe just change them into the word number or maybe classes of numbers and this is something that’s been studied in a in text-to-speech and there’s various machine learning techniques to try to do well on this you might want to also care about spelling correction and do be aware that tokenization this mismatch can really break everything so if you’re using bert you sort of have to use their tokenization because they’ve assumed that and it can be quite hard if you do something else okay that’s everything about works and morphology we will be talking about morphology again later on in more detail but now we’re going to care about splitting out up out into groups and what we want you to do today is we want you to take one of these languages language families often morphology and or aspect of writing and orthography are similar within language families and I’d like you to identify something that you would need to care about if you were trying to do some form of tokenization"
  },
  {
    "objectID": "reviews/paper/2016-code-summerization/index.html",
    "href": "reviews/paper/2016-code-summerization/index.html",
    "title": "A Convolutional Attention Network for Extreme Summarization of Source Code",
    "section": "",
    "text": "Litrature review\nThis is a paper mentioned in the course on multilingual NLP by Graham Neubig. With an interesting idea of an second attention head being used to copy stuff from the input directly to the output."
  },
  {
    "objectID": "reviews/paper/2016-code-summerization/index.html#abstract",
    "href": "reviews/paper/2016-code-summerization/index.html#abstract",
    "title": "A Convolutional Attention Network for Extreme Summarization of Source Code",
    "section": "Abstract",
    "text": "Abstract\n\nAttention mechanisms in neural networks have proved useful for problems in which the input and output do not have fixed dimension. Often there exist features that are locally translation invariant and would be valuable for directing the model’s attention, but previous attentional architectures are not constructed to learn such features specifically. We introduce an attentional neural network that employs convolution on the input tokens to detect local time-invariant and long-range topical attention features in a context-dependent way. We apply this architecture to the problem of extreme summarization of source code snippets into short, descriptive function name-like summaries. Using those features, the model sequentially generates a summary by marginalizing over two attention mechanisms: one that predicts the next summary token based on the attention weights of the input tokens and another that is able to copy a code token as-is directly into the summary. We demonstrate our convolutional attention neural network’s performance on 10 popular Java projects showing that it achieves better performance compared to previous attentional mechanisms. – (Allamanis, Peng, and Sutton 2016)"
  },
  {
    "objectID": "reviews/paper/2016-code-summerization/index.html#outline",
    "href": "reviews/paper/2016-code-summerization/index.html#outline",
    "title": "A Convolutional Attention Network for Extreme Summarization of Source Code",
    "section": "Outline",
    "text": "Outline"
  },
  {
    "objectID": "reviews/paper/2016-code-summerization/index.html#the-paper",
    "href": "reviews/paper/2016-code-summerization/index.html#the-paper",
    "title": "A Convolutional Attention Network for Extreme Summarization of Source Code",
    "section": "The Paper",
    "text": "The Paper\n\n\n\npaper"
  },
  {
    "objectID": "notes/cs11-737-w10-unsupervised-NMT/index.html#cant-we-just-collectgenerate-the-data",
    "href": "notes/cs11-737-w10-unsupervised-NMT/index.html#cant-we-just-collectgenerate-the-data",
    "title": "Unsupervised Machine Translation",
    "section": "Can’t we just collect/generate the data?",
    "text": "Can’t we just collect/generate the data?\n\nAnother question is “Couldn’t we just collect or generate data for these tasks”. To some extent, the answer is yes we could for some but it could be too time-consuming or expensive and it can also be difficult to specify what to generate or even evaluate the quality of generations so if we said generate generate this text like Joe Biden said it many people here you know don’t know what Joe Biden sounds like enough to be able to even do this in the first place. You know it’s difficult and under-specified, and finding people who’d be able to do that would be difficult, and because of this, it often doesn’t result in good-quality data sets."
  },
  {
    "objectID": "notes/cs11-737-w10-unsupervised-NMT/index.html#unsupervised-translation",
    "href": "notes/cs11-737-w10-unsupervised-NMT/index.html#unsupervised-translation",
    "title": "Unsupervised Machine Translation",
    "section": "Unsupervised Translation",
    "text": "Unsupervised Translation\n\nAn unsupervised translation the basic idea is we have some seq2seq task you know translation being the stereotypical example but it could be any of the other ones that I talked about where we instead of using monolingual data to improve an existing NMT system trained on parallel data or reducing the amount of supervision we’d like to talk about can we learn without any supervision whatsoever."
  },
  {
    "objectID": "notes/cs11-737-w10-unsupervised-NMT/index.html#outline",
    "href": "notes/cs11-737-w10-unsupervised-NMT/index.html#outline",
    "title": "Unsupervised Machine Translation",
    "section": "Outline",
    "text": "Outline\n\nThere are some core concepts in unsupervised MT and these core concepts include initialization iterative back translation bidirectional model sharing and denoising auto encoding and actually from the point of view of unsupervised MT. In some cases people have also used older statistical machine translation techniques instead of neural machine translation techniques because they were more robust.\nI’ll talk a little bit about what statistical MT you know means because we haven’t really talked about it here yet and I’ll also explain a little bit about why it is more robust and you know what what we could do to also improve robustness of neural MT as well."
  },
  {
    "objectID": "notes/cs11-737-w10-unsupervised-NMT/index.html#step-1-initialization",
    "href": "notes/cs11-737-w10-unsupervised-NMT/index.html#step-1-initialization",
    "title": "Unsupervised Machine Translation",
    "section": "Step 1: Initialization",
    "text": "Step 1: Initialization\n\nFor step one in initialization, basically a prerequisite for unsupervised mt is that we start out with an initial model that can do something that can do some sort of mapping between sequences in an appropriate way so that we can use it to seed a downstream learning process to do translation and it basically adds a good prior to the state of solutions we want to reach and the way this is done is by using approximate translations of subwords words or phrases usually and the way we take advantage of this is we take advantage of the fact that the context of a word is often similar across languages since each language refers to the same underlying physical world and what we do is we rely on unsupervised word translation."
  },
  {
    "objectID": "notes/cs11-737-w10-unsupervised-NMT/index.html#initialization-unsupervised-word-translatic",
    "href": "notes/cs11-737-w10-unsupervised-NMT/index.html#initialization-unsupervised-word-translatic",
    "title": "Unsupervised Machine Translation",
    "section": "Initialization: Unsupervised Word Translatic",
    "text": "Initialization: Unsupervised Word Translatic\n\nI talked about this a little bit two classes ago. I also called it a bilingual lexicon induction, and the basic idea is that word embedding spaces in two languages are isomorphic and what I mean by this is if you take an embedding space from one language like English in embedding space from another language, let’s say, Spanish we can learn some function that isn’t overly complicated that allows us to map between these two embedding spaces so, for example, we might run a model like word to back or any other you know kind of word embedding induction technique to embed individual words and embedding spaces and then we learn a mapping between them like an orthogonal like a matrix transformation w x equals y maybe with some constraints like the w is orthogonal which basically makes the embedding mutually makes the embedding like bijective so you can mutually map between one embedding space and another embedding space and we hope that by applying this transformation we will end up with something where the words in one embedding space are or the words in both embedding spaces if they’re close together the words are similar semantically or syntactically and this is hard to believe that this would actually work. I actually remember going to a presentation in ACL, I think 2016 where this method was proposed and I was like there’s no way this could possibly work because you’re assuming that you embed words and then just transform them in some way and the distributional properties cause them to line up, in fact, it does work better than you would think it would and there’s a couple of reasons for this one reason for this is that in addition to distributional properties a lot of the word embedding techniques that are used in these mappings here also take into account like sub word information and then if you’re mapping between English and Spanish or English and German.\nA lot of the words have similar spellings and that can give additional indication about whether the words are similar another reason is like words like gato and cat are both common and you know common words tend to map the common words uncommon words so you’re also basically implicitly using word frequency information in the mapping and frequent words in word embedding spaces often tend to have larger norms because they’re updated more frequently and so that kind of implicitly gets added into this calculation as well so there’s a bunch of things working for this nonetheless it doesn’t work perfectly it works kind of well enough to do something and in the case of initialization that’s mainly what we’re looking for we’re mainly looking for something that you know starts us out in some sort of reasonable space"
  },
  {
    "objectID": "notes/cs11-737-w10-unsupervised-NMT/index.html#unsupervised-word-translation-adversarial-t",
    "href": "notes/cs11-737-w10-unsupervised-NMT/index.html#unsupervised-word-translation-adversarial-t",
    "title": "Unsupervised Machine Translation",
    "section": "Unsupervised Word Translation: Adversarial T",
    "text": "Unsupervised Word Translation: Adversarial T\n\nWhat if the words in the two domains are not one to one or what if the words in the languages are not one to one. To give an example let’s say we’re mapping English to Turkish where in English a single verb has you know one or five conjugations and then in Turkish it has like 80. The answer is it basically doesn’t work it doesn’t work it doesn’t work very well so that’s another thing that you need to be concerned about but if the morphology is approximately the same it can it can still do something and the later steps of the unsupervised translation pipeline can also help disambiguate like ambiguous words and other things like that so yeah I saw another hand yeah so it’s a it’s a combination of words being frequent and words being words appearing in the same context. So for example just to give an example proper names proper names don’t work particularly well for this method because especially if you have like a Japanese newspaper and English newspaper the proper names mentioned in the English newspaper tend to be English names and the ones in the Japanese newspaper tend to be Japanese names for example however, there are these clusters of proper names and proper names have a certain frequency profile and they tend to cluster together so at the very least you should be able to get the fact that things in this cluster are proper names for example and you know that like I think intuitively you can see how that would be universal across languages more or less so you can get at least that close and then there’s other things like if two languages have determiners are almost always like the most common things so you could map determiners between languages and get them right uh most of the time stuff but uncertain completely unsupervised methods work kind of okay but not really well so yeah also an auxiliary point is that there’s very few languages in the world where you don’t have any dictionary whatsoever if there are languages in the world that don’t have a dictionary they also probably don’t have written text that you could learn word embeddings from so completely unsupervised you know you might not need it but you might want to do something with like just a dictionary and so another another thing that’s commonly done is like so the way another way to do this distribution matching or to learn this distribution matching is basically to have an adversarial objective and the adversarial objective basically what it does is it tries to prevent a model from being able to distinguish between the this x times w and y. So the idea is you want to move the spaces so close together that like a sophisticated neural network or some sort of you know discriminator is not able to distinguish between them so that’s the actual mechanism for doing the distribution matching another thing that is commonly done which I talked about two classes ago but isn’t included in the slides here is you get an initial first pass where you you find like several points that you’re very confident in like several points that don’t that are mutual nearest neighors of each other but don’t have close other close mutual nearest neighbors and you use those as basically pseudo supervision and then super create a supervised model that tries to make sure that those get mapped as close together as possible while making others farther apart and do an iterative process where you gradually like increase the number of words that are mapped together and that further improves accuracy now if you have like a small amount of supervision if you have like i don’t know 50 words in a dictionary you could use that you could use that to do supervision directly without having to do the unsupervised distribution matching at first and in fact two papers were presented at basically the same time one was a paper and completely unsupervised mapping another was a paper where they only use numbers like numbers were the only thing that they used to cross the language because numbers tend to be written in Roman characters in many languages in the world so sorry let Latin characters in many languages in the world so because of that you could use just that as supervision and then that would get you a long way too so if you have a dictionary that gives you better results. Usually, even a smaller does that actually really work because just because numbers I feel like they’re the information about that number and the word embedding is often not it doesn’t really encode what the number actually is just kind of that it is a number a lot of the time right and but I think basically the idea is if you can get any if you can get any supervision it’s better than no supervision and you’re still gonna have like ideally some sort of distribution matching component in your objective anyway so yeah yes with this method we still ensure that your viewers that’s a really good question so you if you have supervision you wouldn’t necessarily have to do that we’ve done some work on unsupervised embedding induction and it almost always helps to ensure that w is orthogonal and it or it almost always helped us anyway to ensure that w was orthogonal and it almost always helped us to not use anything more complicated than a linear transform like you would think you’d be able to throw a big neural network at it and it would do better but like even in supervised settings I guess the problem is too underspecified and that didn’t help very much not to say it wouldn’t have verb cool okay so the next so the next thing is we pull out our favorite data augmentation method date back translation and so we take for example French and back translated monolingual data into English and we take English and we back translate monolingual data into French and so here we have parallel data here but what we can do instead is we can do like pseudo parallel data which I’ll talk about in a second."
  },
  {
    "objectID": "notes/cs11-737-w10-unsupervised-NMT/index.html#one-slide-primer-on-phrase-based-statistical",
    "href": "notes/cs11-737-w10-unsupervised-NMT/index.html#one-slide-primer-on-phrase-based-statistical",
    "title": "Unsupervised Machine Translation",
    "section": "One slide primer on phrase-based statistical",
    "text": "One slide primer on phrase-based statistical\n\nNext I’d like to explain how we apply these methods to a non-neural MT system and just to give a very brief overview. A one-slide primer on phrase-based machine translation so this is what a lot of people used to do machine translation before neural mt came out and it consists of three steps first the input the source input is segmented into phrases these phrases can be any sequence of words not necessarily linguistically motivated so they don’t need to be like a noun phrase or verb phrase or anything like this then you take out a dictionary and you replace like the dictionary has translations of the phrases into the target language maybe it has you know 10 or so candidates for each phrase and then the phrases are reordered to get into the correct order and this is a nice method for some reasons and the one of the reasons why it’s a nice method is it’s guaranteed to basically cover every word in the inputs and not do any you know if the model is trained okay not do any really really crazy things so if you guys are struggling with assignment two right now already and you’re getting your like low resource machine translation neural machine translation system it’s probably doing things like repeating the same word 300 times in a row or something like that a phrase-based machine translation system would not do this it might give you a bad output but it wouldn’t you know repeat the same thing over and over again or translate a sentence into like an empty sentence or something like that precisely because it has to cover all the words and it can only use a fixed set of translations so because of this it has a strong bias to generate like non-nonsense but it’s also you know not as powerful as neural mt basically so the segmenting into phrases is easy the reordering is not easy but possible but in order to translate each phrase you need parallel data for this that’s a problem in unsupervised MT of course"
  },
  {
    "objectID": "notes/cs11-737-w10-unsupervised-NMT/index.html#unsupervised-statistical-mt",
    "href": "notes/cs11-737-w10-unsupervised-NMT/index.html#unsupervised-statistical-mt",
    "title": "Unsupervised Machine Translation",
    "section": "Unsupervised Statistical MT",
    "text": "Unsupervised Statistical MT\n\nThe way unsupervised statistical mt could work which is also detailed in these two papers by arteza in limple is you learn monolingual embeddings for unigrams diagrams and trigrams you initialize phrase tables from cross-lingual mappings of these embeddings so basically you initialize the phrases that could be used in a phrase-based machine translation system and then you do supervised training based on back translation and iterate so what this what this means is basically you take the phrase tables from cross-lingual mappings you estimate a language model on the target language and then you just translate all of the all of the monolingual data and after you’ve translated all the monolingual data then you can then feed it into your like normal machine translation training pipeline and the key here is that you know you’re inducing like each of these phrase translations just by mapping embeddings of unigram’s diagrams and trigrams cool and what we can see here is that if we start out with the unsupervised phrase table and translate from French to English we get a blue score of 17.5 and then as we add more iterations and translate and translate and translate and learn from the translated data the score gets a bit better every time. Basically, it’s this iterative process of recreating the data back translation in one direction the other direction one direction the other direction in training.\nUnsupervised neural mt the way this works is we create a neural MT system and actually the exact same procedure could also be done for neural mt with the caveat that you can’t create a phrase table so you would need another you would need another method for learning the initial model because you can’t like induce a phrase table from embeddings so in addition to using that same procedure there’s one thing that you can do to improve the neural MT model and basically the way it works is you take the encoder-decoder model and you use the same encoder-decoder for both languages and another thing that you can do is you can initialize the inputs with cross-lingual embeddings and the idea of what you do here is you train the model to output French but you have the inputs be either French so it’s like an auto encoding objective or English here so if you have this French token here this is basically saying, I want you to generate french next so the model is you know basically guaranteed to generate French and but because these input embeddings are initialized with bilingual like coordinated embeddings the inputs look very similar so it’s like the inputs look similar I know I want to generate French, so if we just train on this encoding objective in the bottom which we can do for monolingual data it nonetheless learns how to translate is the hope and dream so we have a cool we have a couple objectives objective is the denoising"
  },
  {
    "objectID": "notes/cs11-737-w10-unsupervised-NMT/index.html#unsupervised-mt-training-objective-1",
    "href": "notes/cs11-737-w10-unsupervised-NMT/index.html#unsupervised-mt-training-objective-1",
    "title": "Unsupervised Machine Translation",
    "section": "Unsupervised MT: Training Objective 1",
    "text": "Unsupervised MT: Training Objective 1\n\nAutoencoder objective and so what we do is we take the target sentence the source sentence we map it into a denoising autoencoder and what a denoising autoencoder does is basically we have a sentence in the source side we add some according to this corruption function c oops this corruption function c so we move x into some other place c we map it into a latent space and then we try to regenerate the original x and I’ll give an example of this in a moment and the other objective that we can use an unsupervised NMT is back translation so what we do is we translate the target to the source and we use this as a quote-unquote supervised example to translate the source to the targets.\nSo one example of but the noising objective would be just to like cross off individual words here and try to reproduce the original sentence from the crosstalk chords so we set their word embeddings to zero or something like this and so that would be going from this x to the c x and then we could just run a seq2seq model to try to take in this noise input and generate the output so that would be one example and in this particular case what we could do is we could try to translate this into the into another language and this translation could either be done with like a pre-trained model that we have already from our a previous iteration about unsupervised translation or it could be done with some sort of heuristic like mapping the words in the input and I think I have a slide about that coming up soon and so the reason why it works is as I said before cross-lingual embeddings and the shared encoder decoder gives the model a good starting point where translating from this is similar to translating from from this here and so then another objective that people use to further enforce this is an adversarial objective and what they try to do essentially is you have an auto encoder example or a back translation example and you apply an adversarial objective to try to force these two phrases together so this is also similar to what they use an unsupervised like word translation where you have a model and you try to get these encoder vectors to be so similar that you fool the model and it’s not able to distinguish between them yes yeah so actually mast lms are a variety of denoising autoencoding it’s basically a subset of denoising yeah well so here we’re not using any parallel data whatsoever so the back translation is we’re not using any real parallel data all of the parallel data we have is obtained through like that translation for example. ## How does it work? Basically there’s two ways that you can you can do this the the first way is with just using an auto encoding or denoising auto encoding objectives so to take the example here we have like I am a student and and so if you have bilingual embeddings or something like that where bilingual embeddings gotten out close together basically you know these word events look the same these sport embeddings maybe look the same and these were embedding the same so overall and so like despite the fact that what you want to do is on the bottom the vector the light blue vectors on the top if each of the word wrappers look similar also looks similar and then you can also do things like randomly masking out words to make it look a little bit like basically to make the problem of auto encoding more difficult and so you need to fill in more information so when you move from this queen English over here with some without words to French you know the problem also gets harder so you kind of need to infer missing things or differences from this but because you’re doing denoising it allows you to do that for back translation back translation is basically like what we talked about before it’s like more or less the same so actually perhaps the more important thing is yeah so wouldn’t that lead to more hallucination from the model yeah basically yes I think it’s going to be very hard to train a model that’s that works perfectly with through purely unsupervised translation but the idea one thing is that any mistakes in your one reason why back translation works in the first place is any mistakes in your training data tend to be more random than the actual correct training data so as you refine the model more and more and get a better and better model the hallucinations because they’re random tend to get washed out whereas the like correct translations tend to be reinforced because they’re more consistent so it still will hallucinate and make mistakes but hopefully the idea is that hopefully they get washed out it works yeah because yes we do not directly use this move by statement during the training the veterans practice later is was trained based on this data so somehow that we use the difference between not that of dsd yeah so this this slide is a little bit deceptive because this would be the ## Step 2: Back-translation example of like supervised back translation here in unsupervised back translation in the back translation in the unsupervised like MT paradigm basically you don’t use any parallel data you just use a denoising auto-encoding objective to seed your back translator and then use that to generate data so you’re never using any like actually parallel data yeah so basically you start out with just a model training using monolingual data so for example, English is ## Performance This is a graph from the original paper. I think there’s a big caveat in this graph so you need to be a little bit careful in interpreting it but basically the horizontal lines are an unsupervised translation model that uses lots of monolingual data but no parallel data. The non-horizontal lines are a model are the supervised translation model and basically what they’re showing here is with no monolingual no parallel data they’re able to achieve scores of about the same level as something trained with 10 to the five so 10 thousand parallel sentences a big caveat here is that they didn’t use any monolingual data in the in the supervised translation system so if they did the supervised translation system would probably be a lot better but still I mean it’s kind of interesting that you’re able to do anything at all with unsupervised translations so I think as long as you’re aware of their caveats it’s kind of an interesting graph·yeah so another so basically I’ll go to the open problems in unsupervised mts so basically this is exactly the problem that Ellen was pointing out which is unsupervised machine translation works in ideal situations so basically languages are fairly similar written with similar writing systems large monolingual data sets in the same domain and match the test domain so in this particular case they were using data from the European parliament and it basically the data in the English and the French or the English and the Germans was from exactly the same distribution and that really helps in like inducing the lexicon or doing translation and so when you have less related languages truly low resource languages diverse domains less monolingual data unsupervised machine translation performs less well and reasons for performance basically small monolingual data in low resource languages can result in bad embedding so if we don’t have lots of data in the low-resource language this won’t work because the embeddings will be too poor to get a good initialization different word frequencies or morphology like the English and Turkish example I talked about before also different content makes things like back translation are less effective in bootstrapping a translation model. So for example if you’re trying to translate Twitter in one language and you have news text in another language that’s not like back translation is not gonna be good for covering what is said on Twitter so just in an interest of time I’ll skip that one but so there are some things that can be used to improve so better initialization recently people have been using things like cross-lingual language models or multilingual language models to improve the initialization of multi of unsupervised translation models so things like masked language modeling across language. So basically what you what you can do is you can train a single monolingual or bilingual single multilingual language model as your encoder or both your encoder and your decoder and you use that to initialize the translation model and this is good because you’re initializing the whole model as opposed to just the input and for various reasons it’s nice to have it’s nice to have a single model that is trained on all the languages and just to give one example even in like Chinese or something that’s written an entirely different script than English there’s still lots of English words so if you’re training the English model in the Chinese model on tons and tons of monolingual data the English words can also help anchor you know things into the same semantic space because they appear in various languages as well another thing is masked sequence sequence modeling so there’s things like mass which basically they have an encoder decoder form formulation of mass language modeling where basically you mask out a piece of the input and you generate that masked-out piece of the output and recently a model that lots of people have been using is this mBART model multilingual bart model and the way it works is basically you mask out words in the input but then you you generate all of the words in the outputs so then you you basically train this on tons and tons of data it’s a pre-trained model that you can then use to initialize your downstream unsupervised NTM And I had some stuff about the unsupervised multilingual 70. I’m going to skip over that in the interest of time, but how practical is this strict unsupervised scenario? So one thing that I definitely can say is that a lot of the techniques that are used in unsupervised mt are practical and semi-supervised learning scenarios where we have a little bit of training data so we can either train the model first with an unsupervised method and then fine-tune using the parallel corpus or train the model using a parallel corpus and update with iterative back translation and part of the reason why it’s why this is particularly good is there’s very few languages in the world that don’t have any parallel data but have lots of monolingual data for example almost every language in the world has parallel data from the bible if it has any amount of monolingual data but the bible is very out of domain very you know small so because of that using that to speed a model but then doing basic almost unsupervised translation seems like a practical way to do like news translation or something like that and then another area where these techniques are practical is if you have a task where you like legitimately cannot get very much data whatsoever so one example being style transfer from informal to informal text where you know there’s not very much data especially in different languages. Another example that has been used recently is the translation between Java and Python for example where there’s like lots of Java lots of Python but very little you know very little parallel data there’s some parallel data but not very much so yeah the discussion question is pick a low resource language or dialect research all of the monolingual or parallel data that you can find online for it would unsupervised or semi-supervised mt methods be helpful and how could you best use the existing resources to set up an unsupervised or something supervised empty for success on this language or dialect and the this is a reference to a paper that you could take a look at to discuss that so cool any questions before we start the discussion."
  },
  {
    "objectID": "notes/cs11-737-w12-qa/index.html",
    "href": "notes/cs11-737-w12-qa/index.html",
    "title": "Multilingual Q&A",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\nVideo 1: Lesson Video\n\n\n\n\n\n\n\n\nThis week’s slides\n\n\n\n\nSupplementary Figure 1"
  },
  {
    "objectID": "notes/cs11-737-w07-data-driven-NMT/index.html#machine-translation-evaluation",
    "href": "notes/cs11-737-w07-data-driven-NMT/index.html#machine-translation-evaluation",
    "title": "Data-driven Strategies for NMT",
    "section": "Machine Translation Evaluation",
    "text": "Machine Translation Evaluation\n\nMT evaluation is a very important and difficult topic in doing machine translation research. I think we’ve gotten to the point where almost evaluating how well we’re doing is maybe as difficult as like actually doing the translation itself The reason for this is: if we output a translation there are many different correct translations. We could have paraphrases where the output is “this is a dog”, “I see a dog”, “there is a dog” here other things like this and all of those would be appropriate for an equivalent sentence in another language."
  },
  {
    "objectID": "notes/cs11-737-w07-data-driven-NMT/index.html#manual-evaluation",
    "href": "notes/cs11-737-w07-data-driven-NMT/index.html#manual-evaluation",
    "title": "Data-driven Strategies for NMT",
    "section": "Manual Evaluation",
    "text": "Manual Evaluation\n\nThe basic evaluation paradigm in MT. There’s two different types. There’s human evaluation or manual evaluation and automatic evaluation. This is the basic evaluation paradigm for automatic evaluation. Where basically what we do is we have a parallel test set where we have an input and an output in the language pair that we’re interested in. We use a system to generate translations and we compare the target translations with references. Before I talk a little bit more detail about that I’d like to talk about kind of manual evaluation. This is the gold standard of doing evaluation of translation systems. Where basically we ask human evaluators to check whether the answer is correct or not. To give an example we’ve taken a source side sentence we generate some outputs and you can either evaluate by having a human evaluator look at the source in the output or look at a reference and in the output and the reference would be like the so-called correct translation looking at the source and the output only works if you’re bilingual in both languages and it’s somewhat difficult to get bilingual speakers or at least more expensive however given the quality of machine translation systems nowadays very often you don’t know if the output of a human translator like your reference is better than the machine translation system so like very often if you hire a person to do evaluation you know they might not try super hard and like I mentioned before so kind of the gold standard is to get somebody who knows the source and the target to do the evaluation there’s a number of different axes along which you can do evaluation I just listed a couple of them here one is adequacy and adequacy is basically whether the meaning of the translation is conveyed properly and the in this case this is the correct answer here you would know this if you knew japanese which you know most people don’t but if you knew japanese you would know the first one is correct so this is perfectly adequate it conveys the target message the middle one is conveys the target message but is this fluent so it would score high on an adequacy scale but on a fluency scale it would score low the one over here is fluent but not adequate so basically it switched the subject to that object for order so it would be wrong and one notable thing about fluency is you don’t need to know the source language to evaluate fluency all you need to know is the target language because it only has to do with whether the output is fluent or not you can also do pairwise evaluation which just says which one of these is better one of the good things about pairwise evaluation is it’s very simple because you just ask question which do you like better which do you think is a better translation the problem with it is it doesn’t give you an absolute idea of how well you’re doing so if you have two really bad systems and say which is better one might be better but they’re both really bad if you have two really good systems and say which is better one might be better but they’re both really good so kind of absolute scales have that advantage another thing is just like you might get bad translators you might get lazy evaluators you know if you hire people on mechanical turks and they’re not very motivated for example so you need to be careful about quality control as well"
  },
  {
    "objectID": "notes/cs11-737-w07-data-driven-NMT/index.html#human-evaluation-shared-tasks",
    "href": "notes/cs11-737-w07-data-driven-NMT/index.html#human-evaluation-shared-tasks",
    "title": "Data-driven Strategies for NMT",
    "section": "Human Evaluation Shared Tasks",
    "text": "Human Evaluation Shared Tasks\n\nThere’s a human evaluation shared tasks so the most famous one is the conference on machine translation shared tasks. I can show you a little bit what this looks like. They basically have a whole bunch of tasks that you can participate in most of these are tasks for actual translation, but they also have evaluation tasks on metrics and quality estimation so basically what you try to do here is you try to create a metric that has the highest correlation with with human evaluation and for quality estimation what this is is this is essentially evaluation without a gold standard reference so you’re just given the input and the output and you want to guess how good the system output is and this is harder obviously because you don’t have an example of what a good translation looks like but it’s also very useful in practical situations where like let’s say you’re a machine translation company and you want to decide whether you need to get a human translator to go in and check the output and correct it or something like that so if that’s the case if you can estimate very accurately whether the input and output are correct or not then that would save you money save you time give you confidence in the results"
  },
  {
    "objectID": "notes/cs11-737-w07-data-driven-NMT/index.html#blue-scores",
    "href": "notes/cs11-737-w07-data-driven-NMT/index.html#blue-scores",
    "title": "Data-driven Strategies for NMT",
    "section": "Blue Scores",
    "text": "Blue Scores\n\nThere’s also other leaderboards and stuff for other seq2seq models but that’s a little bit less important for this multilingual class. There are other metrics like blue scores so blue score is very famous. You know if you’ve done any research on machine translation or even heard of it you probably encountered blue. The exact details of how blue is calculated are what you do is you take the precision of engrams output by the system so for example if you look at the unigram precision that’s out of all the unigrams that the system output how many of them exist in a reference or one of the references that you’re provided if in the case that you’re using multiple references and this gives you a position of like three out of five and then you take the geometric mean of engrams usually one to four and then you also have a penalty for outputting two short sentences because one way to improve your precision is to not output very many things so this is basically to prevent you from gaming system but the important thing is now the details of how blue is calculated probably the important thing is that this is a lexical metric which means that you’re just doing exact match with the references and this has a few issues. One of the issues is essentially that you so there’s there’s two major issues that cause blue to either underestimate how good a translation is or overestimate how good a translation is blue tends to underestimate how good translations are when the translations are paraphrases of the true reference so if you for example had I have like i went I went to the store and bought a book yesterday and you compare that with yesterday I bought a book at the store those are almost identical in meaning but they would get a low blue square because i’ve just rearranged the phrases a little bit so that’s when blue tends to underestimate scores blue tends to overestimate scores when you get very critical words in the sentence wrong but get everything else right\n\nfor example\n“could you please send this big package to philadelphia”\nturns into:\n“could you please send this big package to japan”\nThat would get a very good blue score because most of the words are the same but your package would go to Japan and that’s probably not what you intended by that, otherwise right.\nThat’s the the downside of blue. It’s basically not smart enough with respect to these."
  },
  {
    "objectID": "notes/cs11-737-w07-data-driven-NMT/index.html#shortness-penalty",
    "href": "notes/cs11-737-w07-data-driven-NMT/index.html#shortness-penalty",
    "title": "Data-driven Strategies for NMT",
    "section": "Shortness Penalty",
    "text": "Shortness Penalty\n\nYhe brevity the brevity penalty basically gives you a penalty if your output is shorter than the intended output. If the reference is like 20 words then you’re and your sentence is 15 words you would get a penalty of about 0.75 It’s not exactly like just the ratio it actually drops off faster and stuff like that but that’s a basic idea. That’s a really good question so you pay a penalty in your precision your precision goes down by a lot because there’s no way to get good precision if you output too many things. One thing you should know about BLEU if you’re using it in your research which you might is that it’s very very sensitive to the length. So if your length is a little bit too short or a little bit too long it hurts your BLEU really badly. That’s another problem with BLEU essentially is that it’s not sensitive to paraphrases that are too long or too short as well."
  },
  {
    "objectID": "notes/cs11-737-w07-data-driven-NMT/index.html#bert-score",
    "href": "notes/cs11-737-w07-data-driven-NMT/index.html#bert-score",
    "title": "Data-driven Strategies for NMT",
    "section": "Bert Score",
    "text": "Bert Score\n\nRecently in the past three years or so, there’s been a huge improvement in embedding based metrics which are basically metrics that you know take advantage of recent NLP techniques and one of the first ones was bert score and these can be separated into unsupervised metrics and supervised metrics unsupervised metrics require no annotated data of whether a translation is a good translation or a bad translation supervised metrics are trained to basically regress to an estimation of how good a transplantation is or not so a Bert score is an unsupervised metric that’s based essentially on the similarity between burton betting so it has this matching algorithm where you basically for each word in the output you try to find you know how good it matches with one of the words in the input and this is good because it can do things like handle paraphrases as long as the paraphrases have similar bert embeddings another famous one is BLEUrt"
  },
  {
    "objectID": "notes/cs11-737-w07-data-driven-NMT/index.html#bluert",
    "href": "notes/cs11-737-w07-data-driven-NMT/index.html#bluert",
    "title": "Data-driven Strategies for NMT",
    "section": "BlueRT",
    "text": "BlueRT\nWhat they did essentially was they trained bert to predict human evaluation scores so they essentially solve a regression problem from the sentences like the reference in this system output to an evaluation score so they’re just going in directly predicting evaluation and they have a bunch of other tricks like unsupervised training where they try to predict BLEU or rouge or other lexical metrics beforehand and that makes it more robust the favorite one that we use in our Comet research on mt now is a comet and comet is also similarly it trains the model to predict human evaluation but in addition to using the just the system output in reference it also uses a source sentence which means that essentially I talked about human evaluation right where you can either ask a human evaluator to look only at the the reference or also look at the source and for a similar reason to why we would like a speaker human speaker to do that it’s also useful to have the model do that because the model can look at the source and see if the information is reflected in the target"
  },
  {
    "objectID": "notes/cs11-737-w07-data-driven-NMT/index.html#bart-score",
    "href": "notes/cs11-737-w07-data-driven-NMT/index.html#bart-score",
    "title": "Data-driven Strategies for NMT",
    "section": "Bart Score",
    "text": "Bart Score\nand the final one prism’s another one based on paraphrasing a final one is a bart score environment score is one that I’m a co-author on this is a unsupervised metric that is based on basically a generative model that tries to generate the the system output using the reference or the reference using the system output or the source using the system output et cetera et cetera and bart score I think is good because it’s unsupervised like birth score but it’s essentially more accurate and more controllable so you can do things like calculate recall calculate a precision and other things like that so if that sounds interesting you can take a look at the paper as well but basically if you’re doing empty I would suggest using comment now because it’s well supported it has a nice package it’s pretty widely tested and follow-up reports have suggested that it has very good correlation with human evaluation so that’s my suggestion you can"
  },
  {
    "objectID": "notes/cs11-737-w07-data-driven-NMT/index.html#meta-evaluation",
    "href": "notes/cs11-737-w07-data-driven-NMT/index.html#meta-evaluation",
    "title": "Data-driven Strategies for NMT",
    "section": "Meta Evaluation",
    "text": "Meta Evaluation\nMeta evaluation runs human evaluation and automatic evaluation on the same outputs and calculates the correlation this is what they do in the wmt shared tasks like the wmt metrics task for mt other things for summarization etc and one interesting thing is that evaluation as I mentioned at the beginning is pretty hard especially with good systems so most metrics actually had no correlation with human eval over a subset of the best systems at some of the wmt 2019 tasks which means that basically all of the evaluation metrics we had were kind of broken on like evaluating really good mt systems so fortunately now we have comment we have other things like this that actually seemed to be a lot more robust but it was a major problem so you basically calculate the correlation you calculate pearson’s correlation there’s experiments correlation and the way you do that is you human you do human eval of a whole bunch of sentences or humans develop a whole bunch of systems and you try to find the metric that has the highest correlation between the evaluation scores of the systems and the evaluations given by the humans yep all right they often don’t support that many languages well so that’s a good that’s a really good question I many of them do use something like embert or xlmr which support a lot of languages xlmr actually envert’s a little bit more biased xlmr has pretty good coverage of the most common languages in the world but of course as you go down to less well resource languages that’s going to continue to be a problem there you might be stuck with BLEU for now but honestly if you have really bad systems really bad empty systems I still think BLEU is probably good enough in many cases oh another option is carefu chrf and that’s a character-based evaluation metric for mt that’s particularly good for languages with like rich morphology or something like that so I think when you’re working with low resource languages your mnt systems are also going to be really bad so any metric you have is still going to be like reasonably good at measuring progress so"
  },
  {
    "objectID": "notes/cs11-737-w07-data-driven-NMT/index.html#database-strategies",
    "href": "notes/cs11-737-w07-data-driven-NMT/index.html#database-strategies",
    "title": "Data-driven Strategies for NMT",
    "section": "Database Strategies",
    "text": "Database Strategies\nThe next thing I’d like to talk about is the database strategies to low resource mt there’s not a whole lot of content here so I’ll try to go through it rather quickly to leave time for the discussion but basically we have data challenges in the resource mt so mt of high resource languages with large parallel corpora gives us you know very good translations but low resource languages with small parallel corporate you just train there you can end up with nonsense so this is an example of a system trained on 5 000 languages and the most frequent failure state is basically that a neural nt system will just spit out something that has nothing to do with the original inputs there’s a famous example of this so that says why is google translate spitting out sinister religious prophecies and basically if you put in a dog dog dog dog dog in maori it outputs doomsday clock is three minutes at 12 we are experiencing characters in a dramatic development in which jesus returns [Music] can you guess why this happened exactly they use bible data in training their system and when you use bible data and training your system and your system doesn’t know what to do because it has so few resources or it sees something it doesn’t know it just reverts to using the language model and basically outputs whatever the language model thinks and thinks it looks likely and so you know if your system is trained on bible data that looks like the bible if your system’s trained on something else it looks like something else High and Low Resource Languages so you know that’s basically what happened here as well so some ways to fix this we can transfer from high resource languages to low resource languages so basically what you do is you train on a high resource language or multiple high resource languages and then you adapt to the low resource language one the simplest way to do that is just to continue fine tuning on the low resource language you can also do joint training with the low resource language in the high resource language so just concatenate all the data together and in training so this is okay but there are some problems with this as well one problem is a sub-optimal lexical or syntactic sharing and another problem is it’s not possible to leverage monolingual data because you still require a parallel data here and I’m going to be talking more about like lexical overlap and loanwords and stuff in in the next class so I’ll cover that more there but basically suffice to say the high resource language and the lower resource language are different so training on different data is sub-optimal for information sure"
  },
  {
    "objectID": "notes/cs11-737-w07-data-driven-NMT/index.html#data-augmentation",
    "href": "notes/cs11-737-w07-data-driven-NMT/index.html#data-augmentation",
    "title": "Data-driven Strategies for NMT",
    "section": "Data Augmentation",
    "text": "Data Augmentation\nso if we think about data augmentation data augmentation is basically generating other data that looks like the data that you want to have the very convenient thing about this is generating more training data and feeding it into your existing system is easy but effective in in improving mt performance so it’s actually a pretty widely used technique now so if we look at the available resources we might have a low resource language parallel data a high resource language parallel data and also for example target data which is monolingual hence the m here and what we do is we would like to create augmented data where we have target data and like pseudo low resource language data and train our model on this with the idea being that if we can create this this will be closer to our final evaluation scenario where we where we want to generate the target given a low resource language"
  },
  {
    "objectID": "notes/cs11-737-w07-data-driven-NMT/index.html#back-translation",
    "href": "notes/cs11-737-w07-data-driven-NMT/index.html#back-translation",
    "title": "Data-driven Strategies for NMT",
    "section": "Back Translation",
    "text": "Back Translation\nso the first example is a back translation and the way back translation works is basically we train a target two low resource language system and we take our monolingual target data and we generate fake low resource language data by translating the target data into the low resource language data so this is how it works we take our target to low resource language system we back translate using the system and then we train a low resource language to target language system using the concatenation of this augmented data in the original data and the key point here is that when we are when we’re training like a sequence sequence model or a machine translation model we’re we’re training it to do two things we’re training it to do language modeling on the target side only and we’re trying to do mapping between the source side and the target side and in order to do language modeling we only really need good target site data so even if there’s some degree of error in this like low resource language here we’ll still be able to learn target site data and we’ll be able to learn a the language model from target side data and we’ll be able to learn a mapping you know even if it’s imperfect from the low resource language to the target language"
  },
  {
    "objectID": "notes/cs11-737-w07-data-driven-NMT/index.html#training-schedule",
    "href": "notes/cs11-737-w07-data-driven-NMT/index.html#training-schedule",
    "title": "Data-driven Strategies for NMT",
    "section": "Training Schedule",
    "text": "Training Schedule\nso there’s a couple ways to generate translations when doing back translation the first one is using beam search oh sorry yeah yeah that’s a really that’s a really good question so the question was is there any sort of training schedule that you use when you do this so the the kind of quote-unquote obvious training schedule that you might do is you might train jointly on both of them at first and then fine-tune on this data over here and that would make sense because you know this is good data this is like actually translated data however there’s another issue which actually is not super obvious at first but it’s maybe obvious in hindsight which is that if this data is all from the bible and then you want to translate news then actually fine tuning on bible data will be really out of domain and cause issues for you so in fact in the original black translation paper they threw away this data and only trained on this because it was more in domain and that ended up giving better results but that was predicated on the fact that they have a good you know batch translation system in the first place so it’s not necessarily clear what the ideal schedule is but you would almost certainly benefit from some sort of schedule or balancing or something but that’s a complicated hyper parameter so because it’s a complicated hyper parameter it’s also very common to just concatenate the two and these are good details to know for assignment too by the way because they might make a difference in your final scores"
  },
  {
    "objectID": "notes/cs11-737-w07-data-driven-NMT/index.html#generating-translations",
    "href": "notes/cs11-737-w07-data-driven-NMT/index.html#generating-translations",
    "title": "Data-driven Strategies for NMT",
    "section": "Generating Translations",
    "text": "Generating Translations\n\nHow to generate translations?\nBeam search is one way and basically what this is doing is selecting the highest scoring output. This was done in the original paper. This has the advantage of having higher quality but also lower diversity in the outputs and the potential for bias. You might like for example one result is beam search tends to mostly output pronouns from the majority gender because they’re over-represented. You might get only get male inflections if you do beam search. That’s the type of data bias that could result from here. The other option is sampling. What you do is you randomly sample from the back translation model which gives a lower overall quality but higher diversity. Most reports say this works better at the moment. We had a recent paper which I’m going to introduce in a second but this has kind of a theoretical explanation for why we think sampling should be better which is that it’s a better model of the underlying data distribution that we’re trying to model. I think I’m pretty firmly a believer that sampling is the way to go there’s also a method of iterative back translation."
  },
  {
    "objectID": "notes/cs11-737-w07-data-driven-NMT/index.html#metaback-translation",
    "href": "notes/cs11-737-w07-data-driven-NMT/index.html#metaback-translation",
    "title": "Data-driven Strategies for NMT",
    "section": "Metaback translation",
    "text": "Metaback translation\njust one example of this this is a paper that I just talked about but we have a paper called meta back translation which I think is kind of a an interesting idea so normally when we’re training this system to train the the low resource language to the target language system we’re back propping the gradient from the slow resource language data but we can also do a back propagation step where we basically train oh sorry that arrow is thrown I apologize so the arrow actually should be going from here around this to here so the basic idea I’ll fix this later in the slides but the basic idea is we use the signal that we get from from training the final system that we want to train to update the parameters of the back translation system so we’re essentially training the ideal back translation system to train a good forward translation system so this is a I like the idea behind here which is basically the final goal of the back translation system is to improve the forward translation system so we can directly optimize it to do this"
  },
  {
    "objectID": "notes/cs11-737-w07-data-driven-NMT/index.html#papers",
    "href": "notes/cs11-737-w07-data-driven-NMT/index.html#papers",
    "title": "Data-driven Strategies for NMT",
    "section": "Papers:",
    "text": "Papers:\n\nXia et al. (2019) Generalized data augmentation for low-resource translation.\nTransfer HRL to LRL\n\nZoph et al. (2016) Transfer Learning for Low-Resource Neural Machine Translation\n\nNguyen and Chiang (2017) Transfer Learning across Low-Resource, Related Languages for Neural Machine Translation\n\nJoint training with LRL and HRL parallel data\n\nJohnson et al. (2017) Google’s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation\nNeubig and Hu (2018) Rapid Adaptation of Neural Machine Translation to New Languages\n\nBack Translation\n\nSennrich, Haddow, and Birch (2016) Improving neural machine translation models with monolingual data.\nEdunov et al. (2018) Understanding Back-Translation at Scale\nPham et al. (2021) Meta Back-translation\n\nCurrey, Miceli Barone, and Heafield (2017) Copied Monolingual Data Improves Low-Resource Neural Machine Translation\nFadaee, Bisazza, and Monz (2017) Data Augmentation for Low-Resource Neural Machine Translation\nWord-by-word Data Augmentation\n\nLample et al. (2018) Unsupervised Machine Translation Using Monolingual Corpora Only\n\nWord-by-word Augmentation w/ Reordering\n\nZhou et al. (2019) Handling Syntactic Divergence in Low-resource Machine Translation"
  },
  {
    "objectID": "notes/cs11-737-w07-data-driven-NMT/index.html#iterative-back-translation.",
    "href": "notes/cs11-737-w07-data-driven-NMT/index.html#iterative-back-translation.",
    "title": "Data-driven Strategies for NMT",
    "section": "Iterative back translation.",
    "text": "Iterative back translation.\n\nIterative back translation is particularly useful when you have a large monolingual data in both languages. Again the idea is simple you train a low resource language to target system first. This is going in the direction you originally want to translate. You generate pseudo data with the target language. You use that to train your target to low resource language system. You back translate and then you use this to train your final system so this is now you have three systems your forward translation data augmentation system your back translation data augmentation system and your forward translation final system you can do this as many times as you want obviously you could also do it on the fly in the process of training the system so this can become arbitrarily complicated if you want"
  },
  {
    "objectID": "notes/cs11-737-w07-data-driven-NMT/index.html#metaback-translation-issues",
    "href": "notes/cs11-737-w07-data-driven-NMT/index.html#metaback-translation-issues",
    "title": "Data-driven Strategies for NMT",
    "section": "Metaback translation issues",
    "text": "Metaback translation issues\nso there are a couple issues here the first issue is that back translation often fails in low resource languages or domains and as a solution one thing that we can do is we can use other high resource languages or we can combine them with monolingual data maybe with denoising objectives which we’re covering in a following class and we can perform other varieties of rule-based augmentation so I’m gonna go through these in a little bit in maybe in a few minutes so also actually we’ll have discussion about about these two so maybe I’ll just briefly explain the idea and we can discuss more in the discussion for people who read those papers so using high resource languages High resource languages augmentation and augmentation the problem is target to low resource language back translation might be very low quality so the idea is we can also use a high resource language that’s similar to the low resource language and basically for example if we have something like azerbaijani in turkish azerbaijani and turkish are very highly related so maybe we could use information from azerbaijani to english translation back translate into az into turkish which is certainly going to give us higher quality data and use that to augment our data for azerbaijani english system and then we can just throw away this azerbaijani data that we know is not going to be very useful so that would High resource languages pivoting give us additional high resource language to target language data and another thing we can do is we can augment via pivoting and so basically what that does is that gives us data where we take the high resource language data and we translate that into the low resource language and presumably translation from the high resource language to low resource language is easier because these languages are more related so basically what this does is that gives us a better like low resource language pseudo data here and we can also do a similar thing where we generate more high resource language data and this basically gives us three different ways to create this pseudo-parallel data between the low resource language and target language another simple trick this is kind of"
  },
  {
    "objectID": "notes/cs11-737-w07-data-driven-NMT/index.html#monolingual-data-copying",
    "href": "notes/cs11-737-w07-data-driven-NMT/index.html#monolingual-data-copying",
    "title": "Data-driven Strategies for NMT",
    "section": "Monolingual data copying",
    "text": "Monolingual data copying\nlike frustratingly effective at improving your models is monolingual data copying and the issue is that back translation may help with structure but one of the issues with the resource language systems is that they tend to fail really badly on unusual vocabulary so like for example proper names or something like this so you might get a back translation system that’s very good at getting the structure right but get it gets you know all of your proper names and entities incorrect so basically one thing that you can do here is you just copy the target data into the source data and then you’re done and this kind of guarantees to maintain the entities so or the the rare words so that will help mitigate these issues of like vocabulary being dropped yeah something to point out with copying is that even in languages with different scripts it seems to work really well. &gt; Maybe because of auto and clutter objective stuff yeah even in languages with different scripts"
  },
  {
    "objectID": "notes/cs11-737-w07-data-driven-NMT/index.html#transfer-learning",
    "href": "notes/cs11-737-w07-data-driven-NMT/index.html#transfer-learning",
    "title": "Data-driven Strategies for NMT",
    "section": "Transfer learning",
    "text": "Transfer learning\n\nThere’s actually a nice paper by the same authors who wrote this paper where they examine this. Basically they are trying to figure out transfer learning. Why does transfer learning have this positive effect? One of the things that they show is that even making sure the length is the same or approximately the same or making sure that the words are output in approximately the same order as the input is is effective for improving translation accuracy. If you have a low resource language the translation system might drop half the content or it might like totally mess up the order or something like this This paper is demonstrating that kind of just like a monotonic bias and a bias towards outputting approximately the same number of words gets you a long way in improving the results which of course monolingual data copying would also do"
  },
  {
    "objectID": "notes/cs11-737-w07-data-driven-NMT/index.html#word-alignment",
    "href": "notes/cs11-737-w07-data-driven-NMT/index.html#word-alignment",
    "title": "Data-driven Strategies for NMT",
    "section": "Word alignment",
    "text": "Word alignment\n\nIn order to do this they need to use a tool called word alignment What Word alignment does is, it essentially takes in two parallel corpora. The parallel corpora you want to find which words align to each other in the source and target sentences. This is useful for a number of reasons it’s useful for analysis it’s useful for cross-lingual transfer learning I talked about supervised alignment as a training method last time I believe and there’s a couple methods to do so there’s again traditional symbolic methods which like BLEU are based on exact lexical match or you know some variety of clustering\n\ngiza ++ has some clustering involved in it but recently neural methods have been largely outperforming these and I can recommend a highly our aligner called awesome lion I i didn’t name it I’m far too humble to name my alignment or awesome line but but it’s pretty awesome I have to say and basically it uses multilingual perks and it tries to find things that are similar but it’s also fine-tuned multilingually on supervised data so basically there’s some supervision that goes into it to try to inform the aligner about the outputs and it works on any language that’s included in mvert again like the question before it won’t work on very low resource languages of course so then you might be stuck with keystone plus plus and faster Word by word data augmentation so you can also do things like word by word data augmentation where you simply translate sentences word by word into the target sentence using a dictionary this is another frustratingly you know effective method like monolingual data copying however there are problems like word order and syntactic divergence so if you get like I the new car bought number one the order is strange number two these words don’t actually align with each other so that’s a problem so Reordering other things you can do or you can try to decrease this divergence with reordering or rules so this was also another paper in the potential reading and basically what the idea is that you a priori do some reordering from one language from english into like reordered english and then do data augmentation on top of that and the good thing about this is like english has a lot of analysis tools you could like do syntactic parsing of english get the syntactic structure build reordering rules on top of that and then just apply dictionary-based translation and then the hope would be that you would get something that looks a lot more like japanese than if you just translated english word by word and one interesting thing we showed here was we demonstrated that this was useful for japanese translation but then we applied the exact same reordering rules and also applied it to wigger which is another language that’s completely different different language family but it’s it has a very similar syntax to japanese so because of that the exact same reordering rules for english were still effective in improving the results for weaker english translations so because of that you know it’s not language dependent it’s rather syntax dependent and because there’s syntactic similarities between the language it helps so yeah given that we now have the"
  },
  {
    "objectID": "notes/cs11-737-w07-data-driven-NMT/index.html#assignment",
    "href": "notes/cs11-737-w07-data-driven-NMT/index.html#assignment",
    "title": "Data-driven Strategies for NMT",
    "section": "Assignment",
    "text": "Assignment\nassignment actually this is this slide is missing one of the one of the papers that was a potential paper to read so first before we go to the discussion are there any questions so I kind of breezed through it the last part quickly but hopefully we can also talk about them in the discussion okay if not this time we’re going to try a new experiment we’re going to try to make six groups so the groups are going to be half the size and they’re going to be front middle front right front left back middle back left back middle in that right so we’re gonna ask everybody to talk a little bit more quietly but also you’ll be in a smaller circle so hopefully that’ll be easier and yeah let’s go ahead and actually guys since we’re running a little bit late I think maybe we’ll skip the reporting part this time is that okay and we’ll just you know be within our groups and if there’s anything really interesting we can share on piazza or something okay"
  },
  {
    "objectID": "notes/cs11-737-w01-inro/index.html#papers",
    "href": "notes/cs11-737-w01-inro/index.html#papers",
    "title": "Text Classification",
    "section": "Papers",
    "text": "Papers\n\n\nESPnet"
  },
  {
    "objectID": "reviews/paper/2022-nakdimon/index.html",
    "href": "reviews/paper/2022-nakdimon/index.html",
    "title": "Restoring Hebrew Diacritics Without a Dictionary",
    "section": "",
    "text": "Litrature review\n\n\n\n\n\n\n\nVideo 1: Presentation Video"
  },
  {
    "objectID": "reviews/paper/2022-nakdimon/index.html#abstract",
    "href": "reviews/paper/2022-nakdimon/index.html#abstract",
    "title": "Restoring Hebrew Diacritics Without a Dictionary",
    "section": "Abstract",
    "text": "Abstract\n\nWe demonstrate that it is feasible to accurately diacritize Hebrew script without any human-curated resources other than plain diacritized text. We present Nakdimon, a two-layer character-level LSTM, that performs on par with much more complicated curation-dependent systems, across a diverse array of modern Hebrew sources. The model is accompanied by a training set and a test set, collected from diverse sources. –(gershuni-pinter-2022-restoring?)"
  },
  {
    "objectID": "reviews/paper/2022-nakdimon/index.html#outline",
    "href": "reviews/paper/2022-nakdimon/index.html#outline",
    "title": "Restoring Hebrew Diacritics Without a Dictionary",
    "section": "Outline",
    "text": "Outline"
  },
  {
    "objectID": "reviews/paper/2022-nakdimon/index.html#the-paper",
    "href": "reviews/paper/2022-nakdimon/index.html#the-paper",
    "title": "Restoring Hebrew Diacritics Without a Dictionary",
    "section": "The Paper",
    "text": "The Paper\n\n\n\npaper"
  },
  {
    "objectID": "reviews/paper/2022-data-augmentation-low-resource-NMT/index.html",
    "href": "reviews/paper/2022-data-augmentation-low-resource-NMT/index.html",
    "title": "Data Augmentation for Low-Resource Neural Machine Translation",
    "section": "",
    "text": "Litrature review"
  },
  {
    "objectID": "reviews/paper/2022-data-augmentation-low-resource-NMT/index.html#abstract",
    "href": "reviews/paper/2022-data-augmentation-low-resource-NMT/index.html#abstract",
    "title": "Data Augmentation for Low-Resource Neural Machine Translation",
    "section": "Abstract",
    "text": "Abstract\n\nThe quality of a Neural Machine Translation system depends substantially on the availability of sizable parallel corpora. For low-resource language pairs this is not the case, resulting in poor translation quality. Inspired by work in computer vision, we propose a novel data augmentation approach that targets low-frequency words by generating new sentence pairs containing rare words in new, synthetically created contexts. Experimental results on simulated low-resource settings show that our method improves translation quality by up to 2.9 BLEU points over the baseline and up to 3.2 BLEU over back-translation. –(Fadaee, Bisazza, and Monz 2017)"
  },
  {
    "objectID": "reviews/paper/2022-data-augmentation-low-resource-NMT/index.html#outline",
    "href": "reviews/paper/2022-data-augmentation-low-resource-NMT/index.html#outline",
    "title": "Data Augmentation for Low-Resource Neural Machine Translation",
    "section": "Outline",
    "text": "Outline"
  },
  {
    "objectID": "reviews/paper/2022-data-augmentation-low-resource-NMT/index.html#the-paper",
    "href": "reviews/paper/2022-data-augmentation-low-resource-NMT/index.html#the-paper",
    "title": "Data Augmentation for Low-Resource Neural Machine Translation",
    "section": "The Paper",
    "text": "The Paper\n\n\n\npaper"
  },
  {
    "objectID": "reviews/paper/2017-data-augmentation-low-resource-NMT/index.html",
    "href": "reviews/paper/2017-data-augmentation-low-resource-NMT/index.html",
    "title": "Data Augmentation for Low-Resource Neural Machine Translation",
    "section": "",
    "text": "Litrature review",
    "crumbs": [
      "Home",
      "Papers",
      "Data augmentation for low-resource NMT"
    ]
  },
  {
    "objectID": "reviews/paper/2017-data-augmentation-low-resource-NMT/index.html#abstract",
    "href": "reviews/paper/2017-data-augmentation-low-resource-NMT/index.html#abstract",
    "title": "Data Augmentation for Low-Resource Neural Machine Translation",
    "section": "Abstract",
    "text": "Abstract\n\nThe quality of a Neural Machine Translation system depends substantially on the availability of sizable parallel corpora. For low-resource language pairs this is not the case, resulting in poor translation quality. Inspired by work in computer vision, we propose a novel data augmentation approach that targets low-frequency words by generating new sentence pairs containing rare words in new, synthetically created contexts. Experimental results on simulated low-resource settings show that our method improves translation quality by up to 2.9 BLEU points over the baseline and up to 3.2 BLEU over back-translation. –(Fadaee, Bisazza, and Monz 2017)",
    "crumbs": [
      "Home",
      "Papers",
      "Data augmentation for low-resource NMT"
    ]
  },
  {
    "objectID": "reviews/paper/2017-data-augmentation-low-resource-NMT/index.html#outline",
    "href": "reviews/paper/2017-data-augmentation-low-resource-NMT/index.html#outline",
    "title": "Data Augmentation for Low-Resource Neural Machine Translation",
    "section": "Outline",
    "text": "Outline",
    "crumbs": [
      "Home",
      "Papers",
      "Data augmentation for low-resource NMT"
    ]
  },
  {
    "objectID": "reviews/paper/2017-data-augmentation-low-resource-NMT/index.html#the-paper",
    "href": "reviews/paper/2017-data-augmentation-low-resource-NMT/index.html#the-paper",
    "title": "Data Augmentation for Low-Resource Neural Machine Translation",
    "section": "The Paper",
    "text": "The Paper\n\n\n\npaper",
    "crumbs": [
      "Home",
      "Papers",
      "Data augmentation for low-resource NMT"
    ]
  },
  {
    "objectID": "notes/cs11-737-w07-data-driven-NMT/index.html#resources",
    "href": "notes/cs11-737-w07-data-driven-NMT/index.html#resources",
    "title": "Data-driven Strategies for NMT",
    "section": "Resources",
    "text": "Resources\n\nGizA++\nmgiza\nFast Align\nawesome-align (Dou and Neubig (2021))\n\ndoch jetzt ist der Held gefallen . ||| but now the hero has fallen .\nneue Modelle werden erprobt . ||| new models are being tested .\ndoch fehlen uns neue Ressourcen . ||| but we lack new resources .\n0-0 1-1 2-4 3-2 4-3 5-5 6-6\n0-0 1-1 2-2 2-3 3-4 4-5\n0-0 1-2 2-1 3-3 4-4 5-5"
  },
  {
    "objectID": "notes/cs11-737-w07-data-driven-NMT/index.html#bleu-scores",
    "href": "notes/cs11-737-w07-data-driven-NMT/index.html#bleu-scores",
    "title": "Data-driven Strategies for NMT",
    "section": "BLEU Scores",
    "text": "BLEU Scores\n\nThere’s also other leaderboards and stuff for other seq2seq models but that’s a little bit less important for this multilingual class. There are other metrics like BLEU scores so BLEU score is very famous. You know if you’ve done any research on machine translation or even heard of it you probably encountered BLEU. The exact details of how BLEU is calculated are: What you do is you take the precision of N-grams output by the system. So for example if you look at the unigram precision that’s out of all the unigrams that the system output how many of them exist in a reference or one of the references that you’re provided. If in the case that you’re using multiple references and this gives you a position of like three out of five and then you take the geometric mean of N-grams, usually one to four and then you also have a penalty for outputting two short sentences. Because one way to improve your precision is to not output very many things &gt; This is basically to prevent you from gaming system. But the important thing is now the details of how BLEU is calculated probably the important thing is that this is a lexical metric which means that you’re just doing exact match with the references and this has a few issues. One of the issues is essentially that you so there’s there’s two major issues that cause BLEU to either underestimate how good a translation is or overestimate how good a translation is BLEU tends to underestimate how good translations are when the translations are paraphrases of the true reference so if you for example had I have like i went I went to the store and bought a book yesterday and you compare that with yesterday I bought a book at the store those are almost identical in meaning but they would get a low BLEU square because i’ve just rearranged the phrases a little bit so that’s when BLEU tends to underestimate scores BLEU tends to overestimate scores when you get very critical words in the sentence wrong but get everything else right\n\nfor example\n“could you please send this big package to philadelphia”\nturns into:\n“could you please send this big package to japan”\nThat would get a very good BLEU score because most of the words are the same but your package would go to Japan and that’s probably not what you intended by that, otherwise right.\nThat’s the the downside of BLEU. It’s basically not smart enough with respect to these."
  },
  {
    "objectID": "notes/cs11-737-w07-data-driven-NMT/index.html#dictionary-based-augmentation",
    "href": "notes/cs11-737-w07-data-driven-NMT/index.html#dictionary-based-augmentation",
    "title": "Data-driven Strategies for NMT",
    "section": "Dictionary based augmentation",
    "text": "Dictionary based augmentation\n\nSo for the final things which we’re also reading so we can talk more about them in the discussion. We had dictionary based augmentation and dictionary based augmentation basically finds rare words in the source sentences it could also be in the target sentence and tries to replace the words with other words that are kind of in the same semantic class. It replaces car with motorbike and then using a lexicon. It replaces the words in the targeted sentence as well. It’s basically creating more sentences to augment augmented data with like words that are less frequent in the original"
  },
  {
    "objectID": "notes/cs11-737-w07-data-driven-NMT/index.html#bleu-scores-bilingual-evaluation-understudy",
    "href": "notes/cs11-737-w07-data-driven-NMT/index.html#bleu-scores-bilingual-evaluation-understudy",
    "title": "Data-driven Strategies for NMT",
    "section": "BLEU Scores (BiLingual Evaluation Understudy)",
    "text": "BLEU Scores (BiLingual Evaluation Understudy)\n\nThere’s also other leaderboards and stuff for other seq2seq models but that’s a little bit less important for this multilingual class. There are other metrics like BLEU scores so BLEU score is very famous. You know if you’ve done any research on machine translation or even heard of it you probably encountered BLEU. The exact details of how BLEU is calculated are: What you do is you take the precision of N-grams output by the system. So for example if you look at the unigram precision that’s out of all the unigrams that the system output how many of them exist in a reference or one of the references that you’re provided. If in the case that you’re using multiple references and this gives you a position of like three out of five and then you take the geometric mean of N-grams, usually one to four and then you also have a penalty for outputting two short sentences. Because one way to improve your precision is to not output very many things &gt; This is basically to prevent you from gaming system. But the important thing is now the details of how BLEU is calculated probably the important thing is that this is a lexical metric which means that you’re just doing exact match with the references and this has a few issues. One of the issues is essentially that you So there’s there’s two major issues that cause BLEU to either underestimate how good a translation is or overestimate how good a translation is.\n\n\nBLEU tends to underestimate how good translations are when the translations are paraphrases of the true reference. So for example had I have like “I went I went to the store and bought a book yesterday” and you compare that with “yesterday I bought a book at the store” Those are almost identical in meaning But they would get a low BLEU square because I’ve just rearranged the phrases a little bit. So that’s when BLEU tends to underestimate scores.\n\n\nBLEU tends to overestimate scores when you get very critical words in the sentence wrong but get everything else right\n\nfor example\n“could you please send this big package to philadelphia”\nturns into:\n“could you please send this big package to japan”\n\nThat would get a very good BLEU score because most of the words are the same but your package would go to Japan and that’s probably not what you intended by that, otherwise right.\n\n\nThat’s the the downside of BLEU. It’s basically not smart enough with respect to these."
  },
  {
    "objectID": "notes/cs11-737-w07-data-driven-NMT/index.html#in-class-assignment",
    "href": "notes/cs11-737-w07-data-driven-NMT/index.html#in-class-assignment",
    "title": "Data-driven Strategies for NMT",
    "section": "In-class Assignment",
    "text": "In-class Assignment\nRead one of the cited papers on heuristic data augmentation - Fadaee, Bisazza, and Monz (2017) or - Zhou et al. (2019)\n\nTry to think of how it would work for one of the languages you’re familiar with\nAre there any potential hurdles to applying such a method? Are there any improvements you can think of?"
  },
  {
    "objectID": "notes/cs11-737-w06-translation-models/index.html#outline",
    "href": "notes/cs11-737-w06-translation-models/index.html#outline",
    "title": "Translation Models",
    "section": "Outline",
    "text": "Outline\nHere is a lesson outline for your notes, based on the provided sources:\n\nIntroduction to Language Models\n\nLanguage models are generative models of text.\nThey calculate the probability of a text, P(x), and allow sampling to generate outputs with high probability.\nTraining on specific texts (e.g., Harry Potter) results in the model generating similar text.\n\nConditional Language Models\n\nThese models generate text based on some specifications or conditions.\nThey calculate the probability of an output, y, given an input, x: P(Y|X).\nExamples include:\n\nTranslation (English to Japanese)\nSummarization (document to short description)\nResponse generation (utterance to response)\nImage captioning (image to text)\nSpeech recognition (speech to transcript)\n\n\nCalculating the Probability of a Sentence\n\nThe probability of a sentence is calculated by multiplying the probabilities of each word in the sentence: P(X) = ∏ P(xi | x1, …, xi-1)\nConditional language models add context to this calculation: P(Y|X) = ∏ P(yj | X, y1, …, yj-1)\nThe probability of the next word is conditioned on the previous context.\nThis is also called an autoregressive model, a left-to-right language model, or a causal language model.\n\nSequence-to-Sequence Models and Encoder-Decoder Architecture\n\nRecurrent neural networks (RNNs) can be used to feed in previous words and predict the next word in a sequence.\nFor conditional language models, the conditioning context (e.g., a Japanese sentence) is fed into an encoder, and the output (e.g., an English sentence) is predicted using a decoder.\nThis architecture is called an encoder-decoder model and is widely used.\n\nMethods of Generation\n\nThe generation problem is how to use the model P(Y|X) to generate a sentence.\nTwo main methods:\n\nSampling: Generating a random sentence based on the probability distribution.\n\nAncestral sampling generates words one by one.\n\nArgmax: Generating the sentence with the highest probability.\n\nGreedy search picks the highest probability word one by one.\n\n\nGreedy search is not exact and can cause issues like generating easy words first or preferring multiple common words over one rare word.\nBeam search maintains several paths instead of just one, allowing for a more precise search.\n\nAttention Mechanisms\n\nAttention relaxes the constraint of memorizing the entire input sentence in a single vector, using multiple vectors for each token in the input.\nThe model references these vectors when decoding.\nEach word in the input sentence is encoded into a vector.\nDuring decoding, a linear combination of these vectors, weighted by attention weights, is used to pick the next word.\nA query vector (from the decoder) is compared to key vectors (from the encoder) to calculate attention weights.\nThese weights are normalized using softmax.\nValue vectors are combined using these weights.\nThe result is used in the decoder.\n\nAttention Score Functions\n\nVarious functions can be used to calculate attention scores (how well a query vector matches a key vector):\n\nMulti-layer Perceptron (MLP): flexible, good with large data\nBilinear: allows comparison of different sized vectors\nDot Product: simple, requires same-size vectors\nScaled Dot Product: scales the dot product by the size of the vectors\n\n\nImprovements to Attention\n\nCoverage: Addresses issues with dropping or repeating content.\n\nModels how many times words in the input have been covered, adding a penalty if words are not covered approximately once.\nUses embeddings indicating coverage.\n\nMulti-head attention: Multiple attention “heads” focus on different parts of the sentence.\n\nDifferent heads can focus on different functions (e.g. copying vs regular attention).\nIn standard models, attention heads learn what to focus on independently.\n\nSupervised training: Uses “gold standard” alignments (manual or from a strong alignment model) to train the model to match alignments.\n\nSelf-Attention and Transformers\n\nSelf-attention: Each word in the sentence attends to other words in the same sentence, creating context-sensitive encodings.\n\nIt can be a drop-in replacement for RNNs or CNNs.\n\nSelf-attention advantages:\n\nParallelizable, which allows for faster training on GPUs.\nEasily captures global context.\n\nSelf-attention disadvantage:\n\nQuadratic computation time.\n\nTransformers use both self-attention and regular attention between the source and target.\n\nThe transformer encoder processes input word embeddings with positional encodings and multi-head self-attention, adding the original input and layer normalization, and a feed-forward network repeatedly.\nThe transformer decoder is similar but includes cross-attention to the encoder output.\nTransformers use tricks such as layer normalization, specialized training schedules, label smoothing, and masking.\n\n\nUnified View of Sequence-to-Sequence Models\n\nSequence labeling: feature extraction followed by output prediction for each word.\nSequence-to-sequence modeling: a feature extractor followed by a masked feature extractor, and then prediction of words.\n\nThe Annotated Transformer\n\nProvides an implementation of the transformer model.\nIncludes an encoder, decoder, source embedders, target embedders and a generation policy.\nUses multiple identical encoder and decoder layers.\nThe core encoder consists of a stack of layers with layer normalization and dropout.\nThe core decoder is similar but has self-attention and source attention.\nUses masking for the decoder.\nImplements scaled dot product attention.\nAlso implements multi-head attention.\n\nAssignment 2\n\nTask: Machine translation\nGoals: Understand data processing, train bilingual/multilingual models, learn evaluation, and tackle data scarcity.\nUses the TED talks corpus.\nFocuses on low-resource language pairs (English-Azerbaijani and English-Belarusian).\nData preprocessing includes byte pair encoding (BPE) with sentence piece.\nUses the transformer architecture.\nEvaluation uses BLEU and COMET metrics.\nMinimum requirements involve bilingual and multilingual training, as well as using a provided pre-trained model.\nAdditional extensions include: data augmentation, better transfer languages, different BPE variants, and other model improvements.\nSubmission requires code, analysis, and model outputs.\nGrading is based on implementation, analysis, and novelty of methods."
  },
  {
    "objectID": "notes/cs11-737-w06-translation-models/index.html#papers",
    "href": "notes/cs11-737-w06-translation-models/index.html#papers",
    "title": "Translation Models",
    "section": "Papers",
    "text": "Papers\nHere are the papers mentioned in the lesson outline:\n\nMikolov et al. (2011): This paper is referenced in the context of language models and is cited as “Extensions of recurrent neural network language model”.\nSutskever et al. (2014): This paper is mentioned in the context of sequence-to-sequence models and is cited as “Sequence to sequence learning with neural networks”. This paper is also mentioned in relation to how to pass hidden states in encoder-decoder models and initializing the decoder with the encoder.\nKalchbrenner & Blunsom (2013): This paper is referenced in the context of input at every time step for conditional language models and is cited as “Recurrent continuous translation models”.\nBahdanau et al. (2015): This paper is cited in the context of attention mechanisms and is referenced as “Neural machine translation by jointly learning to align”. It is also referenced in relation to the multi-layer perceptron attention score function.\nLuong et al. (2015): This paper is cited in the context of attention score functions and is referenced as “Effective approaches to attention-based neural”. It is also mentioned in relation to bilinear and dot product attention score functions.\nVaswani et al. (2017): This paper is cited in the context of scaled dot product attention, multi-headed attention, and the Transformer model.\nCohn et al. (2015): This paper is referenced in the context of coverage for attention models and is cited as “Incorporating structural alignment biases into an attentional neural translation model”.\nMi et al. (2016): This paper is also referenced in the context of coverage for attention models and adding embeddings indicating coverage.\nAllamanis et al. (2016): This paper is mentioned in the context of multi-headed attention and is cited as “A convolutional attention network for extreme summarization of source code”.\nLiu et al. (2016): This paper is referenced in the context of supervised training with gold standard alignments.\nCheng et al. (2016): This paper is mentioned in the context of self-attention and is cited as self-attention.\nChen et al. (2018): This paper is mentioned in the context of the accuracy of self-attention/transformers and is cited as “The best of both worlds: Combining recent advances in neural machine translation”.\n\n\nSee also\nBahdanau, Cho, and Bengio (2016)\nLuong, Pham, and Manning (2015)\nReference: Self Attention (Cheng et al. 2016)\nVaswani et al. (2023)"
  },
  {
    "objectID": "notes/cs11-737-w02-sequence-labeling/index.html#outline",
    "href": "notes/cs11-737-w02-sequence-labeling/index.html#outline",
    "title": "Sequence Labeling",
    "section": "Outline",
    "text": "Outline\n\nIntroduction to Text Classification and Sequence Labeling\n\nThese are broad task categories, not tasks in themselves, that can be solved in similar ways.\nText classification involves predicting a categorical label for an input text.\nSequence labeling involves predicting an output label sequence of equal length to the input text.\n\nText Classification Tasks\n\nTopic classification: Assigning a topic to a given text. For example, “I like peaches and pears” would be classified as “food,” while “I like peaches and herb” would be classified as “music”.\nLanguage identification: Identifying the language in which a text is written. This task is particularly important in multilingual learning.\nSentiment analysis: Determining the sentiment of a text (e.g., positive, negative, neutral) at the sentence or document level.\n\nSequence Labeling Tasks\n\nPart-of-speech (POS) tagging: Assigning a part of speech tag to each word in a sentence. For example, “He saw two birds” would be tagged as “PRON VERB NUM NOUN”.\nLemmatization: Identifying the base form of words, which is particularly useful in languages with rich morphology.\nMorphological tagging: Predicting various features of a word, such as tense, number, and verb form.\n\nSpan Labeling\n\nInvolves predicting output spans and labels for input text.\nNamed entity recognition (NER): Identifying and labeling named entities like person, organization, or location. For example, “Graham Neubig” would be labeled as a “person” and “Carnegie Mellon University” as an “organization”.\nSyntactic chunking: Splitting a sentence into noun phrases and verb phrases.\nSemantic role labeling: Identifying the roles of different parts of a sentence, such as actor, predicate, and location.\nSpan labeling can also be treated as a sequence labeling task by predicting beginning, inside, and outside (BIO) tags for each word in the span.\n\nText Segmentation\n\nSplitting an input text into segmented text.\nTokenization: Splitting text into tokens.\nWord segmentation: Identifying word boundaries in languages without spaces between words. For example, Japanese requires word segmentation since it does not use spaces.\nMorphological segmentation: Splitting words into morphemes.\n\nModeling for Sequence Labeling and Classification\n\nThe process involves extracting features from the input text and then predicting labels.\nA feature extractor can extract a single vector for the whole sequence (for text classification) or one vector for each word (for sequence labeling).\nA simple feature extractor is the bag-of-words model, which looks up a feature for each word and adds them together.\nA simple predictor is a linear transform with a softmax function, which converts scores into probabilities.\nMore complex feature extractors include n-grams, syntax-based features, and neural networks.\nNeural networks, which use computation graphs, are now more common because they allow for transfer across languages.\n\nNeural Network Basics\n\nNeural networks are computation graphs with nodes and edges.\nNodes can represent values, and edges represent operations.\nForward propagation involves computing the value of each node in topological order, starting from the input values.\nBackpropagation is used to calculate the derivatives of the parameters with respect to a loss function and update the parameters to improve model performance.\n\nRecurrent Neural Networks (RNNs)\n\nRNNs are used to handle dependencies in language, such as word order, agreement, and semantic congruency.\nThey process sequences by taking the input at the current time step and the features from the previous time step.\nRNNs can be used to represent sentences for tasks like text classification or represent words for tasks like sequence labeling.\nTraining RNNs involves calculating a loss function, summing it up over a sequence, and doing backpropagation through time to update the parameters.\nParameters are tied across time, allowing the network to handle sequences of arbitrary length.\nBi-directional RNNs process the input sequence from both left to right and right to left.\n\nMultilingual Tasks and Datasets\n\nLanguage identification is a task that is inherently multilingual.\nOther multilingual text classification and sequence labeling tasks can be used for benchmarking multilingual models.\nMLDoc corpus is used for multilingual document classification.\nPOS-X is a corpus for paraphrase detection between languages.\nCross-lingual natural language inference (XNLI) is used for textual entailment prediction.\nCross-lingual sentiment classification is another benchmark task.\nUniversal Dependencies (UD) Treebank is a high-quality multilingual corpus that contains syntactic parses, POS tags, and morphological features for 90 languages.\nCoNLL 2002/2003 is a dataset for language-independent named entity recognition.\nWikiAnn is a dataset for entity recognition and linking in 282 languages.\nXTREME and XGLUE are composite benchmarks for multilingual learning, aggregating different sequence labeling and classification tasks.\n\nAssignment 1: Multilingual Part of Speech Tagging\n\nThe goal is to give a practical introduction to multilingual part-of-speech tagging.\nStudents will be given a dataset of sentences in different languages and must output the POS tags for each word.\nThe assignment emphasizes an experimental approach to multilingual problems.\nThe assignment requires the use of a GPU and AWS is recommended.\nFiles containing code and data will be provided to students.\nStudents need to submit code and a report detailing their analysis of results and model improvements.\nGrading criteria include running the code, training on multilingual datasets, and providing detailed analysis of the results.\nStudents can improve their grade by making a non-trivial extension to improve the existing scores."
  },
  {
    "objectID": "notes/cs11-737-w02-sequence-labeling/index.html#papers",
    "href": "notes/cs11-737-w02-sequence-labeling/index.html#papers",
    "title": "Sequence Labeling",
    "section": "Papers",
    "text": "Papers\nHere is a list of all the papers mentioned in the lesson:\n\nJauhiainen et al. (2018) Automatic Language Identification in Texts: A Survey\n\nThis is a survey that provides an overview of automatic language identification methods.\n\nCaswell et al. (2020) Language ID in the Wild: Unexpected Challenges on the Path to a Thousand Language Web Text Corpus\n\nThis paper discusses challenges in language identification, particularly when applied to web text.\n\nSchwenk and Li (2018) MLDoc: A Corpus for Multilingual Document Classification in Eight Languages\n\nThis paper introduces a corpus for multilingual document classification.\n\nQi et al. (2022) Cross-lingual Natural Language Inference (XNLI) corpus\n\nThis paper presents a corpus for cross-lingual natural language inference, a task that involves textual entailment prediction.\n\nYang et al. (2019) PAWS-X: Paraphrase Adversaries from Word Scrambling, Cross-lingual Version\n\nThis paper describes a dataset for paraphrase detection across multiple languages.\n\nPonti et al. (2019) Modeling language variation and universals: A survey on typological linguistics for natural language processing\n\nThis is a survey on typological linguistics for natural language processing.\n\nSang and De Meulder (2003) CoNLL 2002/2003 Language Independent Named Entity Recognition dataset\n\nThese papers introduce datasets for language-independent named entity recognition.\n\nPan et al. (2017) WikiAnn Entity Recognition/Linking in 282 Languages\n\nThis paper presents a dataset for entity recognition and linking extracted from Wikipedia.\n\nHu et al. (2020) XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization dataset\n\nThis paper introduces a benchmark for evaluating cross-lingual generalization across multiple tasks and languages.\n\nLiang et al. (2020) XGLUE: A New Benchmark Dataset for Cross-lingual Pre-training, Understanding and Generation presentation dataset\n\nThis paper presents a benchmark dataset for cross-lingual pre-training, understanding, and generation.\n\n\nThese are all the papers explicitly mentioned in the sources. The lesson also refer to the Universal Dependencies Treebank, Udify, and Stanza as resources for multilingual NLP tasks which are linked below."
  },
  {
    "objectID": "notes/cs11-737-w03-typology/index.html#outline",
    "href": "notes/cs11-737-w03-typology/index.html#outline",
    "title": "Typology: The Space of Languages",
    "section": "Outline",
    "text": "Outline\n\nIntroduction to Linguistic Typology\n\nTypology is a way to structure and organize languages based on similarities.\nLanguages are not fixed points, they exist across regions and often overlap.\nLanguage definition can be political, e.g. Hindi & Urdu.\nRemember the gag: “a language is a dialect with an army”.\nAcknowledged the difficulty of defining the borders between languages.\n\nLinguistic Diversity\n\nThe world has approximately 7,000 languages.\nSome areas have higher concentrations of languages than others.\nIndia has around 460 languages.\nAfrica has an estimated 1,500-2,000 languages from 6 language families.\nMost people in the world are multilingual.\nThe U.S. and Britain are unusual in that many people speak only one language.\nGeographical features can create borders between languages.\n\nLanguage Families\n\nLinguists identify language families by looking at shared features such as lexicon, grammar, and morphology.\nExamples of major language families include:\n\nNiger-Congo (Africa).\nIndo-European (Europe, Middle East, and Northern India).\n\nLanguages within families share common linguistic information.\nSome languages, like Korean, may not be related to other languages in their area.\n\nIdentifying Similarities between Languages\n\nMethods for identifying similarities include:\n\nWord overlap: looking at shared words, especially core vocabulary (body parts, family terms, food, water, air). A specific list called the Swadesh’s list is often used for this.\nPhonetic form: considering phonetic similarities rather than just written forms.\nAreal similarity: geographic proximity and influence between languages.\nGenealogical similarity: language families based on linguistic history.\nTypological similarity: classifying languages based on functional and structural properties.\n\nResources such as Ethnologue and Glottolog try to identify and classify languages.\nThe World Atlas of Language Structures (WALS) is a key resource for typological data.\n\nTypological Features\n\nTypology involves classifying languages based on shared formal characteristics.\nExamples of typological features include:\n\nPhonology: How languages pronounce sounds; the International Phonetic Alphabet (IPA) helps in comparing phonological systems.\nNumeral bases: Different languages use different counting systems, such as decimal or base-20.\nWord order: Languages may have a default word order such as subject-verb-object (English) or subject-object-verb (Japanese).\n\nWALS provides a database of 192 attributes across 2,676 languages.\nSome features may be predictable based on others; for example, languages with rich morphology may have less fixed word order.\n\nTypological Databases\n\nWALS is a major typological database.\nURIEL is another typological database which includes phonology, morphosyntax, and lexical semantics.\n\nURIEL has data for 8,070 languages and 284 attributes.\n\nThese databases are useful in multilingual NLP, providing features for language models.\n\nLinguistic Universals\n\nMost languages have vowels and consonants.\nAlmost all languages distinguish between nouns and verbs.\n\nThe distinction between adjectives is less clear across languages.\n\n\nMultilingual Natural Language Processing (NLP)\n\nTypological features can help in multilingual NLP by providing structured data.\nLow-resource languages benefit from typological information due to the lack of available data.\nMethods in multilingual NLP:\n\nCross-lingual transfer: using a model from a resource-rich language on a resource-poor language.\nZero-shot learning: applying a model from one domain to another with no extra training.\nFew-shot learning: adapting a model using a few examples from a low-resource domain.\nJoint multilingual learning: training a single model on multiple languages.\n\nTypological information can be used to select an appropriate transfer language.\nThe amount of training data and the similarity between languages is important.\n\nOpen Research Problems\n\nHow to automatically extract typological features from existing resources.\nHow to accurately predict typological knowledge while controlling for biases.\nHow to incorporate linguistic typology into models.\nHow to alleviate negative transfer in multilingual models using typological knowledge.\n\nFurther Resources\n\nPapers in computational linguistics conferences.\nWorkshops such as SIGMORPHON, SIGTYP, and AfricaNLP.\n\nDiscussion\n\nIdentifying unique typological features in languages.\nConsidering aspects of phonology, morphology, syntax, semantics, and pragmatics."
  },
  {
    "objectID": "notes/cs11-737-w03-typology/index.html#papers",
    "href": "notes/cs11-737-w03-typology/index.html#papers",
    "title": "Typology: The Space of Languages",
    "section": "Papers",
    "text": "Papers\nHere is a list of the papers covered in the lesson\n\nPonti et al. (2019) Modeling language variation and universals: A survey on typological linguistics for natural language processing\n\nThis paper is a survey on typological linguistics for natural language processing. It is cited as a key resource for understanding how typological information can be used in NLP. The paper also explores modeling language variation and universals.\n\nLin et al. (2019) Choosing Transfer Languages for Cross-Lingual Learning\n\nThis paper discusses how to choose appropriate transfer languages for cross-lingual learning in NLP. It is relevant to the topic of using typological features for low-resource languages.\n\nLittell et al. (2017) URIEL Typological database\n\nThis paper introduces the URIEL typological database. It provides information on phonology, morphosyntax, and lexical semantics across many languages.\n\nMalaviya, Neubig, and Littell (2017) Learning language representations for typology\n\nThis paper is related to the lang2vec representations derived from the URIEL database and explores how to learn language representations for typology.\n\nGeorgi, Xia, and Lewis (2010) Comparing Language Similarity across Genetic and Typologically-Based Groupings\n\nAn example of research in the automatic prediction of typological features."
  },
  {
    "objectID": "notes/cs11-737-w02-sequence-labeling/index.html#lesson-outline",
    "href": "notes/cs11-737-w02-sequence-labeling/index.html#lesson-outline",
    "title": "Sequence Labeling",
    "section": "Lesson Outline",
    "text": "Lesson Outline\n\nIntroduction to Text Classification and Sequence Labeling\n\nThese are broad task categories, not tasks in themselves, that can be solved in similar ways.\nText classification involves predicting a categorical label for an input text.\nSequence labeling involves predicting an output label sequence of equal length to the input text.\n\nText Classification Tasks\n\nTopic classification: Assigning a topic to a given text. For example, “I like peaches and pears” would be classified as “food,” while “I like peaches and herb” would be classified as “music”.\nLanguage identification: Identifying the language in which a text is written. This task is particularly important in multilingual learning.\nSentiment analysis: Determining the sentiment of a text (e.g., positive, negative, neutral) at the sentence or document level.\n\nSequence Labeling Tasks\n\nPart-of-speech (POS) tagging: Assigning a part of speech tag to each word in a sentence. For example, “He saw two birds” would be tagged as “PRON VERB NUM NOUN”.\nLemmatization: Identifying the base form of words, which is particularly useful in languages with rich morphology.\nMorphological tagging: Predicting various features of a word, such as tense, number, and verb form.\n\nSpan Labeling\n\nInvolves predicting output spans and labels for input text.\nNamed entity recognition (NER): Identifying and labeling named entities like person, organization, or location. For example, “Graham Neubig” would be labeled as a “person” and “Carnegie Mellon University” as an “organization”.\nSyntactic chunking: Splitting a sentence into noun phrases and verb phrases.\nSemantic role labeling: Identifying the roles of different parts of a sentence, such as actor, predicate, and location.\nSpan labeling can also be treated as a sequence labeling task by predicting beginning, inside, and outside (BIO) tags for each word in the span.\n\nText Segmentation\n\nSplitting an input text into segmented text.\nTokenization: Splitting text into tokens.\nWord segmentation: Identifying word boundaries in languages without spaces between words. For example, Japanese requires word segmentation since it does not use spaces.\nMorphological segmentation: Splitting words into morphemes.\n\nModeling for Sequence Labeling and Classification\n\nThe process involves extracting features from the input text and then predicting labels.\nA feature extractor can extract a single vector for the whole sequence (for text classification) or one vector for each word (for sequence labeling).\nA simple feature extractor is the bag-of-words model, which looks up a feature for each word and adds them together.\nA simple predictor is a linear transform with a softmax function, which converts scores into probabilities.\nMore complex feature extractors include n-grams, syntax-based features, and neural networks.\nNeural networks, which use computation graphs, are now more common because they allow for transfer across languages.\n\nNeural Network Basics\n\nNeural networks are computation graphs with nodes and edges.\nNodes can represent values, and edges represent operations.\nForward propagation involves computing the value of each node in topological order, starting from the input values.\nBackpropagation is used to calculate the derivatives of the parameters with respect to a loss function and update the parameters to improve model performance.\n\nRecurrent Neural Networks (RNNs)\n\nRNNs are used to handle dependencies in language, such as word order, agreement, and semantic congruency.\nThey process sequences by taking the input at the current time step and the features from the previous time step.\nRNNs can be used to represent sentences for tasks like text classification or represent words for tasks like sequence labeling.\nTraining RNNs involves calculating a loss function, summing it up over a sequence, and doing backpropagation through time to update the parameters.\nParameters are tied across time, allowing the network to handle sequences of arbitrary length.\nBi-directional RNNs process the input sequence from both left to right and right to left.\n\nMultilingual Tasks and Datasets\n\nLanguage identification is a task that is inherently multilingual.\nOther multilingual text classification and sequence labeling tasks can be used for benchmarking multilingual models.\nMLDoc corpus is used for multilingual document classification.\nPOS-X is a corpus for paraphrase detection between languages.\nCross-lingual natural language inference (XNLI) is used for textual entailment prediction.\nCross-lingual sentiment classification is another benchmark task.\nUniversal Dependencies (UD) Treebank is a high-quality multilingual corpus that contains syntactic parses, POS tags, and morphological features for 90 languages.\nCoNLL 2002/2003 is a dataset for language-independent named entity recognition.\nWikiAnn is a dataset for entity recognition and linking in 282 languages.\nXTREME and XGLUE are composite benchmarks for multilingual learning, aggregating different sequence labeling and classification tasks.\n\nAssignment 1: Multilingual Part of Speech Tagging\n\nThe goal is to give a practical introduction to multilingual part-of-speech tagging.\nStudents will be given a dataset of sentences in different languages and must output the POS tags for each word.\nThe assignment emphasizes an experimental approach to multilingual problems.\nThe assignment requires the use of a GPU and AWS is recommended.\nFiles containing code and data will be provided to students.\nStudents need to submit code and a report detailing their analysis of results and model improvements.\nGrading criteria include running the code, training on multilingual datasets, and providing detailed analysis of the results.\nStudents can improve their grade by making a non-trivial extension to improve the existing scores."
  },
  {
    "objectID": "notes/cs11-737-w13-speech/index.html",
    "href": "notes/cs11-737-w13-speech/index.html",
    "title": "Speech",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\nVideo 1: Lesson Video\n\n\n\n\n\n\n\n\nThis week’s slides\n\n\n\n\nSupplementary Figure 1"
  },
  {
    "objectID": "notes/cs11-737-w04-words/index.html#outline",
    "href": "notes/cs11-737-w04-words/index.html#outline",
    "title": "Words, Parts of Speech, Morphology",
    "section": "Outline",
    "text": "Outline\n\nIntroduction to Words in NLP\n\nWords are fundamental in NLP, yet defining them can be non-trivial.\nThe notion of a word boundary is not always straightforward, especially when moving beyond English.\nWhite space separated tokens are a good approximation for words in some languages, but this isn’t a universal rule.\n\nChallenges in Defining Words\n\nClitics: Apostrophe-s (’s) is a clitic, not an isolated word, and it can attach to whole noun phrases.\nCompounds: Some compounds (like “handyman”) are treated as single words, even with spaces, while others are hyphenated (“do-it-yourself”), and these distinctions are not always consistent.\nContractions: Contractions like “kinda” and “isn’t it” can be written in reduced forms and require decisions about tokenization.\nAgglutination: Some languages (e.g., Turkish) join words together with no spaces, resulting in what might be considered phrases in English being single words.\nAmbiguity: In some languages, a single written word can have multiple meanings depending on how it is separated into morphemes (e.g., Hebrew example given).\n\nLinguistic Approaches to Defining Words\n\nPhonetics: Word definitions could be based on pronunciation, such as every word containing a vowel or having a certain syllable structure.\nMorphology: Word definitions could be based on the individual morphemes within the word.\nSyntax: Words can be defined by their class and interchangeability with other words of the same class.\nSemantics: Word definitions could relate to the meaning they convey, including multi-word units with a single idea like “navy blue”.\nPragmatics: Some phrases like “New York” may be used as single words even though they have white space within them.\n\nOrthographic Definitions\n\nWhite Space: Even in English, white space is complex (spaces, tabs, new lines, etc.) and includes non-breakable spaces which may or may not be considered word boundaries.\nWritten vs. Spoken: Most languages are not written, and in spoken language, there are no spaces between words. A definition of words is needed that is independent of the written form.\n\nProsodic Definitions\n\nIntonation: Phrases grouped together in a single intonational phrase (like “in the park”) can be treated as a single word in some languages.\n\nSyntactic Definitions\n\nWords can be grouped into syntactic classes; for example “New York” is a particular city in the same way that “Pittsburgh” is.\nOpen Classes (Nouns, verbs, adjectives, adverbs): These classes are vast and can easily accommodate new words.\nClosed Classes (Prepositions, determiners, pronouns, conjunctions, auxiliary verbs): These classes have a finite set of words, and new words rarely enter these classes.\n\nPart-of-Speech Tagging\n\nTag Sets: Tag sets, like those based on the Penn Treebank (with 43 tags), are used to categorize words into parts of speech.\nUniversal Dependency Tag Sets: A smaller set of tags is used across many languages for universal dependency grammar.\nImportance of Tagging: Tagging provides structured information, which can improve machine learning models, especially with limited data.\n\nMorphology\n\nMorphemes: Morphemes are the smallest atomic parts of a word.\nDerivational Morphology: This changes the meaning and part of speech class of a word. English is rich in derivational morphology.\nInflectional Morphology: This changes syntactic properties such as tense or number, and usually occurs after derivational morphology.\nMorphosyntax: How stems and affixes combine.\nMorphophonemics: Pronunciation and orthographic changes at morpheme boundaries. This can make morphology non-segmental.\n\nMorphological Decomposition\n\nStems/Roots, Prefixes, and Suffixes: Words can be broken into their root forms, prefixes, and suffixes. English usually adds prefixes and suffixes but some languages have infixes.\nInfixes: Some languages have infixes (elements inserted within the word, with examples from Southeast Asia) and even change the consonants and vowels, as in templatic morphology in Semitic languages.\nNeed for Decomposition: Decomposing words into morphemes allows for a more finite set of meaningful units for processing and tokenization.\n\nTypes of Morphology\n\nIsolating/Analytic: Very little morphological change (e.g. Vietnamese, Chinese dialects, English).\nSynthetic/Fusional/Flectional: Things are constantly being created (e.g. German, Greek, Russian).\nTemplatic: Often consonants are the basis, and the vowels change around them.\nAgglutinative: Lots of things joining together (e.g. Japanese, Finnish, Turkish).\nPolysynthetic: Complex joining of many parts, often with a whole phrase in one word (e.g. some Native American languages).\n\nTools for Morphological Analysis\n\nMorphological Analyzers: Existing tools, including those from the UniMorph project, can provide morphological decomposition for many languages.\nFinite State Morphology: Finite state machines are useful for modeling morphology because of its localized nature.\nUnsupervised Methods: Tools like Morfessor can identify prefixes and suffixes.\nStemming: Removing word endings to find the root, useful for information retrieval.\nByte Pair Encoding (BPE): Finds optimal letter sequences for segmentation, also helpful in tokenization.\n\nRelated Problems\n\nTokenization: The process of splitting text into words or tokens.\nLemmatization: Finding the linguistic root of a word after morphological decomposition.\nText Normalization: Replacing numbers, symbols, and abbreviations with standard words.\nSpelling Correction: Addressing spelling errors and social media language.\nTokenization Mismatch: Different tokenization methods can lead to issues, for example when using models like BERT.\nWord Segmentation: In languages without spaces, segmenting words is necessary.\n\nPractical Applications\n\nThe need to consider different language families when developing tokenization methods because morphology and orthography are often similar within a family.\nTokenization decisions impact all downstream tasks, including word embeddings and parsing."
  },
  {
    "objectID": "notes/cs11-737-w04-words/index.html#papers",
    "href": "notes/cs11-737-w04-words/index.html#papers",
    "title": "Words, Parts of Speech, Morphology",
    "section": "Papers",
    "text": "Papers\nHere’s a list of the papers mentioned in the lesson\n\nSproat et al 2001 CSL 15(3): Normalization of Non-standard Words, is referenced in the context of text normalization. It deals with the expansion of non-standard words."
  },
  {
    "objectID": "notes/cs11-737-w04-words/index.html#kannada-in-10-minutes",
    "href": "notes/cs11-737-w04-words/index.html#kannada-in-10-minutes",
    "title": "Words, Parts of Speech, Morphology",
    "section": "Kannada in 10 minutes",
    "text": "Kannada in 10 minutes\nOkay, here is an overview of Kannada, drawing from the provided Wikipedia article.\n\nLanguage Name and Classification:\n\nKannada (ಕನ್ನಡ) is a classical Dravidian language.\nIt belongs to the South Dravidian language family.\nIt evolved from Tamil-Kannada.\nIt was formerly known as Canarese.\n\nSpeakers and Location:\n\nApproximately 44 million native speakers.\nAround 15 million second-language speakers.\nPrimarily spoken in the state of Karnataka in southwestern India.\nSpoken by minority populations in neighboring states, such as Tamil Nadu, Maharashtra, Andhra Pradesh, Telangana, Kerala, and Goa.\nKannada speaking communities also exist in countries like the United States, Canada, Australia, Singapore, and Malaysia.\n\nOfficial Status:\n\nOfficial and administrative language of Karnataka.\nIt has scheduled status in India.\nRecognized as one of India’s classical languages.\n\nHistory and Development:\n\nThe language’s history is divided into: Old Kannada (450-1200 AD), Middle Kannada (1200-1700 AD), and Modern Kannada (1700-present).\nInfluenced by Sanskrit and Prakrit.\nSome scholars suggest a spoken tradition existed as early as the 3rd century BC.\nThe earliest Kannada inscriptions date back to the 5th century AD.\nThe language has an unbroken literary history of approximately 1200 years.\n\nWriting System:\n\nUses the Kannada script.\nThe script evolved from the 5th-century Kadamba script.\nIt is a syllabic script.\nIt contains 49 phonemic letters.\n\nPhonology:\n\nVowels include front, central, and back vowels, with variations in length.\n\nConsonants include labial, dental, retroflex, palatal, velar, and glottal sounds.\nMost consonants can be geminated.\nAspirated consonants are rare in native vocabulary.\n\n\nGrammar:\n\nWord order is typically subject-object-verb (SOV).\nIt is a highly inflected language with three genders and two numbers.\nCompound bases (samāsa) combine two or more words.\n\nDialects:\n\nApproximately 20 dialects of Kannada exist, including Kundagannada, Havigannada, and Are Bhashe.\nDialects are influenced by regional and cultural backgrounds.\nRelated languages include Badaga, Holiya, Kurumba, and Urali.\n\nSignificance to Modern Linguistics:\n\nKannada’s syntactic properties, particularly regarding causative morphemes and negation, have been significant in studies of language acquisition and innateness."
  },
  {
    "objectID": "notes/cs11-737-w02-assignment-1/index.html",
    "href": "notes/cs11-737-w02-assignment-1/index.html",
    "title": "Assignment 1: Multilingual POS Tagging",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\nVideo 1: Recitation of AWS Fundamentals 01\n\n\n\n\n\n\n\n\nVideo 2: Recitation of AWS Fundamentals 02\n\n\n\n\n\n\n\n\nVideo 3: Recitation of AWS Fundamentals 03\n\n\n\n\n\n\n\n\nVideo 4: Recitation of AWS Fundamentals 04\n\n\n\n\n\n\n\n\nassignment slides\n\n\n\n\nSupplementary Figure 1"
  },
  {
    "objectID": "notes/cs11-737-w02-assignment-1/index.html#text-classification-and-sequence-labeling",
    "href": "notes/cs11-737-w02-assignment-1/index.html#text-classification-and-sequence-labeling",
    "title": "Assignment 1: Multilingual POS Tagging",
    "section": "Text classification and sequence labeling",
    "text": "Text classification and sequence labeling\n\nSo text classification and sequence labeling are both very broad. I like to call these task categories. They’re not tasks of themselves but they’re categories of tasks that’ll look very similar and thus can be solved in similar ways. Text classification: given, input text X, predicted output: some categorical label Y. This can be all kinds of different things like topic classification where we take i like peaches and pears and that gives us the topic food and i like peaches and herb and that gives you the topic music because peaches and herb is a old band basically we have language identification which is a particularly important task in multilingual learning and basically this is Taking in language and outputting labels. Taking in text and now putting the language that it’s written in some you know obviously the first is english and the second one is japanese here this becomes very interesting and difficult as i’ll be elaborating later in the lecture portion. Another example that’s very widely known is sentiment analysis and this can be done at the sentence level the document level or you can even do some sentiment analysis with respect to you know individual entities or things like that but from a text classification point of view it would be like sentences or documents so if we have i like peaches and pairs i’d be positive i hate peaches in paris i’d be negative obviously and you know there’s many many other many many other tasks that fall into this sequence labeling on the other hand is given an input text text predict an output label sequence y usually of equal length so this can be formulated as taking in a sequence of words and outputting one tag for each sequence of words so we have part of speech tagging is one example of this so we take in words and output parts of speech another one is lemmatization so what this is doing is it’s basically taking in words and outputting their base form this is kind of this is used sometimes in in english but it’s actually particularly useful or important in languages that have richer morphology than english because essentially if they have lots and lots of conjugation or other things like this the words themselves can become very sparse so identifying the underlying base form helps you understand like what the word is referring to better another variety is morphological tagging in morphological tagging again we’re going to be talking about morphology in about two classes but basically it’s predicting various features of the word based on like for example here we have he saw becomes the past tense and a finite verb form two it’s a number and the type is a cardinal number this is plural for example so this can become more complicated but more languages have more complicated morphologicals and there’s other ones as well there’s also span labeling tasks and sometimes these fan labeling tasks are treated in sequence labeling tasks but they’re actually a little bit different and basically the idea is given in input text x predict output spans and labels y and these include things like named entity recognition where you want to identify the spans and labels on the spans for named entities like person people or things so here we have graham newbig as a person in carnegie mellon university as an organization another example is something called syntactic chunking or shallow syntactic parsing where basically you want to split up the sentence into like noun phrases and verb phrases like this and that’s another example and there’s also semantic role labeling where semantic role labeling basically identifies fans and tries to identify like this is an actor this is a predicate and this is a location so what role each of these arguments is playing with respect to the predicate but as you can see all of these basically have to do with like identifying spans and labeling them in some way so span labeling can also be treated as a sequence labeling task so you predict the beginning in and out tags bio tags for each word in the span so if we take our span labeling task like this where we want to identify a person or organization we convert this into a thing where we basically have one task or one tag for each word so we have beginning of person inside a person out out out which means no no span is identified beginning organization in organization in an organization like this so the good news is then you know if you want to do this span identification task you can just solve it with a sequence labeling model so sequence labeling is nice in that way there might be better ways to handle span identification but this is this is one way to do it so another another task that is slightly different but also can be handled as sequence labeling is text segmentation so given in input text x split it into segmented text y so a very common example of this in many languages is tokenization where you want to take something like a well a well-conceived thought exercise that has like lots of punctuation and intervening hyphens and stuff like this and split it up into things that look a little bit more like just kind of natural word boundaries like like this adding spaces between each of the punctuation etc another variety of this which is necessary for some languages not not so many languages in the world but but some languages is word segmentation and this is a example from japanese because japanese is written with no spaces between words so you can’t just split on white space using your python like split function you have to actually find the locations of the word boundaries and this is a non-trivial task so this is one segmentation that you could have here this is the kind of like quote-unquote correct segmentation but you could also make a mistake and segment like this and this phrase itself if you split it this way means foreign people voting rights or like suffrage for whereas this means the foreign government so basically if you split the correct way you get one meaning if you split the wrong way you get another meeting and this can you know mess up information retrieval systems any translation systems anything that you would think of and then there’s also morphological segmentation which again we’re going to talk about a bit more which is for example we might have something that looks a little bit like this which can be split into different ways i i believe this is turkish i think someone can correct me if i’m wrong here i i actually forgot where i got the example but i believe it’s turkish and so then we have dog and plural or dog paddle and attempts so basically whether you split it one way or the other also affects whether this is like a plural noun or a verb and thank you for confirming that this is a determination so this is another another issue that you could have cool are there any questions about this so far oh is the word segmentation solved with additional context in japanese yeah that’s a really good question and morphological analysis also is similar you know having context so basically yes having context is very important because both of these are both of these are reasonable morphological segmentations in different contexts you know they’re both things that could happen sure this might be a lot more frequent but there certainly are examples where this would occur it’s also the case in japanese although that’s a little bit more rare like one one example is that i can put in the zoom chat this means if you split off the first character that means american nuclear reactor if you split off the second character it means a train that’s departing from maihara which is a place in japan and so depending on whether you’re whether you’re in a news newspaper article or a train schedule either of those would be correct so it’s yeah it is based on context then i had another question explaining the distinction between b and i here so b basically is the first tag in a sequence in a span so it would be applied to the first word in the span i would be applied to every subsequent word at the span so you you if you had a single word span it would just have a b but if you have the multi word span it will be b i i i tell the span finishes and having these two different tags is necessary to distinguish cases where you have two spans in a row so if you have like person and then another person later how do we constrain the model so that b is predicted before i for a particular entity that’s a another really good question so if you i maybe maybe i’ll talk about how we actually do this prediction first and then get back to that question great okay so i’d like to talk a little bit about modeling for sequence labeling and classification so the first question is how do we make predictions so given an input x input text x extract features h and predict labels y and the way this works is basically we have text classification and sequence labeling so we have like i like peaches we have some sort of feature extractor that extracts a whole bunch of features from the whole sequence into like a single vector here and then given that single vector we make a prediction we have for sequence labeling we have a feature extractor that extracts one feature or one vector of features for each word in the input and then we make a prediction from this like that and so either way we need a feature extractor it’s just a matter of whether we are extracting a single vector of features for the whole sequence or one vector for each word so a very simple feature extractor that we might use for text classification for example is bag of words where what we do is we look up a a single feature for each of the words and then we add them together to get a vector a vector representing the number of times a particular word occurred in the sentence and then we feed this into a predictor maybe we have a a matrix multiply that turns this into label label values and then we use this to calculate label probabilities so just to clarify when i say bag of words this we have a one hot vector which means a single value in this vector here is one and all the other values are zeros so this is essentially a a vector that represents the identity of the word and nothing else so as our simple predictor we can have something like a linear transform and a softmax function so what that looks like is we take we take the extracted features we multiply them by a weight matrix we add a bias here which tells you essentially how how likely each of the labels is a priori and then the softmax converts arbitrary scores into probabilities so we exponentiate the score at each of the elements in the resulting vector divided by a normalizer to make sure that all of them add up to one add up to one and then that gives us a probability of our final output so it might look a little bit like this after the after the linear transform and bias addition we might have a score vector that looks like this and we turn it into a probability so i think this should be pretty familiar to a lot of people [Music] no questions about this any questions if not i have i have a little bit of a quiz so like we talked about text classification sorry every time i touch my mouse it moves my my slides it’s a little bit annoying but so we talked about text classification where we extract features for the whole sequence and then we use that to predict the the label probabilities for the whole sequence by adding all of these together what do you think of a bag of words of a similar feature extractor used for sequence labeling does that make any sense whatsoever so we extract one one vector using this kind of bag of words lookup function and then make predictions based on this vector would that would that do anything would there be a reasonable way of solving this problem maybe not so reasonable because it’s neglecting the word order yeah maybe not so reasonable because it’s neglecting the word order yeah that’s a a pretty good idea but it’s actually maybe not that bad right what it what it would end up doing is it would basically look up i and then it would make a prediction of what part of speech tag i would be based on what the most frequent part of speech tag for i is and it would make the prediction of like and it would make the prediction of the most frequent thing for likes so yeah it’s a frequency-based model it would be looking up the majority class for each word and actually for part of speech tagging in english and even more so for other languages this actually isn’t that bad at part of speech tagging it gets like high 80s accuracy just because there’s relatively little like peaches is always a plural noun i i can’t i can’t think of any case where it wouldn’t be a plural so even that would be you know a moderately okay feature extractor both for classification and and sequence labeling however you know it does neglect the word order so if you want to do like even better than just majority class you have to come up with something that way so another issue language is not just a bag of words for classification or labeling so to have some examples we have i don’t love pairs there’s nothing i don’t love about pears if you just look at the words in the sentence like love is the positive word and then you have a negation but negation in itself is not you know very indicative of positive or negative sentiment so both of these would be relatively hard to tackle with a bag of words model so what we want to do is we want to come up with a better futurizer to better pull out features for our sequences for each word in our sequences so one example of a better futurizer might be a bag of engrams model so basically instead of looking up each word we would look up each engram so here for example then don’t love would also become a feature in our model and if don’t love is a feature in our model that’s kind of like a negative feature right it would maybe overpower the the love feature and make move that more negative you could also come up with syntax-based features like subject object pairs or neural networks like recurrent neural networks convolutional neural networks self-attention and so until you know 20 or so it was very common to use these kind of handcrafted features or at least handcrafted feature templates and throw them into a support vector machine model or something like this to do text classification now it’s much more common to use neural networks and they also have some really nice properties like allowing transfer across languages which is a very big you know part of this class so we’re going to mainly be focusing on these models in this class this time i’ll talk about recurrent neural networks just because they’re conceptually easy but we’re also going to be talking about things like self-attention so a neural network which a lot of people know is basically a computation graph that is parameterized in a certain way in order to allow it to make predictions the name neural networks comes from neurons in the brain where basically you know they take an information across synapses and then they fire and they give output over the the outputs and the absolutes but the current conception of how we use them it’s basically a mathematical object that allows us to calculate something take in an input generate an output and in particular in this case it’s going to be our featurizer and our predictor so it’s going to take in an output input output some features for each word or each class or it’s going to take in an input and output a score for each for each class so if we define an expression the way we represent it as a computation graph is through nodes and edges between the nodes and so here we have a single variable maybe a vector and that’s represented as a single node the node can be like a scalar value of vector value matrix value tensor value and we can also have operations over all of these values so like let’s say we transpose the vector that operation would be demonstrated as an edge to another node that implements the transpose so an edge represents a function argument and a node with an incoming edge is a function of that edge’s tail node and a node knows how to compute its value and the value of its derivative with respect to each argument and then functions can be nullary like input values here or unary unity or binary so here’s a something representing a matrix multiply so basically now we have x transpose times times a here and these are directed in the cyclic graphs that allow us to calculate more and more complicated expressions so they allow us to you know do things like calculate features make predictions et cetera and we can also name individual parts of the graph and this allows us to you know calculate values that we would like to be calculating like the score of the the probability of prediction and so some algorithms are graph construction and forward propagation that allow us to calculate values so forward propagation basically we we start out with the input values of the graph and we gradually move through the graph to calculate the final result here and we also have back propagation and back propagation basically what it does is it processes examples in reverse topological order calculating the derivatives of the parameters with respect to the final value and this is usually the loss function the value we want to minimize so in many cases this is the negative log likelihood of the predictions over the true values and by minimizing this negative log likelihood that allows us to maximize the probability of getting the correct answer and then we take the derivatives calculated through back propagation to update the parameters of the model so back propagation basically works like this we start from the end of the graph and gradually move back until we back propagate into parameters so let’s say a was a parameter a b was a parameter and c was a parameter we would back propagate into these values and once we have the derivatives that we calculate through this process of back propagation we can use them to update the parameters to kind of improve the likelihood of getting the correct answer so this is a kind of five-minute intro to neural networks if you’re not very familiar with them there’s lots of good tutorials online in particular we’re going to be using a neural network framework pytorch for examples in this class that i think you know a lot of people are familiar with already and our first assignment this time is aimed to kind of have the dual purpose of allowing you to get familiar with you know building models in high torch and stuff like this and also learn more about kind of the interesting difficulties that you have to deal with when you apply these multilingually so if you’ve already taken a class or already used neural networks pretty widely in your in your work then you know all of this will be old to you and you’ll more or less know this already if you haven’t then definitely take advantage of the ta office hours look at the examples ask lots of questions we can forward you some tutorials online to help you out as well so this will be your chance to catch up before we get into the more involved assignments that happen later so yeah actually maybe i’ll skip that part so to give an example of a type of neural network that can be used for featurizing a text classifier or a sequence labeler we are going to talk briefly about recurrent neural networks and recurrent neural networks are models that allow us to do precisely what was mentioned before as being the [Music] issue with a bag of words model which is handling either short or long distance dependencies in language handling word order handling other things like this and so in language there’s many dependencies that span across whole sentences so for example agreement is one example there’s not a whole lot of agreement in english like for example there’s gender agreement and there’s a plural there’s like number agreement between subjects and verbs so here we can see he and himself need to agree she and herself need to agree also he does needs to agree here so if this were i would be i do he does so that’s an example also word order in general we talked about that last class so we need to have some sort of model that’s able to handle these things these are syntactic characteristics that we need to be able to handle and there’s also semantic characteristics so for example we need to have semantic congruency between rain and queen and rain and clouds here you know they’re they’re just things that make sense make sense semantically and don’t make sense mentally based on our knowledge of the world and we also need to handle these as well so recurrent neural networks basically are are one of the tools that we can use to encode sequences either to get representations for each word or representations for other words so basically what recurrent neural networks do is they look up they look up the context or the input at the current time step do a transformation of this into features and then they feed in the features from the previous time step for the next time steps so to give an example if we have i we would feed it through an rnn and get the next vector here like we would feed this through another rnn function we would calculate a vector corresponding to light this is a parameter of the model and then we feed in the result of running i through the rnn to get the and this input here to get the representation for i like and then we have these and we calculate the representation of these i like these and then we have pairs and we calculate the representation of i like these pairs and basically this is a recursive function so each time you’re using the result of the previous function to calculate the result of the next function so when we represent the sentence for text classification basically it would look a bit like this so we would take the last vector in the sequence to make a prediction and that would be useful for things like text classification condition generation retrieval we’ll be talking about the latter queue later in the class but text classification is the one we’re talking about this time and it can also be used to represent words so if we wanted to predict a part of speech label for i and like and these and pairs we could use the immediate output after inputting that word to try to make that prediction as well and this would allow us to do things like pull in contacts from the left side to make this part of speech prediction for things like sequence labeling language modeling calculating representations for for parsing etc so the way we train the rnns is like let’s say we’re training one for sequence labeling we have the predictions here from these we could calculate a negative log likelihood or a loss function something like this we have the label we use the true label of the output to calculate the loss function and we add them together to get the sequence level total loss so this is one big computation graph for the whole sentence and then we take the total loss and we do back propagation from this total loss into the whole the representations for the whole sentence so the parameters of the model are all tied across time the derivatives are aggregated across all time steps and this gives us something called back propagation through time so you can back propagate through the whole sentence and basically optimize the probability of making the correct predictions for the whole science so what did i mean by parameter tying basically the parameters are shared between this rnn function over the entire sentence and because of this this allows you to apply this to sentences of arbitrary length so if you have a sentence of length 50 or a sentence of length 20 or something like this then this would essentially allow you to represent all of them within the same recurrent neural network by just applying this rnn function 50 times or 20 times or three times for a three word sentence and when doing when doing representation for things like sequence labeling it’s very common to use bi-directional rnns and what i mean by this is basically you take the left side and you run a recurrent neural network that steps from time to step zero to time step one to two to three to four from left to right and then you have another recurrent neural network that steps from right to left in this way and aggregates information from both of the directions congratulates that together and makes a prediction and the reason why this is useful is you never know whether the context to disambiguate a particular word would be available on the left side or the right side so this allows you to pull in information from both sides okay so that’s basically the overview of you know a simple method for doing calculation of either representations for the whole sentence so for example if we’re representing the whole sentence we might take this vector we might concatenate the right side of the left rnn and the left side of the right the right side of the back forward rnn and the left side of the backward arm or if we want to represent individual words we might concatenate together the things in each time step to make predictions so this would allow us to do text classification or sequence labels okay are there any questions about this before i jump into the multilingual part okay i guess not so we can jump into the multi-uh multilingual thing that is part of the name of the class of course so i’m gonna be talking about some text some text classification and sequence labeling tasks most of these are tasks that are applicable to any language so they’re explored quite widely on english as well but some of them are inherently multilingual so for example language identification is one that’s inherently multiple so language identification as i mentioned before is the task of identifying the language that a particular text comes in and this is really important for a very broad number of reasons the first reason might be if you want to create like let’s say you want to show people content in only a language that they speak so you know people when they’re doing search online they’ll probably more appreciate results in the language they speak than in another language another example could be for creating data sets for something like machine translation or language modeling or something like this where you only want data in a particular language and and other things like this so actually one of the largest language identification corporal was created by ralph brown here at at lti it’s a benchmark on 1152 languages from a variety of free sources so this is kind of a widely known data set here if you want off the shelf tools for doing language identification one example of a relatively easy one to use is slang id.pai so you can just download this use it for 90 plus languages another example is there’s the chrome language identifier from the chrome browser i actually don’t have a link here but that’s also pretty widely used by people if you want an off-the-shelf method that you can use there’s also a nice survey it’s a little bit old by now but automatic language identification and texts which i can recommend you can take a look at if you’re interested in this and oh missing a slide that i thought i had added here weird okay so i i will just discuss i’ll just discuss this paper so this is a recent paper from 2020 by people at google working on kind of low resource languages it’s quite interesting it’s called language id in the wild unexpected challenges on the path to a thousand language web text corpus and this is not the only paper that has it has pointed out this problem that language identification doesn’t work well but they have some very interesting insights and they also have a very nice kind of example of a of the issues that you encounter when trying to do language identification on web text and so here here are some mind sentences from the web that were supposedly in one language according to google’s text like language identification model so like a whole bunch of people raising their hand and emojis got classified as amani beri the in this was in twee which is why you lie in why you always lie in written in kind of like strange characters a misrendered pdf was for hadi the non-unicode font i’m not sure what this was\n\nyeah it’s a non-non-unicode font i guess was written in this way here this was as balinese it was just like boilerplate this was also english but it was written in like the cherokee script for stabilization he just wrote me ow that became cooler so you can see basically here when people write in like slightly non-standard language it gets identified as other things like even this is like clearly standard english but there were hints of words that often occur in remote so it got recognized as a remote so this kind of just demonstrates how difficult this this task is when you start applying it to web text and i actually have had a similar experience when i was trying to do twitter there was a certain like face like not emoji but like the the faces written with regular characters it was written in canada characters and so many many things were recognized as commonly just because they use that like popular face so there are lots of large corpora like while i’m at it here i can also introduce the oscar purpose this is a very large corpus huge and multilingual obtained by language classification and filtering of the common crawl corpus it’s gotten a bit better since they first released it in terms of the noisiness but when it was first released it was like extremely noisy just because language id didn’t didn’t work so well so this is actually like a really big problem that you need to be aware of if you’re if you’re starting out are there any questions about language id before i move on to the next okay so also kind of standard text classification like yes i said text classifications were like a class class of tasks that make tasks in itself here are some representative ones that people have used these are mostly used for benchmarking multilingual models as opposed to like actually building anything useful so but still you know sometimes you want to know how good your multilingual representations are so they could be good test beds one example is ml doc corpus which is a corpus of a multilingual document classification there’s also the pos x corpus so the this is a paraphrase detection between languages it’s sentence pair classification where you feed in two sentences also cross-lingual natural language inference so this is textual entailment prediction or natural language inference which is also a sentence pair classification task there’s also cross-lingual sentiment classification in chinese in english so this could be used for facing another thing is part of speech and morphological tagging i’m not going to go into a whole lot of about this because i know it’s going to be covered more when we talk about like words parts of speech and morphology but basically there’s the universal dependencies treebank and the universal dependencies treebank basically it contains syntactic parses like dependency forces but it also contains parts of speech and morphological features for 90 languages and it has a standardized universal part of speech set in universal morphology headset to make things consistent across the languages so this is one of the highest quality like multilingual corpora that i’m aware of it’s you know well controlled well conceived and there are some pre-trained models that can use that can do like syntactic analysis on many languages trained on these datasets like unify and stanza if you’re interested in doing multilingual like syntactic analysis named entity recognition this is going to be what we’re going to be doing for the assignment and there’s different types of named entity recognition data sets there’s a gold standard data set from connell 2002 2003 on language independent named entity recognition this is an english german spanish and dutch with human annotated data i actually forgot to add one that i just remembered now that just came out i actually i helped out with this a little bit but it’s a an identity recognition data set for african languages i called masakonner and this is this is nice because it’s also manually labeled but it’s in african languages that have like a lot fewer resources than english german spanish and dutch so it gives kind of a better idea of [Music] like you know how well we’ll be doing a lower resource languages there’s also this wiki and data set for entity recognition and linking in 282 languages this was extracted from wikipedia using inter page links so in wikipedia of course if we go to carnegie mellon university on wikipedia there are many there are many links so there’s no link here but here’s pittsburgh pennsylvania the melon institute of industrial research andrew carnegie so all of these links link to other pages and then if you look up the type of the page according to some annotations that come on wikipedia you can tell that pittsburgh is a city mellon institute of industrial researchers and organization and andrew carnegie is a is a person and then of course you know this is available in lots of languages so we can go to chinese and find the chinese equivalent of andrew carnegie or the chinese equivalent of pittsburgh and and do the same thing so basically this data creates that in many different languages there’s also several composite benchmarks for multilingual learning so they aggregate many different sequence labeling or classification tasks for testing multilingual models one popular one is extreme it’s a massively multilingual benchmark for 10 different tasks 40 different languages another one that came out at a similar time is exclu with 11 tasks over 19 languages so there’s also a new version of extreme cold extreme r that just came out i had been a little bit involved in both of these and extreme r is they swapped out some easy tasks added some harder tasks and added better analysis so you might also consider looking at that for your class project i would warn you that these these benchmarks are very popular and there’s people with like lots of compute that are competing on these benchmarks so you might not it might be a bit of a challenge to keep up with the state of the art there but i think you could work on individual tasks and still do a very good job like some of the tasks where kind of generic models are not working as well so you can definitely take a look to get inspiration for ideas okay great so that’s all i have are there any questions before we move on to the the like discussion period which in this case we’re not doing discussion we’re having a presentation of the assignment but any question about data sets or tasks or oh sorry the homework was on ner i said it beyond that in er but the top part of speech tagging i apologize cool yeah but we’ll have the description of the the task for assignment one before i go into that i just like to point out that starting next time we will indeed be having discussion and reading assignments so the reading assignment for next time is this modeling language variation in universals a survey on typological linguistics for natural language processing the reading is actually it’s only required for suggested that you do sections one through three but the whole survey is good so if you don’t mind reading 30 pages or so it would be worth taking a look at that as well so required is one through three and then based on what you learned in that reading you can try to think of what are some unique typological features of a language that you know regarding phonology morphologies and dex pragmatics doesn’t you don’t need to cover all of them but you can cover like one or two of these and we’ll have a discussion where everybody will share what they came up with cool and today is assignment one introduction tr vijay or who’s going to be presenting this thing\nare you speaking if you’re speaking around me do i need to unmute me i don’t think he’s on mute okay there we go now we can take gray i think you are talking right can you hear me now yep yes great okay so okay hi hi i’m think gray and ti is going to give some introduction yeah all right so this first assignment is to give you a practical introduction to multilingual parts of speech tagging and i think briefly was mentioned so part of speech is just lexical categories or word classes or tags so in the example sentence he saw two words we assigned the part of speech pronoun to he verb to saw and so on so you’ll get a data set of a sentence sentences in different languages and you want to output the pos tags for each of the words in the sentence yeah so as mentioned previously aside from giving you guys a practical introduction to multilingual pause tagging yeah we want to give you a sort of like an experimental approach to multilingual problems such as investigating the challenges to languages which are low resource meaning that there’s a limited availability of label data and of course just be familiarized with deep learning frameworks aws and multilingual data sets so to do this assignment you would need a machine with a gpu so you can use aws or you can use your own computer if you have a gpu and you’ll have to install some python packages for this assignment so the tricky part here i guess would be like the aws setup so shortly i’ll be posting instructions on piazza on how to request aws credit and i think the assignment will have further instructions on how to set it up and all students should have an aws account using your andrew email and just follow the instructions on how to set it up we tried doing the assignment without a gpu but it’s strongly recommended because i think the next assignments are not really doable without a gpu and i trained it on a very old macbook air and it took me around three to four hours to complete the training and you need to retrain it you know when you’re changing the parameters and so on so yeah do it with the gpu and i think one more thing to take note is that make sure that you stop the aws instance when you’re not doing it because you will be continuously billed so i think the aws setup should be fairly straightforward i’m not sure myself tingerie has done it before without instructions but the instruction instructors and the tas of the intro to deep learning course have provided a very comprehensive aws fundamentals playlist so it will be linked in the assignment handout page as well and you can follow the steps in case you have any difficulties yeah so for this assignment we are going to give you a deep file and this this this file will contain all the code and the data that is to use for this assignment and we will post a link later on psl so in the date file you can find the training data it is the training data for six languages and the right hand side is an example what what the format looks like for this training data so this is an example of one sentence so you can see there are the words and the pls text of that word separated by some new lines and each line contains a word and it’s text separated by a tab but actually you don’t have to worry too much about this format because our code will handle this for you and in this homework we are going to use this simple baseline model by rcm model it is a model with an embedding layer and also a bi-directional stem there and so the input of this model will be a sequence of words and the outputs will be a sequence of pos tags so let’s work through the files in the the file so the first file is the config.json file it is the file that contains the hyper parameters that will be used to trend the model you may have to change this ma this file in order to write a report when you are doing some analysis and this udprs.pipe file is the place where we implement how to read a data set but actually i don’t think you will have to modify this so i won’t go into detail here and this model.pi file is the place where we implement this bi-directional stm model and if you are familiar with pytorch then you can see it’s a very simple model it just simply applies and it’s embedding layer and then lcn there and then finally predict the text using a fully connected layer yeah but if you want to make a stronger model then you may have to modify this file and most of the complex jobs are done in this main.pi file it handles the loading of data and it also do some pre-process and do the thing that have simple into batch and also the part that trends the model so let’s go through this content so the first thing it does is that a load the data set with the function we define and then it build a vocabulary for the input text and also the the output pos text the reason that we want to build this vocabulary is that in modern deep learning framework we are when we are using a embedding layer what the embedding layer does is that a map the index of sound balls into some batters so that’s to say before we can use this embedding layer we need to first build a mapping they can assign each word with an index so we have to iterate through the changing data to see what to see the words that occurs in the training data and for similar reason we have to iterate through the data set to see all the possible possible pos tags in this data so these are the purpose of this device and once we have the mapping that map the tokens to the indices and the mapping then map the pos tag to the index we can define these two functions that converge they convert the train data into indices and with these two functions we can define this collet batch function the purpose of this function is that it pack a bunch of text label pairs into a single batch and then this batch will be used to train the model so yeah so what this function does is that it first converts the words inside the the samples to a tensor by coding the function with the file above and you also can convert the tag into indexes by code by cleaning the function and then the most important thing it does is that it paid those sequence of tokens into the sentence so those tensors can be stacked into a single tensor and that single tensor can be later used to train the model and once we have this correct batch function we can define a data loader a data network is an iterator we can get some batches from it by iterate through this data loader so here we define three data loaders for the training set the validation set and testing set respectively and then we can use the data loader to change our model the way we train our model is that we repeat this process for a certain number of times defined in this hyper-parameter mesh epoch and the process is that we first train the model by using the training data and then evaluate the model using the validation set and if after this epoch the model get a better validation loss then this grip will set the model to some place so at the end of this training process you will have the model that has the minimum validation loss as for what this trend function does it is also very simple it just iterates through the data order the chain data loader and then makes prediction over the text and then compute the laws by comparing its prediction and the quantum tags and then use these those to do backward propagation and then code atomizer to update the parameters in the model yeah so it’s basically what our codes do so what you need to submit for this assignment is code and a write-up so part of the you know how to obtain points for this assignment is you need to run the code you need to make notifications to the model and so on but it’s also equally important to give a detailed explanation on the results that you see or what you do and why do you think your changes have made an effect on the results and you can submit this on campus and i think this is the part that everyone really wants to see is how do i get a good grade in this assignment so there’s a lot of ways there’s a lot of tiers as well so if you just want to get you can get a b by just running the code on the existing english model and just running that running the model on the test set you’ll get a b but if you train the model on the different multilingual data sets and you evaluate them using their respective test sets you’ll get a b plus to get anywhere of an a you need to write a report with detailed analysis so there’s a lot of ways you can comment on the results you can see how the performance varies across different languages you can also see you know which tags are most often misplaced for other tags so you know our pronouns and nouns more easily mistaken for each other and so on so that’s what we want to see and if you have a report you know detailing all these explanation you’ll probably get an a minus to get an a or above you will need to get to create a non-trivial extension to improve the existing scores and there’s really a lot of ways you can do this you can add like cnn input layer to capture character level features you can use pre-trained embeddings and so on so there’s really it’s kind of like an experiment that you need to run on your own and we’re excited to see your results."
  },
  {
    "objectID": "notes/cs11-737-w02-assignment-1/index.html#lesson-outline",
    "href": "notes/cs11-737-w02-assignment-1/index.html#lesson-outline",
    "title": "Assignment 1: Multilingual POS Tagging",
    "section": "Lesson Outline",
    "text": "Lesson Outline"
  },
  {
    "objectID": "notes/cs11-737-w02-assignment-1/index.html#reflection",
    "href": "notes/cs11-737-w02-assignment-1/index.html#reflection",
    "title": "Assignment 1: Multilingual POS Tagging",
    "section": "Reflection",
    "text": "Reflection\nSequence labeling and text classification are broad task categories that can be solved using similar methods. Sequence labeling involves predicting an output label sequence of equal length to the input text, while text classification involves predicting a categorical label for an input text. These tasks are essential in natural language processing and can be used for a wide range of applications, such as sentiment analysis, named entity recognition, and part-of-speech tagging.\nOne point I consider is if there is one or two formats that are more common in the industry or academia. For example, is BIO more common than IOB? Or is there a standard format for representing part-of-speech tags? I would like to know more about the standard practices in the field.\nMore so is there is a format that is a good fit for other tasks like allignment annotaion, tts, and so on."
  },
  {
    "objectID": "notes/cs11-737-w02-assignment-1/index.html#papers",
    "href": "notes/cs11-737-w02-assignment-1/index.html#papers",
    "title": "Assignment 1: Multilingual POS Tagging",
    "section": "Papers",
    "text": "Papers\nHere is a list of all the papers mentioned in the lesson:\n\nJauhiainen et al. (2018) Automatic Language Identification in Texts: A Survey\n\nThis is a survey that provides an overview of automatic language identification methods.\n\nCaswell et al. (2020) Language ID in the Wild: Unexpected Challenges on the Path to a Thousand Language Web Text Corpus\n\nThis paper discusses challenges in language identification, particularly when applied to web text.\n\nSchwenk and Li (2018) MLDoc: A Corpus for Multilingual Document Classification in Eight Languages\n\nThis paper introduces a corpus for multilingual document classification.\n\nQi et al. (2022) Cross-lingual Natural Language Inference (XNLI) corpus\n\nThis paper presents a corpus for cross-lingual natural language inference, a task that involves textual entailment prediction.\n\nYang et al. (2019) PAWS-X: Paraphrase Adversaries from Word Scrambling, Cross-lingual Version\n\nThis paper describes a dataset for paraphrase detection across multiple languages.\n\nPonti et al. (2019) Modeling language variation and universals: A survey on typological linguistics for natural language processing\n\nThis is a survey on typological linguistics for natural language processing.\n\nSang and De Meulder (2003) CoNLL 2002/2003 Language Independent Named Entity Recognition dataset\n\nThese papers introduce datasets for language-independent named entity recognition.\n\nPan et al. (2017) WikiAnn Entity Recognition/Linking in 282 Languages\n\nThis paper presents a dataset for entity recognition and linking extracted from Wikipedia.\n\nHu et al. (2020) XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization dataset\n\nThis paper introduces a benchmark for evaluating cross-lingual generalization across multiple tasks and languages.\n\nLiang et al. (2020) XGLUE: A New Benchmark Dataset for Cross-lingual Pre-training, Understanding and Generation presentation dataset\n\nThis paper presents a benchmark dataset for cross-lingual pre-training, understanding, and generation.\n\n\nThese are all the papers explicitly mentioned in the sources. The lesson also refer to the Universal Dependencies Treebank, Udify, and Stanza as resources for multilingual NLP tasks which are linked below."
  },
  {
    "objectID": "notes/cs11-737-w02-assignment-1/index.html#resources",
    "href": "notes/cs11-737-w02-assignment-1/index.html#resources",
    "title": "Assignment 1: Multilingual POS Tagging",
    "section": "Resources",
    "text": "Resources\n\nlangid.py\nUdify\nStanza\nLTI LangID Corpus\nMLDoc\nPAWS-X\nXNLI\nCross-lingual Sentence Classification\nUniversal Dependencies/POS Tags\nCoNLL NER Tasks\nXTREME\nXGLUE"
  },
  {
    "objectID": "notes/cs11-737-w02-sequence-labeling/index.html#reflection",
    "href": "notes/cs11-737-w02-sequence-labeling/index.html#reflection",
    "title": "Sequence Labeling",
    "section": "Reflection",
    "text": "Reflection\nSequence labeling and text classification are broad task categories that can be solved using similar methods. Sequence labeling involves predicting an output label sequence of equal length to the input text, while text classification involves predicting a categorical label for an input text. These tasks are essential in natural language processing and can be used for a wide range of applications, such as sentiment analysis, named entity recognition, and part-of-speech tagging.\nOne point I consider is if there is one or two formats that are more common in the industry or academia. For example, is BIO more common than IOB? Or is there a standard format for representing part-of-speech tags? I would like to know more about the standard practices in the field.\nMore so is there is a format that is a good fit for other tasks like allignment annotaion, tts, and so on."
  },
  {
    "objectID": "notes/cs11-737-w19-syntax-and-parsing/index.html",
    "href": "notes/cs11-737-w19-syntax-and-parsing/index.html",
    "title": "Syntax and Parsing",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\nVideo 1: Lesson Video\n\n\n\n\n\n\n\n\nThis week’s slides\n\n\n\n\nSupplementary Figure 1\n\n\n\n\n\n\n\n\nThis week’s slides\n\n\n\n\nSupplementary Figure 2"
  },
  {
    "objectID": "notes/cs11-737-w02-assignment-1/index.html#some-ideas",
    "href": "notes/cs11-737-w02-assignment-1/index.html#some-ideas",
    "title": "Assignment 1: Multilingual POS Tagging",
    "section": "Some Ideas",
    "text": "Some Ideas\n\ncrearing a multilingual POS tagger\nusing a hierarchical model that does partial pooling to learn from multiple languages when working on a low-resource language\ncreating a surrogate simulated language which\n\nhas parameters that correspond to the low resource language - resources are drawn from language database like WALS, Ethnologue, and Glottolog Note that the challange then becomes in how to generate the surrogate language based on these parameters. One could try to create real phonetic and morphological rules etc or one might side step this complexity and use a simple mathematical construct to create data to create suitable embeddings.\nUse a phrase book as a template for generating texts in the surrogate languages. The outcome should be a dataset of translations of the phrase book in multiple languages. Note that it could also be feasible to generate multiple variants for the both the source and target language to avoid overfitting on the phrase book.\na priors distribution that follows high resource languages. (i.e. idea that high frequency source words are more likely to be translated to high frequency target words)\na language model that is trained on the high resource language and then used to generate the surrogate language\na model that is trained on the surrogate language and then used to tag the low resource language"
  },
  {
    "objectID": "notes/cs11-737-w19-syntax-and-parsing/index.html#navajo-in-10",
    "href": "notes/cs11-737-w19-syntax-and-parsing/index.html#navajo-in-10",
    "title": "Syntax and Parsing",
    "section": "Navajo in 10",
    "text": "Navajo in 10\n\nLanguage Name and Classification:\n\nNavajo (also known as Navaho).\nNavajo: Diné bizaad [tìnépìz̥ɑ̀ːt] or Naabeehó bizaad [nɑ̀ːpèːhópìz̥ɑ̀ːt].\nIt is a Southern Athabaskan language of the Na-Dené family.\n\nSpeakers and Location:\n\nSpoken primarily in the Southwestern United States, especially in the Navajo Nation.\nOne of the most widely spoken Native American languages.\nThe most widely spoken Native American language north of the Mexico–United States border.\nAlmost 170,000 Americans speaking Navajo at home as of 2011.\n\nNomenclature:\n\nThe word Navajo is an exonym from the Tewa word Navahu, meaning ‘large field’.\nThe Navajo refer to themselves as the Diné (‘People’), with their language known as Diné bizaad (‘People’s language’) or Naabeehó bizaad.\n\nOfficial Status:\n\nOfficial language in Navajo Nation.\n\nHistory and Development:\n\nThe Apachean languages, of which Navajo is one, are thought to have arrived in the American Southwest from the north by 1500.\nSpeakers of the Navajo language were employed as Navajo code talkers during World Wars I and II.\nOrthography developed in the late 1930s and is based on the Latin script.\n\nWriting System:\n\nBased on the Latin script.\nDeveloped between 1935 and 1940.\nUses an apostrophe to mark ejective consonants and mid-word or final glottal stops.\nRepresents nasalized vowels with an ogonek and the voiceless alveolar lateral fricative with a barred L.\n\nPhonology:\n\nHas a fairly large consonant inventory.\nStop consonants exist in three laryngeal forms: aspirated, unaspirated, and ejective.\nHas a simple glottal stop used after vowels.\nFour vowel qualities: /a/, /e/, /i/, and /o/.\nEach vowel exists in both oral and nasalized forms and can be either short or long.\nDistinguishes for tone between high and low.\n\n \nGrammar:\n\nRelies heavily on affixes, mainly prefixes.\nAffixes are joined in unpredictable, overlapping ways that make them difficult to segment.\nBasic word order is subject–object–verb.\nVerbs are conjugated for aspect and mood.\n\nVocabulary:\n\nMost Navajo vocabulary is of Athabaskan origin.\nHas been conservative with loanwords due to its highly complex noun morphology.\nExpanded its vocabulary to include Western technological and cultural terms through calques and Navajo descriptive terms.\n\nRevitalization and Current Status:\n\nBilingual Education Act in 1968 provided funds for educating young students who are not native English speakers.\nNavajo Nation Council decreed in 1984 that the Navajo language would be available and comprehensive for students of all grade levels in schools of the Navajo Nation.\nNavajo-immersion programs have cropped up across the Navajo Nation.\nDiné College offers an associate degree in the subject of Navajo.\nIn December 2024, Navajo Nation President made Navajo language the official language of Navajo Nation."
  },
  {
    "objectID": "notes/cs11-737-w14/index.html",
    "href": "notes/cs11-737-w14/index.html",
    "title": "Automatic Speech Recognition",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\nVideo 1: Lesson Video\n\n\n\n\n\n\n\n\nThis week’s slides\n\n\n\n\nSupplementary Figure 1"
  },
  {
    "objectID": "notes/cs11-737-w14/index.html#warlpiri-in-10-minutes",
    "href": "notes/cs11-737-w14/index.html#warlpiri-in-10-minutes",
    "title": "Automatic Speech Recognition",
    "section": "Warlpiri in 10 minutes",
    "text": "Warlpiri in 10 minutes\n\nLanguage Name and Classification:\n\nWarlpiri (Warlpiri).\nAn Aboriginal Australian language.\nBelongs to the Ngarrkic languages of the large Pama–Nyungan family.\n\nSpeakers and Location:\n\nSpoken by close to 3,000 of the Warlpiri people.\nLocated in the Tanami Desert, northwest of Alice Springs, Central Australia.\nOne of the largest Aboriginal languages in Australia in terms of the number of speakers.\nNative speakers: 2,624 (2021 census).\n\nDialects:\n\nWarlpiri.\nNgaliya.\nWalmala.\nNgardilpa.\nEastern Warlpiri.\nOther probable dialects: Warnayaka (Wanayaga, Woneiga), Wawulya (Ngardilpa).\n\nWriting System:\n\nUses the Latin script.\nAlphabet devised by Lothar Jagst, later modified.\nUses only ordinary letters, with deviations from IPA.\n\nLong vowels are written by doubling the vowel letter: ii, aa, uu.\nRetroflex consonants are written with digraphs formed by prefixing r: rt, rn, rl.\nPalatal stop is written j.\nOther palatals are written with digraphs formed by suffixing y: ny, ly.\nVelar nasal is written ng.\nAlveolar trill is written rr.\nRetroflex flap is written rd.\nRetroflex approximant is written r.\n\nIndicators y (palatal) and r (retroflex) are often dropped in consonant clusters with the same position of articulation.\nAt the beginning of a word, the retroflex indicator r may be omitted.\n\nPhonology:\n\nVowels: standard three-vowel system (similar to Classical Arabic) with phonemic length distinction, creating six possible vowels.\n\nFront: /i/, /iː/.\nBack: /u/, /uː/, /a/, /aː/.\n\nConsonants: distinguishes five positions of articulation and has oral and nasal stops at each position.\n\nOral stops have no phonemic voice distinction, but display voiced and unvoiced allophones.\nStops are usually unvoiced at the beginning of a word and voiced elsewhere.\nNo fricative consonants.\n\nSyllables: constrained structure, all syllables begin with a single consonant, no syllable-initial consonant clusters, and no syllable begins with a vowel.\n\nAfter the consonant is a single long or short vowel, sometimes followed by a single closing consonant.\nOpen syllables are more common than closed ones.\nNo syllable ends with a stop or with the retroflex flap /ɽ/.\n\nStress: not generally distinctive but is assigned by rule.\n\nPolysyllabic words receive primary stress on the first syllable, with secondary stresses tending to occur on alternate syllables thereafter.\n\nVowel harmony: If two adjacent syllables in a Warlpiri morpheme have high vowels, those high vowels are almost always alike: both /u/ or both /i/.\n\nBoth progressive and regressive vowel harmony occur.\n\n\nGrammar:\n\nAvoidance register: special style of language used when certain family relations need to converse.\n\nSame grammar as ordinary Warlpiri but a drastically reduced lexicon.\n\nVerbs: built from a few hundred verb roots, distributed among five conjugation classes.\n\nLarge class of modifying prefixes (preverbs) are used to create verbs with specific meanings.\n\nNouns: assembled from thousands of roots, with derivational techniques such as compounding and derivational suffixes.\n\nPlurals are formed by reduplication of the root.\n\nAuxiliary word: each full Warlpiri clause may contain an auxiliary word, which, together with the verb suffix, serves to identify tense and to clarify the relationship between main and dependent clauses.\n\nThe auxiliary word is almost always the second word of a clause.\nAlso functions as the home for suffixes that specify the person and number of the subject and object of the clause.\n\n\nAdditional Features:\n\nAll Warlpiri words end in vowels."
  },
  {
    "objectID": "notes/cs11-737-w14-ASR/index.html",
    "href": "notes/cs11-737-w14-ASR/index.html",
    "title": "Automatic Speech Recognition",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\nVideo 1: Lesson Video\n\n\n\n\n\n\n\n\nThis week’s slides\n\n\n\n\nSupplementary Figure 1"
  },
  {
    "objectID": "notes/cs11-737-w14-ASR/index.html#warlpiri-in-10-minutes",
    "href": "notes/cs11-737-w14-ASR/index.html#warlpiri-in-10-minutes",
    "title": "Automatic Speech Recognition",
    "section": "Warlpiri in 10 minutes",
    "text": "Warlpiri in 10 minutes\n  \n\nSpoken in the Northern Territory of Australia by the Warlpiri people.\nApproximately 2,500-3,000 native speakers.\nOne of the largest Aboriginal languages in Australia based on the number of speakers.\nOne of only 13 indigenous languages in Australia still being passed down to children.\nAlternative names include Walbiri and Waljbiri.\n\nLanguage Family & History\n\nPama-Nyungan.\nNgarrkic languages.\nThe term Jukurrpa, referring to Aboriginal spiritual beliefs, comes from Warlpiri.\nA writing system was not developed until the 1970s when the language began to be taught in schools.\n\nGrammar\n\nFree word order, but the auxiliary word is almost always the second word in a clause.\nErgative marking. The actor takes a special ending called the ergative ending. The ergative ending marks the subject of a transitive sentence.\nSplit ergativity. Nouns follow one set of rules, while pronouns and auxiliary verbs follow another.\nSuffixes indicate person and number of the subject and object.\nVowel harmony.\n\nPhonology\n\nMost Warlpiri languages have only three vowels.\nNo voicing contrast. Aboriginal languages have no contrast between voiced and voiceless consonants. A sound can sound like a ‘p’ or a ‘b’ depending on its position in the word.\nNo fricative sounds.\nLove the ‘r’ sound. Warlpiri has three ‘r’ sounds.\n\nInteresting Linguistic Features\n\nAvoidance register, a special style of language is used between certain family relations that have a drastically reduced lexicon.\nWarlpiri Sign Language also exists.\nSpeakers are often multilingual, learning each other’s languages.\nA strong tradition exists of not saying the names or showing images of people who have passed away.\n\nPresent Status\n\nWarlpiri is considered a threatened language because children sometimes respond in English even when spoken to in Warlpiri.\nThere are efforts to teach the language in schools and create modern terminology."
  },
  {
    "objectID": "notes/cs11-737-w14-ASR/index.html#outline",
    "href": "notes/cs11-737-w14-ASR/index.html#outline",
    "title": "Automatic Speech Recognition",
    "section": "Outline",
    "text": "Outline\n\nSpeech Recognition Demo and Evaluation Metrics\n\nDemonstration of speech recognition.\nDiscussion of how well it works, and examples of when it fails.\nEvaluation metrics.\n\nEvaluation Metrics\n\nSentence error rate.\n\nDiscuss if the entire sentence is correct.\nExplain why this is too strict of a measure, and the need to consider local correctness.\n\nWord error rate (WER).\n\nUsing edit distance word-by-word.\nCalculating error rate percentage.\nHow to compute WER for languages without word boundaries.\n\nOther metrics.\n\nPhoneme error rate (requires a pronunciation dictionary).\nFrame error rate (requires an alignment).\n\nNIST Speech Recognition Scoring Toolkit (SCTK).\n\n(A bit) Mathematical Formulation of Speech Recognition\n\nSpeech recognition as a conversion from a physical signal to a linguistic symbol.\nExplanation of probabilistic formulation.\n\nMAP decision theory to estimate the most probable word sequence.\nNoisy channel model.\nFactoring and conditional independence.\n\n\nStandard Speech Recognition Pipeline\n\nFeature extraction.\n\nConverting waveform to MFCC.\nLanguage-independent process.\nDesirable representations.\n\nAcoustic Modeling.\n\nConverting speech features to phoneme sequences.\nUsing Hidden Markov Model (HMM) to align speech features and phoneme sequences.\nLanguage-independent.\n\nLexicon.\n\nPronunciation dictionary.\nCMU dictionary.\nMultilingual phone dictionary.\n\nLanguage Model.\n\nUsing N-grams or recurrent neural networks.\nWord selection based on context.\nLanguage-dependent.\n\n\nEnd-to-end Speech Recognition\n\nUsing a single neural network.\nA simpler solution for multilingual ASR."
  },
  {
    "objectID": "notes/cs11-737-w14-ASR/index.html#papers",
    "href": "notes/cs11-737-w14-ASR/index.html#papers",
    "title": "Automatic Speech Recognition",
    "section": "Papers",
    "text": "Papers\n\n\nA look at NIST’s benchmark ASR tests: past, present, and future\n\n\nThe IBM 2015 English Conversational Telephone Speech Recognition System\n\n\nAchieving Human Parity in Conversational Speech Recognition"
  },
  {
    "objectID": "notes/cs11-737-w13-speech/index.html#intro",
    "href": "notes/cs11-737-w13-speech/index.html#intro",
    "title": "Speech",
    "section": "Intro",
    "text": "Intro\nToday’s lecture today is about speech. Today, we also have our assignment release. After this lecture, we will have a walkthrough of the assignment speech. I hope I can finish it earlier so that you guys have enough time to explain the assignment three. We actually put a lot of energy into it, so I hope you guys can enjoy it. anyway, for today’s content, I put the four other items: speech, speech applications, speech databases, and speech hierarchy. I will probably go through at least three items, and the fourth maybe the this time or next Thursday so the next week that I will talk about the"
  },
  {
    "objectID": "notes/cs11-737-w13-speech/index.html#what-is-speech",
    "href": "notes/cs11-737-w13-speech/index.html#what-is-speech",
    "title": "Speech",
    "section": "What is speech???",
    "text": "What is speech???\nThe most important definition is speech. Many people say that “it’s sound produced by humans.” It may probably be true, but in many cases, we actually have a bit more narrow definition, which is “the sound produced by humans for the communication” that is mostly actually used in our kind of other speech processing but generally the wider meaning of the speech can be just by humans that is a kind of our other usual definition and then I will try to ask you guys about the four kinds of five audio and please answer this one is speech or not with water is it speech or not it is? Speech yes it’s very noisy but still speech next probably this is not speech or this thing is speech yeah probably business speech by the way all the sound oh no except for the first one. I picked up from the other free sound so this the link is also useful you guys can get a lot of kind of interesting sound and actually using it for your research. it’s also copyright free, so quite kind of easy to use the Saturn is this speech or not it’s actually the difficult so my definition still speech because laughing and so on actually accelerate our communication right so my definition are usually yeah they’re using it as a speech yeah even people are typing to do some communication but this is not human voice so this is not speech right and the last one what do you think is this speech or not so if this is the wide the sense of the the sound to produce a human this is speech but this is not free for the communication especially singing voice well some people can use singing words for the communication like opera and so on but in general it is not used for the communication so again this definition is not very clear but usually people actually regarding the speech and the singing voice are separate okay so this is the definition of the speech and the next a speech is made by the sun right and what is sound sound is just a change of the air pressure and that is captured my microphone so it’s quite physical phenomena so compared with the various topics we have been studying in multilingual nlp many of the kind of the problems are more like a syntax semantic other textual information but speech is actually quite that comes from this the physical phenomena and the actually this other sound"
  },
  {
    "objectID": "notes/cs11-737-w13-speech/index.html#speech-waveform",
    "href": "notes/cs11-737-w13-speech/index.html#speech-waveform",
    "title": "Speech",
    "section": "Speech waveform",
    "text": "Speech waveform\npressure is captured by microphone and they converted either this other one-dimensional waveform and usually in this lecture, I use this one-dimensional waveform as an input to our problem in speech other processing but it’s actually not one-dimensional like for example human years a very good. example we actually at least have two two-dimensional other waveforms and the many of the the smart speakers including this one or the Alexa and so on. They also have the other more microphones but in general each of the kind of channel of the signal is the one other dimensional time domain waveform and since this is a waveform so this is governed by well-known physical properties. I am not sure how many people remember this kind of properties attenuation deflection the the the diffraction super position and so on so actually these properties are quite a making speech signal to be ditch information and due to this kind of property especially superposition and reflection is a kind of very important the property to include a lot of the information inside the waveform this becomes the least information in the speech so first no second question actually the second question to you guys is"
  },
  {
    "objectID": "notes/cs11-737-w13-speech/index.html#what-kind-of-information-does-speech-sound-contain",
    "href": "notes/cs11-737-w13-speech/index.html#what-kind-of-information-does-speech-sound-contain",
    "title": "Speech",
    "section": "What kind of information does speech sound contain?",
    "text": "What kind of information does speech sound contain?\nwhat kind of information does speech sound contain of course I will talk about the speech condition so this means that the speech includes linguistic content transcription and so on right and the speech also has a speaker characteristics is there any other information that speech can deliver can someone tell me phonemic signals like emphasis and so on yes exactly exactly any other notion emotion yes good good are there any other examples actually quite a lot by the way for example one of them yeah you can tell if somebody’s sick yes if they have a cold yes they if they talk too much the previous night yes yes exactly exactly you know other other faculties here also that try to kind of detect the the person is copied or not from the voice signal so voice can also include the health information voice also that speech also includes a lot of other information for example we can speech information even include the location you guys can identify at least you know which direction speech comes right this is not purely speech but it’s a kind of multiple other signal of the speech we can identify other the the the location speech comes from speech also include the reverberation so from the evaporation we can even approximately say the room size and so on is there any other information maybe a lot that I can come up with other the many of them and actually this other information is that corresponds to various other speech applications so now move to the the speech applications so the speech applications are depend on the other problem but they actually the the many of the problem is to try to capture what information is delivered from speech speech condition is a good example right and there’s speech emotion recognition try to capture the emotion and of course we also have our audi the opposite problem given the text to generate the speech speech synthesis this is also the speech processing research topics and the we actually hover tons of the other problem in speech speaker recognition as I mentioned speech delivered speaker characteristics right so that from the speech signal we can actually identify who is speaking oh I forgot to mention about it this is actually very important for the modern thing or nlp language recognition from speech sound we can also identify the language and the there are a lot of the speech the program by the way these topics i extracted from the topics in the intel speech which is one of the biggest speech conference and the deadline is next month right one month right after right after this that day one month right after this today so if people here are thinking about doing something for speech inter speech is a good target but I think if we link it to the project here maybe it’s a bit too late okay so we have a tons of the speech application and by the way among them the next question among them what is the most widely used technique there’s you know under some how they say the definition what is other than the mostly used but I think everyone can agree if I mention the answer so can someone say what is the yes yes do not check the next line because people usually cannot answer so yeah speech recognition probably in terms of the number of the researchers the the biggest one is speech recognition and the next biggest the second biggest would be speech synthesis and other kind of following and the speech coding is actually a bit the the already sorted technique so there’s not so much research in this area but actually speech coding is the most used technique and the actually the speech coding is a unique compared with all others all the other tried to harder say some given input to extract some information or enrich some information by generating that and so on the speech coding is try to keep the original information but try to kind of compress the"
  },
  {
    "objectID": "notes/cs11-737-w13-speech/index.html#speech-coding",
    "href": "notes/cs11-737-w13-speech/index.html#speech-coding",
    "title": "Speech",
    "section": "Speech coding",
    "text": "Speech coding\ninformation as much as possible and actually this was the one of the first or the most active study in the early the speech the processing research since at that time compression is quite important and also compression actually tells us what is speech what information is delivered is a quite important if we consider the speech coding and so on so that’s why people are starting to work on the speech coding and then the that we moved to various other applications and another the many of them are actually getting quite matured I would say and one of the techniques I would like to mention is a speech recognition and that can be covered"
  },
  {
    "objectID": "notes/cs11-737-w13-speech/index.html#automatic-speech-recognition-asr",
    "href": "notes/cs11-737-w13-speech/index.html#automatic-speech-recognition-asr",
    "title": "Speech",
    "section": "Automatic Speech Recognition (ASR)",
    "text": "Automatic Speech Recognition (ASR)\nin Thursday and after the break I will also other revisit the sr part again so using the two lectures to explain about a speech condition but the speech recognition is one of the very complicated system if you guys really want to understand a speech recognition it may take one semester so then please take the speech recognition and understanding in the next fall semester if you really want to know that the next the most the second the biggest obligation is speech synthesis"
  },
  {
    "objectID": "notes/cs11-737-w13-speech/index.html#speech-synthesis-tts-text-to-speech",
    "href": "notes/cs11-737-w13-speech/index.html#speech-synthesis-tts-text-to-speech",
    "title": "Speech",
    "section": "Speech Synthesis (TTS: Text to Speech)",
    "text": "Speech Synthesis (TTS: Text to Speech)\nand the speech synthesis is also are they given the text to generating the web form and how the wider and I think this is the possibly the the second biggest applications than speech recognition I believe because every and now you know are that hard about that the tts synthesize the voice right so in terms of the application this also has a big success and as everyone here may note that the young guys are lucky so professor aram black he is the pioneer of the text speech and he will have a lecture about dds I’m also very looking forward to that okay so there are a couple of others and I try to pick up not everything but some of them that are interesting or are important the next one that may be very related"
  },
  {
    "objectID": "notes/cs11-737-w13-speech/index.html#speech-translation",
    "href": "notes/cs11-737-w13-speech/index.html#speech-translation",
    "title": "Speech",
    "section": "Speech Translation",
    "text": "Speech Translation\nto this course is speech translation so directly converting the source speech to that target text so in this example a japanese speech it’s translated to english text how to do it one simple approach is to combine automatic speed recognition first and then machine translation and this is probably the other most kind of other the widely used way and the I think other that this way should be also considered but these are the two system combining two systems a bit complicated and also if we have our error in the asl side it cannot be well recovered in the second machine translation side so people also are starting to directly are using an end-to-end neural network to solve the japanese source speech like a Japanese speech to a english text directory without outputting Japanese text and so on and this approach is quite important if some languages don’t have their own scripts but they often kind of have a kind of a translation to their colonial languages and so on there’s such a lot of such kind of other databases actually and then this speech to speech to text direct information is also quite important in these cases and the this is to the speech to still text but if we try to use it for the interface to communicate to the foreign people in the foreign countries seamlessly we also want to develop the source a speech to speech translation in this case source speech is english and it is converted to the target speech and this is also started to be very popular not only as a research but also other other product and many of the systems are currently using the combination of the speech recognition first machine translation in the middle and then other other dts takes to speech but there are a lot of kind of a decent emergence techniques based on the end-to-end single neural network to even the model this kind of a very complicated pipeline based on a single architecture and I think this is one of the the ultimate goals of multilingual energy right if we have a perfect speech to speech translation systems probably we can solve many of the problems that discussed in this work but this problem is quite difficult and a lot of studies are all going on now so these are the related to marginal nlp and I will also introduce some of the other speech applications which will be very related to our problems so I just kind of combined the speaker recognition language construction speech emotion recognition all the event classification and detection so actually speech information has a lot of profiling information so as I mentioned the speech recognition is one of the example this means a linguistic information right and we speech also other the speaker gender aging information as well age is not very correct but we can still actually predict some range of the age based on speech and the language we can also add get the information from speech emotion that we have discussed that is also other extracted from speech and this is not completely speech information but the let’s say environment is important for us to understand about the communication scene and the song right and then audio event so when I am speaking the fun noises you know working on somewhere right this kind of information all that kind of informations are included in speech so superposition property is quite nice in terms of packing many of the information into the one sound wave form"
  },
  {
    "objectID": "notes/cs11-737-w13-speech/index.html#privacy-in-speech",
    "href": "notes/cs11-737-w13-speech/index.html#privacy-in-speech",
    "title": "Speech",
    "section": "Privacy in speech",
    "text": "Privacy in speech\nso this the property is very good but it’s getting a bit more other difficult not difficult but challenging problem privacy in speech since speech has a lot of information so that the working on speech means we potentially touch a lot of information of that other person so the people are also seriously thinking about the privacy and on-device approach becomes quite important due to this i’m setting the how many slides I can have I have to finish maybe this is very cool so I can also either introduce one this one so as I mentioned the super position property is very cool in terms of the getting various information in the environment right but this information is not always useful actually many cases we just want to focus on target speaker right so then the speech enhancements peace separation techniques are also very important it’s not very related to this other lecture but I will just explain about it because many people actually often ask me the the my asl doesn’t work and most of the cases it comes from noise so I just want to emphasize that and there are way to deal with that and there are several types of the the noise"
  },
  {
    "objectID": "notes/cs11-737-w13-speech/index.html#speech-enhancement-several-types-of-problems",
    "href": "notes/cs11-737-w13-speech/index.html#speech-enhancement-several-types-of-problems",
    "title": "Speech",
    "section": "Speech enhancement Several types of problems",
    "text": "Speech enhancement Several types of problems\nwe have to deal with the first is a kind of how does this the the gesture or the environmental sound that adding to our the the usual sound you guys I guess can steal the recognize it right computer actually cannot at all probably you guys can do better than computer so to solve this problem we also cover other noising techniques this is actually after the denoising sounds better right this is actually very good for computer and the next debug variation is also another very the annoying issue again for you guys it is not so difficult to recognize but for computer this evaporation echo is quite harmful so there’s a technique to suppress the evaporation called the reverberation if you guys using the headset microphone you guys can even clearly see the the find that the the echo is removed since this room is a bit large so actually echo is further accomplished but I believe still people can understand it next one is the separation I think I prepared a very cool demonstration by oh yes I prepared this demonstration this is actually the he is my former colleague when I was working at the mitsubishi electric research laboratory jonathan and he actually it’s he was planning to come here this friday but it was cancelled so for it is unfortunate but instead other I just play his cool demonstration you can see that the the mixed speech is completely separated right so this is one of the pioneering work of using deep running for speech separation in the first time and this is already very clearly separated right but now the technology becomes mature and this other kind of a simple speech separation problem is almost solved however we have a lot of other difficult separation problems okay so I think these are more like the applications that I want to cover there are lots of other application if I have time I can also want to introduce the some of them like a spoken dialogue systems and so on but I just skipped them due to that time limitation okay so next the I will talk about speech databases to work on this kind of our core speech problems we have to have our database"
  },
  {
    "objectID": "notes/cs11-737-w13-speech/index.html#speech-variations-speaking-styles-and-environments",
    "href": "notes/cs11-737-w13-speech/index.html#speech-variations-speaking-styles-and-environments",
    "title": "Speech",
    "section": "Speech variations Speaking styles and environments",
    "text": "Speech variations Speaking styles and environments\nand the speech as I mentioned has a variations a lot of variations right and the there are many axes but mostly we separate classify the speech database with the speaking styles and the environment and so on, but I will just talk about the speaking styles for now so one style is wet speech and the other is spontaneous or non-bit speech, and I will explain how it is different maybe I can play some of them, so this one is the first one versus journal this is one of the most famous speech recognition corporates so if you guys have some cool idea for the speech equation techniques and the the showing the kind of other improvement in this abortion channel you guys can write the paper so it is a bit so the game is small, but it is a so-called red speech and the why it is called versus journal people are leading the world’s regional standards so that the the disco pacifi named work to each other switchboard this is an example of spontaneous speech. I think it is obvious that the second one it’s more difficult, right, but the second one is more the natural conversation and the third one this is the corpus called liberty speech and I mentioned that wall street just now is one of the most famous speech corporal, but now that this liberal speech is the most famous the most frequently used the corpus why it is used so popularly this is simply because the license is quite flexible wall street journal has a kind of a little bit limited license while river speech doesn’t have so much restriction for the license and also the amount of data is also quite large thousand hours so that’s the reason that people are using the legal speech so okay I will try to explain a bit more about what is red speech I kind of mentioned already about some of them so that speech is usually corrected as follows first other we have some sentence showing in the prompt, and then we can just read this prompt which are corresponding to we get the paradigm of the prompt and corresponding speech data that’s it right it’s very easy I believe i prepared this how many people knows this the the web page this kind of activity come on boys this is why widely the the used the speech the the data collection scheme led by the modular and this actually has tons of the multilingual speech resources now so if you guys want to get some speech data the one of the first phrase you will check is this common voice and this is a common voice the correction is based on the red speech prompt difficult for example if you click this one it’s working but she has also served as chair of the board of bomb health new zealand oh yes he has also served as jail the board of bomb health new zealand and the next prop this comes and I will speak that it is very cool system right and then we can actually create a lot of languages and again this supports many many languages now I was super happy when you know they started to support japanese and I spent you know half day just speaking probably many of the my voices are using this are the database however be careful people sometimes don’t speak correctly how to other than deal with that they actually are not only recording they actually hover oh sorry maybe I can reload it this is a bit difficult because I have to we also have another action listen this actually confirms whether the recorded speech is correct or not and I I haven’t tried this but I am only speaking in my Japanese so not sure if you know correctly you know they recognize my japanese and they registered it those are the useful corpus or not so this is the that speech"
  },
  {
    "objectID": "notes/cs11-737-w13-speech/index.html#read-speech-examples",
    "href": "notes/cs11-737-w13-speech/index.html#read-speech-examples",
    "title": "Speech",
    "section": "Read speech examples",
    "text": "Read speech examples\nso again this is easy to connect but I have a lot of experience that the people don’t speak correctly, so we actually need to check whether they are speaking correctly and also easy to anonymize compared to the spontaneous speech because it is prepared sentence it is not connected with the speaker’s identity, so if the speaker there is completely anonymized that we don’t care about it, but the problem is that let’s speech is not a real conversation, so this is a very good spot to starts to build a speech function system but if I put it in in this system in the conversational scenario it will not work so well"
  },
  {
    "objectID": "notes/cs11-737-w13-speech/index.html#spontaneous-speech",
    "href": "notes/cs11-737-w13-speech/index.html#spontaneous-speech",
    "title": "Speech",
    "section": "Spontaneous speech",
    "text": "Spontaneous speech\nso instead we also used a spontaneous speech, and this one is very tough. It’s actually hard to transcribe actual recording yeah again i prepared an example by the way, how many people use this to own that city did you know that audacity is developed here cmu yeah again you guys are very lucky cmu can provide any of the other tools for other speech and the nlp research and the I think I’ve prepared the wait a moment the way to transcribe the speech the situation speech I think, okay oh, I actually put the latest rider give it a moment my computer is not working now. Okay, now it’s working, so if I have more skills you know switch your screen and so on I can do some demonstration in front of you, but it’s very difficult to do it, you know, by checking the other back monitor and this monitor so instead i will just upgrade the play the audio a play the video of how to transcribe the spontaneous speech check the segment part manually, then type okay check the segment [Music] okay, just to transcribe this I don’t know five seconds of speech it takes a very long time compared with the left speech right so this is quite time consuming and it is quite rather expensive to collect such kind of data like for example this switchboard this is a switchboard corpus which is one of the other famous speech condition coppers, but this is spontaneous speech and the when the in the previous my lecture i actually had a homework to the student to transcribe two minutes of this dashboard conversation I think, yeah the 30 minutes is actually a shorter side most of the people takes random work this is the one of the most honestly biggest complaint of my homework actually since it is very very hard of course, transcribers are very professional, so they can finish it a little bit earlier but still the the spontaneous speech the the recognition and transcript transcription there is a very very difficult I think I need to finish in five minutes or so so this is the the just speech is very fluent but a dear situation is more difficult like for example the in the real situation the meeting scenario or compensation scenario this is a mostly we use for the speech right and then the situation is even more difficult As I mentioned last in the speech enhancement demonstration, we have background noise we have our interference speaker, and we have a reverberation and so on but this is kind of our again our usual the everyday conversation scenes right and we can easily recognize this kind of sound speech while the alexa and so on I guess you guys have like some other experience just you know, putting it to the the, and then we started to conversate it they actually cannot be recognized well they even don’t know that of each other person they recognize and so on it’s very difficult problem but again human can do it easily and if I am actually working on this area a lot so not only for the spontaneous speech but also in the kind of real everyday conversation scenario and the project is called chime project and this is we actually collected such kind of a very the adverse the the environment of this collected speech in the very adverse environment and then the also are organizing the challenge making a baseline and so on I can play some of them so compared with the previous conversation it has a lot of over by the way also noise and so on, but again, this is a natural conversation it makes the speech very difficult so I think yeah"
  },
  {
    "objectID": "notes/cs11-737-w13-speech/index.html#where-we-found-the-speech-data",
    "href": "notes/cs11-737-w13-speech/index.html#where-we-found-the-speech-data",
    "title": "Speech",
    "section": "Where we found the speech data?",
    "text": "Where we found the speech data?\nI will finish it in a minute, so there are again a lot of types of the speech and many researchers many other the institutes actually collecting the data for the research and the development for the community and the we can actually the access to such data easily with some of the repository one of the most frequent one is ldc linguistic data consortium where you can actually get famous asr benchmarks like a teammate a world regional switchboard that I mentioned before however as I mentioned that this has some kind of the the restriction for the use we have to pay they do the I get this kind of data but again fortunately cmu is a member so everyone can actually get lgc data really so the for this part you guys would not cover any kind of issue, I believe, and another only for the fdc. There are a a lot of other institutes g,overnment institute or some the university and so on hosting a lot of other speech corporal so you guys can actually get such kind of the speech data through this kind of repository and the second are the bullet items are getting the more popular now the books first open the center of common boys are denoted and the common voice is that I mentioned before and this this data usually have a less restricted license a mostly creative command so that the that we may not go through the LDC and they can easily get the data and even redistribute the data and so on, so due to this reason, recently, many people actually starting to use this data in the box for your common voice and so forth, especially people working on the likewise machine learning and not in the nlp or speech some people may not have an access to the LDC and then people using this kind of repository to get the data, so these are also important resources by the way, these are important resources for assignment three as well, so that’s why I kind of spent some time for the explanation, and the last one is the audiobooks or public recordings with captions youtube actually the ten percent of the youtube videos hazard captions they are set by the the uploader so we can use speech and corresponding captions and the same for the podcast for data token and so on generally, they have some captions so that we can actually use such kind of data and so on, but for this data, maybe you guys also check the license. Sometimes it’s very strict, sometimes not but for most of the kind of research purpose in the academic institute, it shouldn’t be a problem, but the more difficult part is that this recording is required for just one hour and a half of the long recording and corresponding very long the caption so it’s it is just too long for us to use it so we need to correctly segment it and also this is caption is very noisy so it is not easy to use this kind of caption another very important difficulty is that this kind of data is updated frequently so they’re often deleted so we cannot actually others assure the reproducibility if we’re using this one and one of the the famous the databases actually cmu’s wireless the wiredness database game that professor alan black collected and it’s actually has a 700 languages wow that is very cool but it is also already has some issues written here like a bit noisy or API if API is changed it is not easy to get the data and so on, but in general, this is also very good source and the last very last this is important important for the assignment for speech recognition we often use our other unit so please remember I even saw that this is a common the sense for everyone and that they use it without caring about the your prior knowledge so I also want to actually define we often use the hour and, let’s say, more than a thousand dollars, we can make a commercial system but it requires a lot of computational cost 100 hours a speech equation started to be working so we can do the research and so on lesser 100 hour it is categorized as a low resource language in speech recognition so other please understand this range of the data if you try to practice speech recognition and then I would like to pass it to the assignment three but maybe I can accept one or two quick question there any questions yes, like annotating this spontaneous speech could we you first pass it into a voice activity detection: oh yeah segment it and then annotate it yes usually there are such kinds of assistance to a lot of assisted tools. Even we use a pre speech condition and then do it but the voice activity detection is super important so yes that we usually either have such kind of our other voice activity direction but voice activity detection is not working in the noisy environment so much so anyway most of the cases are the human segmentation is also quite important and in question yes definitely we have such kind of other issues since we tried to capture more like a similar closer information and then the the great program for the same gender and then speech separation performance is big world and it’s working very well in a male female and so on so it’s definitely that these kind of things exist but still the current technology almost perfectly separated we almost perfectly separate the speech even in the same gender but it’s a good question okay so maybe that’s it and I want to pass it to this chiang kai and patrick"
  },
  {
    "objectID": "notes/cs11-737-w15/index.html",
    "href": "notes/cs11-737-w15/index.html",
    "title": "Speech",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\nVideo 1: Lesson Video\n\n\n\n\n\n\n\n\nThis week’s slides\n\n\n\n\nSupplementary Figure 1\n\n\n\n\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\nWhat is speech?\nSpeech applications\nSpeech databases\nSpeech hierarchy\n\n\n\n\n\n\n\n\n\nTranscript\n\n\n\n\n\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{bochman2022,\n  author = {Bochman, Oren},\n  title = {Speech},\n  date = {2022-03-01},\n  url = {https://orenbochman.github.io/notes-nlp/notes/cs11-737-w15/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2022. “Speech.” March 1, 2022. https://orenbochman.github.io/notes-nlp/notes/cs11-737-w15/."
  },
  {
    "objectID": "notes/cs11-737-w13-speech/index.html#papers",
    "href": "notes/cs11-737-w13-speech/index.html#papers",
    "title": "Speech",
    "section": "Papers",
    "text": "Papers\nHere is a list of papers mentioned in the lesson:\n\nKim, Hori, and Watanabe (2017) Joint CTC-Attention based End-to-End Speech Recognition using Multi-task Learning\nBaevski et al. (2020) wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations\nHsu et al. (2021) HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\nChang et al. (2021) An Exploration of Self-Supervised Pretrained Representations for End-to-End Speech Recognition\nHershey et al. (2016) Deep clustering based speech separation"
  },
  {
    "objectID": "notes/cs11-737-w11-code-switching/index.html#outline",
    "href": "notes/cs11-737-w11-code-switching/index.html#outline",
    "title": "Code Switching, Pidgins, Creoles",
    "section": "Outline",
    "text": "Outline\nHere is an outline of the lesson, based on the sources you provided:\n\nLanguage Change Languages are always changing.\n\nThis can be due to a number of factors, including laziness/efficiency, emphasis/clarity, politeness, misunderstanding, group identity, and structural reasons.\n\nLanguage Contact Language contact, where multiple languages are used in the same place at the same time, is a major driver of language change.\nLexical Borrowing Lexical borrowing is common in languages.\n\nThis involves identifying words that are similar across languages and likely to be mutual translations.\n\nLexicon Structure Languages have a core-periphery lexicon structure.\nTransliteration Models These models, including FSTs and LSTMs, can help map lexicons across languages.\nTransliteration Evaluation This can be done intrinsically (word accuracy, fuzziness, ranking) or through downstream evaluation like machine translation.\nCognates and Loanwords These are important in language contact, with phonological and morphological integration.\nBilingual Lexicon Induction This involves learning monolingual embeddings, finding alignment between embedding spaces, and finding nearest neighbors to induce a lexicon.\nCode-Switching This usually occurs between fluent speakers in casual speech or text, often with a matrix language determining word order.\n\nThe amount of code-switching can be measured.\n\nPidgins and Creoles\n\nPidgins are simplified mixes of multiple languages learned by non-native speakers.\nCreoles develop when pidgins become native languages.\nThese terms can also be used politically.\n\nMajor Code-Switching Dialects Common examples include Hinglish, Chinglish, Spanglish, and African American English.\nWhen Code-Switching Occurs This can be influenced by vocabulary coverage, “showing off,” sentiment, and entrainment.\nWhy NLP of Non-Standard Language Matters Code-switching is how people communicate and can define group membership.\nChallenges in Code-Switching NLP Limited data, noise, non-standard spelling, and the confusion of word embeddings.\nCode-Switching Data Social media sites like Twitter, YouTube, and Reddit are good sources.\nCode-Switching: LT Tasks These include language ID, speech recognition/synthesis, spelling normalization, POS tagging, named entity recognition, sentiment analysis, and question answering.\nCode-Switching: Techniques These are similar to low-resource language techniques, including finding appropriate data, bootstrapping labeling, and data augmentation.\nMining Data This involves labeling a small amount of data, building a classifier, and using it to label more data.\nBootstrapping Labeling This involves building a generator from limited data and using classifiers to distinguish real from generated data.\nData Augmentation/Generation This can involve paraphrasing existing data or replacing named entities.\nEvaluation Techniques These include task evaluation of held-out data and using standard techniques.\nDiscussion Points The lesson includes discussion points such as picking a language and analyzing its influence on others, analyzing code-switching in another language, and considering cross-lingual morphological issues."
  },
  {
    "objectID": "notes/cs11-737-w12-qa/index.html#outline",
    "href": "notes/cs11-737-w12-qa/index.html#outline",
    "title": "Multilingual Q&A",
    "section": "Outline",
    "text": "Outline\n\nIntroduction to Question Answering (QA)\n\nHumans have long envisioned asking computers questions.\nQA is now part of daily life.\nExamples of QA systems:\n\nLUNAR\nIBM Watson\nSQuAD\n\nTraditional QA systems are mostly English-language based.\n\nThe Importance of Multilingual QA\n\nMultilingual QA helps people access information easily.\nThere is a steep drop in QA technology quality after the top languages.\n\nTypes of Question Answering\n\nOpen-Retrieval QA (aka Open Domain QA)\n\nGiven a question, the system finds an answer from a text corpus.\n\nKnowledge Graph QA\n\nGiven a question, the system finds an answer from a knowledge graph.\n\n\nOpen-Retrieval QA in Detail\n\nSteps:\n\nPassage Retrieval: Extracting the document from a corpus that likely contains the answer.\nReading Comprehension: Extracting the succinct answer from the retrieved document.\n\n\nPassage Retrieval Techniques\n\nTF-IDF\n\nSelects documents containing many query terms, discounting terms that occur frequently in many documents.\n\nBM25\n\nA modification of TF-IDF that discounts term weight based on document length.\nDoes not require a statistical model.\n\nDense Passage Retrieval (DPR)\n\nEncodes queries and documents into vectors using models like BERT.\nIndexes the search corpus offline.\nRequires fine-tuning on a dataset of questions with relevant documents.\n\n\nReading Comprehension Techniques\n\nExtractive QA\n\nThe answer is assumed to be a span in the passage.\nA common approach is to concatenate the question and passage, encode each token, and predict the start and end tokens of the answer.\n\nGenerative QA\n\nThe answer is freely generated using the passage as evidence.\nA sequence-to-sequence model can be used to generate the answer token by token.\n\n\nMultilingual QA Problem Settings\n\nMultilingual QA\n\nQuestions, corpus, and answers are all in the same language.\n\nCross-lingual QA\n\nQuestions, corpus, and answers can be in different languages.\n\n\nApproaches to Multilingual QA\n\nZero-Shot Transfer\n\nFinetune a multilingual model on an English QA dataset and transfer it to a new language.\nAdding a few target-language examples can improve performance.\n\nTranslation-Based Adaptation\n\nTranslate the question to English, use an English QA system, and translate the answer back.\n“Translate-Train” involves translating the full training data to the target language.\n\nMultilingual Retriever-Generator\n\nUses a multilingual DPR to retrieve passages in any language and a multilingual generator to produce the answer.\nRequires iterative self-training due to lack of training data.\n\n\nEvaluation Metrics for QA\n\nF1 Score\n\nCalculates precision and recall of token overlap between the generated and actual answers.\n\nExact Match (EM)\n\nMeasures how often the generated answer exactly matches the true answer.\n\nBLEU Score\n\nN-gram overlap between the generated and actual answers.\n\n\nMultilingual QA Datasets\n\nXQuAD and MLQA\n\nMachine reading comprehension datasets based on translating English datasets.\n\nMKQA\n\nOpen-retrieval QA dataset based on translating Natural Questions.\n\nTyDi QA\n\nQA pairs collected naturally in multiple languages.\n\nXOR-QA\n\nBased on TyDi QA with added English translations for unanswerable questions.\n\n\nKnowledge Graph QA\n\nInvolves converting a question into a query against a structured knowledge graph.\nMethods include zero-shot transfer and translation-based adaptation.\n\nOpen Problems in Multilingual QA\n\nCross-lingual QA performance is still far from English QA or human-level performance.\nLeveraging non-linguistic cues (images, fact tables).\nAnswering code-mixed questions."
  },
  {
    "objectID": "notes/cs11-737-w12-qa/index.html#papers",
    "href": "notes/cs11-737-w12-qa/index.html#papers",
    "title": "Multilingual Q&A",
    "section": "Papers",
    "text": "Papers\nThe papers mentioned in the context of multilingual question answering are: * One Question Answering Model for Many Languages with Cross-lingual Dense Passage Retrieval (Asai et al. 2021) * Improving Zero-Shot Cross-lingual Transfer for Multilingual Question Answering over Knowledge Graph (Zhou et al. 2021) * Dense Passage Retrieval for Open-Domain Question Answering (Karpukhin et al. 2020) * TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages (Clark et al. 2020) * A BERT Baseline for the Natural Questions (Alberti, Lee, and Collins 2019) * Bootstrap Pattern Learning for Open-Domain CLQA (Shima and Mitamura 2010) * Natural Questions: A Benchmark for Question Answering Research (Kwiatkowski et al. 2019) * On the Cross-lingual Transferability of Monolingual Representations (Artetxe et al. 2019) * RuBQ 2.0: An innovated Russian question answering dataset (Rybin et al. 2021) * QALD-9-plus: A Multilingual Dataset for Question Answering over DBpedia and Wikidata Translated by Native Speakers (Perevalov et al 2022) * Multi-domain Multilingual Question Answering (Ruder 2021 [blog post])"
  },
  {
    "objectID": "notes/cs11-737-w11-code-switching/index.html#papers",
    "href": "notes/cs11-737-w11-code-switching/index.html#papers",
    "title": "Code Switching, Pidgins, Creoles",
    "section": "Papers",
    "text": "Papers\n\nDanescu-Niculescu-Mizil et al. 2013\nGibson 2019\nKnight & Graehl ’98\nRosca & Breuel’16\nWu & Cotterell’19\nMann & Yarowsky ’01\nDellert ’18\nKondrak ’01\nKondrak, Marcu & Knight ’03\nBouchard-Côté et al. ’09\nHall & Klein ’10\nHall & Klein ’11\nTsvetkov & Dyer ’16\nSoisalon-Soininen & Granroth-Wilding ’19\nYip ’93\nKang ’03\nKenstowicz & Suchato ’06\nBenson ’59\nFriesner ’09\nSchwarzwald ’98\nOjo ’77\nSchadeberg ’09\nJohnson ’14\nHaspelmath & Tadmor ’09\nHolden ’76\nVan Coetsem ’88\nAhn & Iverson ’04\nKawahara ’08\nHock & Joseph ’09\nCalabrese & Wetzels ’09\nKang ’11\nRabeno ’97\nRepetti ’06\nWhitney ’81\nMoravcsik ’78\nMyers-Scotton ’02\nGuy ’90\nMcMahon ’94\nSankoff ’02\nAppel & Muysken ’05\nRudra et al 2016\n\nA survey of code-switched speech and language processing\n\nKhanuja et al. (2020) GLUECoS : An Evaluation Benchmark for Code-Switched NLP\nCommon Amigos (Ahn et al 2020, Parekh et al 2020)\n\nNote that many of these citations do not have links in the provided sources. The citations that have URLs are linked."
  },
  {
    "objectID": "notes/cs11-737-w15-seq2seq-ASR/index.html",
    "href": "notes/cs11-737-w15-seq2seq-ASR/index.html",
    "title": "Speech",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\nVideo 1: Lesson Video\n\n\n\n\n\n\n\n\nThis week’s slides\n\n\n\n\nSupplementary Figure 1"
  },
  {
    "objectID": "notes/cs11-737-w15-seq2seq-ASR/index.html#outline",
    "href": "notes/cs11-737-w15-seq2seq-ASR/index.html#outline",
    "title": "Speech",
    "section": "Outline",
    "text": "Outline\n\nIntroduction to End-to-End Speech Recognition\n\nSequence-to-sequence model for speech recognition, also known as end-to-end speech recognition\nThe sequence-to-sequence model uses a single neural network to directly model speech\nSpeech recognition is a sequence-to-sequence problem that converts an input sequence of speech features into an output sequence such as a word or character sequence\nFive methods to address the sequence-to-sequence problem\n\nHMM-based pipeline system\nConnectionist Temporal Classification (CTC)\nAttention-based encoder-decoder\nJoint CTC/attention (Joint C/A)\nRNN transducer (RNN-T)\n\n\nHMM-Based Pipeline System\n\nComposed of feature extraction, acoustic modeling, lexicon, and language modeling\nFeature extraction converts the waveform to speech features such as MFCC\nAcoustic modeling is a probabilistic model to convert the observation to phoneme sequences\nLexicon converts the phoneme sequence to a word sequence\nLanguage modeling provides the most likely word sequence\nRequires knowledge of digital signal processing, pattern recognition, machine learning, linguistics, NLP, and statistical modeling\nDifficult to master all areas and implement all components\n\nConnectionist Temporal Classification (CTC)\n\nOne of the first end-to-end methods developed in speech recognition\nCovers acoustic modeling but does not explicitly cover language modeling\nUses an encoder to perform feature extraction\nThe output is character or textual tokens\nUses dynamic programming to address the alignment problem\nCan be used for online processing\nHas a conditional independence assumption\n\nAttention-Based Encoder-Decoder\n\nIncludes a language model\nThe attention decoder can have a language model-like behavior, predicting the next sequence based on the previous history of the predicted sequence\nThe encoder is initially represented by a recurrent neural network but is now often replaced with a transformer or conformer\nDoes not have a conditional independence assumption\n\nJoint CTC/Attention (Joint C/A)\n\nCombines CTC and attention during training based on multitask learning\nCombines CTC and attention during inference based on score combination\nThe idea is to combine the monotonic attention of CTC with the attention mechanism\nCTC loss regularizes the network to be monotonic\n\nRNN Transducer (RNN-T)\n\nSimilar to CTC but deducts the conditional independence assumption, especially for the label part\nHandles language model-like architecture\nGood for online streaming systems\nHas some conditional independence assumptions at the input feature level"
  },
  {
    "objectID": "notes/cs11-737-w15-seq2seq-ASR/index.html#papers",
    "href": "notes/cs11-737-w15-seq2seq-ASR/index.html#papers",
    "title": "Speech",
    "section": "Papers",
    "text": "Papers\nThe papers covered in the lesson, according to the sources, are:\n\nConnectionist Temporal Classification (CTC): Graves, Alex, et al. “Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks.” Proceedings of the 23rd international conference on Machine learning. 2006.\nAttention-Based Models: Chorowski, Jan K., et al. “Attention-based models for speech recognition.” Advances in neural information processing systems 28 (2015).\nJoint CTC/Attention: Watanabe, Shinji, et al. “Hybrid CTC/attention architecture for end-to-end speech recognition.” IEEE Journal of Selected Topics in Signal Processing 11.8 (2017): 1240-1253.\nRNN Transducer: Graves, Alex, “Sequence transduction with recurrent neural networks,” in ICML Representation Learning Workshop, 2012."
  },
  {
    "objectID": "notes/cs11-737-w16/index.html",
    "href": "notes/cs11-737-w16/index.html",
    "title": "Speech",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\nVideo 1: Lesson Video\n\n\n\n\n\n\n\n\nThis week’s slides\n\n\n\n\nSupplementary Figure 1\n\n\n\n\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\nWhat is speech?\nSpeech applications\nSpeech databases\nSpeech hierarchy\n\n\n\n\n\n\n\n\n\nTranscript\n\n\n\n\n\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{bochman2022,\n  author = {Bochman, Oren},\n  title = {Speech},\n  date = {2022-03-01},\n  url = {https://orenbochman.github.io/notes-nlp/notes/cs11-737-w16/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2022. “Speech.” March 1, 2022. https://orenbochman.github.io/notes-nlp/notes/cs11-737-w16/."
  },
  {
    "objectID": "notes/cs11-737-w18-morphology/index.html",
    "href": "notes/cs11-737-w18-morphology/index.html",
    "title": "Morphology",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\nVideo 1: Lesson Video from 2020 as the video was missing in 2022\n\n\n\n\n\n\n\n\nThis week’s slides\n\n\n\n\nSupplementary Figure 1\n\n\n\n\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\nWhat is speech?\nSpeech applications\nSpeech databases\nSpeech hierarchy\n\n\n\n\n\n\n\n\n\nTranscript\n\n\n\n\n\n\nhi everyone welcome back to multilingual natural language processing and today we start the next part of the course in which we go deeper into individual topics in multilingual nlp and we will talk about morphosyntax and we talk about morphosyntax because the same meaning can be grammaticalized in different ways in different languages for example some languages will grammaticalize the meaning of future as word others as morpheme and others as grammatical structure and today we will talk about morphology morphological processing morphological analysis and inflection generation and in the next class we will talk about syntax we’ll start with a reminder from the beginning of the course when we discussed linguistic typology we have discussed earlier the definition of a word what is a word is a slippery topic and we have agreed on the following definition that words are the building blocks of phrases and sentences that are members in a syntactic category so they have a part of speech that words tend to be able to move around relatively to each other and be composed into sentences so until now what i mentioned is a syntactic definition and words are linguistic units that usually have main stress sometimes as secondary stress this is a phonological definition and they can correspond to a unit of mean this is a semantic definition so all this together is a definition of a word which works rather robustly although there are still some problems for example related to idioms and multi-word expressions such as an expression by and large now words themselves are not atoms they have internal structure and morphology is the study of the formation and internal structure of words as you can see in this drawing a fundamental concept in morphology is that of morphine morphine is a minimal pairing of form and meaning by form we mean the sequence of sounds like on the bottom of this drawing 3 and it can be a sequence of letters for example spelling of the word 3 and this is a mapping between the here the phonetic string 3 and the meaning or content of the word and this mapping is called is a morpheme and we say that morpheme is a minimal pairing because we require that morpheme is a pairing that cannot be reduced to smaller subunits so for example the plural form of the word three trees will contain two more themes three and s plural marker okay so words are made of morphemes and what i highlighted in a sentence on the top or in the example of rich morphology of english on the bottom is what you can see is the concatenations of [Music] morphemes and there are three morphemes morphemes that occur independently for example the word establish and there are bound morphemes those that are attached to other words so in the word this establishment established is a free morpheme but their prefix this and suffix meant are bound more things words can undergo morphological processes or formal operations what are these morphological processes these are for example concatenation it is the most common morphological process if we concatenate the stem which is the core of the word with the affixes which are purely grammatical morphemes that cannot occur in isolation this process is called a fixation so in the example this establishment the stem is established and the there are prefix and suffix these and meant and the concatenation of if you concatenate prefix and then stem and the suffix both from two sides this process is called circumfixation so these are concatenative processes uh there are also non-concatenative processes uh for example when we add an infix in there so this when we add a morpheme in the middle of a string and another common type of morphological process is compounding when rather than concatenating bound morphemes we concatenate three morphemes we concatenate several stems like in the word dishwasher or in the word skyscraper so you can see that there are two types of morphological processes and these morphological several types of morphological processes more than two and these morphological processes can change the meaning of the word like for example with compounding or with [Music] the the modification of establishing to disestablish or they can change the grammatical function for example cats turning is a plural form of the word cat but the meaning has not changed here are some examples of interesting languages and their morphologies tagalog language is spoken in philippines it illustrates a great many form of morphological processes including prefixations fixation in fixation and reduplication so here the singular form is formed with prefix ma and then the plural form is formed with both prefixing prefixation and prefixing reduplication the infix boo and you can read more examples of interesting examples in the book that i reference below by laurie levin and david mortensen this is still a draft but a very nice draft human languages for artificial intelligence here are some other interesting examples arabic is a semitic language that uses templatic morphology in these languages root consists of two or four consonants and then morphology is uh represented by vowels and consonants which are inserted in between the root consonants so the root and the pattern of specific vowels each function like separate morphemes uh so they are combined like a beads in the stream and the uh on the bottom we can see chinese so although chinese has poor inflectional morphology it has many compound words and so there are compounding processes you can see examples below which i cannot read okay morphological functions so talking about morphological processes uh such as concatenation so compounding only tells us part of a story our next question in what are the functions of these operations the two broadest functional categories of morphology are derivation and inflection derivational morphemes examples are on the top are used to create new words they change meaning they can result in a new part of speech but of course not always for example the same example establish and disestablish and then another modification is this establishment then when we move from the verb to noun the second type of morphological function is uh and the important class of morphemes are inflectional morphemes they use to mark grammatical distinctions like cat and the suffix s will turn it into a cat but it does not change the meaning of the word so how do we formally discuss morphological properties of a language and learn about languages morphology in linguistics so in linguistics we introduce a interlinear gloss text igt interlinear gloss text is a standard way to explain the structure and properties of the language via examples it is described in detail in a document that i link on the top of the slide and i recommend you read this document if the topic of morphology is interesting to you and on the bottom i paste the simplest example of igt interlinear gloss text from the same document so you can see that there are three lines the top line corresponds to the original source language or object language which is in our case indonesian the third line is a translation of the first line into a meta language which is typically in linguistic papers this is english this is a lingua franca the second line is the gloss and it is between the first and the third one this is a line this is why it’s called interlinear every token in the second line is written in a meta language in english but it is not an english sentence this is an alignment with talking with tokens in their object language so you can see that the word they in jakarta now are aligned with the first line all tokens are aligned and this is important because this gloss contains information more complicated examples of this gloss contain information about morphosyntax of the object language which is indonesian and in this example we can see from this even simple example of igt we can see the difference between english and indonesian and in particular how english uses copula verb r b while indonesian doesn’t and here is another more complicated example of the interlinear gloss text between the morphologically rich russian this is our object language the top line and the bottom line is that translation of the sentence into english which is a meta language and in between you can see two lines representing a little bit different information this is optional both variants are possible this these are interlinear gloss text so the word the the fourth token in the russian sentence [Music] is split into three more themes corresponding to stem past tense and plural and you can see this specification in the igt lines two and three so this would be a stem go and then past tense and the number plural and the word bas of tobusan its fifth token in russian is split into two more themes corresponding to stem and suffix marking the case instrumental case so you can see from this linguistic description and alignment between the two sentences and the morphological analysis of individual talking in the top sentence how the morphologies of russian and english are different and you can see that morphology of russian is significantly more complex than the morphology of english in this slide which i will not read to you but you can use it later as a reference i list the types of morphological categories across parts of speech all languages possess the same set of about 25 categories number gender tense in if each of which have several functions so there are roughly 25 categories and roughly 100 functions for example a category for nouns a category [Music] number can of the object can be singular plural dual across different languages this corresponds to number another example of a category is noun class which uh noun classes are linked to grammatical gender in indo-european languages but in other languages this can be arbitrary set of categories and all nouns must belong to a category so an interesting example of noun classes uh is in swahili which is an african bantu language uh there are several classes i don’t remember i think it’s seven there is a class corresponding to anime animate nouns denoting people animals insects birds fish there is a different class corresponding to trees plants body parts and another chorus class corresponding to foods and so on and there is a separate class for singular animated nouns versus plural animate now so these are semantic classes and the different morphological rules different prefixes and suffixes are applied to different classes so languages differ in how they express these categories some use like seems like chinese vietnamese these are morphologically what we call poor languages some use freestanding grammatical morphemes such as pronouns prepositions so which means that they express the same categories but through syntax and some languages use affixes prefixes suffixes and this means that they express these categories through morphology so this is what it means when i said in the beginning of the class that different languages grammaticalize the this relationships and the cutting function categories and functions differently through words or through syntax or through morphology and this is why we have different types of languages which we categorize into morphological typology so there is also a disagreement on the typology but i present uh an analysis from david morton sense and the lori levine’s book and traditionally languages have been divided into four times types based on how morphologically rich they are languages like chinese vietnamese and a little less english are called isolating or analytic languages these languages have relatively little morphology and very little inflectional morphology with the exception of compounding like i showed in the example of chinese so most words consist of a single frame or theme languages that similarly to isolating languages have relatively few three more themes per word but unlike isolating languages have many prefixes and suffixes are called fusional or flexional languages examples include greek russian german there is a special subclass of fusional languages which are called templatic languages as i showed in previous example arabic and hebrew is also like this the next category is agglutinative or agglutinating languages these languages often have many more themes within a world examples include finnish turkish and swahili and these languages have very long words and finally the most morphologically rich languages are polysynthetic which can include the whole sentences whole propositions uh they can concatenate them into a single topic so analytic languages have lower morpheme to word ratio and higher restrictions of word order in a sentence languages with richer morphology synthetic languages typically have higher morpheme to word ratio in more free word order there are tight token curves that also can visualize how [Music] properties of morphologically rich languages so we can see that the richer the morphology of a language the higher is type to token ratio in a language and consequently the higher is the sparsity of tokens in the corpus here for example [Music] this paper by 2012 they took a multi-parallel corpus consisting of six languages with varying degrees of morphological complexity so these are the same sentences this is a multi-parallel corpus but written in different languages in english chinese german arabic turkish and korean and you can see that as the morphological complexity of the language increases the number of types in the corpus increases resulting in a steeper curve the most morphologically rich in this drawing is innoctitude which is a polysynthetic language it concatenates many morphemes together into single words and on top of this it has it has complex morphophonological uh processes between morphemes so uh in for inuktitut at 1 million tokens it it it has approximately 225 000 types compared to english which is for around 1 million tokens has only 30 000 types okay so due to morphological complexity and the diversity of languages there are several challenges in processing morphologically rich languages first is related to high lexical sparsity due to the variability of morphological forms this leads to sparse statistics many forms of words appear only once or few times even in very large corpus and there are many plausible word forms that do not appear at all these are called out of vocabulary words or ovs and these out of vocabulary or rare words that appear only once so twice lead to errors in systems such as machine translation speech recognition dialogue question answering and so on so on so this is the first challenge of processing morphologically rich languages the corpora no matter how large they are they are very sparse but on top of this morphologically rich languages are often resource poor so the problem of sparsity is much more acute specifically for these languages an additional challenge of morphologically rich languages is their complex word agreement this leads to agreement errors in language generation especially when we need to model agreement between words that appear far from each other in a sentence just because as i mentioned in some of the in the few slides before that the morphologically rich sentences are languages also have more free word order okay so on top of this high variability there is also a mismatch between morphologies of variability between morphologists across languages and language families and this is so mapping between morphologies of different languages is difficult this leads to difficulties in transfer learning when we want to transfer training corpora from language one language to another uh this can lead to problems this can also lead problems in translation errors for example when we translate from languages that don’t have the concept of definiteness like chinese or russian and so chinese and russian don’t have definite indefinite articles and when we would like to translate from chinese or russian into english that have definitely indefinite articles so these translations tend to contain errors and another problem is that differences in mapping across morphologies of different languages can lead to exacerbation or amplification of social biases for example when we translate from languages like turkish or hungarian that do not mark gender of third person pronouns like he or she and we translate from turkish or hungarian into english so it was shown in previous work that for example translation of lower status occupation would be translated with the female pronouns whereas the translations of higher status occupations would be translated with male pronouns so a turkish sentence that is written basically they are a nurse would be translated as she is a nurse and they are a scientist would be translated as he is a scientist and this just amplifies biases in english training data so to summarize main challenges for processing morphologically rich languages are high sparsity different errors in generation for example caused by uh in for the need to enforce agreement between words and the differences in morphological properties across languages that are different to map across languages and because of these challenges we do morphological processing and incorporate morphological knowledge into models explicitly so what are the types of morphological processing first morphological analysis and we can talk about morphological parsing or morphological segmentation the second is related to generation and the task can be inflection generation or full paradigm completion i will show later an example and the third less common task would be in acquisition of morphology although we have resources for hundreds of languages where we know their morphology uh thousands of languages we still don’t have enough knowledge about their mythological properties so the task of acquisition of morphology would focus on learning languages morphologies automatically so the most common task is morphological analysis and also this it is called morphological parsing our input is a word and our output is a word stem and features expressed by other morphemes for example given in input cats morphological analyzer’s output will be cat plus n which corresponds to noun plus pl which corresponds to the plural form so morphological parsing is the process of determining what are those morphemes given an input word and as you can see in these examples uh this information morphological information comes from three major sources first from lexicon so we need to know the stem second from morpho tactics we need to know morphosyntactic rules and third is from spelling or pronunciation rules because we need to know uh for example um the world cities uh we need to know that as a stem city which ends with a y in a plural form will be changed into cities with eye having this information about lexicon about morphotag tactic rules and about spelling rules we can build a morphological analyzer classical approaches to morphological analysis used finite state transducers and they achieved the high accuracies high results good results so uh in these approaches we have two tapes uh lexical and surface as in this example in the drawing on the top these drawings are from the girovsky martin textbook so and we build a finite state transducer so either one so it can be inverted so either lexical form can be in in surface form can be an input and the output can be stem plus morphological analysis or stem plus morphological analysis can be an input and the output would be a surface form so we will build a transducer so that given an input word cats it would read the word the input letter c and output c lead and read a output a read t output t then read the plus n and would output an empty string epsilon and then it would read plus pl plural and would output s so the output would be cats and it’s coming from lexical interpretation to the surface interpretation so on the bottom we have a simplified fst for morphological parsing cats goes from q0 to q1 from state q0 to state q1 this is a regular noun then q1 outputs an epsilon the for for the noun plus n and then we have we can go from q4 to output s a plural form and we have the end of string in q7 so we can build such an fst finance state transducer as a recognizer and it would say is it a legal string if we if we accept it and or if it’s not a legal string if it’s not a word illegal word in a language in this case the fst would reject it and we can build this fst as a translator in which we input a string of characters from the surface form and output the string of morphemes okay so in 2016 there were a surge of papers that started to replace fin state transducers with current neural networks so this is one of the many examples of papers that i present here by katarina khan and her collaborators ryan cotterell and henrik schutzen and they showed how to implement the same task of morphological analysis using recurrent neural networks and i present specifically this paper because it’s interesting for two reasons first they show how to do canonical segmentation rather than surface level segmentation of words so canonical segmentation so you can see in this example for the word achievability uh how would for the input achievability how would an output surface segmentation look like and how would output canonical segmentation look like so for a surface segmentation we will just segment the input sequence into substrings in canonical segmentation we don’t only segment the sequences into substrings but also replace substrings with the canonical form this canonical segmentation has several representational advantages [Music] because it shows more themes that are not obfuscated by morphology so it reduces the sparsity more significantly but it is also more challenging for example it would be difficult to implement with finite state transducers uh because it doesn’t only segment uh words into substrings but also reverses orthographic changes so how did they do it they first used the standard character based bi-directional uh gru encoder with attention and then they have a unidirectional gre decoder so this is a very standard sequence to sequence architecture uh run end with attention and then a novel component they introduced was a neural re-ranking for segments to identify canonical segments so in particular when they kind of produce the segmentation of a word they first output [Music] a surface level segmentation but then they did re-ranking and specifically looked for segments that are more frequent when they occur as an independent word is in a lexicon and for this they incorporated simple lexical information into word embeddings that marked an additional that added an additional dimension to the word embedding a binary feature that said whether this word is appears as an independent word in the lexicon or not so this is a standard sequence to sequence character-based model plus a re-ranker on top of it to prioritize words that appear more frequently as independent words in the lexicon so in case in this case the in the segmentation of a word achievability the word so the second token able would appear more frequently than the token abil so how do we evaluate morphological analysis there are several evaluation measures that are described in the paper in the previous slide and in several other papers first is error rate defined as 1 minus the proportion of guesses that are completely correct so proportion of 1 minus proportion of outputs of the model that are completely correct second is the added distance so when the first first is a hard measure it’s uh the correct or incorrect output eleven stain distance would measure the similarity between the gas and the gold standard form and finally morpheme f1 score calculates the precision and recall of correctly generated morphemes where the training corpora usually come from in this paper and other papers on morphological analysis a most common resource to work with is unimorph it annotates hundreds of languages using a unified schema it includes currently annotations for 110 languages and i put the link in the slide okay so this was about morphological analysis uh the second type of prominent type of direction of research is focusing on morphological generation in particular inflection generation there are two tasks one is uh to generate a single inflection so our input would be a lemma and grammatical features for example [Music] this is an example in german cow which is a calf and the grammatical features such as case nominative number plural infraction generator will output calvary and the second task which is a generalization of inflection generation task introduced relatively recently in single phone competitions that given a lemma as an input we would need to fill a table and this table this means we need to general generate all possible inflections of a word for all morphological categories so an example where inflection generation either inflection generation or paradigm completion is useful is for reducing sparsity of the corpus for example in machine translation machine translation suffers from data sparsity as i mentioned before when translated thing morphologically rich languages since every surface form is considered every token is considered as an independent entity and to alleviate the problem of sparsity we can translate into lemmas in the target language and then apply inflection generation as a post-processing step so these tasks morphological analysis morphological inflection generation paradigm completion they are widely explored in particular in the sigmar phone competitions so sigma phone is a special interest group on computational morphology and for phonology and they organize workshop and workshops and share tasks every language and i put here some interesting summaries of workshop tasks that you can look into and there is a lot of research so i really gave a high level overview morphological inflection generation in general the base model is a usually a simple bile stem with attention so some recurrent sequence to sequence model uh and then while the base model is simple there are several interesting challenges specific to morphology an interesting challenge for example is how to enforce monotonic alignment between input and output because we unlike in machine translation morphology we don’t need to do a lot of reordering to output to the output sequence uh second interesting challenge is how to better learn longer distance relationships between characters to model variation in inflection due to spelling and phonological rules in a language so some of the papers focusing on shared tasks specifically focus on for example adding a crf layer on top of the decoder to better model relationships between characters in the outputs and if you want to know more about the field you can start with the sigma phone 2020 share task summary which provides an overview and the historical development of the task and the links to interesting papers and to read a particularly interesting paper about morphological generation let’s read this paper that i put in this slide so it particularly focuses on low resource settings so morphological inflection has been pretty thoroughly studied in monolingual high resource setting but in this paper adonis and graham focus on um low resource perspective and uh on cross-lingual transfer across languages and your task for the discussion is just to read this paper and provide your insights for example you can provide the critique about an individual component of the model so the model in the paper consists of several different components so you can talk about an individual component or individual experiment or experimental setup and in particular you can propose ideas for follow-up work or how a specific modeling decision could be adapted to a language of your choice for example a language that you speak and that’s it thank you and see you tomorrow\n\n\n\n\n\n\nCitationBibTeX citation:@online{bochman2022,\n  author = {Bochman, Oren},\n  title = {Morphology},\n  date = {2022-03-01},\n  url = {https://orenbochman.github.io/notes-nlp/notes/cs11-737-w18-morphology/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2022. “Morphology.” March 1, 2022. https://orenbochman.github.io/notes-nlp/notes/cs11-737-w18-morphology/."
  },
  {
    "objectID": "notes/cs11-737-w17/index.html",
    "href": "notes/cs11-737-w17/index.html",
    "title": "Speech",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\nVideo 1: Lesson Video\n\n\n\n\n\n\n\n\nThis week’s slides\n\n\n\n\nSupplementary Figure 1\n\n\n\n\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\nWhat is speech?\nSpeech applications\nSpeech databases\nSpeech hierarchy\n\n\n\n\n\n\n\n\n\nTranscript\n\n\n\n\n\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{bochman2022,\n  author = {Bochman, Oren},\n  title = {Speech},\n  date = {2022-03-01},\n  url = {https://orenbochman.github.io/notes-nlp/notes/cs11-737-w17/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2022. “Speech.” March 1, 2022. https://orenbochman.github.io/notes-nlp/notes/cs11-737-w17/."
  },
  {
    "objectID": "posts/desidirata/index.html",
    "href": "posts/desidirata/index.html",
    "title": "Further Desiderata for Emergent Language",
    "section": "",
    "text": "Some additional desiderata for emergent language come from:\n\nUsing the emergent language as surrogate for low resource languages. Though in a Bayesian setting we may consider them as priors. We may want to have multiple surrogates for a low resource language so that we can fit to the set of surrogates rather then to the lexemes of one specific surrogate! Fitting to a set of languages should teach the neural model about relations. A single surrogate would be less useful as it models would tend to overfit to features like lexemes which are made-up.\nUsing games where agents learn multiple languages. This allows to study the field of language contact. In this case speakers speak more than one language and the languages influence each other. To make things more concreate we may want to evolve languages that are similar to certain specification that come from the WALS database. 1\nA third idea that may be of interest is to evolve languages that can be used as a universal interlingua. This are emergent language that are minimally ambiguous have capture most deep features. It would be created using a large state space and be given a long time for planning or to evolve and become more uniform, the verbs more regular and transformations more compositional. Ideally the verbs and nouns would only require one form to master all possible forms and their meanings. Perhaps the nouns could even be derived from the verbs using an automatic process, to reduce the learning load even further. Why multiple interlingua? Since we propose to use these for translations it may be possible to create smaller-interlinguas that are used for a restricted language hierarchy. A second reason is that again we may not want to overfit to a single interlingua but to some set of interlinguas captring diverse deep features and surface features. Idealy some interlinguas would be a better fit for certain languages then others and an attention mechanism could be used to relay on the interlinguas that are the best fit for the language being translated. Think of using different shortcuts to get from one place to another. (e.g. An interlingua with politeness and honorifics would be a better fit for Japanese then one without.)\n\n1 this is at last another good reason for multiple senders to be used and for them to send different languages."
  },
  {
    "objectID": "posts/desidirata/index.html#three-applications-for-emergent-language",
    "href": "posts/desidirata/index.html#three-applications-for-emergent-language",
    "title": "Further Desiderata for Emergent Language",
    "section": "",
    "text": "Some additional desiderata for emergent language come from:\n\nUsing the emergent language as surrogate for low resource languages. Though in a Bayesian setting we may consider them as priors. We may want to have multiple surrogates for a low resource language so that we can fit to the set of surrogates rather then to the lexemes of one specific surrogate! Fitting to a set of languages should teach the neural model about relations. A single surrogate would be less useful as it models would tend to overfit to features like lexemes which are made-up.\nUsing games where agents learn multiple languages. This allows to study the field of language contact. In this case speakers speak more than one language and the languages influence each other. To make things more concreate we may want to evolve languages that are similar to certain specification that come from the WALS database. 1\nA third idea that may be of interest is to evolve languages that can be used as a universal interlingua. This are emergent language that are minimally ambiguous have capture most deep features. It would be created using a large state space and be given a long time for planning or to evolve and become more uniform, the verbs more regular and transformations more compositional. Ideally the verbs and nouns would only require one form to master all possible forms and their meanings. Perhaps the nouns could even be derived from the verbs using an automatic process, to reduce the learning load even further. Why multiple interlingua? Since we propose to use these for translations it may be possible to create smaller-interlinguas that are used for a restricted language hierarchy. A second reason is that again we may not want to overfit to a single interlingua but to some set of interlinguas captring diverse deep features and surface features. Idealy some interlinguas would be a better fit for certain languages then others and an attention mechanism could be used to relay on the interlinguas that are the best fit for the language being translated. Think of using different shortcuts to get from one place to another. (e.g. An interlingua with politeness and honorifics would be a better fit for Japanese then one without.)\n\n1 this is at last another good reason for multiple senders to be used and for them to send different languages."
  },
  {
    "objectID": "posts/desidirata/index.html#section",
    "href": "posts/desidirata/index.html#section",
    "title": "Further Desiderata for Emergent Language",
    "section": "??",
    "text": "??\nSo the main thrust is that we want to evolve language that are similar to a WALS specification. THis may require too much work though and we may prefer to consider certain aspects of language in the database that are both common, easy to implement and measure and ideally can become surrogates that are more like a certain language then any high resource language (e.g. Turkish).\nWhile a number of tricks may be used to make the emergent language more like a low resource language, the more systematic route is to consider a suitable set of states for the lewis game. Spontaneous symmetry breaking may then serve us. In other cases we may be able to incorporate different aggregation rule\nHowever to get started I think one needs to simply generate suitable states. The desedirate has a duality with the states. The number and structure of the state will shape the size and complexity of the emegent languages. Since these will likely be large languages we may also need to find algorithms that allow these to emerge quickly.\nAnother idea is to make to make three views of the state space. - an image based on a chart - a frame based view\n\nWe may want to express propositions using a ‘Block world’\nWe may want to express possession using a ‘Possession world’\nWe may want to first breakdown the verb to indicate tense, aspect, mood, number, politeness, formality & counterfactual.\nFor thematic roles we may start the figure from (Winston 1992, 211) and the frame it shows.\nWinston points out that the number of roles which can range from 6 to 24 is less important, so long as we can learn the constraints verbs place on the roles when forming a sentence. This seems to be even more important if we are going to learn these constraints from data using an attention mechanism.\nFilamore explains that semantic roles are a tool to eliminate the polisemy inherent in verbs.\n\n\nDistributional Similarity - to a NL\n\nlexeme should be distributed similarly to a corresponding word in a natural langauge.\nthis is desirable because when translating from high frequency words should map to high frequency words.\nthis seems a challenge but lets recall that we also wanted to have a power law distribution.\nif lexemes are distributed following a power law. so fee words are high frequency and almost all are low frequency.\nmetrics for this could be cosine similarity, KL divergence, or some other measure of distributional similarity.\nprobability theorem has convergence in distribution and it may be of interest here, particularly as we may be interested in more then the lexemes distribution but of other probabilistically modeled aspects of the language.\n\nWe may want to choose a fully emergent language with phonology, morphology, syntax, and semantics. This might be more complexity than we need though. We might want to work with a system that is simpler but can be used to make embeddings that are useful for approximating low resource languages. In such a case we may use concatentaed numeric codes for representing the lexems.\nverb tense, aspect, mood, conterfactuals\n\ntense is a time reference\n\npast, present, future.\n\naspect is the way the action is viewed e.g. \n\nperfective means: the action is viewed as a whole (e.g. “I have eaten”)\nimperfective means: the action is viewed as ongoing (e.g. “I am eating”)\nprogressive means: the action is viewed as ongoing (e.g. “I am eating right now, but I will call you when I am done”)\nhabitual means: the action is viewed as a habit (e.g. “I eat breakfast every day”)\nperfect means: the action is viewed as completed (e.g. “I have eaten”)\n\n\nmood is the attitude of the speaker some examples are:\n\nindicative means: the speaker is making a statement of fact (e.g. “I am happy”)\nsubjunctive means: the speaker is expressing a wish, a doubt, or a hypothetical (e.g. “I wish I were happy”)\nimperative means: the speaker is giving a command (e.g. “Be happy!”)\nconditional means: the speaker is expressing a condition (e.g. “If I were happy, I would be smiling”)\ninterrogative means: the speaker is asking a question (e.g. “Are you happy?”)\nexclamatory means: the speaker is making an exclamation (e.g. “How happy I am!”)\n\ncounterfactuals are statements that are contrary to fact. (e.g. “In the best of possible worlds, I would be smiling”)\n\nthematic roles\n\nagent\npatient\nexperiencer\ntheme\ngoal\nsource\ninstrument\nlocation\nbenefactor\n\npossession world\n\npossessor\npossessed States:\n\nWe may want to have verbs and nouns and other parts of speech.\n\nstates could be frames for verbs with slots for subjects, objects. this can\n\n\nPropositions could be materialized using a block world.\nPossession could be materialized using a possession world.\n\nblock world (Winston 1992, 47–60) , c.f. Mover and SHRDLU by Terry Winograd"
  },
  {
    "objectID": "notes/cs11-737-w17-ASR-TTS/index.html",
    "href": "notes/cs11-737-w17-ASR-TTS/index.html",
    "title": "Models for multilingual ASR and TTS",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\nVideo 1: Lesson Video\n\n\n\n\n\n\n\n\nThis week’s slides\n\n\n\n\nSupplementary Figure 1\n\n\n\n\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\nWhat is speech?\nSpeech applications\nSpeech databases\nSpeech hierarchy\n\n\n\n\n\n\n\n\n\nTranscript\n\n\n\n\n\n\n\nToday’s lecture uh it is about multilingual speech processing i will mainly uh explain about the speech recognition but it also covers most of the techniques that are quite similar in the tts of speech translation and so on. First uh i will talk about n2 and base system because previous lectures we have run the end-to-end speech recognition so it may be better to start from that and then i will actually also uh explain our form-based system which is i call it the hm-based system and uh they actually are the multilingual processing there are some benefits of using phone based system so i will explain about it and for this uh marketing uh speech processing actually uh the cmu has been uh the contributing a lot of research the activities to the community and i’d like to also introduce this kind of activity okay so uh let me first start the uh the end to end the direction and this is actually one of the slider that i already showed when i introduced the end to end speech recognition so uh the the one of the kind of uh the uh difficulty of hm based speech recognition pipeline uh when i introduced this other figure uh is that requiring the linguistic resources and this is actually okay for the major languages like uh the english japanese mandarin and storm but if you if we move to the uh the other languages uh getting such kind of a linguistic resource is more difficult but still it’s possible so uh the one of the benefit i emphasize uh is that the end-to-end system doesn’t require a such kind of linguistic resources especially for the other phonetic uh the information and we can build other speech recognition so first uh the emphasize this kind of our order the other characteristics and then explained about the multilingual processing based on end-to-end speech definition however i uh emphasized several times uh this can be a process and that i will explain it later so first uh that i will explain about uh my experience of working on the end-to-end speech condition i started to work on the end-to-end speech recognition around 2015. i said one of the early adapter of other moving to entrance other systems compared with other researchers and then we actually tried to uh welcome the first uh english and it’s working quite well like the other people reported and then i am thinking about trying to the other languages which is japanese and the japanese is actually not a sr friendly language i would say so this is a typical sentence i actually extracted from one of my other japanese articles and this sentence is actually quite rather difficult to handle for speech recognition first there is no world boundary compared with other languages there is no word boundary and the other are the properties are that the someone may recognize anyway there are totally four scripts the mixed in this one sentence one is hiragana the other is katakana and the third one is kanji which is the original from china and roman alphabet and so on this kind of a mix of the script happened uh so often and actually that this other variety also making the pronunciation are very difficult it depends on the context on the it depends on the uh the uh the meaning uh some of the other the characters uh change the pronunciations uh so this uh the uh the pronouns providing the pronunciation uh is also very very difficult and the last uh the part is that one of the character may include many actually uh the phonemes like for example this uh and which is like the other languages but this character it actually has several pronunciations depending on the context but the one of the uh the way of uh the pronunciation uh we call this the character as kokoro jasi which other are 10 a phonemes five syllables only one character but the the cerebral are the language and the phoneme lengths are very very different so in general uh that it was regarded as that dealing with the japanese is very difficult due to this kind of four uh three uh reasons maybe there are several others and then what we will do is actually using the tokenizer or actually it’s just we don’t actually theory just doing the tokenization it’s actually trying to solve all this problem jointly uh by other concerns the pronunciation and some kind of a mathematical analysis and so on so this kind of a tokenizer morphological analyzer is very important when we start to work on japanese and the uh to do that uh japanese are the researchers because uh it’s more like a highly sourced side and there are a lot of researchers so we actually have a lot of very great tools to perform the tokenization which is very very good with that we can actually make speech definition work in japanese however the tokenizer also has an issue first it has some mistakes but it’s if the tokenizer is getting better and better probably we can ignore this kind of issues one of the most difficult issue for me is that it’s changed the result depending on the tokenizer if you’re using a different software different targets different dictionary uh tokenizer output will be changed and which we do the kind of speech recognition experiments we actually cannot compare them so this is actually quite a difficult problem not only program itself is kind of difficult problem if we’re using this as an other speech recognition uh this actually becomes a quite large barrier and actually uh the one of the example is that for example one company wants to use a tokenizer that they’re made by that company and the other company they want to use the other organizer developed by the other company and then that we are not easily going to compare the water since the unit of the world is different it happens in the company the divers belong to by the way so there is a such kind of issue anyway that is what pognasa is doing so this is one of the examples are using the tokenizer most widely used one megaboo and we just throw this text and then tokenize actually how to say the spirit uh each of the characters and as i mentioned this is not the the only things that we also have to care about the uh the pronunciation and the part of speech and so on so it’s actually this tool is not only tokenizing it but also providing a pronunciation this one is actually pronunciation and the part of speech information and so on so that we have to use this kind of tool to perform uh the speech condition which is great but it actually has a lot of problems as i mentioned and then i one of my research goal is actually want to remove uh tokenizer uh from the speech recognition uh the other process and i feel sorry about gram because gram is one of the person that make another very famous tokenizer are called kitty kitty yeah so actually a lot of people actually working on the tokenizer which is great and i also really appreciate this effort but at the same time as a pure sr purpose i actually want to skip this direction so that’s why they i actually started to apply end-to-end speech recognition to a japanese probably this is the first trial i’m not sure but at least in terms of the how does a paper uh our group is one of the first team that performing the end-to-end speech recognition uh with japanese without that the tokenizer and so on and actually the performance was very good surprisingly very good uh this is the uh calculate ten percent uh when i first tried this one and the data with a lot of techniques now it goes to five percent or less than five percent in the famous japanese benchmark called other corpus of spontaneous uh japanese and this is a wizard uh tokenizer to reaching this kind of performance so this experience was very good to me to actually moving to a multilingual end-to-end asr so as i mentioned japanese is in terms of the written form it’s very complicated but still just through the paired data it started to be working so maybe we can use this architecture to the multilingual and 2 and asl so this is actually given this story i started to apply and the speech equation to that multilingual processing and also color switching end to end asl and let me start to introduce the what are the margining of n2 and sr is doing so this is a kind of a typical pipeline of using the speech recognition and then if we uh they extend this one to the multilingual speech recognition first uh we have to do is detect which speak the which language is spoken by using the language detector or asking the user to put this information abroad anyway after the language detector we actually came almost completely separately built a speech recognition although of course uh this part which extraction part can be other than the unified and the part of the acoustic modeling part can be unified but generally to build uh the speech recognition in a multilingual method we actually have to uh prepare a lot of kind of linguistic resources or lot of kind of engineering to build various languages and then my attempt is to make it with a single neural network okay let’s start to discuss what kind of techniques i am using but actually this is super simple what i use is just first collect 10 languages and then train train regarding each other the one corpus and then uh the train the single neural network basically that’s it and i didn’t do any kind of special things except for by following the machine translation convention i also put the language id in the beginning of the sentence so that first network predict the language id which correspond to the language detector and then the other uh the transcription of that language is following so basically that’s it so uh i uh this is more like just using the data augmentation but no sorry data preparation and then added i can either make this kind of a neural network and note that i don’t use any other pronunciation other dictionary and so on and this is one of the uh the most kind of a difficult part when we don’t have a knowledge to get the kind of other access to the various language resources of course if there are some kind of knowledge definitely we can do but in general it is not easy to have such kind of access of that so due to that this process is quite easy and let’s check the performance actually performance was also okay i tried this one with the 10 languages mostly yeah the the languages in europe and the i mixed the chinese and japanese and the bruven is a language dependent uh which means that we actually built a sr system for each language and the red one is just uh they combine everything and the one single neural network to recognize this mixed 10 language speech and surprisingly it’s working well note that some languages especially when the uh the data is enough it’s actually degraded performance however some cases the improvement is quite large and this improvement comes from the data sharing structure since we kind of mix all the data and at least encoder part would be possibly doing a language independent processing so by mixing this kind of the other language data which is very helpful to regularize the encoder part of the list and then it’s working quite well in this experiment and the language prediction is almost perfect uh almost hundred percent in the each of the luggage pair except for the uh the spanish italian it has some kind of uh the similarity in the language and the language recognition is a little bit difficult but other than that our language recognition part is also perfect and the uh this is uh the applied to the other major languages and then i also moved to the other kind of low resource languages uh which is collected from the available bubble project and by the way some of the these all represent the hero and some of them are actually uh when i cut and paste the characters it’s as this but the information is disappeared this kind of how they say issues uh happens often so uh when you apply the monitoring of processing please be careful about copy and paste sometimes it’s not working and then that even for this uh the language uh the uh low resource uh language cases i had a kind of similar trend that mixing the language and the building a single neural network seemed to be working quite well and actually finally i uh my colleagues actually even uh extend this direction to uh using the 100 uh language it’s it’s not exactly 190 something languages are using the cmu uh wilderness data that professor black collected for this approach and then i we got similar performance uh based on that okay so uh this is actually a question so how many people were importing this development so 10 of course you know given that we have an n2 and airsoft and then uh we have to build 10 other language speech recognition how many people involved in the development and someone answered okay the answer is only one that many people actually expect only me actually and how long did it take to build this system 10 language good question very very good question the answer is that date of preparation one day gpu one week what 10 days sorry at that time yeah now if we use you know other several gpas probably uh that two to three days to finish this kind of uh the training and the linguistic i would say that only what i use is the unique order and this is actually one one of my mistake i use a python two i should have used python three and then the the things are more easy but the uh uh anyway uh we didn’t don’t have to use this kind of linguistic knowledge um with this kind of effort of course after that we actually write a paper about this one and when writing the paper we also need to have our uh other researchers help to uh do additional experiments and so on uh so they totally the uh two that finish this paper uh we actually uh they have a three or four also but the most of the work was all done by only me and then actually we submitted this paper to sru which is one of the uh the most kind of famous uh speech recognition a workshop and then this paper i got the best paper candidate not the best paper the selected are the uh nominated that are the best paper but we kind of lost today the best paper but i think it’s okay you know the other people spending you know a lot of time you know three months or half half year and a lot of kind of resources to and then finally light of paper this one as i mentioned most of the work one day of the scripting work and then a light of paper and then we got the kind of uh the best candidate so i’m actually very satisfied with this result okay so then uh the the important thing uh is that this kind of effort can be done just using the data preparation this is a very kind of a cool part of the uh using the this kind of end-to-end system and then i actually from now not i we i would say that my colleagues they started to extend this methodology even for the uh the other code switching and what we did in the very beginning of this experiment was that we just concatenate two sentences which is different languages that’s it and then simulate the code sitting uh we know that this is simulation you know there’s uh the the code switching uh the code reaching may happen in the uh the same speaker and even in the within the one sentence the code of sitting may happen but as a proof of concept we started to work on uh this kind of code switching with just mixing the data and then it’s actually other working other quite well i mean by using the uh the wizard quarter switching uh the of course uh the the water rate is quite uh high however uh by using the other code switching uh the simulation uh we can actually uh the correctly detect the other language boundary and then under the perform the code switching a speech condition uh the in this other experiment however i i just want to emphasize that this is accumulation the most difficult part of the code switching is actually we don’t have a real data so to do that in addition to the simulation uh we also have to the the uh the care about how to uh make simulation data to be close to real data which is a very important research direction now so anyway i will play with some of the audio and corresponding uh the uh the system out of it difference is the uh the difference uh the the ground tools and the sr is the other sr output and we use this kind of our other audio us exports rose in the month but not nearly as much as imports superior like this yeah okay it’s a simulation but uh the just using the data preparation uh we can handle uh this kind of code switch so this is a kind of uh uh one uh nice part of the end to end speech recognition uh we just uh try to make our program to be data preparation and then we can get to some other straightforward result of course to improve that to the real data is so long we need more effort but as a first step to do something it’s actually quite good and the other example is actually similar to the uh today’s uh the language stand that we are also working on the language uh indian jada language documentation andy uh is is actually uh working with the uh jonathan amis uh gets broken correct he will be a guest lecturer here actually uh this is the collaboration happened suddenly jonasson sent me email he wants to use speech question and then i am answered we have an end-to-end asl so if we have a you have data that we can do it and usually uh after the conversation this kind of a conversation happens often but usually it would be failed because the data size is very small however jonathan is great he actually collected over a hundred hours uh of this other endangered language called ios mistake which is in one of the village uh in mexico and he and his colleagues actually uh often visited uh this village and the other uh this area and collecting uh the data and the well-defined format so that we can actually perform the end-to-end sr but still the problem is that the uh the there are two kind of issues one is the other the transcription uh the the bottleneck it is very difficult to transcribe uh this kind of the uh the endangered language because it doesn’t cover uh the standard of the script form that we discussed before and the second one is that due to that we actually using a quite a linguistic oriented uh that script to uh transcribe the audio of this kind of endangered luggage which means that the transcriber must have a very good knowledge about linguistics so that the transcriber or the shortages is also very important uh the difficulty in the endangered languages and our end-to-end asl it’s actually as partly solving this club problem our performance is actually quite good uh it’s comparable to the novice transcriber it’s their process is actually two steps one step is a novice transcriber to transcribe it and then later expert will either correct it or expert will directly add that do it and the novice is just kind of a training uh by using the uh the the others uh the expat transcript and so on but anyway uh in this kind of a transcription process we usually have such kind of layers and the other end-to-end sr at least are comparable to this novice transcriber so which can mitigate possibly mitigate the transcriber shortage so uh that we start at the end-to-end sr is actually uh they’re showing some kind of uh the advantages uh they based on removing uh this problem in the endangered languages but i just want to emphasize that this is a very unique case this is other uh based on the great effort by other generations and historians to collect large amount of data hundred hours of data and then we can realize it in the other cases uh usually we don’t have so many uh transcribed data so the the other usual endangered language cases it is still very difficult for us this end-to-end area okay so anyway the end-to-end sl is somehow working but the uh the problem as i mentioned is that in general uh many languages don’t cover enough data and then what kind of technology we can use uh this is a transparent or fine tuning this is also very similar to the other machine translation and other other multilingual nlp techniques so the first we try to build a seed model which is uh that uh trained by uh the high resource language or uh the mix of the the multilingual data like i showed before and then uh making a big seed asl model and then given this kind of seed asl model we just have a small amount of target speech data and then added by using a seriousl model to transfer uh to the target language sr model by using the small amount of training data okay and then actually the one question so to transfer the model to the target language since the neural network has too many parameters right 100 million parameters and so on for standard speech recognition cases and then we usually freeze some of the layers and only fine tune some of the layers and then the question which layer we should fine tune in the language the transfer learning strategy in general i think everyone must have a correct answer in your mind actually uh we usually that only train higher layers that is enough first of course we have to change the uh the script from the given language to the uh the target language if they are not included right and then the last linearizer must be uh that are initialized must be fine-tuned and then the uh the speech recognition enter in the system or not in the system acoustic model or neural network in general lower layer more like doing some feature extraction to try to capture uh the invariant uh the phoneme like representation so if the kind of layer goes to gradually higher uh the it’s actually uh the uh goes to more kind of linguistic information and then the uh this kind of initial other fast layers are more towards to represent the acoustic information so that if we try to convert other per home the transfer learning from the big seed model to the target language model we should focus on tuning or fine-tuning the higher layers of a deep neural network in general by the way the question if for example the data is very noisy and the target is not language but to target to some kind of noisy speech feature layer we should fine-tune it should be shadow layers right so this is in general general property by the way so uh i am very lazy so i actually added other fine-tune all the layers because it’s rather easy to do it but yes yes yes but so i guess um fine tuning like just the early layers or fine-tuning just the light layer it’s just a very extreme version of regularization right you’re either like you’re basically regularizing some layers to not move at all then you’re regular you’re not regularizing right you’re basically allowing them to move freely and i wonder if there’s anything like in between those two i mean obviously that adds more hyper parameters to the model or something like is there is there anywhere that kind of more generally says well the layers at the bottom probably shouldn’t be moving much or they should be moving in a particular way um i don’t know so much about this book but definitely it should be a very good idea uh but the most people are just kind of analyzed these kind of things with the layer-wise other processing there are some work it’s not completely what you mean to say but some work is actually uh focusing on the like for example the uh some particular part of the network like for example inside the transformer some people only kind of adapt the feed of other parts and some people don’t control the freezer query key value and so on there are such kind of work exist but i actually don’t know so much about this kind of direction okay so uh there by doing that we can actually either perform the target target uh the transfer learning to the target uh the language and this kind of uh techniques are quite popular and mostly using uh for many other speech other processing tasks when we used uh try to have our target language uh the the the speech recognition and the tts and so on okay so uh the uh these are more like when we have anyway uh the pair data in the seed model but actually uh in some cases we have a huge amount of uh the speech-only data we don’t have so much uh the paired data uh how to kind of solve this problem and then actually people are using the cell supervised running in this other framework in the current speech recognition technologies and i just list the three techniques i will not dig into the uh each of the technique so if you are interested in that you can actually check some of the references one of them are web to back 2.0 actually 2.0 means that there is a prior study a web to vector which was okay pictures starting to be working but the after uh the the more effort uh the basically the making the training strategies simple and then by using the large uh the huge amount of the uh the speech on the data web dubac 2.0 started to be working and i’ll say that this is the first success in the speech recognition in terms of breaking the record of various benchmarks and then the later hubert the based cell supervised learning also is proposed by the uh the same other group uh the other meta people and this is actually i would say that simplified version compared with web to 3.0 web developer 2.0 is other very difficult to train in the uh the contrastive loss part while the cube part is actually quite similar to the other famous uh training criteria like masked language model and how to kind of make masks language model work in the hub is that instead of using the continuous speed representation uh they’re just using a k means and then this uh the hubert uh is using this k-means as a target to pre-train the model the k means it’s really simple k means like uh that we usually use in the other first or second uh the the courses in the pattern recognition and they decently uh the google actually uh pre uh the proposed the father simplified model called london projection quantizer and this is actually quite uh the sensational to me so before uh the uh london projection quantizer hubert or web feedback 2.0 anyway they try to somehow imitate phoneme as a target by using k means a contrastive loss or whatever however other london projection quantizer still using the some quantized target but how to get this quantized target they just convert mscc to the high dimensional uh features by using a random projection and then by you by sampling the some of the uh the uh the uh the centroid and then by using this other target for neural network so this london projection space is not close to volume at all however anyway this problem is very difficult and then by using this one as a target uh the neural network is somehow learning something and then it can be used as a fine-tuning uh of the initial model and the uh this runner projection quantizer is also other comparable performance to cuba to our website 2.0 and these are now very popular especially we have dubek 2.0 is very very popular right how many people harder the web to back the most people right i actually the people should know that because one of the advanced part of the assignment three is to use the either of the web to make the other human um and the i think uh the many people are using the uh s3pll shanghai right yeah so uh the our group is actually uh the the contributing not to making a pre-trained model but to make a how does a benchmark for this uh the speech self-supervised learning called the spark benchmark which is uh we try to kind of make a uh general benchmark including the speech recognition spoken language understanding the uh speaker recognition and so on and see whether this kind of or other self-supervised learning features are generally working on various speech recognition downstairs tasks and then through this kind of project we developed uh some kind of uh uh the mainly national taiwan university uh but that we actually uh developed together with them the software called s3prl which is a moroccan interface to use the various uh the pre-trend the cell supervised language model for speech recognition and then this one is actually called in espnet so that you guys can just changing the configuration file you can play with the hubert or web 2 back to uh and so on okay so this is about the end-to-end system and now i move to that form-based system doesn’t have so much time um so form-based system i mentioned that the uh without expert knowledge may have approximate accounts and i will explain about that actually some uh that many cases uh english technology is actually quite important to build a speech pension system for example uh remember our kind of a pipeline acoustic model lexicon language model and it looks like we need a paired data of the input and output right but please uh the focus on this part this is form well mostly people are using phonemes but they it can be universal form and then phone is actually language independent so if we collect a lot of kind of a speech data like english or mandarin and so on and making a phone recognizer and then later if we hover some kind of a long election of the target language we can actually connect all of this part so how to do it first build acoustic model but based on the form based language acoustic model and which can be possibly language independent and try to find the phonetic dictionary of the target language by us using the weak scenario or whatever are the using the linguistic resource and then sometimes if we don’t have enough uh the word coverage we can use the graphing dupont g2p techniques nowadays modeling part we don’t need a pair data right so we’re just using the text data to build language model so it turns out that we do not need a parallel data we just need to have our other descent and the phone recognizer and the language model uh to build the uh the speech recognition system for the target language so this approach is actually one of the method if we don’t have so much training data or if we don’t have our uh we if we don’t have so much training data or if don’t have any training data and this methodology is very difficult when we use it in the end-to-end system because we don’t have this kind of clear modularity right so actually in many of the other the low resource speech equation still form based hrm based system is popular and actually better than other end-to-end system if we don’t have enough other training data and lastly i would like to introduce some of our ongoing other project first one is end-to-end air cell this is actually you guys are contributing this project our group and everyone now is try to actually build uh the speech recognition for many languages as many as possible public data reproducible recipes are preparing the model and our cases we use the esp net but any other toolkit is fine but the intention of our kind of uh uh this other project is anyway try to cover uh the uh the speech recognition uh for other many languages as many as possible and again you guys are a very important contributor for this project and the other approach is a phone-based asl and this is actually possibly we can build 2000 languages of course it is very difficult to evaluate it so we are still kind of a remaining 100 languages at an evaluation but actually by using the home based techniques even we can expand our 2000 languages are based on our effort and there are a lot of other activities in cmu uh two that are working on the multilingual speech processing not only speech recognition tts speech translation and any other kind of spoken language processing and so on and i want to show you one of the web page that so this is under construction but we are now trying to make a kind of this kind of cmu merging or speech database and the as we said that we have our many languages like 8 000 languages and in terms of this espnet project uh model our coverage is actually only this area there are some missing information by the way we also have some other the models here and so on so the coverage can be more but at least our the speech requisition language coverage is now only 0.59 percent and even uh the the compasses can be more but still the uh the if we don’t include the bible purpose the coverage is very small but if we include a viable purpose it becomes nearly uh the other one and so on and the recipes are also very small so uh this uh the showing that that uh we actually need to work more to improve the coverage of this part and your assignment is actually contributing to improve the coverage to be a larger size okay uh this is the end of my uh the talk and the today’s discussion uh is how to improve the coverage of asl and tts technology what kind of effort we can do for the uh to improve the coverage and there are a lot of kind of dimensions that we can consider and let’s try to discuss and let’s try to kind of come up with a very cool ideas to improve the coverage of the monitoring of speech process okay let’s start the discussion\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{bochman2022,\n  author = {Bochman, Oren},\n  title = {Models for Multilingual {ASR} and {TTS}},\n  date = {2022-03-01},\n  url = {https://orenbochman.github.io/notes-nlp/notes/cs11-737-w17-ASR-TTS/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2022. “Models for Multilingual ASR and TTS.”\nMarch 1, 2022. https://orenbochman.github.io/notes-nlp/notes/cs11-737-w17-ASR-TTS/."
  },
  {
    "objectID": "posts/desidirata/index.html#block-world",
    "href": "posts/desidirata/index.html#block-world",
    "title": "Further Desiderata for Emergent Language",
    "section": "Block world",
    "text": "Block world"
  },
  {
    "objectID": "posts/desidirata/index.html#zookeeper",
    "href": "posts/desidirata/index.html#zookeeper",
    "title": "Further Desiderata for Emergent Language",
    "section": "Zookeeper",
    "text": "Zookeeper\n\nin Zookeeper (Winston 1992, 121–25) we need to classify animals using deduction.\nhttps://www.kaggle.com/uciml/zoo-animal-classification/data\nwe can give each of 101 animal 18 properties in a table via a zoo.csv. We can then sample from the table to create sets of animals with different properties.\nwe might then play games based on this data:\n\nwe may need to identify the animal based on it properties.\neach property may come from a different sender.\nan early response may be to attack, evade or ignore based on a partial identification.\nthis requires some kind of reasoning about the properties of the animals in the ‘zoo’\n\n\nThis means states for animals and their properties."
  },
  {
    "objectID": "posts/desidirata/index.html#samurai-world---sub-states-that-imbue-an-emergent-language-with-politeness-and-formality",
    "href": "posts/desidirata/index.html#samurai-world---sub-states-that-imbue-an-emergent-language-with-politeness-and-formality",
    "title": "Further Desiderata for Emergent Language",
    "section": "Samurai world - Sub-States that imbue an emergent language with politeness and formality",
    "text": "Samurai world - Sub-States that imbue an emergent language with politeness and formality\nThis is a small state space which is used as a model for politeness and formality in natural language based on Japanese which has a more sophisticated system of politeness and formality then most languages.\nIn this game agents interact in a framing game. In the lewis sub-game they need to coordinate a system in which they observe politeness and formality to avoid being decapitated by their superiors, yet avoid losing face, i.e. without being too polite or formal to their inferiors.\n\nJapanese has three forms of politeness and formality:\n\ninformal\n\nthe informal form is used with friends, family, and people of lower social status.\n\nPolite (desu/masu)\n\nthe polite form is used in formal situations, with strangers, and with people of higher social status.\nVerbs end in -masu (affirmative) or -masen (negative); copula is desu.\n\nHonorific Language Keigo\n\nthe honorific form is used to show respect to the listener or the subject of the conversation.\nSonkeigo (Respectful Language): Elevates the subject’s actions (e.g., o-hanashi ni naru for “to speak”).\nKenjougo (Humble Language): Lowers the speaker’s actions to show deference (e.g., moushiageru for “to say”).\nTeineigo (Polite Language): The desu/masu form falls under this when speaking politely without changing perspective.\n\n\nstate:\n\nspeaker’s status\n\ngender, age, social status, relationship to subject\n\nsubject’s status\n\n\nMinsky’s K-lines - physical world 214 - mental world 215 - ownership world 216 - Kanade (1980) A Theory of Origami World"
  },
  {
    "objectID": "posts/2025-02-17-samurai-world/index.html",
    "href": "posts/2025-02-17-samurai-world/index.html",
    "title": "A Samurai’s World",
    "section": "",
    "text": "“if proof theory is about the sacred, then model theory is about the profane” – (Dalen 2012, 3:1)"
  },
  {
    "objectID": "posts/2025-02-17-samurai-world/index.html#motivation",
    "href": "posts/2025-02-17-samurai-world/index.html#motivation",
    "title": "A Samurai’s World",
    "section": "Motivation",
    "text": "Motivation\nIn the previous post, “Further Desiderata for Emergent Language”, I discussed the need to adapt emerging languages to conform with established properties of natural languages. You can read more about motivations in that post.\nRather then considering all the ways this could be achieved in a single essay, I believe that this should be broken down and considered one aspect at a time. This can make both the linguistics and the development easier to understand.\nOnce enough examples are available it should become clearer on how to integrate the desiderata using suitable states or other mechanisms."
  },
  {
    "objectID": "posts/2025-02-17-samurai-world/index.html#the-task",
    "href": "posts/2025-02-17-samurai-world/index.html#the-task",
    "title": "A Samurai’s World",
    "section": "The task",
    "text": "The task\nMoving on I’d like to tackle the capacity of certain languages to encode politeness and honorifics. This is a feature that is present in many natural languages, and it it bering some awareness that may be absent from many ai systems where agents lack a sense of social structure.\nIn this post I’d like to consider the emergence of politeness in a multi-agent language game. We saw that politeness emerged as a way to avoid conflict and to maintain social harmony. In this post, we will explore the emergence of honor in a multi-agent language game. Honor is a concept that is closely related to politeness, but it has some distinct features that make it an interesting topic of study."
  },
  {
    "objectID": "posts/2025-02-17-samurai-world/index.html#how-is-politeness-encoded-in-japanese",
    "href": "posts/2025-02-17-samurai-world/index.html#how-is-politeness-encoded-in-japanese",
    "title": "A Samurai’s World",
    "section": "How is politeness encoded in Japanese?",
    "text": "How is politeness encoded in Japanese?\n\nPoliteness persists into modern Japanese, and is a challange for learners of the language who wish to master it as they must become aware of social aspects in which they are participating.\nJapanese has three forms of politeness and formality:\n\ninformal\n\nthe informal form is used with friends, family, and people of lower social status.\n\nPolite (desu/masu)\n\nthe polite form is used in formal situations, with strangers, and with people of higher social status.\nVerbs end in -masu (affirmative) or -masen (negative); copula is desu.\n\nHonorific Language Keigo\n\nthe honorific form is used to show respect to the listener or the subject of the conversation.\nSonkeigo (Respectful Language): Elevates the subject’s actions (e.g., o-hanashi ni naru for “to speak”).\nKenjougo (Humble Language): Lowers the speaker’s actions to show deference (e.g., moushiageru for “to say”).\nTeineigo (Polite Language): The desu/masu form falls under this when speaking politely without changing perspective.\n\n\n\n\nQuestions\n\nwhat is changing perspective?\n\nIn Japanese, the speaker must change their perspective when using honorific language. This means that the speaker must consider the listener’s perspective and use language that is appropriate for that perspective.\n\nAre politeness and honorifics only encoded in the verb inflection or do they further manifest in the subject object as agreements or other forms ?"
  },
  {
    "objectID": "posts/2025-02-17-samurai-world/index.html#samurai-world---sub-states-that-imbue-an-emergent-language-with-politeness-and-formality",
    "href": "posts/2025-02-17-samurai-world/index.html#samurai-world---sub-states-that-imbue-an-emergent-language-with-politeness-and-formality",
    "title": "A Samurai’s World",
    "section": "Samurai world - Sub-States that imbue an emergent language with politeness and formality",
    "text": "Samurai world - Sub-States that imbue an emergent language with politeness and formality\nThis is a small state space which is used as a model1 for politeness and formality in natural language based on Japanese which has a more sophisticated system of politeness and formality then most languages.\n1 in the sense of logicIn this game agents interact in a framing game. In the lewis sub-game they need to coordinate a system in which they observe politeness and formality to avoid being decapitated by their superiors, yet avoid losing face, i.e. without being too polite or formal to their inferiors.\n\nstate:\n\nspeaker’s status\n\ngender, age, social status, relationship to subject\n\nsubject’s status\n\n\nOne could evolve the use of poltiness in a reconstruciton game or a discrimination game.\n\nIn the reconstruction task, Sender gets an input item, it sends a message to Receiver, and Receiver must generate an output item identical to Sender’s input.\nIn the discrimination task, Sender gets an input item (the target); Receiver gets multiple input items (the same target and a number of distractors, in random order).\n\nNote that reconstruction is very much like the original lewis signaling game. While the discrimination easier task as the receiver could in theory fail to reconstruct, or have to choose at random from a very large, perhaps even infinite set of possible reconstructions and that having a just K-distractions, it is down to 1/k probability of success. More so if it is equipped with the ability of learning from errors he might score the distractions and make progress with much higher information levels then in the reconstruction game."
  },
  {
    "objectID": "posts/2025-02-16-more-desidirata/index.html",
    "href": "posts/2025-02-16-more-desidirata/index.html",
    "title": "Further Desiderata for Emergent Language",
    "section": "",
    "text": "Some additional desiderata for emergent language come from:\n\nUsing the emergent language as surrogate for low resource languages. Though in a Bayesian setting we may consider them as priors. We may want to have multiple surrogates for a low resource language so that we can fit to the set of surrogates rather then to the lexemes of one specific surrogate! Fitting to a set of languages should teach the neural model about relations. A single surrogate would be less useful as it models would tend to overfit to features like lexemes which are made-up.\nUsing games where agents learn multiple languages. This allows to study the field of language contact. In this case speakers speak more than one language and the languages influence each other. To make things more concreate we may want to evolve languages that are similar to certain specification that come from the WALS database. 1\nA third idea that may be of interest is to evolve languages that can be used as a universal interlingua. This are emergent language that are minimally ambiguous have capture most deep features. It would be created using a large state space and be given a long time for planning or to evolve and become more uniform, the verbs more regular and transformations more compositional. Ideally the verbs and nouns would only require one form to master all possible forms and their meanings. Perhaps the nouns could even be derived from the verbs using an automatic process, to reduce the learning load even further. Why multiple interlingua? Since we propose to use these for translations it may be possible to create smaller-interlinguas that are used for a restricted language hierarchy. A second reason is that again we may not want to overfit to a single interlingua but to some set of interlinguas captring diverse deep features and surface features. Idealy some interlinguas would be a better fit for certain languages then others and an attention mechanism could be used to relay on the interlinguas that are the best fit for the language being translated. Think of using different shortcuts to get from one place to another. (e.g. An interlingua with politeness and honorifics would be a better fit for Japanese then one without.)\n\n1 this is at last another good reason for multiple senders to be used and for them to send different languages."
  },
  {
    "objectID": "posts/2025-02-16-more-desidirata/index.html#three-applications-for-emergent-language",
    "href": "posts/2025-02-16-more-desidirata/index.html#three-applications-for-emergent-language",
    "title": "Further Desiderata for Emergent Language",
    "section": "",
    "text": "Some additional desiderata for emergent language come from:\n\nUsing the emergent language as surrogate for low resource languages. Though in a Bayesian setting we may consider them as priors. We may want to have multiple surrogates for a low resource language so that we can fit to the set of surrogates rather then to the lexemes of one specific surrogate! Fitting to a set of languages should teach the neural model about relations. A single surrogate would be less useful as it models would tend to overfit to features like lexemes which are made-up.\nUsing games where agents learn multiple languages. This allows to study the field of language contact. In this case speakers speak more than one language and the languages influence each other. To make things more concreate we may want to evolve languages that are similar to certain specification that come from the WALS database. 1\nA third idea that may be of interest is to evolve languages that can be used as a universal interlingua. This are emergent language that are minimally ambiguous have capture most deep features. It would be created using a large state space and be given a long time for planning or to evolve and become more uniform, the verbs more regular and transformations more compositional. Ideally the verbs and nouns would only require one form to master all possible forms and their meanings. Perhaps the nouns could even be derived from the verbs using an automatic process, to reduce the learning load even further. Why multiple interlingua? Since we propose to use these for translations it may be possible to create smaller-interlinguas that are used for a restricted language hierarchy. A second reason is that again we may not want to overfit to a single interlingua but to some set of interlinguas captring diverse deep features and surface features. Idealy some interlinguas would be a better fit for certain languages then others and an attention mechanism could be used to relay on the interlinguas that are the best fit for the language being translated. Think of using different shortcuts to get from one place to another. (e.g. An interlingua with politeness and honorifics would be a better fit for Japanese then one without.)\n\n1 this is at last another good reason for multiple senders to be used and for them to send different languages."
  },
  {
    "objectID": "posts/2025-02-16-more-desidirata/index.html#section",
    "href": "posts/2025-02-16-more-desidirata/index.html#section",
    "title": "Further Desiderata for Emergent Language",
    "section": "??",
    "text": "??\nSo the main thrust is that we want to evolve language that are similar to a WALS specification. THis may require too much work though and we may prefer to consider certain aspects of language in the database that are both common, easy to implement and measure and ideally can become surrogates that are more like a certain language then any high resource language (e.g. Turkish).\nWhile a number of tricks may be used to make the emergent language more like a low resource language, the more systematic route is to consider a suitable set of states for the lewis game. Spontaneous symmetry breaking may then serve us. In other cases we may be able to incorporate different aggregation rule\nHowever to get started I think one needs to simply generate suitable states. The desedirate has a duality with the states. The number and structure of the state will shape the size and complexity of the emegent languages. Since these will likely be large languages we may also need to find algorithms that allow these to emerge quickly.\nAnother idea is to make to make three views of the state space. - an image based on a chart - a frame based view\n\nWe may want to express propositions using a ‘Block world’\nWe may want to express possession using a ‘Possession world’\nWe may want to first breakdown the verb to indicate tense, aspect, mood, number, politeness, formality & counterfactual.\nFor thematic roles we may start the figure from (Winston 1992, 211) and the frame it shows.\nWinston points out that the number of roles which can range from 6 to 24 is less important, so long as we can learn the constraints verbs place on the roles when forming a sentence. This seems to be even more important if we are going to learn these constraints from data using an attention mechanism.\nFilamore explains that semantic roles are a tool to eliminate the polisemy inherent in verbs.\n\n\nDistributional Similarity - to a NL\n\nlexeme should be distributed similarly to a corresponding word in a natural langauge.\nthis is desirable because when translating from high frequency words should map to high frequency words.\nthis seems a challenge but lets recall that we also wanted to have a power law distribution.\nif lexemes are distributed following a power law. so fee words are high frequency and almost all are low frequency.\nmetrics for this could be cosine similarity, KL divergence, or some other measure of distributional similarity.\nprobability theorem has convergence in distribution and it may be of interest here, particularly as we may be interested in more then the lexemes distribution but of other probabilistically modeled aspects of the language.\n\nWe may want to choose a fully emergent language with phonology, morphology, syntax, and semantics. This might be more complexity than we need though. We might want to work with a system that is simpler but can be used to make embeddings that are useful for approximating low resource languages. In such a case we may use concatentaed numeric codes for representing the lexems.\nverb tense, aspect, mood, conterfactuals\n\ntense is a time reference\n\npast, present, future.\n\naspect is the way the action is viewed e.g. \n\nperfective means: the action is viewed as a whole (e.g. “I have eaten”)\nimperfective means: the action is viewed as ongoing (e.g. “I am eating”)\nprogressive means: the action is viewed as ongoing (e.g. “I am eating right now, but I will call you when I am done”)\nhabitual means: the action is viewed as a habit (e.g. “I eat breakfast every day”)\nperfect means: the action is viewed as completed (e.g. “I have eaten”)\n\n\nmood is the attitude of the speaker some examples are:\n\nindicative means: the speaker is making a statement of fact (e.g. “I am happy”)\nsubjunctive means: the speaker is expressing a wish, a doubt, or a hypothetical (e.g. “I wish I were happy”)\nimperative means: the speaker is giving a command (e.g. “Be happy!”)\nconditional means: the speaker is expressing a condition (e.g. “If I were happy, I would be smiling”)\ninterrogative means: the speaker is asking a question (e.g. “Are you happy?”)\nexclamatory means: the speaker is making an exclamation (e.g. “How happy I am!”)\n\ncounterfactuals are statements that are contrary to fact. (e.g. “In the best of possible worlds, I would be smiling”)\n\nthematic roles\n\nagent\npatient\nexperiencer\ntheme\ngoal\nsource\ninstrument\nlocation\nbenefactor\n\npossession world\n\npossessor\npossessed States:\n\nWe may want to have verbs and nouns and other parts of speech.\n\nstates could be frames for verbs with slots for subjects, objects. this can\n\n\nPropositions could be materialized using a block world.\nPossession could be materialized using a possession world.\n\nblock world (Winston 1992, 47–60) , c.f. Mover and SHRDLU by Terry Winograd"
  },
  {
    "objectID": "posts/2025-02-16-more-desidirata/index.html#block-world",
    "href": "posts/2025-02-16-more-desidirata/index.html#block-world",
    "title": "Further Desiderata for Emergent Language",
    "section": "Block world",
    "text": "Block world"
  },
  {
    "objectID": "posts/2025-02-16-more-desidirata/index.html#zookeeper",
    "href": "posts/2025-02-16-more-desidirata/index.html#zookeeper",
    "title": "Further Desiderata for Emergent Language",
    "section": "Zookeeper",
    "text": "Zookeeper\n\nin Zookeeper (Winston 1992, 121–25) we need to classify animals using deduction.\nhttps://www.kaggle.com/uciml/zoo-animal-classification/data\nwe can give each of 101 animal 18 properties in a table via a zoo.csv. We can then sample from the table to create sets of animals with different properties.\nwe might then play games based on this data:\n\nwe may need to identify the animal based on it properties.\neach property may come from a different sender.\nan early response may be to attack, evade or ignore based on a partial identification.\nthis requires some kind of reasoning about the properties of the animals in the ‘zoo’\n\n\nThis means states for animals and their properties.\nMinsky’s K-lines - physical world 214 - mental world 215 - ownership world 216 - Kanade (1980) A Theory of Origami World"
  },
  {
    "objectID": "posts/2025-02-17-samurai-world/index.html#code",
    "href": "posts/2025-02-17-samurai-world/index.html#code",
    "title": "A Samurai’s World",
    "section": "Code",
    "text": "Code\nHere is a data generation script that may be used with (Kharitonov et al. 2021) EGG emergence game toolkit to model politeness and formality in a multi-agent language game."
  },
  {
    "objectID": "posts/2025-02-17-samurai-world/index copy.html",
    "href": "posts/2025-02-17-samurai-world/index copy.html",
    "title": "Knights and Knaves world",
    "section": "",
    "text": "“if proof theory is about the sacred, then model theory is about the profane” – (Dalen 2012, 3:1)"
  },
  {
    "objectID": "posts/2025-02-17-samurai-world/index copy.html#motivation",
    "href": "posts/2025-02-17-samurai-world/index copy.html#motivation",
    "title": "Knights and Knaves world",
    "section": "Motivation",
    "text": "Motivation\nIn the previous post, “Further Desiderata for Emergent Language”, I discussed the need to adapt emerging languages to conform with established properties of natural languages. You can read more about motivations in that post.\nRather then considering all the ways this could be achieved in a single essay, I believe that this should be broken down and considered one aspect at a time. This can make both the linguistics and the development easier to understand.\nOnce enough examples are available it should become clearer on how to integrate the desiderata using suitable states or other mechanisms."
  },
  {
    "objectID": "posts/2025-02-17-samurai-world/index copy.html#the-task",
    "href": "posts/2025-02-17-samurai-world/index copy.html#the-task",
    "title": "Knights and Knaves world",
    "section": "The task",
    "text": "The task\nToday I want to consider the ability of language to encode logical reasoning. Logic is such a big field it can encampass all of mathematics and philosophy. So for this post we need to narrow things down.\nI’d like the agents not only to learn to speak in a langauge that captures logical reasoning, but also to be able to reason about states and statements made in that language.\nI am targeting reconstruction and discrimination games as the inner game which is used to evolve the language. The frameing game might be drawn from\n\nKnights and Knaves puzzles\nLewis Carroll’s The Game of Logic - for\nJon Barwise and John Etchemendy created Tarski’s World - for first order logic"
  },
  {
    "objectID": "posts/2025-02-17-samurai-world/index copy.html#knights-and-knaves-worlds",
    "href": "posts/2025-02-17-samurai-world/index copy.html#knights-and-knaves-worlds",
    "title": "Knights and Knaves world",
    "section": "Knights and Knaves worlds",
    "text": "Knights and Knaves worlds\nIn “What Is the Name of This Book” and his other works Raymond Smullyan use this as a framework to cover this puzzle to cover from propositional logic to the problem of undecidability…\nThe initial state is simple - each individual is either a knight or a knave. Knights always tell the truth, and knaves always lie.\nIn most puzzles we need to determine the type of each individual from a set of statements made by them.\nAnother type of variation is that we want to find out some fact about the world but must ask the right question to get the correct answer regardless of the individual’s type.\nAnother variant that seems salient in this context is when the people in the puzzle respond in their own language, which is unkown to us. In this case we need to deduce the meaning of the words from the context.\nTo spice his puzzles up Smullyan introduced individuals that, who can lie or tell the truth as he pleases these might be called “Normals”\nIn his Transilvanian puzzles he introduced the notion that half the population are insane and have false beliefs e.g. that 2+2\\ne4 and they are also devided into truthfull and lying types this time humans and vampires. In another book he introduced monkeys that look like humans. The only real difference was that monkeys have a tail and humans don’t. In terms of logic it just add another collumn to the truth table for each individual.\nWe have for n individuals we have 2^n possible states.\nnext comes the creative part of the task we want to automate. The statements the individuals make. While each individual can make a statement tht reveals their ground truth we the idea is to\n\nensure all the ground truths are revealed\nuse a minimal number of statements (i.e. by omitting a statement the problem should be rendered unsolvable)\nensure that the ground truths are unique\n\nIn the website https://christopherphelps.trinket.io/sites/knight_knave_puzzler the generator can be used to generate a number of statements:\n\nmeta statements - is the puzzle solvable\nname calling - calling some one a knight or knave a normal a monkey, insane etc.\nAscriptive statements - where an someone says what some type of individual would say about another speaker\nPrime statements- statements on the prime number of knights or knaves in the group\nindependent statements - statements that don’t seem to be related to the puzzle\n\nOne property of the puzzle is if no one makes a statement about an individual then his type is unconstrained and could be swapped without affecting the consistency of the puzzle."
  },
  {
    "objectID": "posts/2025-02-17-samurai-world/index copy.html#how-is-logic-encoded-in-the-knights-and-knaves-puzzles",
    "href": "posts/2025-02-17-samurai-world/index copy.html#how-is-logic-encoded-in-the-knights-and-knaves-puzzles",
    "title": "Knights and Knaves world",
    "section": "How is logic encoded in the knights and knaves Puzzles?",
    "text": "How is logic encoded in the knights and knaves Puzzles?\nSo it is interesting to consider how one give a minimal general solution for such puzzles. In reality most puzzles do have short solutions but in general when we consider logic and language we can’t be certain that there is a neat solution or that the puzlle has a unique answer or that the puzzle is solvable.\nSo here is a general approach to solve these puzzles using boolean logic:\n\nWe encode all possible combinations of sub-states of the world using as inputs for a truth table. I.e. a column for each individual titled with their name and stating that they are a knight. We don’t need to encode the knave column as the two are mutually exclusive. If there are other sub-states like being a monkey, insane, a spy we would need to add a column for these too.\nFor each statement said we should rewrite it as a boolean expression in terms of these states.\nWe need to verify the outcome of the statement for each combination of\n\nThen we encode the statements made by the individuals as a column in the truth table.\nlet’s look at some examples with just knights and knaves\n\nName calling\n\nA says “I am knave.”\n\nnot possible for a knight (False)\nnot possible for a knave (True)\nThis will therefore not appear on it own.\nThis will not be part of a conjunction made by a knight. i.e. “and …”\nIt can be used conjunction with a truthy statement made by a knave. … and I am a knave\n\nA says “I am knight.”\n\nif A is a knight (True)\nif A is a knave (False)\n\nA says “B is a knave.”\n\nif A is a knight and B is knave (True)\nif A is a knave and B is a knight (False) note: that both types will call the other type a knave. so this only tells us the speaker is the same type as another individual.\n\nA says “B is a knight.”\n\nif A is a knight and B is a knight (True)\nif A is a knave and B is a knave (False)\n\nA says “I am the same type as B” same situation as #3\nA says “B would say that I am a knave”\nA says “B would say that I am a knight”\nThere are a prime number of knaves in the group.\nThere are a prime number of knights in the group.\n“The puzzle is solvable” means there isn’t a contradiction in the statements made by the individuals.\n“The puzzle is unsolvable” means there is a contradiction in the statements made by the individuals.\n\n\n\n\nA\nB\nA:#1\nB:#1\n\n\n\n\n1\n1\n0\n0\n\n\n1\n0\n0\n1\n\n\n0\n1\n1\n0\n\n\n0\n0\n1\n1\n\n\n\n\n\nQuestions"
  },
  {
    "objectID": "posts/2025-02-17-samurai-world/index copy.html#samurai-world---sub-states-that-imbue-an-emergent-language-with-politeness-and-formality",
    "href": "posts/2025-02-17-samurai-world/index copy.html#samurai-world---sub-states-that-imbue-an-emergent-language-with-politeness-and-formality",
    "title": "Knights and Knaves world",
    "section": "Samurai world - Sub-States that imbue an emergent language with politeness and formality",
    "text": "Samurai world - Sub-States that imbue an emergent language with politeness and formality\nThis is a small state space which is used as a model1 for politeness and formality in natural language based on Japanese which has a more sophisticated system of politeness and formality then most languages.\n1 in the sense of logicIn this game agents interact in a framing game. In the lewis sub-game they need to coordinate a system in which they observe politeness and formality to avoid being decapitated by their superiors, yet avoid losing face, i.e. without being too polite or formal to their inferiors.\n\nstate:\n\nspeaker’s status\n\ngender, age, social status, relationship to subject\n\nsubject’s status\n\n\nOne could evolve the use of poltiness in a reconstruciton game or a discrimination game.\n\nIn the reconstruction task, Sender gets an input item, it sends a message to Receiver, and Receiver must generate an output item identical to Sender’s input.\nIn the discrimination task, Sender gets an input item (the target); Receiver gets multiple input items (the same target and a number of distractors, in random order).\n\nNote that reconstruction is very much like the original lewis signaling game. While the discrimination easier task as the receiver could in theory fail to reconstruct, or have to choose at random from a very large, perhaps even infinite set of possible reconstructions and that having a jusk K distractorsit is down to 1/k probability of sucess. More so if it is equipped with the ability of learning from errors he might score the distractors and make progress with much higher information levels then in the reconstruction game."
  },
  {
    "objectID": "posts/2025-02-17-samurai-world/index copy.html#code",
    "href": "posts/2025-02-17-samurai-world/index copy.html#code",
    "title": "Knights and Knaves world",
    "section": "Code",
    "text": "Code\nHere is a data generation script that may be used with (Kharitonov et al. 2021) EGG emergence game toolkit to model politeness and formality in a multi-agent language game.\nimport random import numpy as np"
  },
  {
    "objectID": "posts/2025-02-18-logic-worlds/index.html",
    "href": "posts/2025-02-18-logic-worlds/index.html",
    "title": "Knights and Knaves world",
    "section": "",
    "text": "“if proof theory is about the sacred, then model theory is about the profane” – (Dalen 2012, 3:1)"
  },
  {
    "objectID": "posts/2025-02-18-logic-worlds/index.html#motivation",
    "href": "posts/2025-02-18-logic-worlds/index.html#motivation",
    "title": "Knights and Knaves world",
    "section": "Motivation",
    "text": "Motivation\nIn the previous post, “Further Desiderata for Emergent Language”, I discussed the need to adapt emerging languages to conform with established properties of natural languages. You can read more about motivations in that post.\nRather then considering all the ways this could be achieved in a single essay, I believe that this should be broken down and considered one aspect at a time. This can make both the linguistics and the development easier to understand.\nOnce enough examples are available it should become clearer on how to integrate the desiderata using suitable states or other mechanisms."
  },
  {
    "objectID": "posts/2025-02-18-logic-worlds/index.html#the-task",
    "href": "posts/2025-02-18-logic-worlds/index.html#the-task",
    "title": "Knights and Knaves world",
    "section": "The task",
    "text": "The task\nToday I want to consider the ability of language to encode logical reasoning. Logic is such a big field it can encampass all of mathematics and philosophy. So for this post we need to narrow things down.\nI’d like the agents not only to learn to speak in a langauge that captures logical reasoning, but also to be able to reason about states and statements made in that language.\nI am targeting reconstruction and discrimination games as the inner game which is used to evolve the language. The frameing game might be drawn from\n\nKnights and Knaves puzzles\nLewis Carroll’s The Game of Logic - for\nJon Barwise and John Etchemendy created Tarski’s World - for first order logic"
  },
  {
    "objectID": "posts/2025-02-18-logic-worlds/index.html#knights-and-knaves-worlds",
    "href": "posts/2025-02-18-logic-worlds/index.html#knights-and-knaves-worlds",
    "title": "Knights and Knaves world",
    "section": "Knights and Knaves worlds",
    "text": "Knights and Knaves worlds\nIn “What Is the Name of This Book” and his other works Raymond Smullyan use this as a framework to cover this puzzle to cover from propositional logic to the problem of undecidability…\nThe initial state is simple - each individual is either a knight or a knave. Knights always tell the truth, and knaves always lie.\nIn most puzzles we need to determine the type of each individual from a set of statements made by them.\nAnother type of variation is that we want to find out some fact about the world but must ask the right question to get the correct answer regardless of the individual’s type.\nAnother variant that seems salient in this context is when the people in the puzzle respond in their own language, which is unkown to us. In this case we need to deduce the meaning of the words from the context.\nTo spice his puzzles up Smullyan introduced individuals that, who can lie or tell the truth as he pleases these might be called “Normals”\nIn his Transilvanian puzzles he introduced the notion that half the population are insane and have false beliefs e.g. that 2+2\\ne4 and they are also devided into truthfull and lying types this time humans and vampires. In another book he introduced monkeys that look like humans. The only real difference was that monkeys have a tail and humans don’t. In terms of logic it just add another collumn to the truth table for each individual.\nWe have for n individuals we have 2^n possible states.\nnext comes the creative part of the task we want to automate. The statements the individuals make. While each individual can make a statement tht reveals their ground truth we the idea is to\n\nensure all the ground truths are revealed\nuse a minimal number of statements (i.e. by omitting a statement the problem should be rendered unsolvable)\nensure that the ground truths are unique\n\nIn the website https://christopherphelps.trinket.io/sites/knight_knave_puzzler the generator can be used to generate a number of statements:\n\nmeta statements - is the puzzle solvable\nname calling - calling some one a knight or knave a normal a monkey, insane etc.\nAscriptive statements - where an someone says what some type of individual would say about another speaker\nPrime statements- statements on the prime number of knights or knaves in the group\nindependent statements - statements that don’t seem to be related to the puzzle\n\nOne property of the puzzle is if no one makes a statement about an individual then his type is unconstrained and could be swapped without affecting the consistency of the puzzle."
  },
  {
    "objectID": "posts/2025-02-18-logic-worlds/index.html#how-is-logic-encoded-in-the-knights-and-knaves-puzzles",
    "href": "posts/2025-02-18-logic-worlds/index.html#how-is-logic-encoded-in-the-knights-and-knaves-puzzles",
    "title": "Knights and Knaves world",
    "section": "How is logic encoded in the knights and knaves Puzzles?",
    "text": "How is logic encoded in the knights and knaves Puzzles?\nSo it is interesting to consider how one give a minimal general solution for such puzzles. In reality most puzzles do have short solutions but in general when we consider logic and language we can’t be certain that there is a neat solution or that the puzlle has a unique answer or that the puzzle is solvable.\nSo here is a general approach to solve these puzzles using boolean logic:\n\nWe encode all possible combinations of sub-states of the world using as inputs for a truth table. I.e. a column for each individual titled with their name and stating that they are a knight. We don’t need to encode the knave column as the two are mutually exclusive. If there are other sub-states like being a monkey, insane, a spy we would need to add a column for these too.\nFor each statement said we should rewrite it as a boolean expression in terms of these states.\nWe need to verify the outcome of the statement for each combination of\n\nThen we encode the statements made by the individuals as a column in the truth table.\nlet’s look at some examples with just knights and knaves\n\nName calling\n\nA says “I am knave.”\n\nnot possible for a knight (False)\nnot possible for a knave (True)\nThis will therefore not appear on it own.\nThis will not be part of a conjunction made by a knight. i.e. “and …”\nIt can be used conjunction with a truthy statement made by a knave. … and I am a knave\n\nA says “I am knight.”\n\nif A is a knight (True)\nif A is a knave (False)\n\nA says “B is a knave.”\n\nif A is a knight and B is knave (True)\nif A is a knave and B is a knight (False) note: that both types will call the other type a knave. so this only tells us the speaker is the same type as another individual.\n\nA says “B is a knight.”\n\nif A is a knight and B is a knight (True)\nif A is a knave and B is a knave (False)\n\nA says “I am the same type as B” same situation as #3\nA says “B would say that I am a knave”\nA says “B would say that I am a knight”\nThere are a prime number of knaves in the group.\nThere are a prime number of knights in the group.\n“The puzzle is solvable” means there isn’t a contradiction in the statements made by the individuals.\n“The puzzle is unsolvable” means there is a contradiction in the statements made by the individuals.\n\n\n\n\nA\nB\nA:#1\nB:#1\n\n\n\n\n1\n1\n0\n0\n\n\n1\n0\n0\n1\n\n\n0\n1\n1\n0\n\n\n0\n0\n1\n1"
  },
  {
    "objectID": "posts/2025-02-18-logic-worlds/index.html#samurai-world---sub-states-that-imbue-an-emergent-language-with-politeness-and-formality",
    "href": "posts/2025-02-18-logic-worlds/index.html#samurai-world---sub-states-that-imbue-an-emergent-language-with-politeness-and-formality",
    "title": "Knights and Knaves world",
    "section": "Samurai world - Sub-States that imbue an emergent language with politeness and formality",
    "text": "Samurai world - Sub-States that imbue an emergent language with politeness and formality\nThis is a small state space which is used as a model1 for politeness and formality in natural language based on Japanese which has a more sophisticated system of politeness and formality then most languages.\n1 in the sense of logicIn this game agents interact in a framing game. In the lewis sub-game they need to coordinate a system in which they observe politeness and formality to avoid being decapitated by their superiors, yet avoid losing face, i.e. without being too polite or formal to their inferiors.\n\nstate:\n\nspeaker’s status\n\ngender, age, social status, relationship to subject\n\nsubject’s status\n\n\nOne could evolve the use of poltiness in a reconstruciton game or a discrimination game.\n\nIn the reconstruction task, Sender gets an input item, it sends a message to Receiver, and Receiver must generate an output item identical to Sender’s input.\nIn the discrimination task, Sender gets an input item (the target); Receiver gets multiple input items (the same target and a number of distractors, in random order).\n\nNote that reconstruction is very much like the original lewis signaling game. While the discrimination easier task as the receiver could in theory fail to reconstruct, or have to choose at random from a very large, perhaps even infinite set of possible reconstructions and that having a jusk K distractorsit is down to 1/k probability of sucess. More so if it is equipped with the ability of learning from errors he might score the distractors and make progress with much higher information levels then in the reconstruction game."
  },
  {
    "objectID": "posts/2025-02-18-logic-worlds/index.html#code",
    "href": "posts/2025-02-18-logic-worlds/index.html#code",
    "title": "Knights and Knaves world",
    "section": "Code",
    "text": "Code\nHere is a data generation script that may be used with (Kharitonov et al. 2021) EGG emergence game toolkit to model politeness and formality in a multi-agent language game.\n\nimport random\nimport numpy as np\n\n\nstate = dict()\n\ndef add_politenes_state(state):\n\n    state[\"speaker_status\"] = random.choice([\"0\", \"1\", \"2\", ])\n    state[\"subject_status\"] = random.choice([\"0\", \"1\", \"2\", ])\n    return state\n\nfor i in range(20):\n\n    state = dict()\n    state = add_politenes_state(state)\n    print(f\"{i},{state}\")\n\n0,{'speaker_status': '1', 'subject_status': '2'}\n1,{'speaker_status': '2', 'subject_status': '0'}\n2,{'speaker_status': '2', 'subject_status': '2'}\n3,{'speaker_status': '2', 'subject_status': '0'}\n4,{'speaker_status': '0', 'subject_status': '1'}\n5,{'speaker_status': '1', 'subject_status': '0'}\n6,{'speaker_status': '2', 'subject_status': '0'}\n7,{'speaker_status': '1', 'subject_status': '2'}\n8,{'speaker_status': '1', 'subject_status': '2'}\n9,{'speaker_status': '2', 'subject_status': '0'}\n10,{'speaker_status': '2', 'subject_status': '0'}\n11,{'speaker_status': '0', 'subject_status': '2'}\n12,{'speaker_status': '0', 'subject_status': '0'}\n13,{'speaker_status': '0', 'subject_status': '2'}\n14,{'speaker_status': '1', 'subject_status': '2'}\n15,{'speaker_status': '1', 'subject_status': '2'}\n16,{'speaker_status': '0', 'subject_status': '0'}\n17,{'speaker_status': '1', 'subject_status': '2'}\n18,{'speaker_status': '2', 'subject_status': '2'}\n19,{'speaker_status': '1', 'subject_status': '1'}"
  },
  {
    "objectID": "posts/2025-02-18-logic-worlds/index.html#states-for-verbs-and-nouns",
    "href": "posts/2025-02-18-logic-worlds/index.html#states-for-verbs-and-nouns",
    "title": "Knights and Knaves world",
    "section": "states for verbs and nouns",
    "text": "states for verbs and nouns\nOne issue is that there that I have not yet written up the states needed for differentiating between nouns and verbs. Not the states for creating inflected verbs.\nBoth of these are bigger tasks and I will need to write them up quickly.\nAnyhow is this case we should assume that there are already\n\nnouns and verbs and possible other parts of speech.\nverbs may be inflected.\n\nnow we wish to split certain verb states by adding sub-states that correspond to a politeness and formality flag."
  },
  {
    "objectID": "posts/2025-02-17-samurai-world/index.html#states-for-verbs-and-nouns",
    "href": "posts/2025-02-17-samurai-world/index.html#states-for-verbs-and-nouns",
    "title": "A Samurai’s World",
    "section": "states for verbs and nouns",
    "text": "states for verbs and nouns\nOne issue is that there that I have not yet written up the states needed for differentiating between nouns and verbs. Not the states for creating inflected verbs.\nBoth of these are bigger tasks and I will need to write them up quickly.\nAnyhow is this case we should assume that there are already\n\nnouns and verbs and possible other parts of speech.\nverbs may be inflected.\n\nnow we wish to split certain verb states by adding sub-states that correspond to a politeness and formality flag."
  },
  {
    "objectID": "posts/2025-02-17-samurai-world/index.html#code-1",
    "href": "posts/2025-02-17-samurai-world/index.html#code-1",
    "title": "A Samurai’s World",
    "section": "Code",
    "text": "Code\nHere is a data generation script that may be used with (Kharitonov et al. 2021) EGG emergence game toolkit to model politeness and formality in a multi-agent language game.\n\nimport random\nimport numpy as np\n\n\nstate = dict()\n\ndef add_politenes_state(state):\n\n    state[\"speaker_status\"] = random.choice([0, 1, 2, ])\n    state[\"subject_status\"] = random.choice([0, 1, 2, ])\n    return state\n\nfor i in range(20):\n    state = dict()\n    state = add_politenes_state(state)\n    print(f\"{i},{state} should be polite {state['subject_status'] &gt; state['speaker_status']} \")\n\n \n## TODO: estimate politeness and face in a states\n## TODO: estimate politeness and face in an utterance\n## TODO: the politeness loss is the formality of the utterance and the politeness of the state. I.e. if the state is more polite then the utterance there is a loss associated with that.\n\ndef politeness_loss_penalty(state):\n    if state[\"subject_status\"] &gt; state[\"speaker_status\"]:\n        return 0\n    else:\n        return 1\n\n0,{'speaker_status': 0, 'subject_status': 1} should be polite True \n1,{'speaker_status': 0, 'subject_status': 1} should be polite True \n2,{'speaker_status': 2, 'subject_status': 2} should be polite False \n3,{'speaker_status': 2, 'subject_status': 0} should be polite False \n4,{'speaker_status': 2, 'subject_status': 2} should be polite False \n5,{'speaker_status': 1, 'subject_status': 2} should be polite True \n6,{'speaker_status': 1, 'subject_status': 2} should be polite True \n7,{'speaker_status': 1, 'subject_status': 1} should be polite False \n8,{'speaker_status': 1, 'subject_status': 0} should be polite False \n9,{'speaker_status': 1, 'subject_status': 1} should be polite False \n10,{'speaker_status': 0, 'subject_status': 1} should be polite True \n11,{'speaker_status': 0, 'subject_status': 1} should be polite True \n12,{'speaker_status': 1, 'subject_status': 0} should be polite False \n13,{'speaker_status': 2, 'subject_status': 2} should be polite False \n14,{'speaker_status': 0, 'subject_status': 1} should be polite True \n15,{'speaker_status': 1, 'subject_status': 0} should be polite False \n16,{'speaker_status': 1, 'subject_status': 0} should be polite False \n17,{'speaker_status': 1, 'subject_status': 0} should be polite False \n18,{'speaker_status': 1, 'subject_status': 0} should be polite False \n19,{'speaker_status': 1, 'subject_status': 2} should be polite True \n\n\nok now that we have the states we can consider\n\nensuring that the speaker’s status is encoded in the message\n\nwe can do this indirectly as part of the reconstruction.\nwe can also do this as a penalty coming from the framing game.\n\nRude speakers are penalized!\nLosing face is also penalized.\n\n\nwe can refine the model by adding flags for informal_settings, friendship_settings and gender_speaker gender_subject age_speaker age_subject to further refine the politeness and formality of the language.\nwe may also want to consider the setting where the subject social status is unknown to the speaker and thus the speaker must use a default politeness setting.\n\n\nhttps://blog.duolingo.com/japanese-politeness-formal-language"
  },
  {
    "objectID": "posts/2025-02-17-samurai-world/index.html#over-thinking-politeness",
    "href": "posts/2025-02-17-samurai-world/index.html#over-thinking-politeness",
    "title": "A Samurai’s World",
    "section": "Over thinking politeness",
    "text": "Over thinking politeness\nIs politeness is not some random flag in some language?\nI think that language is a social construct and that politeness is a social construct. It is not just a flag that is set in a language, but rather a complex set of rules and norms that govern how people interact with each other.\nThe social and biological basis for politeness can be viewed as an expression of dominance hierarchies and submission. In this view, politeness is a way of signaling respect and deference to others, and it is a way of maintaining social harmony and avoiding conflict associated with reestablishing dominance hierarchies when there are disputes within a social group.\nThis suggests three phenomena that are relevant to the emergence of politeness in a multi-agent language game:\n\nFace for a speaker has a dual. It is the ability and a requirement of an individual to express their dominance and status with respect to his subordinates in society as well as the acknowledgment of the , but more so it due to subordinates giving the speaker their due respect. Thus loss of face can happen in private but is more significant in public. Face can be lost when the an individual is shamed by actions as well as words.\n\nAny loss of face in public may be viewed as a challenge to the speaker’s dominance and status. Such a challange when viewed by some third party may be viewd by others at large as destablizing the social harmony of the established order embodied in the entire hierarchy.\ncould be seen as a challenge to the speaker’ 2. Informal speech may be used when there is no conflict or challenge to dominance hierarchies. 3. Politeness forms that are ingrained in a language are means to permanently establish dominance hierarchies. and to avoid conflict by signaling respect and deference to others."
  },
  {
    "objectID": "reviews/paper/2016-neural-morphological-segmentation/index.html",
    "href": "reviews/paper/2016-neural-morphological-segmentation/index.html",
    "title": "Neural Morphological Analysis: Encoding-Decoding Canonical Segments",
    "section": "",
    "text": "Literature review",
    "crumbs": [
      "Home",
      "Papers",
      "Neural Morphological Analysis Encoding-Decoding Canonical Segments"
    ]
  },
  {
    "objectID": "reviews/paper/2016-neural-morphological-segmentation/index.html#podcast",
    "href": "reviews/paper/2016-neural-morphological-segmentation/index.html#podcast",
    "title": "Neural Morphological Analysis: Encoding-Decoding Canonical Segments",
    "section": "Podcast",
    "text": "Podcast",
    "crumbs": [
      "Home",
      "Papers",
      "Neural Morphological Analysis Encoding-Decoding Canonical Segments"
    ]
  },
  {
    "objectID": "reviews/paper/2016-neural-morphological-segmentation/index.html#abstract",
    "href": "reviews/paper/2016-neural-morphological-segmentation/index.html#abstract",
    "title": "Neural Morphological Analysis: Encoding-Decoding Canonical Segments",
    "section": "Abstract",
    "text": "Abstract\n\nCanonical morphological segmentation aims to divide words into a sequence of standardized segments. In this work, we propose a character-based neural encoder-decoder model for this task. Additionally, we extend our model to include morpheme-level and lexical information through a neural reranker. We set the new state of the art for the task improving previous results by up to 21% accuracy. Our experiments cover three languages: English, German and Indonesian. – (Kann, Cotterell, and Schütze 2016)",
    "crumbs": [
      "Home",
      "Papers",
      "Neural Morphological Analysis Encoding-Decoding Canonical Segments"
    ]
  },
  {
    "objectID": "reviews/paper/2016-neural-morphological-segmentation/index.html#the-paper",
    "href": "reviews/paper/2016-neural-morphological-segmentation/index.html#the-paper",
    "title": "Neural Morphological Analysis: Encoding-Decoding Canonical Segments",
    "section": "The Paper",
    "text": "The Paper\n\n\n\npaper",
    "crumbs": [
      "Home",
      "Papers",
      "Neural Morphological Analysis Encoding-Decoding Canonical Segments"
    ]
  },
  {
    "objectID": "reviews/paper/2016-neural-morphological-segmentation/index.html#outline",
    "href": "reviews/paper/2016-neural-morphological-segmentation/index.html#outline",
    "title": "Neural Morphological Analysis: Encoding-Decoding Canonical Segments",
    "section": "Outline",
    "text": "Outline\n\nIntroduction\n\nDiscusses morphological segmentation and its applications in NLP.\nExplains the difference between surface segmentation and canonical segmentation, providing an example.\nHighlights the advantages of canonical segmentation and the algorithmic challenges it introduces.\nPresents a neural encoder-decoder model for canonical segmentation and a neural reranker to incorporate linguistic structure.\n\nNeural Canonical Segmentation\n\nFormally describes the canonical segmentation task, mapping a word to a canonical segmentation.\nExplains the probabilistic approach to learn a distribution p(c | w).\nDetails the two parts of the model: an encoder-decoder RNN and a neural reranker.\nDescribes the neural encoder-decoder model based on Bahdanau et al. (2014), using a bidirectional gated RNN (GRU) as the encoder.\nExplains how the decoder defines a conditional probability distribution over possible segmentations.\nExplains the attention mechanism and how attention weights are computed.\nExplains the neural reranker’s role in rescoring candidate segmentations from a sample set generated by the encoder-decoder.\nDescribes the reranking model’s ability to embed morphemes and incorporate character-level information.\n\nRelated Work\n\nDiscusses various approaches to morphological segmentation.\nMentions unsupervised methods like LINGUISTICA and MORFESSOR.\nDescribes supervised approaches using conditional random fields (CRFs).\nDistinguishes the approach from surface morphological segmentation methods using a window LSTM.\nRelates the approach to other applications of recurrent neural network transduction models.\n\nExperiments\n\nDescribes the dataset used for comparison to earlier work.\nSpecifies the three languages used in the experiments: English, German, and Indonesian.\nNotes the potential cause of the high error rate for German due to its orthographic changes.\nExplains the data extraction process from CELEX, DerivBase, and MORPHIND analyzer for English, German, and Indonesian, respectively.\nDetails the training setup, including the use of an ensemble of five encoder-decoder models.\nDescribes the training of the reranking model, including sample set gathering and optimization.\nDescribes the baseline models used for comparison: JOINT model and a weighted finite-state transducer (WFST).\nOutlines the evaluation metrics used: error rate, edit distance, and morpheme F1.\n\nResults\n\nPresents the results of the canonical segmentation experiment, showing improvements over baselines with both the encoder-decoder and reranker.\nDiscusses the additional improvements achieved by the reranker due to access to morpheme embeddings and existing words.\nAnalyzes cases where the right answer is not in the samples and errors due to annotation problems.\nDiscusses cases where the encoder-decoder finds the right solution but assigns a higher probability to an incorrect analysis.\nExplains how the reranker corrects some errors based on lexical information and morpheme embeddings.\nInvestigates whether segments unseen in the training set are a source of errors.\n\nConclusion and Future Work\n\nSummarizes the developed model consisting of an encoder-decoder and neural reranker for canonical morphological segmentation.\nStates the model’s improvement over baseline models.\nDiscusses the potential for further performance increase by improving the reranker.",
    "crumbs": [
      "Home",
      "Papers",
      "Neural Morphological Analysis Encoding-Decoding Canonical Segments"
    ]
  },
  {
    "objectID": "posts/2025-02-17-samurai-world/index.html#establishing-a-hierarchy.",
    "href": "posts/2025-02-17-samurai-world/index.html#establishing-a-hierarchy.",
    "title": "A Samurai’s World",
    "section": "Establishing a hierarchy.",
    "text": "Establishing a hierarchy.\nThe establishment of hierarchies, particularly in a multi-agent system, is a complex process that involves a variety of factors. This may well be a bigger topic than we need to consider here and may easily confuse the issues I wish to discuss here which is the minimal example for the emergence of politeness and honorifics in a multi-agent language game. I therefore moved further discussion to the next post titled Establishing Hierarchies."
  },
  {
    "objectID": "posts/2025-02-17-samurai-world/index.html#emergent-hierarchies",
    "href": "posts/2025-02-17-samurai-world/index.html#emergent-hierarchies",
    "title": "Domination Games",
    "section": "Emergent Hierarchies",
    "text": "Emergent Hierarchies\nDominance hierarchy is common in the nature. While most of the time it is established by physical characteristics, we have many examples where other mechanisms. The use of brute force is risky for the individual and in the long term for the group.\n\nEgalitarian\nAge group - initially all have the same age\nDominance - some agents are stronger\nEgalitarian meaning no hierarchy\nPartiarchal\nMatriarchal\nOligarchy - some agents are more fit in harvesting resources and may be able to increase this advantage via trade. This should lead to specialization and division of labor under suitable circumstances.\nOligrachy with welfare. In Japan the CEO’s pay may only be 20x that of the lowest paid employee. Another way to improve welfare is to require an agent to share wealth when they come into contact with agents which are worse off. These gifts though can be used to keep track of social status based on amount given and received!\nMeritocracy - this is a form of hierarchy where agents are ranked by their ability to perform a task. This means that there are many possible hierarchies in a group. However specialization implies we will consider individuals in a group for their top roles for skills that are have unmet demand by more qualified agents."
  },
  {
    "objectID": "posts/2025-02-17-samurai-world/index.html#games-of-domination",
    "href": "posts/2025-02-17-samurai-world/index.html#games-of-domination",
    "title": "Domination Games",
    "section": "Games of domination",
    "text": "Games of domination\nLeaders will wish to deter challenges to their authority i.e. disagreements over their actions or decisions for the group. This can be done in many ways. Here are a few:\n\nthey can do this by being requiring that challengers first qualify\ni.e. win a tournament that places them as the current challenger\nthey may need to also bring some resource, i.e. a wager, to the table\nthe challenges require a quorum and may take place only at certain times like once a year.\nthe challenged may also decide the terms of the challenge like the weapons used and the stakes involved.\n\nThey may go further yet by outlawing such challenges and punishing those who do so. This leads to selectorate theory where the leader may want to keep the selectorate small to reduce the risk of being challenged. I.e. the pool of people who can pose challenges are kept small and given high incentives to support the leader. This may be done by rewarding them with resources or status well beyond they could achieve on their own.\nSo in terms of this game\nif X &lt; Y \\wedge X &gt; Z\\ \\forall(Z &lt; Y) X then X may challenge Y. Y may however consider his and X’s interinsics and skill abd set the terms most favorable to him."
  },
  {
    "objectID": "posts/2025-02-17-samurai-world/index.html#winner-takes-all",
    "href": "posts/2025-02-17-samurai-world/index.html#winner-takes-all",
    "title": "Domination Games",
    "section": "winner takes all:",
    "text": "winner takes all:\n\nThe most extreme format of domination, yet one which may also deter challenges to the leader\nThe winner takes the loser’s life, status, possession, mates, and can kill his progeny and may impose similar punishments to the loser’s group.+\nLeaders will want to avoid having to play in this game and may\n\nIt seems that our agents might need a leaders. And the leaders need a leader too. This might be a prequisite for planning in which an agent assigns tasks to other agents. This wouldn’t work well if all agents shared the same role.\nOne way to go is by age of the agent. The older agents are the leaders. But what if there are no older agents in the group? Another aspect is skill and specialization. To be assigned certain roles agents need to demonstrate skills. This may involve rites of passage or other tests to initiate agents into a new role. E.g. masai hunting a lion solo with a spear\nWe need a game for establishing a hierarchy.\nThe game of domination can be used to create social hierarchy of agents. Domination can be risky and costly. But once established it can be used to assign tasks. This means that any tasked one is given may be reassigned to a lower status agent.\nThe rewards for such social tasks may be shared. - they may be shared by the group - they may be given to the leader to keep/share/distribute/assign - they may be given\n\nIt requires time.\nIt requires witnesses.\nAgents are at risk of losing face\nAgents may also risk injury or death if the opponent is stronger.\nAn agent of lower or equal status may challange another agent to a duel.\nThe challange may be\nThe winner takes the loser’s status.\nThe loser is demoted to the status of the winner.\nThe winner may also take the loser’s resources.\nThe winner may also take the loser’s mate.\nThe winner may also take the loser’s life.\nThis has a cost in terms of resources and status.\nViewers may wager on the outcome of the duel.\nThe challenged decides the terms of the duel.\n\nThe duel may be to the death or to first blood.\nThe duel may be non-lethal e.g. a mating dance or call\n\nNote: that the ideal leader is not always the strongest or most effective killer. But perhaps the one who can best assign tasks and resources to make the group most effective.\nduels to the death may be effective for deterring challenges to the leader, but they also risk the loss of one or two valuable member of the group each time they take place.\nthus a challenge to the leader may need to qualify, say by dominating all the warriors younger then the leader.\nso we can see there may be soft or hard mechanisms for establishing a hierarchy.\nthere could be multiple hierarchies in a group.\n\nhunters may not want to participate in foraging tasks and so the foragers may establish their own hierarchy.\nhunters may also decide rank by ability with a spear pow or throwing rocks.\nelders may establish their own hierarchy and act as advisors to the leaders.\n\n\nWe probably want the hierarchy to be transitive."
  },
  {
    "objectID": "posts/2025-02-18-domination/index.html",
    "href": "posts/2025-02-18-domination/index.html",
    "title": "Domination Games",
    "section": "",
    "text": "In a previous post titled Samurai’s world, I considered the complex states and a framing game of a Lewis signaling game for which the requisite linguistics aspects of politeness and formality should emerge out of language evolution\nToday though I’d like to pause from signaling for a bit and consider the establishment of hierarchies in a multi-agent system. This is also an extension that seems of interest to Sugarscape and other agent based models where one might be interested in the emergence of social hierarchies.\nFor language generation we may assign the hierarchy arbitrarily. But to speed learning we may want both the language and the hierarchy to emerge together. This can let us consider how different social structures may result in different linguistic structures."
  },
  {
    "objectID": "posts/2025-02-18-domination/index.html#motivation",
    "href": "posts/2025-02-18-domination/index.html#motivation",
    "title": "Domination Games",
    "section": "",
    "text": "In a previous post titled Samurai’s world, I considered the complex states and a framing game of a Lewis signaling game for which the requisite linguistics aspects of politeness and formality should emerge out of language evolution\nToday though I’d like to pause from signaling for a bit and consider the establishment of hierarchies in a multi-agent system. This is also an extension that seems of interest to Sugarscape and other agent based models where one might be interested in the emergence of social hierarchies.\nFor language generation we may assign the hierarchy arbitrarily. But to speed learning we may want both the language and the hierarchy to emerge together. This can let us consider how different social structures may result in different linguistic structures."
  },
  {
    "objectID": "posts/2025-02-18-domination/index.html#establishing-a-hierarchy.",
    "href": "posts/2025-02-18-domination/index.html#establishing-a-hierarchy.",
    "title": "Domination Games",
    "section": "Establishing a hierarchy.",
    "text": "Establishing a hierarchy.\nSo lets lay down some ground rules. In our society the agents are initially an Egalitarian society meaning they are without a hierarchy or language. They are heterogenous and they can also evolve and learn via RL.\nThe game takes a decentralized forms for agent interactions and we are generally interested in finding an form of incentive often embodied as rule and or a utility that can when maximized individually leads to better overall performance in the society. This can be in the carrying capacity, mean wealth, or social welfare or expected progeny.\nAs the simulation progresses conditions may change to favor different strategies. For example if the carrying capacity of the environment changes, the agents may need to change their strategy to adapt. Agents less fit may die out, while fitter agents may migrate to greener pastures.\nEventually though the agents will have to interact with each other strategically to maximize their utility. Agents that don’t learn to do so will face greater risks of being eliminated. Further on agents may need to cooperate, coordinate and compete as groups to survive. It is somewhere along this axis that learning language and establishing social hierarchies may be increasingly beneficial.\nFinally although the focus here is growing hierarchies I think that we should agree that that we are interested in the interplay of social structures and language."
  },
  {
    "objectID": "posts/2025-02-18-domination/index.html#dual-view-of-hierarchies",
    "href": "posts/2025-02-18-domination/index.html#dual-view-of-hierarchies",
    "title": "Domination Games",
    "section": "Dual view of Hierarchies",
    "text": "Dual view of Hierarchies\nIn terms of society the cost of a leader’s actions and decisions may have far reaching impact on the future of group. The benefits and costs may be in proportion to the group’s size. Given the costs of bad decisions larger groups have vested interests in seeing that the leaders are competent and that the group’s resources are used wisely.\nThe leader on the other hand may like to do as they please, avoid criticism and challenges to their authority. They may also wish to avoid the costs of bad decisions by foisting them onto others. To do this they would like to reduce the group they are accountable to.\n\nThe principal-agent dilemma\nThe ability of leaders to pick members of the hierarchy based on loyalty rather then merit subverts the society’s goals yet this is often the paths taken by leaders.\nLooking at emergent hierarchies we would like to study how the group can best select a hierarchy and leaders that primarily serve the group’s interests. The leaders of the hierarchy may try to game the system to serve their own interests. This is called the principal-agent dilemma and is a common problem in economics and politics. And we should formalize this aspect of the the game as it appears there are no political systems that are immune to this problem."
  },
  {
    "objectID": "posts/2025-02-18-domination/index.html#emergent-hierarchies",
    "href": "posts/2025-02-18-domination/index.html#emergent-hierarchies",
    "title": "Domination Games",
    "section": "Emergent Hierarchies",
    "text": "Emergent Hierarchies\nDominance hierarchy is common in the nature. While most of the time it is established by physical characteristics, we have many examples where other mechanisms. The use of brute force is risky for the individual and in the long term for the group.\n\nEgalitarian\nAge group - initially all have the same age\nDominance - some agents are stronger\nEgalitarian meaning no hierarchy\nPartiarchal - succession is by the oldest son progeny of the leader\nMatriarchal - succession is by the oldest daughter progeny of the leader\nOligarchy - some agents are more fit in harvesting resources and may be able to increase this advantage via trade. This should lead to specialization and division of labor under suitable circumstances.\nOligrachy with welfare. In Japan the CEO’s pay may only be 20x that of the lowest paid employee. Another way to improve welfare is to require an agent to share wealth when they come into contact with agents which are worse off. These gifts though can be used to keep track of social status based on amount given and received!\nMeritocracy - this is a form of hierarchy where agents are ranked by their ability to perform a task. This means that there are many possible hierarchies in a group. However specialization implies we will consider individuals in a group for their top roles for skills that are have unmet demand by more qualified agents.\nAristocracy - authority is based on birthright and passes by rules of succession. Land ownership and rents dervided from these are restricted to a few families. Power is derived from wealth and influence can be increased by wealth, marriage, and alliances.\n\nAnother aspect of dominance hierarchies is that they can lead to chaos or disharmony whenever the leader’s ability falls into question. This can lead to a challenge to the leader’s authority."
  },
  {
    "objectID": "posts/2025-02-18-domination/index.html#games-of-domination",
    "href": "posts/2025-02-18-domination/index.html#games-of-domination",
    "title": "Domination Games",
    "section": "Games of domination",
    "text": "Games of domination\nLeaders will wish to deter challenges to their authority i.e. disagreements over their actions or decisions for the group. This can be done in many ways. Here are a few:\n\nthey can do this by being requiring that challengers first qualify\ni.e. win a tournament that places them as the current challenger\nthey may need to also bring some resource, i.e. a wager, to the table\nthe challenges require a quorum and may take place only at certain times like once a year.\nthe challenged may also decide the terms of the challenge like the weapons used and the stakes involved.\n\nThey may go further yet by outlawing such challenges and punishing those who do so. This leads to selectorate theory where the leader may want to keep the selectorate small to reduce the risk of being challenged. I.e. the pool of people who can pose challenges are kept small and given high incentives to support the leader. This may be done by rewarding them with resources or status well beyond they could achieve on their own.\nSo in terms of this game:\nif X &lt; Y \\wedge X &gt; Z\\ \\forall(Z &lt; Y) X then X may challenge Y. Y may however consider his and X’s interinsics and skill abd set the terms most favorable to him."
  },
  {
    "objectID": "posts/2025-02-18-domination/index.html#winner-takes-all",
    "href": "posts/2025-02-18-domination/index.html#winner-takes-all",
    "title": "Domination Games",
    "section": "winner takes all:",
    "text": "winner takes all:\n\nThe most extreme format of domination, yet one which may also deter challenges to the leader\nThe winner takes the loser’s life, status, possession, mates, and can kill his progeny and may impose similar punishments to the loser’s group.+\nLeaders will want to avoid having to play in this game and may\n\nIn the next three sections we will consider social structure that are common both family and state levels and are setup to maintain a heirarchy."
  },
  {
    "objectID": "posts/2025-02-18-domination/index.html#patriarchy",
    "href": "posts/2025-02-18-domination/index.html#patriarchy",
    "title": "Domination Games",
    "section": "Patriarchy",
    "text": "Patriarchy\n\nthe oldest son progeny of the leader takes the leader’s social role in the event of the leader’s death.\nthe leader may also decide the heir to the throne is someone else other than his oldest son."
  },
  {
    "objectID": "posts/2025-02-18-domination/index.html#matriarchy",
    "href": "posts/2025-02-18-domination/index.html#matriarchy",
    "title": "Domination Games",
    "section": "Matriarchy",
    "text": "Matriarchy\n\nthe oldest daughter progeny of the leader takes the leader’s social role in the event of the leader’s death."
  },
  {
    "objectID": "posts/2025-02-18-domination/index.html#succession",
    "href": "posts/2025-02-18-domination/index.html#succession",
    "title": "Domination Games",
    "section": "Succession",
    "text": "Succession\n\nthe leaders oldest son or daughter may be the heir to the throne.\nif the leader has no children, the succession is by the oldest direct descendant of the most recent leader.\n\nIt seems that our agents might need a leaders. And the leaders need a leader too. This might be a prequisite for planning in which an agent assigns tasks to other agents. This wouldn’t work well if all agents shared the same role.\nOne way to go is by age of the agent. The older agents are the leaders. But what if there are no older agents in the group? Another aspect is skill and specialization. To be assigned certain roles agents need to demonstrate skills. This may involve rites of passage or other tests to initiate agents into a new role. E.g. masai hunting a lion solo with a spear\nWe need a game for establishing a hierarchy.\nThe game of domination can be used to create social hierarchy of agents. Domination can be risky and costly. But once established it can be used to assign tasks. This means that any tasked one is given may be reassigned to a lower status agent.\nThe rewards for such social tasks may be shared. - they may be shared by the group - they may be given to the leader to keep/share/distribute/assign - they may be given\n\nIt requires time.\nIt requires witnesses.\nAgents are at risk of losing face\nAgents may also risk injury or death if the opponent is stronger.\nAn agent of lower or equal status may challange another agent to a duel.\nThe challange may be\nThe winner takes the loser’s status.\nThe loser is demoted to the status of the winner.\nThe winner may also take the loser’s resources.\nThe winner may also take the loser’s mate.\nThe winner may also take the loser’s life.\nThis has a cost in terms of resources and status.\nViewers may wager on the outcome of the duel.\nThe challenged decides the terms of the duel.\n\nThe duel may be to the death or to first blood.\nThe duel may be non-lethal e.g. a mating dance or call\n\nNote: that the ideal leader is not always the strongest or most effective killer. But perhaps the one who can best assign tasks and resources to make the group most effective.\nduels to the death may be effective for deterring challenges to the leader, but they also risk the loss of one or two valuable member of the group each time they take place.\nthus a challenge to the leader may need to qualify, say by dominating all the warriors younger then the leader.\nso we can see there may be soft or hard mechanisms for establishing a hierarchy.\nthere could be multiple hierarchies in a group.\n\nhunters may not want to participate in foraging tasks and so the foragers may establish their own hierarchy.\nhunters may also decide rank by ability with a spear pow or throwing rocks.\nelders may establish their own hierarchy and act as advisors to the leaders.\n\n\nWe probably want the hierarchy to be transitive."
  },
  {
    "objectID": "reviews/paper/2016-neural-morphological-segmentation/index.html#reflections",
    "href": "reviews/paper/2016-neural-morphological-segmentation/index.html#reflections",
    "title": "Neural Morphological Analysis: Encoding-Decoding Canonical Segments",
    "section": "Reflections",
    "text": "Reflections\n\nwhy has this not caught on.\nwhy are people using byte pair encoding and morphological segmentation.\nwhat resources are needed to train this kind of model on a new language?\n\nA dataset of words with canonical segmentation\nAccess to a lexicon or a large corpus to determine if a canonical segment occurs as an independent word in the language. What is it is a bound morpheme that never appears alone or part of a root-template morphological system? How should we verify that we are deleing with a morpheme and not a surface phonemic fragment.\n\nCan we do this without a canonical segmentation dataset. More specifically can we induct morphology by processing surface forms of words and induct the canonical morphological forms using one of three loss function that\n\naffix loss (prefix,stem, suffix)\ntemplate loss (root,template) loss\nagglunative loss (stem suffix sequence)",
    "crumbs": [
      "Home",
      "Papers",
      "Neural Morphological Analysis Encoding-Decoding Canonical Segments"
    ]
  },
  {
    "objectID": "reviews/paper/2018-PTWM-NMT/index.html",
    "href": "reviews/paper/2018-PTWM-NMT/index.html",
    "title": "When and Why are Pre-trained Word Embeddings Useful for Neural Machine Translation?",
    "section": "",
    "text": "Video 1: Talk covering this paper by Roee Aharoni",
    "crumbs": [
      "Home",
      "Papers",
      "PTWM NMT"
    ]
  },
  {
    "objectID": "reviews/paper/2018-PTWM-NMT/index.html#podcast",
    "href": "reviews/paper/2018-PTWM-NMT/index.html#podcast",
    "title": "When and Why are Pre-trained Word Embeddings Useful for Neural Machine Translation?",
    "section": "Podcast",
    "text": "Podcast",
    "crumbs": [
      "Home",
      "Papers",
      "PTWM NMT"
    ]
  },
  {
    "objectID": "reviews/paper/2018-PTWM-NMT/index.html#abstract",
    "href": "reviews/paper/2018-PTWM-NMT/index.html#abstract",
    "title": "When and Why are Pre-trained Word Embeddings Useful for Neural Machine Translation?",
    "section": "Abstract",
    "text": "Abstract\n\nThe performance of Neural Machine Translation (NMT) systems often suffers in lowresource scenarios where sufficiently largescale parallel corpora cannot be obtained. Pretrained word embeddings have proven to be invaluable for improving performance in natural language analysis tasks, which often suffer from paucity of data. However, their utility for NMT has not been extensively explored. In this work, we perform five sets of experiments that analyze when we can expect pre-trained word embeddings to help in NMT tasks. We show that such embeddings can be surprisingly effective in some cases – providing gains of up to 20 BLEU points in the most favorable setting.. –(Qi et al. 2018)",
    "crumbs": [
      "Home",
      "Papers",
      "PTWM NMT"
    ]
  },
  {
    "objectID": "reviews/paper/2018-PTWM-NMT/index.html#outline",
    "href": "reviews/paper/2018-PTWM-NMT/index.html#outline",
    "title": "When and Why are Pre-trained Word Embeddings Useful for Neural Machine Translation?",
    "section": "Outline",
    "text": "Outline\n\n**Introduction\n\nDescribes the problem of low-resource scenarios in Neural Machine Translation (NMT) and the potential utility of pre-trained word embeddings.\nHighlights the success of pre-trained embeddings in natural language analysis tasks and the lack of extensive exploration in NMT.\nPoses five researcb questions:\n\nQ1 Is the behavior of pre-training affected by language families and other linguistic features of source and target languages? (§3)\nQ2 Do pre-trained embeddings help more when the size of the training data is small? (§4)\nQ3 How much does the similarity of the source and target languages affect the efficacy of using pre-trained embeddings? (§5)\nQ4 Is it helpful to align the embedding spaces between the source and target languages? (§6)\nQ5 Do pre-trained embeddings help more in multilingual systems as compared to bilingual systems? (§7)\n\n\nExperimental Setup\n\nDetails the five sets of experiments conducted to evaluate the effectiveness of pre-trained word embeddings in NMT.\nDescribes the datasets used, including the WMT14 English-German and English-French translation tasks.\nOutlines the models and training procedures employed in the experiments.\n\nResults and Analysis\n\nPresents the results of the experiments, showing the impact of pre-trained word embeddings on NMT performance.\nDiscusses the observed gains in BLEU scores and the factors influencing the effectiveness of pre-trained embeddings.\nAnalyzes the relationship between the quality of pre-trained embeddings and the performance of NMT systems.\n\nAnalysis\n\nConsiders the implications of the findings for NMT research and practice.\nDiscusses the potential benefits and limitations of using pre-trained word embeddings in NMT tasks.\n\nConclusion\n\nThe sweet-spot is wgere there is very little training data yet enough to train the system.\nPTWE are more effective if there are more similar translation pairs.\nA priori alignment of embeddings may not be necessary in bilingual scenarios, but is helpful in multi-lingual training scenarios.",
    "crumbs": [
      "Home",
      "Papers",
      "PTWM NMT"
    ]
  },
  {
    "objectID": "reviews/paper/2018-PTWM-NMT/index.html#the-paper",
    "href": "reviews/paper/2018-PTWM-NMT/index.html#the-paper",
    "title": "When and Why are Pre-trained Word Embeddings Useful for Neural Machine Translation?",
    "section": "The Paper",
    "text": "The Paper\n\n\n\npaper",
    "crumbs": [
      "Home",
      "Papers",
      "PTWM NMT"
    ]
  },
  {
    "objectID": "reviews/paper/2018-ELMo/index.html",
    "href": "reviews/paper/2018-ELMo/index.html",
    "title": "ELMo - Deep contextualized word representations",
    "section": "",
    "text": "Litrature review",
    "crumbs": [
      "Home",
      "Papers",
      "ELMO"
    ]
  },
  {
    "objectID": "reviews/paper/2018-ELMo/index.html#podcast",
    "href": "reviews/paper/2018-ELMo/index.html#podcast",
    "title": "ELMo - Deep contextualized word representations",
    "section": "Podcast",
    "text": "Podcast",
    "crumbs": [
      "Home",
      "Papers",
      "ELMO"
    ]
  },
  {
    "objectID": "reviews/paper/2018-ELMo/index.html#abstract",
    "href": "reviews/paper/2018-ELMo/index.html#abstract",
    "title": "ELMo - Deep contextualized word representations",
    "section": "Abstract",
    "text": "Abstract\n\nWe introduce a new type of deep contextualized word representation that models both\n\ncomplex characteristics of word use (e.g., syntax and semantics), and\nhow these uses vary across linguistic contexts (i.e., to model polysemy).\n\nOur word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
    "crumbs": [
      "Home",
      "Papers",
      "ELMO"
    ]
  },
  {
    "objectID": "reviews/paper/2018-ELMo/index.html#outline",
    "href": "reviews/paper/2018-ELMo/index.html#outline",
    "title": "ELMo - Deep contextualized word representations",
    "section": "Outline",
    "text": "Outline\n\nI. Introduction\n\nIdeally, word representations should model complex characteristics of word use, such as syntax and semantics, and how these uses vary across linguistic contexts, to model polysemy.\nThe authors introduces a new type of deep contextualized word representation (ELMo) that addresses these challenges, integrates easily into existing models, and improves state-of-the-art results.\n\nII. ELMo (Embeddings from Language Models)\n\nELMo representations are functions of the entire input sentence, not just individual tokens.\nThey are computed using a bidirectional LSTM (biLM) trained on a large text corpus with a language model objective.\nELMo representations are deep, in the sense they are a function of all internal layers of the biLM.\nA linear combination of the vectors stacked above each input word is learned for each end task.\nInternal states are combined to create rich word representations\nHigher-level LSTM states capture context-dependent aspects of word meaning (semantics),\nLower-level states model aspects of syntax.\nExposing all of these signals allows learned models to select the most useful types of semi-supervision for each end task.\n\nIII. Bidirectional Language Models (biLM)\n\nA forward language model predicts the next token given the history of previous tokens.\nA backward language model predicts the previous token given the future context.\nA biLM combines both forward and backward LMs, maximizing the log-likelihood of both directions.\nThe biLM uses tied parameters for token representations and the Softmax layer in both directions, but maintains separate parameters for LSTMs in each direction.\n\nIV. ELMo Specifics\n\nFor each token, an L-layer biLM computes a set of 2L+1 representations.\nELMo collapses all biLM layers into a single vector using a task-specific weighting of all layers.\nA scalar parameter scales the entire ELMo vector.\nLayer normalization can be applied to each biLM layer before weighting.\n\nV. Integrating ELMo into Supervised NLP Tasks\n\nThe weights of the pre-trained biLM are frozen, and then the ELMo vector is concatenated with the existing token representation before being passed into the task’s RNN.\nFor some tasks, ELMo is also included at the output of the task RNN.\nDropout is added to ELMo, and sometimes the ELMo weights are regularized.\n\nVI. Pre-trained biLM Architecture\n\nThe biLMs are similar to previous architectures but modified to support joint training of both directions and add a residual connection between LSTM layers.\nThe model uses 2 biLSTM layers with 4096 units and 512-dimension projections, with a residual connection.\nThe context-insensitive type representation uses character n-gram convolutional filters and highway layers.\nThe biLM provides three layers of representation for each input token.\n\nVII. Evaluation\n\nELMo was evaluated on six benchmark NLP tasks, including question answering, textual entailment, and sentiment analysis.\nAdding ELMo significantly improves the state-of-the-art in every case.\nFor tasks where direct comparisons are possible, ELMo outperforms CoVe.\nDeep representations outperform those derived from just the top layer of an LSTM.\n\nVIII. Task-Specific Results\n\nQuestion Answering (SQuAD): ELMo significantly improved the F1 score.\nTextual Entailment (SNLI): ELMo improved accuracy.\nSemantic Role Labeling (SRL): ELMo improved the F1 score.\nCoreference Resolution: ELMo improved the average F1 score.\nNamed Entity Extraction (NER): ELMo enhanced biLSTM-CRF achieved a new state-of-the-art F1 score.\nSentiment Analysis (SST-5): ELMo improved accuracy over the prior state-of-the-art.\n\nIX. Analysis\n\nUsing deep contextual representations improves performance compared to just using the top layer.\nELMo provides better overall performance than representations from a machine translation encoder like CoVe.\nSyntactic information is better represented at lower layers, while semantic information is better captured at higher layers.\nIncluding ELMo at both the input and output layers of the supervised model can improve performance for some tasks.\nELMo increases sample efficiency, requiring fewer training updates and less data to reach the same level of performance.\nThe contextual information captured by ELMo is more important than the sub-word information.\nPre-trained word vectors provide a marginal improvement when used with ELMo.\n\nX. Key Findings\n\nELMo efficiently encodes different types of syntactic and semantic information about words in context.\nUsing all layers of the biLM improves overall task performance.\nELMo provides a general approach for learning high-quality, deep, context-dependent representations.",
    "crumbs": [
      "Home",
      "Papers",
      "ELMO"
    ]
  },
  {
    "objectID": "reviews/paper/2018-ELMo/index.html#the-paper",
    "href": "reviews/paper/2018-ELMo/index.html#the-paper",
    "title": "ELMo - Deep contextualized word representations",
    "section": "The Paper",
    "text": "The Paper\n\n\n\npaper",
    "crumbs": [
      "Home",
      "Papers",
      "ELMO"
    ]
  },
  {
    "objectID": "reviews/paper/2023-MBR-all-the-way-down/index.html",
    "href": "reviews/paper/2023-MBR-all-the-way-down/index.html",
    "title": "It’s MBR All the Way Down: Modern Generation Techniques Through the Lens of Minimum Bayes Risk",
    "section": "",
    "text": "Litrature review",
    "crumbs": [
      "Home",
      "Papers",
      "MBR all the way down"
    ]
  },
  {
    "objectID": "reviews/paper/2023-MBR-all-the-way-down/index.html#abstract",
    "href": "reviews/paper/2023-MBR-all-the-way-down/index.html#abstract",
    "title": "It’s MBR All the Way Down: Modern Generation Techniques Through the Lens of Minimum Bayes Risk",
    "section": "Abstract",
    "text": "Abstract\nMinimum Bayes Risk (MBR) decoding is a method for choosing the outputs of a machine learning system based not on the output with the highest probability, but the output with the lowest risk (expected error) among multiple candidates. It is a simple but powerful method: for an additional cost at inference time, MBR provides reliable several-point improvements across metrics for a wide variety of tasks without any additional data or training. Despite this, MBR is not frequently applied in NLP works, and knowledge of the method itself is limited. We first provide an introduction to the method and the recent literature. We show that several recent methods that do not reference MBR can be written as special cases of MBR; this reformulation provides additional theoretical justification for the performance of these methods, explaining some results that were previously only empirical. We provide theoretical and empirical results about the effectiveness of various MBR variants and make concrete recommendations for the application of MBR in NLP models, including future directions in this area. – (Bertsch et al. 2023)",
    "crumbs": [
      "Home",
      "Papers",
      "MBR all the way down"
    ]
  },
  {
    "objectID": "reviews/paper/2023-MBR-all-the-way-down/index.html#outline",
    "href": "reviews/paper/2023-MBR-all-the-way-down/index.html#outline",
    "title": "It’s MBR All the Way Down: Modern Generation Techniques Through the Lens of Minimum Bayes Risk",
    "section": "Outline",
    "text": "Outline\n\nAbstract\n\nDescribes Minimum Bayes Risk (MBR) decoding.\nPresents MBR as a widely applicable method that often improves performance over beam search and single-sample decoding.\nShows that several recent generation methods can be framed as special cases of MBR, thereby providing theoretical grounding for their empirical success.\n\nIntroduction\n\nPresents Minimum Bayes Risk (MBR) decoding as a simple yet powerful decoding method.\nDiscusses how recent methods in natural language processing (NLP) unknowingly replicate aspects of MBR.\n\nFormalization\n\nStandard decoding\n\nDescribes the standard decoding methods for autoregressive models such as greedy decoding, sampling, and beam search.\n\nMinimum Bayes Risk decoding\n\nDefines the theoretical foundation of Minimum Bayes Risk (MBR) decoding, including the concept of risk based on expected error.\nExplains how risk computation is typically approximated using Monte Carlo methods due to the intractability of calculating the full expectation.\n\n\nTaxonomy of MBR\n\nNotes four key design considerations for implementing an MBR method: the hypothesis set, the evidence set, the gain/error function, and the evidence distribution.\nSampling a hypothesis set\n\nHighlights several works that show improvements in MBR by filtering the hypothesis set to contain only higher-quality candidate outputs.\n\nSampling an evidence set\n\nBriefly discusses various sampling strategies, with most work focusing on drawing unbiased samples from the model distribution.\n\nWhat metric do we want to maximize?\n\nExplores the impact of different gain (or error) functions, noting that using a specific metric as a gain function in MBR tends to lead to improved performance on that metric.\n\nWhat probability distribution should we use to estimate risk?\n\nBriefly discusses the choice of the distribution used to estimate risk in MBR, with most methods using the model’s score distribution over outputs, and some work using alternative distributions like a human or true distribution.\n\n\nMBR as a frame for other methods\n\nHighlights the framing of self-consistency, range voting, output ensembling, and density estimation as special cases of MBR.\nSelf-consistency as MBR\n\nShows how self-consistency, a method where the most frequent answer from multiple model generations is selected, can be formulated as MBR.\nExplains that the best performing sampling strategies for self-consistency are those closest to ancestral sampling due to its unbiased estimation properties.\n\nOutput Ensembling as MBR\n\nPresents output ensembling, where a set of models is used to generate outputs and a combined output is selected, as a form of MBR with a mixture distribution.\n\nMBR as Density Estimation\n\nEstablishes the connection between MBR and kernel density estimation, noting that both can be seen as mode-seeking methods.\n\nRange Voting as MBR\n\nShows that range voting, where candidates are assigned scores by voters, can be formulated as MBR by treating candidates as hypotheses and voters as evidence.\n\n\nDesign Decisions Impact MBR Performance\n\nExamines cases where the choices made in designing an MBR method significantly affect its performance.\nExperimental Details\n\nPresents the datasets and models used for evaluating MBR in abstractive summarization and machine translation tasks.\n\nThe MBR metric matters–but perhaps not as much as the hypothesis set\n\nDemonstrates that MBR using different gain functions (ROUGE-1, BEER, BERTScore) improves abstractive summarization performance.\nNotes that the choice of hypothesis set has a more significant impact than the choice of gain function.\n\nVarying the risk distribution: lessons from beam search don’t translate to MBR\n\nInvestigates the effects of correcting for length bias in the evidence distribution used for estimating risk in MBR.\nFinds that while length correction benefits beam search, it hurts MBR performance, possibly due to a high-variance estimator of risk.\n\n\nMBR applications in NLP\n\nPresents a historical overview of MBR applications in NLP, from its early use in statistical models to its recent resurgence in neural models.\nHistorical context\n\nTraces the roots of MBR to Bayesian decision theory and its use in parsing, speech recognition, and machine translation since the 1990s.\nExplains how early MBR applications were constrained by graph-based model structures, requiring complex algorithms for exact decoding.\n\nRecent usage\n\nDiscusses the revival of MBR in neural text generation tasks, with much of the recent work focusing on machine translation.\nNotes the decline in the explicit use of the term “MBR” in favor of newer terminologies like “self-consistency.”\n\n\nConclusion\n\nDiscusses the terminology drift in NLP that leads to the renaming of MBR as different methods.\nReemphasizes the importance of connecting modern techniques with their historical roots for a better understanding of why they work.\n\nAppendix A: More details on importance sampling for MBR\n\nProvides a detailed explanation of importance sampling and its application to MBR, specifically when estimating risk under a length-corrected distribution.\n\nAppendix B: Contextualizing this work within philosophy of science\n\nExplores the broader implications of the work within the context of meta-analysis of scientific research.\nDiscusses the phenomena of citational amnesia and terminology drift in scientific literature and their possible consequences.",
    "crumbs": [
      "Home",
      "Papers",
      "MBR all the way down"
    ]
  },
  {
    "objectID": "reviews/paper/2023-MBR-all-the-way-down/index.html#the-paper",
    "href": "reviews/paper/2023-MBR-all-the-way-down/index.html#the-paper",
    "title": "It’s MBR All the Way Down: Modern Generation Techniques Through the Lens of Minimum Bayes Risk",
    "section": "The Paper",
    "text": "The Paper\n\n\n\npaper",
    "crumbs": [
      "Home",
      "Papers",
      "MBR all the way down"
    ]
  },
  {
    "objectID": "reviews/paper/2019-generalized-data-augmentation-for-NMT/index.html",
    "href": "reviews/paper/2019-generalized-data-augmentation-for-NMT/index.html",
    "title": "Generalized Data Augmentation for Low-Resource Translation",
    "section": "",
    "text": "covered in CMU Multilingual NLP 2020 (8): Data Augmentation for Machine Translation"
  },
  {
    "objectID": "reviews/paper/2019-generalized-data-augmentation-for-NMT/index.html#abstract",
    "href": "reviews/paper/2019-generalized-data-augmentation-for-NMT/index.html#abstract",
    "title": "Generalized Data Augmentation for Low-Resource Translation",
    "section": "Abstract",
    "text": "Abstract\n\nAn effective method to improve neural machine translation with monolingual data is to augment the parallel training corpus with back-translations of target language sentences. This work broadens the understanding of back-translation and investigates a number of methods to generate synthetic source sentences. We find that in all but resource poor settings back-translations obtained via sampling or noised beam outputs are most effective. Our analysis shows that sampling or noisy synthetic data gives a much stronger training signal than data generated by beam or greedy search. We also compare how synthetic data compares to genuine bitext and study various domain effects. Finally, we scale to hundreds of millions of monolingual sentences and achieve a new state of the art of 35 BLEU on the WMT’14 English-German test set. –(Edunov et al. 2018)"
  },
  {
    "objectID": "reviews/paper/2019-generalized-data-augmentation-for-NMT/index.html#outline",
    "href": "reviews/paper/2019-generalized-data-augmentation-for-NMT/index.html#outline",
    "title": "Generalized Data Augmentation for Low-Resource Translation",
    "section": "Outline",
    "text": "Outline"
  },
  {
    "objectID": "reviews/paper/2019-generalized-data-augmentation-for-NMT/index.html#the-paper",
    "href": "reviews/paper/2019-generalized-data-augmentation-for-NMT/index.html#the-paper",
    "title": "Generalized Data Augmentation for Low-Resource Translation",
    "section": "The Paper",
    "text": "The Paper\n\n\n\npaper"
  },
  {
    "objectID": "reviews/paper/2025-xLSTM/index.html",
    "href": "reviews/paper/2025-xLSTM/index.html",
    "title": "xLSTM: Extended Long Short-Term Memory",
    "section": "",
    "text": "Literature Review\n\n\n\n\n\n\n\nVideo 1: Review by AI Bites\n\n\n\n\n\n\n\n\nVideo 2: Review by Yannic Kilcher\n\n\n\n\n\n\n\n\nVideo 3: LSTM: The Comeback Story? Talk with Sepp Hochreiter after the xLSTM paper",
    "crumbs": [
      "Home",
      "Papers",
      "xLSTM 2018-PTWM-NMT"
    ]
  },
  {
    "objectID": "reviews/paper/2025-xLSTM/index.html#introduction",
    "href": "reviews/paper/2025-xLSTM/index.html#introduction",
    "title": "xLSTM: Extended Long Short-Term Memory",
    "section": "Introduction",
    "text": "Introduction\n\n\n\n\n\n\nMotivation\n\n\n\nSo this is my first review of the RNN papers. I found that since the courses covering LSTM seemed to be over simplified and confusing, I should go back to the source. Since time is limited, it helps to review the papers in reverse chronological order. Each new papers explains the limitations that are generally not apparent when the paper is first published. The authors of the later papers also have the benefit of hindsight which they share in the obligatory literature review wherein they provide lots of context we cannot get by reading the original papers. Given all that there is also a benefit to reading the original paper - the real innovators while harder to understand are remarkable thinkers and often reveal motivations and deep insights that elude later interlocutors.\n\n\n\nPodcast & Other Reviews\nThis paper is a bit heavy on the technical side. But if we are still here, we may enjoy this podcast reviewing the paper. Though AI generated I found it quite useful to get oriented with the paper and understand that the experiments the authors run to validate this new architectures.\nWe also cover something I found absent from the Course on sequence models . The limitations of LSTMs as viewed by the authors when considered in the context of transformers and Mamba.\n\n\n\nAlthough this paper is recent there are a number of other people who cover it.\n\nIn Video 2 Yannic Kilcher covers the paper in his youtube channel. He does a great job of explaining the paper in a way that is easy to understand. Kilcher is all too human, he makes many mistakes, stresses opinions over facts. If we don’t mind all that he is a great resource for understanding the paper. I find he rubs me the wrong way but at least I find this motivating to do better in my own reviews!\n\n\n\n\n\n\n\nWhat can we expect from xLSTM?\n\n\n\n\n\nxLSTM has the potential to considerably impact other fields like Reinforcement Learning, Time Series Prediction, or the modeling of physical systems. – (Beck et al. 2024)\n\nIn his book Understanding Media (McLuhan 1988) Marshal McLuhan introduced his Tetrad. The Tetrad is a mental model for understanding how a technological innovation like the xLSTM might disrupt society. To work the model we ask the following four questions that can be applied to any new technology to understand its impact. The four questions in this instance are:\n\nWhat does the xLSTM enhance or amplify?\n\n\nThe xLSTM enhances the LSTM by introducing exponential gating and novel memory structures, boosting its capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models.\n\n\nWhat does the xLSTM make obsolete or replace?\n\n\nThe xLSTM makes the traditional LSTM obsolete by addressing its limitations, such as the inability to revise storage decisions, limited storage capacities, and lack of parallelizability.\n\n\nWhat does the xLSTM retrieve that was previously obsolesced?\n\n\nThe xLSTM retrieves the LSTM’s relevance in large-scale language modeling by scaling it to billions of parameters and integrating modern LLM techniques while mitigating known limitations. 4. What does the xLSTM reverse or flip into when pushed to its limits?\n\n\nThe xLSTM, when pushed to its limits, can compete with current Large Language Models based on Transformer technology, offering a competitive alternative for sequence modeling tasks.\n\nThe xLSTM paper by (Beck et al. 2024) is a great example of the tetrad in action. The authors take the LSTM, a technology that has stood the test of time, and enhance it with exponential gating and novel memory structures. They address the limitations of traditional LSTMs and push the technology to new limits, creating a new architecture that can compete with state-of-the-art Transformers and State Space Models. Recent work by (Chen et al. 2024) suggest that Transformers and The State Space Models are actually limited in their own ways.\n\n\n\n\n\n\n\n\nTetrad\n\n\n\n\nSupplementary Figure 1",
    "crumbs": [
      "Home",
      "Papers",
      "xLSTM 2018-PTWM-NMT"
    ]
  },
  {
    "objectID": "reviews/paper/2025-xLSTM/index.html#abstract",
    "href": "reviews/paper/2025-xLSTM/index.html#abstract",
    "title": "xLSTM: Extended Long Short-Term Memory",
    "section": "Abstract",
    "text": "Abstract\n\nIn the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining:\n\nsLSTM with a scalar memory, a scalar update, and new memory mixing,\n\nmLSTM that is fully parallelizable with a matrix memory and a covariance update rule.\n\n\nIntegrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling. — (Beck et al. 2024)\n\n\n\n\n\n\n\n\narchitecture\n\n\n\n\nFigure 1: The extended LSTM (xLSTM) family. From left to right:\n1. The original LSTM memory cell with constant error carousel and gating.\n2. New sLSTM and mLSTM memory cells that introduce exponential gating. sLSTM offers a new memory mixing technique. mLSTM is fully parallelizable with a novel matrix memory cell state and new covariance update rule.\n3. mLSTM and sLSTM in residual blocks yield xLSTM blocks.\n4. Stacked xLSTM blocks give an xLSTM\n\n\n\n\n\n\n\n\nblocks\n\n\n\n\nFigure 2: LSTM limitations.\n- Left: Nearest Neighbor Search problem in terms of mean squared error (MSE). Given a reference vector, a sequence is scanned sequentially for the most similar vector with the objective to return its attached value at sequence end. LSTM struggles to revise a stored value when a more similar vector is found. Our new xLSTM overcomes this limitation by exponential gating. - Right: Rare Token Prediction. The perplexity (PPL) of token prediction on Wikitext-103, in partitions of token frequency. LSTM performs worse on predicting rare tokens because of its limited storage capacities, whereas our new xLSTM solves this problem via a matrix memory.\n\n\n\n\n\n\n\n\nblocks\n\n\n\n\nFigure 3: xLSTM blocks.\nLeft: A residual sLSTM block with post up-projection (like Transformers): The input is fed into an sLSTM — with an optional convolution — followed by a gated MLP.\nRight: A residual mLSTM block with pre up-projection (like State Space models): mLSTM is wrapped inside two MLPs, via a convolution, a learnable skip connection, and an output gate that acts componentwise.\n\n\n\n\n\n\n\n\nblocks\n\n\n\n\nFigure 4: xLSTM blocks.\nLeft: A residual sLSTM block with post up-projection (like Transformers): The input is fed into an sLSTM — with an optional convolution — followed by a gated MLP.\nRight: A residual mLSTM block with pre up-projection (like State Space models): mLSTM is wrapped inside two MLPs, via a convolution, a learnable skip connection, and an output gate that acts componentwise.\n\n\n\n\n\n\n\n\nblocks\n\n\n\n\nFigure 5: xLSTM blocks.\nLeft: A residual sLSTM block with post up-projection (like Transformers): The input is fed into an sLSTM — with an optional convolution — followed by a gated MLP.\nRight: A residual mLSTM block with pre up-projection (like State Space models): mLSTM is wrapped inside two MLPs, via a convolution, a learnable skip connection, and an output gate that acts componentwise.",
    "crumbs": [
      "Home",
      "Papers",
      "xLSTM 2018-PTWM-NMT"
    ]
  },
  {
    "objectID": "reviews/paper/2025-xLSTM/index.html#sec-outline",
    "href": "reviews/paper/2025-xLSTM/index.html#sec-outline",
    "title": "xLSTM: Extended Long Short-Term Memory",
    "section": "Paper Outline",
    "text": "Paper Outline\n\nIntroduction\n\nDescribes the constant error carousel and gating mechanisms of Long Short-Term Memory (LSTM). &gt; “In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories…”\nDiscusses three main limitations of LSTMs:\n\nThe inability to revise storage decisions.\nLimited storage capacities.\nLack of parallelizability due to memory mixing.\n\nHighlights the emergence of Transformers in language modeling due to these limitations. &gt; Transformer Era: The rise of Transformers with parallelizable self-attention has largely overshadowed LSTMs due to their superior scalability.\n\nExtended Long Short-Term Memory\n\nIntroduces Extended Long Short-Term Memory (xLSTM), which incorporates exponential gating and novel memory structures.\nPresents two new LSTM variants:\n\nsLSTM with a scalar memory, a scalar update, and new memory mixing.\nmLSTM with a matrix memory and a covariance update rule, which is fully parallelizable.\n\nDescribes the integration of these LSTM variants into residual block modules, resulting in xLSTM blocks.\nPresents xLSTM architectures that residually stack xLSTM blocks.\n\nRelated Work:\n\nMentions various linear attention methods to overcome the quadratic complexity of Transformer attention.\nNotes the popularity of State Space Models (SSMs) for language modeling.\nHighlights the use of Recurrent Neural Networks (RNNs) as a replacement for Transformers.\nMentions the use of gating in recent RNN and SSM approaches.\nNotes the use of covariance update rules1 to enhance storage capacities in various models.\n\nExperiments\n\nPresents the experimental evaluation of xLSTM compared to existing methods with a focus on language modeling.\nDiscusses the effectiveness of xLSTM on synthetic tasks, including:\n\nFormal languages.\nMulti-Query Associative Recall.\nLong Range Arena tasks.\n\nPresents a method comparison and ablation study, where different models are trained on 15 billion tokens from the SlimPajama dataset.\nAssesses the scaling behavior of different methods based on validation perplexity.\nConducts a large-scale language modeling experiment:\n\nTraining different model sizes on 300B tokens from SlimPajama.\nEvaluating models on length extrapolation.\nAssessing models on validation perplexity and performance on downstream tasks.\nEvaluating models on 571 text domains of the PALOMA language benchmark dataset.\nAssessing the scaling behavior with increased training data.\n\n\nLimitations\n\nHighlights limitations of xLSTM, including:\n\nLack of parallelizability for sLSTM due to memory mixing.\nUnoptimized CUDA kernels for mLSTM.\nHigh computational complexity of mLSTM’s matrix memory.\nMemory overload for longer contexts due to the independence of mLSTM’s memory from sequence length.\n\n\nConclusion\n\nConcludes that xLSTM performs favorably in language modeling compared to Transformers and State Space Models.\nSuggests that larger xLSTM models will be competitive with current Large Language Models based on Transformer technology.\nNotes the potential impact of xLSTM on other deep learning fields such as reinforcement learning, time series prediction, and physical system modeling.\n\n\n1 explain covariance",
    "crumbs": [
      "Home",
      "Papers",
      "xLSTM 2018-PTWM-NMT"
    ]
  },
  {
    "objectID": "reviews/paper/2025-xLSTM/index.html#briefing-xlstm---extended-long-short-term-memory",
    "href": "reviews/paper/2025-xLSTM/index.html#briefing-xlstm---extended-long-short-term-memory",
    "title": "xLSTM: Extended Long Short-Term Memory",
    "section": "Briefing : xLSTM - Extended Long Short-Term Memory",
    "text": "Briefing : xLSTM - Extended Long Short-Term Memory\n\nIntroduction and Motivation:\nLSTM’s Legacy: The Long Short-Term Memory (LSTM), introduced in the 1990s, has been a cornerstone of deep learning, especially in sequence modeling and early Large Language Models (LLMs). LSTMs are known for their “constant error carousel” and gating mechanisms, designed to combat vanishing gradients in recurrent neural networks.\n\n“In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories…”\n\nTransformer Era: The rise of Transformers with parallelizable self-attention has largely overshadowed LSTMs due to their superior scalability.\n\n“However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale.” xLSTM Question: This research aims to explore the capabilities of LSTMs when scaled to billions of parameters, by integrating modern LLM techniques while addressing known limitations.\n\n\n“We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs?”\n\n\n\nKey Limitations of Traditional LSTMs:\nThe paper identifies three major limitations of traditional LSTMs:\n\nInability to Revise Storage Decisions: LSTMs struggle to update previously stored information when more relevant information arrives later in the sequence. The paper uses a “Nearest Neighbor Search” task to demonstrate this.\n\n\n“LSTM struggles to revise a stored value when a more similar vector is found…”\n\n\nLimited Storage Capacity: LSTMs store information in scalar cell states, which can lead to information compression and loss. This impacts performance on tasks requiring detailed memory, like predicting rare tokens.\n\n\n“LSTM performs worse on predicting rare tokens because of its limited storage capacities…”\n\n\nLack of Parallelizability: LSTMs process data sequentially due to the recurrent connections between hidden states, hindering parallel computation.\n\n\n“Lack of parallelizability due to memory mixing, i.e., the hidden-hidden connections between hidden states from one time step to the next, which enforce sequential processing.”\n\n\nxLSTM Innovations:\n\nThe paper introduces Extended Long Short-Term Memory (xLSTM), designed to address the above limitations, by modifying the original LSTM using two main approaches:\nExponential Gating:Implemented to allow for better revision of storage decisions. Normalization and stabilization techniques are used to avoid overflows in calculations due to the exponential function.\n\n“To empower LSTMs with the ability to revise storage decisions, we introduce exponential gates (red) together with normalization and stabilization.” Novel Memory Structures: Two new LSTM variants are created with these structures:\n\nsLSTM: A scalar memory LSTM with a scalar update and new memory mixing technique (within heads, not across). Features multiple memory cells and multiple heads, with memory mixing within each head.\n\n“The new sLSTM (see Section 2.2) with a scalar memory, a scalar update, and memory mixing…”\n\nmLSTM: A matrix memory LSTM, fully parallelizable, with a covariance update rule for storing key-value pairs. Does not have memory mixing between cells or heads.\n\n“the new mLSTM (see Section 2.3) with a matrix memory and a covariance (outer product) update rule, which is fully parallelizable.”\n\n\nsLSTM Details:\n\n\nExponential Gates: Introduces exponential activation functions for input and forget gates with normalization and a stabilizer state to prevent overflow.\nNormalizer State: A state used to normalize the hidden state output by accumulating a weighted sum of input and forget gates.\nMemory Mixing: sLSTM allows memory mixing between cells, and also introduces a new form of memory mixing through multiple heads, mixing between cells within a head, but not across heads.\n\n\n“The new sLSTM can have multiple heads with memory mixing within each head but not across heads. The introduction of heads for sLSTM together with exponential gating establishes a new way of memory mixing.”\n\n\nmLSTM Details:\n\nMatrix Memory: Replaces the scalar cell state with a matrix, increasing storage capacity.\nKey-Value Storage: The mLSTM memory is based around a key-value storage system inspired by Bidirectional Associative Memories (BAMs) using covariance updates, where keys and values are updated via an outer product.\n\n“At time t, we want to store a pair of vectors, the key k_t ∈ R^d and the value v_t ∈ R^d… The covariance update rule… for storing a key-value pair…”\n\nParallelization: mLSTM is fully parallelizable due to abandoning hidden-hidden recurrent connections and memory mixing. Computations can be performed efficiently on GPUs using standard matrix operations.\n\n“…the mLSTM… which is fully parallelizable.”\n\n\nxLSTM Architecture:\n\nxLSTM Blocks: The sLSTM and mLSTM variants are integrated into residual blocks.\nsLSTM blocks use post up-projection (like Transformers).\nmLSTM blocks use pre up-projection (like State Space Models).\n\n“Integrating these new LSTM variants into residual block modules results in xLSTM blocks…”\n\nResidual Stacking: xLSTM architectures are created by residually stacking these blocks, using pre-LayerNorm backbones similar to current LLMs.\n\n“An xLSTM architecture is constructed by residually stacking build-ing blocks…” Cover’s Theorem: Used to justify mapping information into a higher-dimensional space for better separation of histories or contexts.\n\n\n“We resort to Cover’s Theorem (Cover, 1965), which states that in a higher dimensional space non-linearly embedded patterns can more likely be linearly separated than in the original space.”\n\n\nPerformance and Scaling:\n\nLinear Computation & Constant Memory: Unlike Transformers, xLSTM networks have linear computation and constant memory complexity with respect to sequence length.\n\n“Contrary to Transformers, xLSTM networks have a linear computation and a constant memory complexity with respect to the sequence length.”\n\nSynthetic Tasks: xLSTM showed improved performance over regular LSTMs on “Nearest Neighbor Search” and “Rare Token Prediction” synthetic tasks.\nSlimPajama Experiments: The xLSTM models were trained on 15B and 300B tokens from the SlimPajama dataset for language modelling evaluation.\nCompetitive Performance: xLSTMs demonstrate competitive performance against state-of-the-art Transformers and State Space Models on language modelling benchmarks, both in terms of next token prediction perplexity and downstream tasks.\n\n“Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.”\n\nAblation studies show importance of gating techniques.\n\nMemory & Speed:\n\nsLSTM: Not parallelizable due to memory mixing, although the authors have created a fast CUDA implementation (less than twice as slow as mLSTM).\n\n“While mLSTM is parallelizable analog to FlashAttention… sLSTM is not parallelizable due to the memory mixing…”\n\nmLSTM: Parallelizable, but currently slower than FlashAttention or Mamba due to non-optimized CUDA kernels. Matrix memory is computationally complex but does not require parameters and can be parallelized.\n\n“The memory of mLSTM does not require parameters but is computationally expensive through its d×d matrix memory and d× d update… the computations can be done in parallel on GPUs, therefore these computations have only a minor effect on the wall clock time.”\n\n\nLimitations:\n\nsLSTM Parallelization: sLSTM’s memory mixing is non-parallelizable.\n\n“(i) In contrast to mLSTM, memory mixing of the sLSTM prohibits parallelizable operations, and therefore does not allow a fast parallel implementation.”\n\nmLSTM CUDA Optimization: The mLSTM CUDA kernels are not fully optimized.\n\n“(ii) The CUDA kernels for mLSTM are not optimized, and therefore the current implementation is about 4 times slower than FlashAttention or the scan used in Mamba.”\n\nmLSTM Matrix Memory: High computational complexity for mLSTM due to matrix memory operations.\nForget Gate Initialization: Careful initialization of the forget gates is needed.\nLong Context Memory: The matrix memory is independent of sequence length, and might overload memory for long context sizes.\n\n“(v) Since the matrix memory is independent of the sequence length, increasing the sequence length might overload the memory for longer context sizes.”\n\nHyperparameter Optimization: xLSTM is not yet fully optimized due to computational load, indicating further performance gains are possible.\n\n“(vi) Due to the expensive computational load for large language experiments, we did neither fully optimize the architecture nor the hyperparameters, especially for larger xLSTM architectures.”\n\n\nRelated Work:\n\nThe paper highlights connections of its ideas with the following areas:\nGating: Many recent models such as HGRN, GLA, Gated State Space models, Mamba, and others also utilize gating mechanisms. Covariance Update Rule: The covariance update rule used in mLSTM is related to other methods such as Fast Weight Programmers, Retention, and Linear Transformers.\n\nConclusion:\n\nThe xLSTM presents a compelling alternative to Transformers in sequence modeling, especially in contexts where memory efficiency and longer sequences are required. By combining insights from the original LSTM with exponential gating and novel memory structures, the paper shows that LSTMs can be competitive at scale. While limitations exist in terms of parallelization and hyperparameter tuning, the research shows promising potential for further development. The authors posit further research and improvements could allow xLSTM to reach its full potential.",
    "crumbs": [
      "Home",
      "Papers",
      "xLSTM 2018-PTWM-NMT"
    ]
  },
  {
    "objectID": "reviews/paper/2025-xLSTM/index.html#my-thoughts",
    "href": "reviews/paper/2025-xLSTM/index.html#my-thoughts",
    "title": "xLSTM: Extended Long Short-Term Memory",
    "section": "My Thoughts",
    "text": "My Thoughts\n\n\n\n\n\n\n\nVideo 4: Review of the sigmoid function\n\n\n\n\n\n\n\n\nResearch questions\n\n\n\n\nHow does the constant error carousel mitigate the vanishing gradient problem in the LSTM?\n\nThe constant error carousel is a way to keep the error constant over time. This helps to prevent the vanishing gradient problem in the LSTM.\n\nHow are the gates in the original LSTM binary?\n\nSigmoid, saturation and a threshold at 0.5\n\nWhat is the long term memory in the LSTM?\n\nthe cell state c_{t-1}\n\nWhat is the short term memory in the LSTM?\n\nthe hidden state h_{t-1}\n\nHow far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs\n\n\n\n\n\n\n\n\n\n\nSupplementary Figure 2: limits of the sigmoid function\n\n\n\n\n\n\n\n\nbias\n\n\n\n\nSupplementary Figure 3: The inductive bias of the sigmoid decision function.\n\n\n\n\nBinary nature of non exponential Gating Mechanisms\nIf we try to understand why are the gating mechanisms in the original LSTM is described here as binary?\nIt helps to the properties of the sigmoid functions that are explained in Video 4.\n\nSupplementary Figure 1 shows that The sigmoid function has a domain of \\mathbb{R} and a range of (0,1). This means that the sigmoid function can only output values between 0 and 1.\nIt has a very sharp gradient around 0 if the bias is set to 0. This means that the sigmoid function acts as a binary function around 0.\nThe Sigmoid function tends to saturate. Now sigmoid function around 0 isn’t quite quite binary but outside of (-6,+6) it becomes saturated and acts very much as a binary function.\nIf we use it as a loss function or a decision function, the sigmoid function is used with a threshold of 0.5. This means that if the output of the sigmoid function is greater than 0.5, the decision is 1, and if it is less than 0.5, the decision is 0. That is how the sigmoid function can become binary.\nNote that we can use thresholding during inference to make the decision binary and we can keep them as continuous values during training so we can have a smooth gradient for back-propagation.\nEven without thresholding the sigmoid function can become binary this is due to the saturation of the sigmoid function. I recall that Hinton in his Coursera course on Neural Networks and Deep Learning mentioned that activation function that become saturated are problematic. I can say that at last I think I understand what he meant:\nI see that that sigmoid functions can fall off the manifold of the data and this can be a problem. This is why the ReLU function is used in the Gated Recurrent Unit (GRU) and the Transformer. Falling off the manifold of the data means that the sigmoid function becomes saturated (above 6 and below -6). At this point the back-propagation steps are no longer able to change the weight or bias since the gradient is effectively so learning stops.\n\nThis becomes a is a problem in two senses. The sigmoid unit is likely to be stuck in a setting that only representative of part of the training data. The rest of the network which is not stuck now has to learn to compensate for this.\nThis issues may well be the root cause LSTM limitation number 1 - the inability to revise storage decisions. It seems that the authors decided to use the exponential gating function to address this issues.\nThis is why the gating mechanisms in the original LSTM are binary. The sigmoid function is used to control the flow of information in the LSTM by deciding which information to keep and which to discard. The sigmoid function is used to decide how much of the new input information should be added to the cell state and how much of the old cell state should be retained. This binary nature of the sigmoid function is what allows the LSTM to control the flow of information and decide what to remember and what to forget.",
    "crumbs": [
      "Home",
      "Papers",
      "xLSTM 2018-PTWM-NMT"
    ]
  },
  {
    "objectID": "reviews/paper/2025-xLSTM/index.html#the-paper",
    "href": "reviews/paper/2025-xLSTM/index.html#the-paper",
    "title": "xLSTM: Extended Long Short-Term Memory",
    "section": "The Paper",
    "text": "The Paper\n\n\n\npaper",
    "crumbs": [
      "Home",
      "Papers",
      "xLSTM 2018-PTWM-NMT"
    ]
  },
  {
    "objectID": "reviews/paper/2025-xLSTM/index.html#references",
    "href": "reviews/paper/2025-xLSTM/index.html#references",
    "title": "xLSTM: Extended Long Short-Term Memory",
    "section": "References",
    "text": "References\n\nThe paper on open review has some additional insights from the authors\nXLSTM — Extended Long Short-Term Memory Networks By Shrinivasan Sankar — May 20, 2024",
    "crumbs": [
      "Home",
      "Papers",
      "xLSTM 2018-PTWM-NMT"
    ]
  },
  {
    "objectID": "notes/cs11-737-w19-syntax-and-parsing/index.html#introduction",
    "href": "notes/cs11-737-w19-syntax-and-parsing/index.html#introduction",
    "title": "Syntax and Parsing",
    "section": "Introduction",
    "text": "Introduction\n\nThis time we’re going to be talking about syntax in general but dependency parsing in particular. and when we talk about linguistic structure and syntax there’s two types of linguistic structure that we talk about.\n\n\nThe first one is dependency structure which focuses on the relations between words and it looks a little bit like this: For example we have the word saw and the word saw as the root of the sentence and this is connected to other words like i and with and girl\n\n\nWe also have phrase structure which is focuses on the kind of structure of the sentence as opposed to the relationships between words Both of these are common expressions of syntax or common ways to represent syntax. In particular phrase structure was widely used in English and you know kind of developed by chomsky and other very influential linguists from 1950s 1960s until now However recently there’s been a big move towards dependency structure and the reason why is because they’re relatively straightforward to express. And in particularly relatively straightforward to express across a wide variety of languages. and so for example we can do things like say saw is the subject of the sentence so or saw is the root of the sentence and then it has a subject it has a direct object and it has a prepositional phrase and these kinds of things are relatively you know constant across languages maybe not prepositional phrases but you know a phrase like this and it’s particularly good for multilinguality because in some free word order languages it’s also possible to have basically words intervene into a phrase which makes it very difficult to say this is like a particular phrase and what i mean by this is if you have a dependency tree and words cross it’s very hard to come up with an example of this in English. I don’t know if anybody knows one of the top of your head like ellen okay it’s like i went i saw a movie yesterday that was good pt so this is a relatively natural English sentence but actually yesterday yesterday is a child of saw and movie that is a child of movie so you can see the dependency is crossing here basically the only place where we get this in in English is is with adverbs adverbs can be like are basically the only thing with really free word order in English but they break this kind of phrase structure representation because you can’t say that any there’s a consistent phrase here so basically that’s an issue and there are other languages where these are like extremely common where the word order is free for all different kinds of phrases so because of this a lot of syntactic analysis is kind of moving in the direction of using dependencies instead of phrase structure"
  },
  {
    "objectID": "notes/cs11-737-w19-syntax-and-parsing/index.html#universal-dependencies",
    "href": "notes/cs11-737-w19-syntax-and-parsing/index.html#universal-dependencies",
    "title": "Syntax and Parsing",
    "section": "Universal Dependencies",
    "text": "Universal Dependencies\nThere is an amazing effort called the universal dependencies treebank that gives you a standard format for parse trees in many languages this started out of stanford dependencies and also universal part of speech tags and universal dependencies created by google and the basic motivation for this was they wanted to be able to build like one parser and use it for all of the languages instead of have having 400 parsers for 400 languages so if you come up with something in a unified format that you can that you can parse in process then that makes deploying to many different languages much easier the disadvantage of doing this is that in order to make something universal you have to give up on some language specific details so there are details that don’t easily fit within this universal dependencies format but in most cases they cover you know a lot of the major things that you see in different languages"
  },
  {
    "objectID": "notes/cs11-737-w19-syntax-and-parsing/index.html#syntax-vs-semantic",
    "href": "notes/cs11-737-w19-syntax-and-parsing/index.html#syntax-vs-semantic",
    "title": "Syntax and Parsing",
    "section": "Syntax vs Semantic",
    "text": "Syntax vs Semantic\nAn other thing that you should know about if you’re considering dependencies or kind of like dependency structure is that there’s actually different types of dependencies – Syntactic and Semantic. And they look very different. Syntactic dependencies basically are trying to as closely as possible reflect the phrase structure of the sentence and how sentences are put together So for example in a prepositional phrase the head of the prepositional phrase will be a preposition. Whereas for semantic dependencies the head of a prepositional phrase will be like the main noun or verb in that prepositional phrase.\nThe idea being that syntactic dependencies are good for understanding the structure of the language whereas semantic dependencies might be good, for example, for Question Answering because if you ask a question if you have a sentence like the one i had here “I saw a movie yesterday that was good” or “I saw a movie yesterday at the movie theater” and you want to say where did the person watch the movie and the syntactic dependency movie would be connected to at so you would have to like jump down the tree several nodes wherein semantic dependencies movie would be connected to movie theater. It would have an appropriate label so you could just like look up the edge and connect the two together to answer that question in a single hop instead of multiple hops. This is really important! Aditi might talk about things related to this later and you need to choose which one you want based on which application you’re interested in"
  },
  {
    "objectID": "notes/cs11-737-w19-syntax-and-parsing/index.html#semantics",
    "href": "notes/cs11-737-w19-syntax-and-parsing/index.html#semantics",
    "title": "Syntax and Parsing",
    "section": "Semantics",
    "text": "Semantics\nOne of the very interesting things about yeah sorry god the last play the the semantics how do we club all the nouns together like we need some external knowledge to understand the semantic of the norm do you mean like a noun phrase or no no just to know just a name of something so like it’s a concept right like so that’s a good question so how how do we understand the semantics of a noun or something so even semantic dependencies this is only talking about the relationship between words in the sentence it’s now telling you about the underlying semantics of the nouns so if you wanted to know about like underlying semantics of a noun you would have to have something else like a semantic like knowledge base or something like this the typical version of this is something like wordnet which basically says well let’s say we have chevrolet chevrolet is a type of car which is a type of vehicle which is a type of you know man-made object which is a type of object or something like this and i think some people might have used wordnet it’s now like lesson style than it was before but it’s it’s basically telling you this information and there’s actually since we’re in a multilingual class i can tell you about something like babelnet which is a multilingual like a multilingual version of wordnet so if i put in [Music] oh i’m sorry i was searching in English no wonder so if i search for this in japanese it can tell you that this word is a concept for like automobile and it’s it has is a has part part of relations and all of these are linked across languages so for example it’s in i can’t find the language link but here yeah here yeah so you can see that it links to car automobile other things like this so it’s all linked together so if you want to specifically talk about semantics of words on their own then you can either use things like this or you can use word embeddings also which give a concept so can’t you get a semantic graph so what word net tells you is it tells you about the semantics of words it doesn’t tell you about the semantics of like words relating to each other so it doesn’t tell you for example that challenge is the object of love it doesn’t tell you that a challenge is being loved which is what this this semantic dependency tells you and then if you go even a little bit farther there’s something called predicate argument analysis or frame semantic parsing or something that gives you an even more abstract version but semantic dependencies are kind of like something related to that so regular regular universal dependencies are semantic sud is syntactic so you should know the difference between them so there’s a lot of cross-lingual differences in structure so we’ve talked about this a lot before like word ordering so we have svo we have hindi which is a verb final and arabic which is verb initial and i got this actually from the pud tree bank which is the parallel universal dependencies treebank it has a whole bunch of translated sentences in different languages along with their dependency trees and the interesting thing is these sentences all i guess mean the same thing hindi speakers can confirm that’s actually the case yes nobody’s saying no so i’ll assume i’ll assume yes but you can see the structure is very different so we have like is and then in English we have we have is is in the middle of the sentence with the noun first and the verb second and then what i can what i can see is we in handy we have an auxiliary verb but then we have a verb here and then we have the object and i guess an oblique indirect object and stuff like this but these all come on the left side of the verb so we can tell that hindi is verb final and then for arabic we can tell that arabic is verb initial and we have a noun here at position sorry a subject and then an oblique here so you can tell the difference in the structure even though i i can’t even read the script in arabic and hindi i can still tell that just by looking at the dependency"
  },
  {
    "objectID": "notes/cs11-737-w19-syntax-and-parsing/index.html#dependencies",
    "href": "notes/cs11-737-w19-syntax-and-parsing/index.html#dependencies",
    "title": "Syntax and Parsing",
    "section": "Dependencies",
    "text": "Dependencies\nso what can we do with dependencies so i actually previously they were used for feature engineering and systems and they’re still useful in some cases but now our default is just to you know pull up m mbert or xlmr and fine tune it and get reasonable accuracy on a lot of tasks that we care about and in fact you know dependencies are probably captured somewhat to some extent implicitly in these models so why care about syntax i would argue that these are more useful now in human-facing applications and a while ago june 3rd i think last year i actually asked a question on twitter what are convicts are i guess two years ago i asked a question on twitter what are convincing use cases in 2020 for dependencies and i got 39 answers and just to give some examples they still can be useful for incorporating inductive bias into neural models so biasing self-attention to follow syntax or other things like this i think in it still is useful to encourage models to be right for kind of like the right reasons instead of the wrong reasons because this improves model robustness especially out of domain and other things like this and syntactic conductive biases can provide you a way to do this another thing is understanding language structure and this is an example from the aditi’s work which he’s going to present in much more detail in a few minutes so i’ll let her talk to that another very interesting example is searching over parsed corpora so like i talked about before like let’s say we want to find examples with that are talking about x was the founder of y so we want to find it lots of examples of founders of something or other you could try to do this with a regular expression but it’d be pretty tough to come up with a regular expression that gives you this you know with high precision high recall but if instead you find where founder is the verb the subject and it has a subject and an object here then you can just search all examples of this and it actually highlights the appropriate ones here so this is from the spike system created by ai2 and another thing is analysis of other linguistic phenomena or like you know if you want to identify for example when this is coincidentally i actually one one of these is from Emma Strubell you know assistant professor here another one is from Martin Sap who will be an assistant professor here I actually made the slides before I knew he was going to be a professor here but but anyway this is examining whether film scripts demonstrate that people have power or agency and analyzing it along the gender of the participants in the film so whether male or female characters are you know like demonstrating more power agency and film scripts and this is used to answer like social and sociologically interesting questions for example and this is made a lot easier by analyzing the syntax because then you can do things like say who did what to whom more easily"
  },
  {
    "objectID": "notes/cs11-737-w19-syntax-and-parsing/index.html#parsing",
    "href": "notes/cs11-737-w19-syntax-and-parsing/index.html#parsing",
    "title": "Syntax and Parsing",
    "section": "Parsing",
    "text": "Parsing\nThis is kind of a motivation for like what our dependency parses why would you want to be using dependency parts or syntax in general so to talk a little bit more about dependency parsing how would you get these especially in a multilingual fashion parsing is predicting linguistic structure from an input sentence and there’s a couple methods that we use to do this the first one is transition based models and basically the way they work is they step through some steps one by one until we we can turn those steps into a tree another one is graph based models and basically they calculate the probability for each edge in a dependency parse and perform some sort of dynamic programming over them and if you’re familiar with like part of speech tagging from the first assignment transition based models are kind of like a history based model for part of speech tagging and what this would look like is if you if you had like an encoder decoder model where the next tag was always conditioned on the previous tag for graph based models we didn’t really cover crfs here but if you’re familiar with CRFs the graph based models are a little bit like these they calculate some scores and then they have a dynamic program to get the best output so just to give a very brief flavor of what these look like"
  },
  {
    "objectID": "notes/cs11-737-w19-syntax-and-parsing/index.html#shift-reduce",
    "href": "notes/cs11-737-w19-syntax-and-parsing/index.html#shift-reduce",
    "title": "Syntax and Parsing",
    "section": "Shift Reduce",
    "text": "Shift Reduce\nShift reduce parsing basically it processes words one by one left to right and it maintains two data structures one is a queue of unprocessed words another is a stack of partially processed words and at each point we choose to either shift moving one word from the queue to the stack reduce left where the top word on the stack becomes the head of the second word or reduce right where the second word on the stack is becomes ahead of the top word and we learn how to choose each of these actions with a classifier so just to give an example we want to parse the sentence i saw a girl so what we do is we first shift and move something sorry this says buffer it’s the same thing as q there’s multiple ways to say this but just think buffer equals q we move one thing from the the queue to the stack we move another word from the queue to the stack and then sorry that’s another typo this should be reduced left and so we reduce left and we get a left arc here then we shift again and then we shift again then we reduce left we reduce right and then we reduce right and then we have a final basically parse tree here so basically what you can see is at each point we choose an action and based on the action we add we either move something from the queue to the stack or we add an arc between the top two things on the stack so you probably won’t need to implement this yourself there are plenty of good dependency parsers out there but just to get an idea of what the algorithm looks like in case you’re interested in doing that"
  },
  {
    "objectID": "notes/cs11-737-w19-syntax-and-parsing/index.html#classification",
    "href": "notes/cs11-737-w19-syntax-and-parsing/index.html#classification",
    "title": "Syntax and Parsing",
    "section": "Classification",
    "text": "Classification\nso the way we do classification for any variety of shift-reduced parsing is basically we take in the stack and the buffer and we need to choose between one of the actions shift left and right so this is a regular classification problem three-way classification problem and we can encode the stack configuration with any variety of recurrent neural network or auto-regressive neural network so one example of this is where we encode basically the stack the previous history of actions and the buffer and feed them into the into the model an even simpler way of doing this is you just encode the words in the input sentence and then have a decoder that generates the actions for you so you could even just throw that into like a regular transformer model and train it as well it would just be you know input is the entire words in the the source sentence and then the output is the sequence of actions so that’s a basic like very quick overview of shift-reduce pricing are there any questions or yeah so you said that we use the dependency passing as an auxiliary fast increase of westminster’s products and inputs yeah so now we have some methods about like adding adapters into xlr to make them more robust for a given language so with adapters do we still need these to increase robustness or if you have a car first and find an adapter and yeah so that’s a really good question if we have something like adapters like multilingual adapters do we still need something like an auxiliary test like dependency parsing and i i would say it’s quite likely that those two things are kind of orthogonal in that adapters are allowing you to more effectively adapt to individual tasks whereas the supervision that you would get from a dependency parsing objective would essentially enforce the model to more strictly follow the syntax of the sentence as linguists kind of describe it so i think that both probably would stack together but i i don’t have empirical results or i i don’t know immediately off the top of my head about a paper that demonstrates maddox yeah so i know medics but i don’t remember if they used like dependency parsing it’s an auxiliary task but they show performance improvement on very low resource languages that xlr is failing and then they used access to improve the performance but i’m not sure whether it was one of the things and so basically the the comment to repeat it for other people who couldn’t hear the so there’s a paper called man x that basically does adapters we talked a little bit about adapters in class i think but basically they demonstrated that it improves on very low resource languages but i it’s not perfect still after using medx right and i think this and that could be combined together probably to improve a bit but as i said i think that’s not the main good use case of dependencies right now i think the better use cases of the dependencies are they give very like intuitive human facing interfaces if you want to do like analysis of corpora or extract detected phenomena on a more holistic level or other things like that so it was actually inspired by one of your tweets you had put a tweet i think a year back that when a person releases the model for 100 languages doesn’t mean it works languages so yeah that changed my perspective that okay it’s not perfect then i kind of searched for these people yeah so i to repeat in case people in the back couldn’t hear i i said something on twitter about a year ago where it’s like when somebody releases a model for 100 languages that doesn’t necessarily mean it works on 100 languages it means it does something on 100 languages probably and it’s probably better than nothing but it’s not perfect for sure so and i’m sure you guys all noticed this as you were implementing your various assignments as well cool."
  },
  {
    "objectID": "notes/cs11-737-w19-syntax-and-parsing/index.html#graph-based",
    "href": "notes/cs11-737-w19-syntax-and-parsing/index.html#graph-based",
    "title": "Syntax and Parsing",
    "section": "Graph based",
    "text": "Graph based\nThe other alternative is a graph based parsing and graph based parsing basically what it looks like is we express the sentence as a fully connected directed graph which means that we have all pairs of words as potential candidates for a dependency edge existing between them another way you can think of it is a matrix of scores for each for each edge where the rows are the head and the the columns are the children and the diagonal obviously something can’t be ahead of itself so it would it’s irrelevant but you predict all of the other things there and then after you do that you score each edge independently so you basically calculate the values of that that score matrix and then you have some algorithm that allows you to find the maximal spanning tree which is basically the highest scoring set of edges that form a tree in the form a tree with a root in it"
  },
  {
    "objectID": "notes/cs11-737-w19-syntax-and-parsing/index.html#graphbased-vs-transitionbased",
    "href": "notes/cs11-737-w19-syntax-and-parsing/index.html#graphbased-vs-transitionbased",
    "title": "Syntax and Parsing",
    "section": "Graphbased vs Transitionbased",
    "text": "Graphbased vs Transitionbased\nand i have a comparison of graph based versus transition based one ex one advantage of transition based parsing is that it allows you to condition on infinite context basically so it allows you to condition on all of your previous actions so theoretically it has as much expressiveness as you want just like a regular encoder decoder model can do you know can condition on all your previous sections however for transition based parsing greedy search algorithms can cause short-term mistakes to propagate into damaging your long-term performance so you know if you accidentally connect an edge too early there’s no way to recover from it on the other hand graph based parsing you can find the exact best global solution via dynamic programming however you have to make some local independence assumptions so you can’t like necessarily easily condition you know the choice of one edge on whether you chose one edge or at another time so for the dynamic programming"
  },
  {
    "objectID": "notes/cs11-737-w19-syntax-and-parsing/index.html#dynamic-programming",
    "href": "notes/cs11-737-w19-syntax-and-parsing/index.html#dynamic-programming",
    "title": "Syntax and Parsing",
    "section": "Dynamic Programming",
    "text": "Dynamic Programming\nalgorithm i’m not going to go into a lot of detail here because as i said like people are probably not going to be implementing this on your own you’re probably going to be using a parser but basically we have a graph and want to find it spanning trees so what we do is we greedily select the best incoming edge to each node and we subtract its score from all other incoming edges because we want a tree and not anything with any cycles in it what we do is we contract the cycles together into a single node and resolve the cycles within that node and basically we recursively call the algorithm on the graph with the contracted node and then we can expand the contracted node deleting at an edge from the root node there so if you’re more interested in exactly how this works you can look at jaravsky and martin’s textbook on dependency parsing or something like this but basically that’s the general idea for these models basically what we do is we extract features over the whole sequence so we feed in all of the you know all of the words in the input into a feature extractor in this particular in this particular example it says lstm but in reality now it’s xlmr or bert or you know whatever"
  },
  {
    "objectID": "notes/cs11-737-w19-syntax-and-parsing/index.html#definition-of-required-agreement",
    "href": "notes/cs11-737-w19-syntax-and-parsing/index.html#definition-of-required-agreement",
    "title": "Syntax and Parsing",
    "section": "Definition of required agreement",
    "text": "Definition of required agreement\nmorphological agreement so agreement is a quite complex process wherein basically words in a sentence often change their forms or morphology based on some other words in the same sentence based on some category like gender number and person so i’ll give a quickly an example from number agreement in spanish so here you can see that girl is in singular and the word for verb has also been singular so now if i change the word for girl to be in a plural form then the words form also changes to the plural form so basically any change in the subjects number has to bring about a change in the verbs number so we call this as subject verb required agreement now if you look at the object and the word they are also both in the singular form in the first sentence and the second sentence when the form of the object dog has become plural the form of the word still remains in a singular form so essentially any change in the object’s number is not bringing a change in the verbs number so this is it’s not required agreement so any sort of agreement that we may observe between object and work is purely by chance so we call this as object work chance agreement so to basically understand what are the rules which govern subject agreement and orders or how to require agreement you basically formulate it into a classification task"
  },
  {
    "objectID": "notes/cs11-737-w19-syntax-and-parsing/index.html#prediction-task",
    "href": "notes/cs11-737-w19-syntax-and-parsing/index.html#prediction-task",
    "title": "Syntax and Parsing",
    "section": "Prediction task",
    "text": "Prediction task\nso the task is here of predicting required agreement versus chance agreement and how do we extract these rules automatically just from the raw text so here i have an example of greek this is a greek sentence and we first automatically perform some syntactic analysis which basically gives us what is the part of speech of each word what is this morphological features what are the dependency links between them now from this syntactic analysis we basically create our training data for this prediction task so here’s an example so on this box here you basically have a dependency link between the determiner and the proper noun now the gender of the determinant and the proper noun are both matching they’re both in feminine gender so basically we can create a training data point saying that proper noun and determiner when they are in a relation then the gender is matching so the agreement value becomes yes now another dependency link here is between the noun and the proper noun now here the gender values are not matching so our data point here becomes that any relation between noun and proper noun in this example the agreement value is known so essentially we are basically creating a binary classification task from this example and we create this data set for all the sentences we then basically learn a model on top of this training data from which we extract rules where the rules are telling us which of these rules are actually leading to a required agreement and which are leading to a chance agreement so essentially this is an example of the model so again going back to the previous slide where i mentioned about human centric applications so we want to understand and extract these rules which are understandable to humans so we want to use a model which is more interpretable so here i have used the model of decision trees the decision trees can exactly tell you what are the features which led to one decision so once we have applied a decision tree style model on our training data it gives us a leaf node the leaf here is inducing a distribution of agreement over these examples the leaf 3 here is showing us that there are 58 000 examples where the gender values were matching and 778 but they were not matching but how do we know whether this distribution is leading to a required agreement or a chance agreement and to automatically extract this label we basically apply a statistical threshold i won’t go into the much details of it but essentially we apply a significance test which tells us whether the observed agreement distribution is significant or not and this can tell us whether the leaf is truly capturing a required agreement or not and this is like an example of the label decision tree where for spanish gender agreement this is the leaf or this is the tree after the leaf labeling stage and the leaf three here has been marked as required agreement so basically from this leaf three we can extract some discrete rules which says okay if determinant and noun are in the following relation then they need to agree on gender so basically from the raw text we started training data over which we train an interpretable model from which we then extracted some discrete and very simple rules and this is just a very basic pipeline and now we are trying to extract this"
  },
  {
    "objectID": "notes/cs11-737-w19-syntax-and-parsing/index.html#linguistic-questions",
    "href": "notes/cs11-737-w19-syntax-and-parsing/index.html#linguistic-questions",
    "title": "Syntax and Parsing",
    "section": "Linguistic questions",
    "text": "Linguistic questions\nsort of or apply this pipeline for potentially any linguistic question so we saw this for agreement where our linguistic question was when do syntactic heads show agreement with their dependence on gender number and so on another interesting question was for case marking in case marking we’re interested to know when does a particular class of words like nouns take nominative case over the other so for example in this sentence anna has food anna is in the nominated case but if it becomes anna’s food then ani is a generative base so we want to basically if you are learning this language you need to know that when to add an apostrophe is when you are basically showing a possession another kind of interesting linguistic phenomena is of word order like you need to know how to arrange the words appropriately and even in English typically when we say about word order people just say it’s one word that English follows svo but it’s even within English there are multiple word orders so for instance if if i’m saying English is sorry anna is eating an apple then your word order is simply subject verb and object but if i’m asking a question what is anna eating then these order changes it now becomes object subject and work so if i’m learning English i need to know that when i’m asking a question what is the typical order but if i am just using or saying a decorative sentence what is the what order so essentially for any linguistic question we want to formulate it as a prediction task and learn and extract its rules and this is the final general framework which we have been working on from the raw text you basically extract some"
  },
  {
    "objectID": "notes/cs11-737-w19-syntax-and-parsing/index.html#feature-extractor",
    "href": "notes/cs11-737-w19-syntax-and-parsing/index.html#feature-extractor",
    "title": "Syntax and Parsing",
    "section": "Feature Extractor",
    "text": "Feature Extractor\nYour favorite feature extractor is the classifier that you use to basically apply scores to each of the nodes of the input is a by affine classifier this looks a little bit scary if you look at the equations but it’s actually super simple what you do is you learn specific representations through an mllp for each head word in each dependent word so basically you feed the representations that you get from burke or whatever else into an mlp where you have a separate mlp when you’re considering the word as a head and when you’re considering the word as a child or a dependent and then you calculate the score of each arc where sorry i thought i had an animation here but basically the first part of this here is calculating how likely a child word is to connect to a head word so it’s calculating basically the congruence between a child word and a head word and of course you know this is optimized end to end but it will be considering things like how close together are the words how likely is a child to be a child of a head like so nouns tend to be children of verbs et cetera et cetera determines children of nouns and then in addition it also calculates the score saying how likely is this word to be a head word in the first place so for example determiners are almost never head words in in semantic dependencies and so they’ll get a very low score according to this thing down here whereas verbs are very often head words so we’ll get a high score"
  },
  {
    "objectID": "notes/cs11-737-w19-syntax-and-parsing/index.html#difficulty",
    "href": "notes/cs11-737-w19-syntax-and-parsing/index.html#difficulty",
    "title": "Syntax and Parsing",
    "section": "Difficulty",
    "text": "Difficulty\nFor multi-lingual dependency parsing,the difficulty in multilingual dependency parsing is, that actually syntactic analysis is a particularly hard multilingual task. I think it’s harder than named entity recognition; it’s probably harder than part of speech tagging and it might even be on the same level of difficulty as translation. Yeah maybe not maybe not on the same level of difficulty as translation but it’s hard it’s a hard task and the reason why is because it’s on a global level not just a word by word level so you need to consider long distance dependencies within the sentence and syntax varies widely across different languages so like English is very very different from Hindi and both are very very different from Arabic so you can’t like just say you know well like named entities look pretty similar in all languages they all have like low frequency words and is a key that always works for named entities but you can’t do something like that when deciding syntax is easily"
  },
  {
    "objectID": "notes/cs11-737-w19-syntax-and-parsing/index.html#dependency-parsing",
    "href": "notes/cs11-737-w19-syntax-and-parsing/index.html#dependency-parsing",
    "title": "Syntax and Parsing",
    "section": "Dependency parsing",
    "text": "Dependency parsing\nthis is one of my papers i like it a lot i i wish more people liked it a lot but but basically the i think it’s a really really cool paper so i i’m trying to sell it to everybody but basically we came up with a generative model for dependency parsing and the idea of the generative model for dependency parsing is that you have a model that jointly generates the dependency tree and the sentence and one of the nice things about generative models is that you can train them unsupervised as well so you can take the unsupervised model and learn the structure that’s used in the unsupervised model together with like generating the sentences on unlabeled data and the reason why i think this is cool is because you can take a model that was trained on English and then train it unsupervised on the language that you’re interested in parsing and it improves the performance by a lot especially if the languages are very unrelated so i think this is another cool tool if you have a language that is from like a different language family for example the input is the dependency tree and your model just generates a random sentence in the target so the input is it’s a generative it’s you can think of it like a language model but it calculates instead of just calculating the probability of the output sentence it calculates the joint probability of the dependency tree and the output sentence so if you’ve given a sentence like it reconstructs the sentence right and it can do other things like it can get the the highest probability dependency tree given a sentence that’s called like latent variable inference and other things like that basically in this case it’s initialization it can be initialization with regularization towards the original parameters as well does that imply supervised data for the chemistry it does not it only it only requires text and basically the way it works is it it you have a dynamic program that finds the dependency tree that iterates over the dependency trees and it optimizes the parameters of like the dependency parser and the output at the same time but the probability of the output will go up basically if you have a more coherent dependency tree for a completely different family yes yeah the different family could have a completely different type of synthetic structure yes exactly yes you read read the paper i’m not lying so that’s a good that’s a good question and that will link it to what aditi is going to talk about in a second too we didn’t we didn’t evaluate it that you know like that extensively so i think that would be a natural interesting next question for any of these improvements that i’m talking about here does do these improvements lead to a more coherent you know grammatical sketch for the language or something like that if we extract the grammatical sketch cool yeah so i’m i’m"
  },
  {
    "objectID": "notes/cs11-737-w19-syntax-and-parsing/index.html#order-insensitive-encoder",
    "href": "notes/cs11-737-w19-syntax-and-parsing/index.html#order-insensitive-encoder",
    "title": "Syntax and Parsing",
    "section": "Order insensitive encoder",
    "text": "Order insensitive encoder\nso there’s a bunch of improvements that people have proposed i had some examples of these and the papers that i suggested for reading for the discussion one example is that people have shown that when you’re transferring to very very different languages you can remove the bias on ordering in encoders so for example if you’re using a transformer based model you can remove the positional encodings so you basically get an order insensitive encoder and that transfers much better to very very different languages so in this case they always use English as the source language and transferred to another language but can you think of any other examples where this might be useful can you think of it can you think of an example of a language where it would be very hard to get another language with similar syntax that’s included in your dependency treatment yeah i mean if you’re studying a low resource language that’s part of a language family that only has other low resource languages yeah exactly so if you’re studying a low resource language with a language family that only has other low research languages i could give an example what about navajo that’s probably the highest resource language in its language family but it’s extremely low resource and i think that’s i think it’s also useful because one of the or one of it’s also important because dependency parsing is particularly useful for things like linguistic inquiry or human facing interfaces when you can’t just train an end to end model easily so you know that’s a particularly salient use case another paper that i"
  },
  {
    "objectID": "notes/cs11-737-w19-syntax-and-parsing/index.html#linguistic-informed-constraints",
    "href": "notes/cs11-737-w19-syntax-and-parsing/index.html#linguistic-informed-constraints",
    "title": "Syntax and Parsing",
    "section": "Linguistic informed constraints",
    "text": "Linguistic informed constraints\nas i said i i’m excited about that so come talk to me if you if you want to hear more a final thing is linguistically informed constraints so there are big atlases of data about linguistic structures like the world atlas of language structures which i believe ellen or somebody talked about earlier and they they tell you things like is an adjective before a noun and so using this what this paper that i’m introducing here does is they use something called posterior regularization which basically says we’re going to parse our whole corpus we’re going to look at the proportion of the time that an adjective appears before a noun or after a noun and then if our big atlas of language structure says that adjectives should happen before nouns but our dependency parser is putting adjectives after nouns much more often then we are going to decrease the probability of it putting an adjective after the noun and increase the probability of it putting an adjective before the noun so this is a way to introduce basically prior knowledge into the predictor in order to make it work better so these are just three examples of cross-lingual dependency parsing but they demonstrate some you know ways to incorporate intuitions and stuff cool and any other things if not i’ll turn it over to aditi to talk in a bit of detail we might or might not have time for discussion formal discussion but we could have maybe all like class discussion and and it’s the keynote slides yes these are the keywords so can i open the demo on the google chrome this one yes thank you and then yeah that works cool and that’s the mind sure okay hi everyone i’m aditi i’m a phd student working with graham and today i’m going to show you a part of my research where i’ve used these dependency analysis okay so we just saw some because it’s kind of annoying okay so dependency analysis basically told us on a high level how words are related to each other so it’s information is useful but we also need to understand a more complex linguistic phenomena if you truly want to understand the language as a whole so some of these complex linguistic phenomena are like morphological agreement word order case marking suffix usage to name a few and these are important not just for like a language communication or understanding but also has some concrete applications so there are some human-centric or human-facing applications for instance like if you want to like learn the language then you need to know how to arrange these words when does the ordering of the word changes what kind of suffix to use when what happens gender is like for each gender you might have a different word ending and so on another important application which for navajo we also saw was language documentation because languages are getting extinct quite frequently and quite quickly also so language documentation is a way where linguists document the salient grammar points of a language not just for preservation but also as a way to like also create pedagogical resources maybe even create language technologies from that so another kind of application is from machine centric applications some examples that we saw where dependency analysis were used to give inductive bias into models another application is like we sort of used these rules that we extract automatically to evaluate a machine output so often across languages as we saw syntax is quite different so we want to have a way to automatically evaluate how grammatically correct let’s say a machine translation output is so basically to achieve both human centric application and machine centric applications we need to extract rules which explain this phenomena in a format which is both human readable and machine readable and i’m going to quickly explain like how we do this using a process of"
  },
  {
    "objectID": "notes/cs11-737-w19-syntax-and-parsing/index.html#general-framework",
    "href": "notes/cs11-737-w19-syntax-and-parsing/index.html#general-framework",
    "title": "Syntax and Parsing",
    "section": "General framework",
    "text": "General framework\nfeatures but in our case the features were part of speech tags dependency analysis from which we then extract rules for each of this phenomena and i’ll show you what the rules look like in a minute okay so now what are the kind of syntactic analysis which we can use so graham just showed you the universal dependencies project within that is also the sud tree banks so basically if for a language we have this kind of data available which more or less is annotated by language experts we can directly use these sud treatment as a starting point but for many more languages we even don’t have annotations so in that case your multilingual parsers come into the picture where you can take some raw text first parse it using these models and then apply the same approach okay so this is the toolkit"
  },
  {
    "objectID": "notes/cs11-737-w19-syntax-and-parsing/index.html#toolkit",
    "href": "notes/cs11-737-w19-syntax-and-parsing/index.html#toolkit",
    "title": "Syntax and Parsing",
    "section": "Toolkit",
    "text": "Toolkit\nwhich we have developed and okay this basically this is an autolex framework where we have extracted such kind of rules for different linguistic behaviors for a bunch of languages sure so for each language we extract a bunch of features and i’ll just go through some of them here so let’s say we want to explore the spanish agreement features and let’s go into gender so okay so this is exactly the so if you look at the first thing here basically say that okay mostly in spanish gender usually agrees between the determiner and its head but there are some significant number of cases when this does not hold true and some of these cases we have highlighted here that if let’s say a determiner is governed by an adjective in some cases the gender did not agree so we basically extracts rules in a human readable format and further for each of these rules because rules alone can often get quite overwhelming we also show some illustrative examples so here basically if you look here so here the determiner gender is masculine and the adjective gender is feminine so here is one example where although the determiner is governed by an adjective the gender is not matching but then there are some other examples where the determiner’s gender is masculine and the adjectives gender is also masculine here the general values are matching so these are some sort of exceptions to the language general oriented and knowing these exceptions are also important because they do occur quite frequently in the language again we have some rules for word order also so let’s say we want to can go adjective noun so for instance so typically in spanish unlike English most adjectives come after the noun that’s the typical ordering of objectives but there are very specific cases or or very specific adjectives which come before the nouns and the model has correctly identified some of them for instance this rule is telling us that if the adjective has the lima primero then the adjectives come before the noun and again for each of the rules we also ex like show some this illustrative examples where indeed these adjective which has the lima primero is coming before the noun but again language is not that simple there are even exceptions to exceptions so there are again examples which show that even when the lima is primarily there are certain times when this rule is not followed so we are trying to show a more softer view of the rule also that this rule is not 100 applicable every time there are conditions where this rule is not followed so essentially here we have used dependencies as our feature base to explain the rules in a human readable format and i guess one another important aspect here is the quality of the rules also depend a lot on the quality of the underlying analysis so as we improve the multilingual dependency parsing the quality of the rules should also improve so we also applied this system for an endangered language variety called hmong so hmong is spoken in north america also china thailand vietnam and laos and one of its variety is close to endangered and we were trying to we simply had access to a bunch of monk sentences and we wanted to analyze some of his interests and linguistic properties and we had david in lti who also speaks more and knows a lot about it so we basically presented such kind of rules for mong to david and there were a couple of interesting observations first the quality of rules was not as good as what we were seeing for spanish but despite that the model had still been able to identify some cluster of examples which showed rules which david was not aware about so such kind of data-driven approaches is also able to capture or identify some rules which the linguist was not initially aware of so i think dependency analysis is a very useful tool which has a lot of applications especially for exploring a language because there’s so much data out there the dependency analysis helps us find the key components to it so i can go into more detail but these are the main features sure so right now we basically i showed you grammar rules and how it was"
  },
  {
    "objectID": "notes/cs11-737-w19-syntax-and-parsing/index.html#summary",
    "href": "notes/cs11-737-w19-syntax-and-parsing/index.html#summary",
    "title": "Syntax and Parsing",
    "section": "Summary",
    "text": "Summary\nuseful from let’s say a linguist or language documentation point of view we’re also now using these tools to actually teach languages to a more less linguistically aware audience because often learners or teachers they don’t go into the linguistic jargon that much but they’re still interested in knowing the grammar rules and especially we are focusing on two languages marathi and kanada these are both indian languages and there are schools in north america where they are teaching these languages to English speakers so a lot of the times there are immigrants who are settled here and they want their children to learn these languages so that so there are English native speakers here and the schools here are interested in teaching them these languages from English so here we basically extract a lot of interesting features so for example for example here we have basically extracted some of the most common words observed in kannada what is the transliteration in English and what are the different kind of forms they are like what are the different forms these lemma have been observed across different genders so this lemma do here i don’t know i’m pronouncing it correctly but you can see here how this lima is used for a masculine gender or how this lima is used in the feminine gender and such kind of tables are pretty common when you are teaching how a word should be used in a language so we are automatically again performing syntactic analysis on these languages extracting such kind of useful insights and then presenting them to actual teachers so that they can check if this material is useful for their teaching process so we are trying to ease the teachers job so that they can focus on the creative aspect of teaching and yeah i guess those are the main points great thank you thank you aditi yeah so i’m sorry we’re right up against the the time so i don’t think we’re gonna have time for discussion this time i really apologize i try not to do this but i packed too much stuff in but i also invited the dt and didn’t want to ever prepare for nothing because of this if you took time to like prepare for the discussion today and read the things if you want to send like a short summary of the things that you prepared we can give like extra credit it won’t be required but we i’ll give like one discussion worth of extra credit for that are there any questions for about the stuff that you was talking about here yeah obviously mentioned like different regions and different species like america are there any like grammatical differences region-wise or are they like that’s a good question so there are these different varieties so we worked so the question was about because hmong is spread across so many different regions are there any difference in the grammatical properties so there are because there are different hmong varieties and we worked on one of the variety which is predominantly spoken in north america so we didn’t have the chance to investigate how the grammar rules changes across these different varieties but the interesting thing so we did another sort of a separate set of experiments where the universal dependencies project they have a lot of free bang for the same language for different domains so there are data from grammar books they have data from news articles and even within the same language across these domains the grammar structure varies quite a lot so in this tool we are basically also offering linguists to like check okay these are the do’s extracted from one domain how do they change in another domain is their model able to account for instances of agreement where there’s not exact matching values for instance if the subject is marked as duo so that would be considered as that if the agreement is not happening because the value is not matching i’m not sure if the universal dependency tree bank actually has that level of detail it might i maybe you have experience with that but i think the universal dependency annotations are actually quite i don’t know coarse so that my level of detail might not show up in the first place but it might yeah that is like an important point that this analysis has been done on one schema the schema used here is the ud schema or the sud schema which is the the we purposefully chose sud or ud because first of all it’s available for a lot more languages and it has a consistent annotation format so the kind of rules we extract now they’re also consistent across languages but that being said it might not consider the language the system is that well that is like a drawback for that but potentially you can apply this pipeline or this model to any other annotation statement it will give you rules according to that schema okay great thanks a lot everyone we can answer other other questions"
  },
  {
    "objectID": "notes/cs11-737-w19-syntax-and-parsing/index.html#outline",
    "href": "notes/cs11-737-w19-syntax-and-parsing/index.html#outline",
    "title": "Syntax and Parsing",
    "section": "Outline",
    "text": "Outline\n\nIntroduction to Syntax and Linguistic Structure\n\nSyntax focuses on the structure of sentences and the relationships between words.\nTwo main types of linguistic structure:\n\nDependency structure: Focuses on the relations between words.\nPhrase structure: Focuses on the structure of the sentence.\n\nDependency structures are often more straightforward to express across various languages and are thus useful in multilingual applications.\n\nDependency Parsing\n\nDependency parsing involves predicting linguistic structure from an input sentence.\nDifferent types of dependencies:\n\nSyntactic dependencies: Reflect the phrase structure of the sentence.\nSemantic dependencies: Useful for applications like question answering by connecting semantically related words.\n\n\nUniversal Dependencies Treebank\n\nA standard format for parse trees across many languages, facilitating the development of parsers that can be used for multiple languages.\nAllows for a unified format that simplifies processing across different languages.\nTo achieve universality, it sometimes requires sacrificing language-specific details.\n\nCross-Lingual Differences in Structure\n\nLanguages can vary significantly in word order (e.g., SVO, verb-final, verb-initial).\nDependency trees can visually highlight these structural differences, even without understanding the specific language.\n\nDependency Parsing Methods\n\nTransition-based models: Step through actions to build a tree. They use a queue of unprocessed words and a stack of partially processed words, choosing actions like shift and reduce.\nGraph-based models: Calculate the probability of each edge and use dynamic programming to find the best tree. They express a sentence as a fully connected directed graph and find the maximal spanning tree.\n\nApplications of Dependency Parsing\n\nHuman-facing applications: Useful for analyzing corpora, understanding language structure, and other linguistic phenomena.\nAdding inductive bias to neural models: Improving model robustness and encouraging models to be correct for the right reasons.\nSearching parsed corpora: Finding examples based on syntactic relationships.\nAnalysis of linguistic phenomena: Examining power dynamics or other sociological questions in film scripts.\nLanguage learning and documentation: Extracting morphological agreement rules and documenting salient grammar points.\nEvaluating machine output: Assessing the grammatical correctness of machine translation outputs.\n\nMultilingual Dependency Parsing\n\nSyntactic analysis is a challenging multilingual task due to global-level considerations and wide syntax variations.\nTechniques to improve cross-lingual transfer:\n\nRemoving bias on ordering in encoders.\nUsing generative models for unsupervised training.\nApplying linguistically informed constraints.\n\n\nExtracting Linguistic Insights Automatically\n\nGoal: To extract rules that explain linguistic phenomena in a human-readable and machine-readable format.\nMethod: Formulate linguistic questions as prediction tasks and extract rules from raw text using syntactic analysis.\nExample: Extracting morphological agreement rules by predicting required agreement versus chance agreement.\nGeneral Framework: Extract features (POS tags, dependency parses) from raw text, then extract rules for agreement, case marking, word order, etc..\n\nPractical uses of automatically extracted rules\n\nLanguage documentation and preservation.\nCreating pedagogical resources and language technologies.\nAiding language learners and teachers by presenting grammar rules and common word usages.\nEvaluating and correcting machine-generated text."
  },
  {
    "objectID": "notes/cs11-737-w19-syntax-and-parsing/index.html#reflection",
    "href": "notes/cs11-737-w19-syntax-and-parsing/index.html#reflection",
    "title": "Syntax and Parsing",
    "section": "Reflection",
    "text": "Reflection\nMorphological agreements discussion by Aditi is of particular interest.\nIt seems that understanding agreement is central to ‘second order’ model that need to beat the baseline. To get this to happen in an interpretable form we may want to train an attention head that is looking at thing that are in agreement and responding to the feature of agreement and disagreement as both are signals of some value.\nI belief that we can learn a probabilistic model of agreement between different cases but that they may be affected by all sorts of constraints like clause and phrase boundaries and the degree of morphological markings. So this is why attention seems so appropriate as it can learn what to focus on. And what we want is to make it interpretable so we can also extract the rules that it has learned!\nAditi talks about require agreement v.s. chance agreement for verb-subject and verb-object. I think that we may want to have a theory that looks at agreement in winder context and create a model that can predict agreement in a wider context so that we can consider the entropy of a sentence once agreement has been accounted for. In fact my thinking is that we may want to go further and determine the functional load of the agreement and have a more redined metric that can be used to evaluate the entropy of a sentence accounting to agreement in a functional context. (Error correction ambiguity, Reducing cognitive load, Reducing ambiguity, Parsing)"
  },
  {
    "objectID": "notes/cs11-737-w19-syntax-and-parsing/index.html#papers",
    "href": "notes/cs11-737-w19-syntax-and-parsing/index.html#papers",
    "title": "Syntax and Parsing",
    "section": "Papers",
    "text": "Papers\n\n\nAutomatic Extraction of Rules Governing Morphological Agreement EMNLP 2020. video\n\n\nThis paper is related to extracting morphological agreement rules using dependency relations.\n\n\nOn difficulties of cross-lingual transfer with order differences: A case study on dependency parsing\n\n\nThis paper discusses challenges in cross-lingual transfer due to word order differences.\n\n\nCross-lingual syntactic transfer through unsupervised adaptation of invertible projections\n\n\nThis paper is about cross-lingual syntactic transfer using unsupervised adaptation of invertible projections.\n\n\nTarget language-aware constrained inference for cross-lingual dependency parsing.”\n\n\nThis paper focuses on target language-aware constrained inference for cross-lingual dependency parsing.\n\n\nOn the Shortest Arborescence of a Directed Graph\n\n\nOptimum Branchings*\n\n\nThese papers are related to the Chu-Liu-Edmonds algorithm.\n\nKiperwasser and Goldberg (2016) Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations\n\nThis paper concerns Sequence Model Feature Extractors.\n\n\nDeep Biaffine Attention for Neural Dependency Parsing**\n\n\nThis paper discusses the BiAffine Classifier.\n\nYamada and Matsumoto (2003) Statistical Dependency Analysis with Support Vector Machines\n\nThese paper describes Arc Standard Shift-Reduce Parsing.\n\n\nAn Efficient Algorithm for Projective Dependency Parsing\n\nThese paper describes Arc Standard Shift-Reduce Parsing.\n\n\n\nAutoLEX: An Automatic Framework for Linguistic Exploration project\n\n\nEvaluating the Morphosyntactic Well-formedness of Generated Texts video\n\n\nData augmentation for low resource neural machine translation\n\n\nHandling syntactic divergence and low resource translation"
  },
  {
    "objectID": "notes/cs11-737-w19-syntax-and-parsing/index.html#hard-to-parse-in-english",
    "href": "notes/cs11-737-w19-syntax-and-parsing/index.html#hard-to-parse-in-english",
    "title": "Syntax and Parsing",
    "section": "Hard to parse in English",
    "text": "Hard to parse in English\n\nAlice drove down the street in her car\n– prepositional phrase attachment ambiguity\n\n\ntime flies like an arrow. Fruit flies like bananas\n– polysemic ambiguity & garden path sentences\n\n\nI drove my car to the hospital in town on Saturday\n– Linear projection in English\n\n\nYou cannot add flavour to a bean that isn’t there\n– Non-linear projection in English\n\n\nAlex went to Sam’s house, where he told her that they would miss his show.\n– Coreference resolution ambiguity\n\n\nI saw an elephant yesterday in my pajamas\n– Non Projectives in English\n\n\nBill loves and mary hates soccer\n– Non Projectives in English"
  },
  {
    "objectID": "notes/cs11-737-w08-contact/index.html#outline",
    "href": "notes/cs11-737-w08-contact/index.html#outline",
    "title": "Language Contact and Change",
    "section": "Outline",
    "text": "Outline\n\nIntroduction: Language Contact and Change\n\nLanguage is Dynamic: Languages are always evolving, not static. Examples from Isaac Newton’s letter (1672) compared to Wilbur Wright’s (1899) and modern email illustrate language change over time.\nRelevance to Multilingual NLP: Multilingual NLP must consider language evolution, especially with cross-lingual transfer approaches.\n\nWhy Languages Change\n\nChanges in the World:\n\nNew technologies and concepts lead to new words (e.g., “email”).\nObsolete terms disappear (e.g., “radiogram”).\n\nLaziness/Efficiency:\n\nWords are shortened for efficiency (e.g., “telephone” to “phone”).\n\nEmphasis/Clarity:\n\nChanges to avoid confusion, like pronoun shifts (e.g., “he/heo/hi” to “he/she/they”).\n\nPoliteness:\n\nAvoiding offensive or culturally insensitive language (e.g., Google’s word list).\n\nMisunderstanding:\n\nWord meanings change due to misinterpretation (e.g., “bead” from “prayer” to “small ball”).\n\nGroup Identity/Prestige:\n\nAdopting language to fit in or sound sophisticated (e.g., aroma -&gt; smell). French influence in English, where posh words like “constitution” come from French.\n\nStructural Reasons:\n\nAdapting words to fit the phonetic and morphological rules of a language.\n\n\nLexical Changes\n\nCognates: Words with a shared ancestor, evolving from a parent language. Example: “Two” in English and “zwei” in German.\nLoanwords: Words adopted from another language. Example: “Orchestra” from Greek to Japanese (オーケストラ), then adapted to “karaoke”.\n\nLanguage Contact and the Lexicon\n\nDefinition: Language contact is the use of multiple languages in the same place at the same time.\nDriving Force: Language contact is a major driver of language change.\n\nCase Study: Arabic-Swahili\n\nHistorical Context: Trade between 800-1920 AD led to significant language contact.\nInfluence: Approximately 40% of Swahili words are borrowed from Arabic.\n\nLexical Borrowing\n\nPervasiveness: Common in languages, especially from resource-rich or socially influential ones.\n\nCross-Lingual Lexical Similarities\n\nBridging Languages: Identifying orthographically or phonetically similar words across languages that are likely mutual translations.\n\nLexicon Structure\n\nCore: Basic, frequently used words (e.g., beer, bread).\nAssimilated: Integrated loanwords (e.g., cookie, sugar, coffee, orange).\nPeripheral: Proper nouns and specialized terms (e.g., New York, Luxembourg).\n\nCross-Lingual Lexical Learning\n\nCross-Lingual Lexicon Induction: Identifying word meanings across languages.\n\nTransliteration Models\n\nModels: Finite State Transducers (FSTs), LSTMs with attention.\nEvaluation: Word accuracy, F-score, ranking metrics like Mean Reciprocal Rank (MRR).\nResources: Databases of named entities.\n\nCognates and Loanwords: Adaptation\n\nPhonological and Morphological Integration:\n\nSyllable structure adaptation.\nConsonant cluster removal (degemination).\nVowel substitution.\nApplication of target language morphology.\nConsonant adaptation.\n\nExamples (Arabic-Swahili): Adaptation of words like “minister” and “palace”.\n\nLinguistic Research on Lexical Borrowing\n\nCase studies in language pairs (e.g., Cantonese, Korean).\nPhonological, morphological, and syntactic integration.\nSociolinguistic phenomena in borrowing.\n\nCognate and Loanword Models\n\nPhonologically-weighted Levenshtein distance.\nPhonetic and semantic distance.\nGenerative models of sound laws and word evolution.\nOptimality-theoretic constraint-based learning.\n\nCognate and Lexical Borrowing Databases\n\nCognate databases with millions of cognate pairs across many languages.\nLexical borrowing databases like the World Loanword Database.\n\nBilingual Lexicon Induction\n\nSteps:\n\nLearn monolingual embeddings.\nAlign embedding spaces.\nFind nearest neighbors to induce lexicon.\nPerform supervised alignment to minimize distance between lexicon items.\n\n\nDiscussion Points\n\nHow knowledge of lexical sharing can improve applications.\nExamples of using morphological and phonological embeddings for information sharing between languages.\nDiscussion options:\n\nEfficiency in language.\nHistorical influences and loanwords in specific languages."
  },
  {
    "objectID": "notes/cs11-737-w08-contact/index.html#introduction",
    "href": "notes/cs11-737-w08-contact/index.html#introduction",
    "title": "Language Contact and Change",
    "section": "Introduction",
    "text": "Introduction\n\nThis time we’re going to talk about language contact and change which i think is a you know very interesting and relevant topic if we are going to be thinking about models it’s relevant for a couple reasons one reason is because we’re going to be using lots of cross-lingual transfer approaches in this class another reason is like knowing that language is not like when we talk about english we’re not just talking about you know a single language but we’re talking about a language that’s constantly evolving constantly changing and you know same with any other language is a very important factor of multilingual nlp you know it’s multiple languages even within the same thing that we’re talking about so just to give an example of this from English which presumably everybody knows about I i have a letter from isaac newton from 1672 a letter from wilbur wright who invented the airplane in 1899 in an email from me in 2021 not because I think i’m on the same caliber as any of the other people here but rather from 2021 it was just convenient to get my email but if you look here you can see the letter from isaac newton which says stir to perform my late promise to you I shall without further ceremony acquaint you that in the beginning of the year 1666 at which time I applied my self to the grinding of optic glasses of other figures than spherical I procured me a triangular glass prism to try to there with the celebrated phenomena of colors this is by the way a letter to the editor of the journal so this is what you had to write when you wanted to submit a paper in in 1672. the letter from wilbur wright says dear sirs I have been interested in the problem of mechanical and human flight ever since as a boy I constructed a number of baths of various sizes after the style of Cayley’s and penalties machines my observations since have only convinced me more firmly that human flight is possible and practicable and this is a letter from wilbur wright to the smithsonian asking them for some like materials the final one is my email auto response so much less consequential but you’ll notice actually the letter from overwrite this doesn’t seem so strange it’s like a little bit more formal than we write things nowadays but you know it’s pretty close to the language we speak now it’s also written in a kind of modern typeface similar to corey or new or whatever that you see on your on your computer here from isaac newton it’s pretty different like you know all of the s’s are kind of the old style s that looks like an f you can see that he uses some words that we don’t use very much anymore like there with he also I procured me this isn’t the grammatical form that we really use anymore so you can see that the grammar is actually changing as well oh and actually it’s using another letter that we don’t we don’t use anymore the a a and e combine together so why do languages change if you"
  },
  {
    "objectID": "notes/cs11-737-w08-contact/index.html#why-do-languages-change",
    "href": "notes/cs11-737-w08-contact/index.html#why-do-languages-change",
    "title": "Language Contact and Change",
    "section": "Why do languages change",
    "text": "Why do languages change\n\nwant to learn more about this topic I highly recommend this book why do languages change and it’s pretty amusing it’s about 150 pages so it’s not a heavy read and i’ve learned lots of fun facts about why football is called sorry why soccer is called soccer in english and the name the reason behind the various names of cities in britain and stuff like this but it has this nice categorization in the second chapter that says languages change because of changes in the world so to give an example the way we communicate has changed a lot so I communicate via email as I showed you in the previous slide and so now we have a word email which we didn’t have you know 30 years ago or 40 years ago actually maybe more maybe more than that and then on the other hand there’s a word radiogram which we don’t use anymore because we don’t communicate by radiogram anymore so this word is moving out of use in the english language and this is just one of many examples you know we have new technology all the time we have varieties of concepts that we don’t use very much anymore another reason why languages change is laziness or efficiency so for example we started out with telephone and this kind of converged down to phone this also happens in more recumbent words like you all becoming y'all in you know southern southern us dialect and i’m sure you can think of other things in the languages you’re aware of as well another reason is the opposite which is instead of being lazy and abbreviating things away we change things for emphasis or clarity so one of the examples in this book is that in old english the pronouns in in english for he she and they were i’m probably gonna pronounce them completely wrong but hey hill and high or something like that and these are too close together so they were confusable so they you know added additional things onto them to make this more clear and I think some of the people might have read this gibson 2019 survey on how efficiency shapes human language and so basically we have two pressures we want to be lazy we want to say things as efficiently as we can but on the other hand we can’t like say things so efficiently that everything converges into the same pronunciation in the same words because then we couldn’t communicate our thoughts so we have this pressure of making things simpler to make it more efficient and making things more complicated to make it more clear another thing is politeness and as a"
  },
  {
    "objectID": "notes/cs11-737-w08-contact/index.html#politeness",
    "href": "notes/cs11-737-w08-contact/index.html#politeness",
    "title": "Language Contact and Change",
    "section": "Politeness",
    "text": "Politeness\n\nvery interesting example of this politeness we have this google google word list here and this google word list is all of the words that people in google are not allowed to use in documentation or variable names or things like this and just to give some examples you’re not supposed to use well you’re not supposed to use agnostic because agnostic is a name for that’s also used in the us for non-religious and that might affect religious people [Music] another thing i’m assuming this sorry I don’t like you can confirm with google if that’s correct but that’s my guess not supposed to use the word below for some reason maybe it is like indicative of like inferiority or something like this in general you’re not supposed to use words that are indicative of race for example especially if they have a negative or positive connotation cell phone is an americanism so that’s that’s they’re just trying to be more clear and make it so everybody can understand and say mobile phone chubby because that could be derogatory for for people who are like overweight or something so you can see that there’s all these all these things that people like change the way they used to say things for politeness another example is misunderstanding"
  },
  {
    "objectID": "notes/cs11-737-w08-contact/index.html#misunderstanding",
    "href": "notes/cs11-737-w08-contact/index.html#misunderstanding",
    "title": "Language Contact and Change",
    "section": "Misunderstanding",
    "text": "Misunderstanding\n\nThe word bead apparently in english originally meant prayer but it got mistaken because when people are from the catholic faith they they count prayers on little balls and when people heard this they thought bead meant ball instead of prayer so they started using it that way another thing is group identity and prestige this isn’t a really wonderful paper that talks about how people who belong to a certain group in this case forums about beer adopt the way that people speak on that forum and this is something called entrainment it’s a thing where you start adopting the way people speak in order to fit into the group and in this case the group started saying that beer had aroma but then some at some point they started everybody started saying smell maybe an influencer said smell or something like this and then they all switched over to bad so if you’re using the appropriate way of saying things then you know that makes you more cool and so so you do that and another good example of this is actually we were talking about german and how english is a germanic language but actually I don’t know the actual numbers but I think probably half or more of the words in english are from french and the reason why is because you know french people conquered england or much of england at some point and so all of the kind of posh you know like words about government and stuff like that like constitution or election or legislation come from french whereas words like hand and mouth and you know just kind of common everyday words come from german so there was a prestige element with saying things in french so all the kind of higher society words came from french and also structural reasons like if you adopt words from other languages they’re hard to say in that language so you you change them around and stuff like this so basically what I want to point out here is you know language is not static we’re not saying it the same way that we did before but there’s very good reasons why we’re not you know why we’re changing the way we speak I know there was a lot of different things were there any questions about these for comments"
  },
  {
    "objectID": "notes/cs11-737-w08-contact/index.html#lexical-changes",
    "href": "notes/cs11-737-w08-contact/index.html#lexical-changes",
    "title": "Language Contact and Change",
    "section": "Lexical Changes",
    "text": "Lexical Changes\n\nWe also have lexical changes and these can be including things like cognates and lone words that arise from language contact so cognates and loanwords are two varieties of words that are shared across different languages cognates are words that basically start out in a some sort of parent language and and propagate their way into the languages just by you know being used over and over and over again but they change due to the underlying phonetic changes in the language or other factors so this is an example that I gave in the first in the first class but I just posted it here again this is an example of how the word two in english evolved from proto-indo-european. You can see that in english it turned into two in german it’s something like zwei you know there are many many other ones and everything else in the indo-european family.\nOne interesting thing is words like q are so frequent that they don’t you know have major changes they don’t get overcome by words in other languages or things like this so basically you know like the more frequent the word is the more likely it is to have descended from some sort of original language and there’s also interesting research not a whole lot of it but on like reconstruction of you know the original pronunciation of the word based on all these different things I see you nodding yes"
  },
  {
    "objectID": "notes/cs11-737-w08-contact/index.html#loanwords",
    "href": "notes/cs11-737-w08-contact/index.html#loanwords",
    "title": "Language Contact and Change",
    "section": "Loanwords",
    "text": "Loanwords\n\nanother example is loanwords so this is just one of my favorite examples the word orchestra originally came from greek it was adopted into the english language just because it’s a cognate you know it was english descended from from the same family as greek but that got adopted into japanese is a loanword and it became orchestra so they changed the pronunciation because you need to change it in order for it to be pronounced pronounceable in japanese and then there was the change in the world in that we we’re not talking about orchestras as much and instead we started having singing machines where it would play music for you and then you would sing and this is called karaoke in japanese where para is an original japanese word that stands for empty and okay is the words from orchestra so it’s basically empty orca and then that got readapted back into english is karaoke so it’s kind of like the okay is from orchestra originally and it came in the circular path here so loanwords you know don’t always have as interesting a story as this but very often they have to do with technology that was invented in that country it got a name in that language and then it gets moved back so the orchestra was invented in europe I got imported to japan and then karaoke was invented in japan and got imported into or well invented in east asia i’m not sure exactly what was invented but got reimported back into the us so like one words very often happen for this precisely this reason it’s to express a concept that wasn’t really around before so it comes back"
  },
  {
    "objectID": "notes/cs11-737-w08-contact/index.html#language-contact",
    "href": "notes/cs11-737-w08-contact/index.html#language-contact",
    "title": "Language Contact and Change",
    "section": "Language Contact",
    "text": "Language Contact\n\nso if we look at language contact in the lexicon this is a very good introduction of how language contact works and how it’s the major driving factor behind language change and to give just another example of this in addition to the english example to demonstrate that this happens all over the world there’s swahili which is a major language in southeast africa a hundred million speakers and there was a lot of trade between 800 a.d and 1920 and because of this there was a lot of contact between swahili speaking places and arabic speaking places and so there are estimates that say 40 of swahili types are borrowed from arabic largely probably because a lot of the concepts corresponding to these types came together with with trade and this is a very very common this is a very common trend and often it’s a resource-rich donor language that has lots of you know economic influence in the places where the recipient language has received these and basically it goes both directions obviously it’s not just like english to japanese or japanese english it goes in both ways but the amount also you know often tends to be proportional to the number of speakers in the influence"
  },
  {
    "objectID": "notes/cs11-737-w08-contact/index.html#crosslingual-lexical-similarities",
    "href": "notes/cs11-737-w08-contact/index.html#crosslingual-lexical-similarities",
    "title": "Language Contact and Change",
    "section": "Crosslingual lexical similarities",
    "text": "Crosslingual lexical similarities\n\nso because of this there are now cross-lingual lexical similarities where you know one one language borrows words from another language but it changes them to be pronounceable or they change due to evolution and other things like this and so often one thing that you would like to do is you’d like to identify things that are you know equivalent across languages and likely to be mutual translations and this is an example of falafel which I think also was i it was likely adopted into into english I guess from hebrew or arabic perhaps probably hebrew"
  },
  {
    "objectID": "notes/cs11-737-w08-contact/index.html#stratified-lexicon-structure",
    "href": "notes/cs11-737-w08-contact/index.html#stratified-lexicon-structure",
    "title": "Language Contact and Change",
    "section": "Stratified lexicon structure",
    "text": "Stratified lexicon structure\n\nThere’s a kind of stratified lexicon structure and the core lexicon is words that originally appeared in the language they’re words that just you know have been here for forever like the word to or the word hand or something like this there’s also or beer or bread there’s also fully assimilated words in for example in english cookie sugar coffee orange other examples would be like election constitution or things like this we don’t even think that they’re loan words and then we have we have partially assimilated words and partially assimilated words are words that could be essentially you can tell that they’re foreign origin based on their like based on just the fact that they haven’t been there for very long like a priori or something like this or deus ex machina or something where it’s like you know you don’t you have a reason to believe that they’re from other languages and then peripheral are kind of things like entities or you know place names or things like this"
  },
  {
    "objectID": "notes/cs11-737-w08-contact/index.html#peripheral-vocabulary",
    "href": "notes/cs11-737-w08-contact/index.html#peripheral-vocabulary",
    "title": "Language Contact and Change",
    "section": "peripheral vocabulary",
    "text": "peripheral vocabulary\n\nPeripheral vocabulary are proper names in specialized terms and very often these will have very similar or exactly the same pronunciation across many different languages borrowing they might be similar they might have similar spelling and then you have content words in the in the core lexicon like like night from one ancestral language"
  },
  {
    "objectID": "notes/cs11-737-w08-contact/index.html#lexicon-induction",
    "href": "notes/cs11-737-w08-contact/index.html#lexicon-induction",
    "title": "Language Contact and Change",
    "section": "lexicon induction",
    "text": "lexicon induction\n\nIn order to to take advantage of this one thing that you can do is cross-lingual lexicon induction and basically what this is is this is identifying words that mean the same thing across different languages and of course this is this is easier if they’re words of similar origin because if they are then the spelling could be the same the pronunciation could be the same this could give you hints about how which words correspond to each other so lexicon induction the task itself is basically in a way a classification task where you’re given a word in a language like this arabic word here and you have a very large number of words in the in the lexicon from the other language like hebrew and you try to pick which one is the correct translation here so this is obviously a hard task because you know any classification problem with like 100k way classification problem is going to be difficult but you can take advantage of these kind of structural similarities to do a good job of this"
  },
  {
    "objectID": "notes/cs11-737-w08-contact/index.html#transliteration",
    "href": "notes/cs11-737-w08-contact/index.html#transliteration",
    "title": "Language Contact and Change",
    "section": "Transliteration",
    "text": "Transliteration\n\nso for peripheral things especially proper names or things where we can assume that the spelling is similar there are models of transliteration and transliteration is essentially a a problem of taking in a word in one language and trying to output the word in the other language knowing that the only thing that differs between them is the pronunciation so this would be an example like new york where this is translated basically pronunciation by pronunciation into the other one into the other language and there’s lots of models for this early models for this used finite state transducers there’s also lstms with attention or you know just using a regular sequence sequence model to solve this but in addition to this one of the features of transliteration is that transliteration tends to be done monotonically so the input and the output tend to be in the same in the same order so there’s also models that take explicit like advantage of this so yeah this is this is one one task that you can do with respect to cross-linkable lexicon cross-lingual lexical learning there’s with respect to like if you think transliteration is interesting oh sorry i forgot one one important thing so just because something’s an entity doesn’t mean it’s just a transliteration problem doesn’t mean that like the only thing you need to do is map the pronunciation so just to give an example carnegie mellon university if you think about it in your language i think it’s probably likely that you don’t just pronounce university unless university in your language is also pronounced similarly to university like for example in in japanese that would be hanagi meron where carnegie mellon is transliterated but daigaku is the japanese version of university so because of this named entity translation is not just transliteration it’s a mix of transliteration and translation that being said there are shared tasks on named entity transliteration and you can do things like measuring word accuracy in the top one measure like the f score of words in the top one or you can treat it as a transliteration mining task where you have entities in one language and a huge list of entities in the other language and see how well you can identify entities there and there’s also downstream evaluations and things like empty and information extraction as well so also relatively recently there was this paper presented on a large-scale name transliteration resource with 1.6 million named entities across 180 languages so if you’re interested in entities or entity linking or any anything with regards to this you can take a look at this paper here"
  },
  {
    "objectID": "notes/cs11-737-w08-contact/index.html#cognate-loan-words",
    "href": "notes/cs11-737-w08-contact/index.html#cognate-loan-words",
    "title": "Language Contact and Change",
    "section": "Cognate loan words",
    "text": "Cognate loan words\n\nNext we have cognates and loan words and cognates and loanwords are the ones that I talked about before and one of the interesting things about this is there’s a lot of different factors that go into what actually happens when you borrow a word and here are two examples from the previous arabic or sorry three examples from the previous arabic and swahili example but you can have things like adopting the syllable structure to be appropriate for the for the language degemination so swahili doesn’t allow consonant clusters so it removes consonant cluster and vowel substitution because certain vowels are more natural here you can also modify the morphology apply morphology from the target language and other things like this or adapt consonants appropriately so what you can see is basically this is assimilating the word into the language to make it seem like natural pronunciation in the language so there’s a huge amount of linguistic"
  },
  {
    "objectID": "notes/cs11-737-w08-contact/index.html#lexical-borrowing",
    "href": "notes/cs11-737-w08-contact/index.html#lexical-borrowing",
    "title": "Language Contact and Change",
    "section": "lexical borrowing",
    "text": "lexical borrowing\n\nresource research on lexical borrowing like case studies of lexical borrowing across language pairs what kinds of phonological morphological phenomena happened in borrowing case studies of socio-linguistic phenomena and borrowing like what what words tend to be borrowed what words tend to not be borrowed etc"
  },
  {
    "objectID": "notes/cs11-737-w08-contact/index.html#cognates",
    "href": "notes/cs11-737-w08-contact/index.html#cognates",
    "title": "Language Contact and Change",
    "section": "cognates",
    "text": "cognates\n\nThere’s a fairly large number of like cognate in loanword models that do things like phonologically weighted levenshtein distance so basically this waits likely to happen phonological substitutions to upgrade ones that are more likely like you might swap a vowel into a vowel but not a consonant into a vowel and other other things like the ones that I listed below and there’s also similar"
  },
  {
    "objectID": "notes/cs11-737-w08-contact/index.html#cognate-databases",
    "href": "notes/cs11-737-w08-contact/index.html#cognate-databases",
    "title": "Language Contact and Change",
    "section": "cognate databases",
    "text": "cognate databases\n\ncognate databases across 338 languages 3.1 million cognate pairs and 35 writing systems so if you’re interested in in studying more about this you can do this too and there’s also a lexical"
  },
  {
    "objectID": "notes/cs11-737-w08-contact/index.html#lexical-databases",
    "href": "notes/cs11-737-w08-contact/index.html#lexical-databases",
    "title": "Language Contact and Change",
    "section": "lexical databases",
    "text": "lexical databases\n\nborrowing database world load word database so i’m not gonna go into a whole lot unsupervised lexicon induction of detail about this here but there’s also unsupervised models of lexicon induction and basically the the way that most of these models work now is they work either as an unsupervised task of identifying lexicon items like I talked about before you know given a certain word what are all the what is the word in the other language that corresponds to this word and in the unsupervised setting what you do is you learn monolingual embeddings first and these monolingual embeddings basically capture the the either spelling or distributional characteristics of each of the words you then find an alignment between the embedding spaces by like rotating one of the two embedding spaces to match the other one using a distributional similarity between the the two aligned embedding spaces you find nearest neighbors to induce a lexicon and you perform supervised alignment to minimize the distance between lexical items further so it’s kind of an iterative process where you start out with an unsupervised objective identify a few pairs with high confidence and then provide propose sorry then perform supervised alignment and there’s a very nice extensive survey of different methods that you can use to do this here if if this is of more interest so basically the this is a method that’s used to obtain bilingual dictionaries that you can then use for downstream tasks as well cool so basically that that’s all I have for the lecture part for today and I talked a little bit about language contact language change and how this affects the lexicon are there any questions before we go into the discussion part"
  },
  {
    "objectID": "notes/cs11-737-w08-contact/index.html#assimilated",
    "href": "notes/cs11-737-w08-contact/index.html#assimilated",
    "title": "Language Contact and Change",
    "section": "assimilated",
    "text": "assimilated\n\nit’s something that like obviously english speakers you know recognizes something that didn’t originate in english like if you ask anybody they they would know and so I think that would say that’s basically partially assimilated you know a lot of a lot of people know what lasse fear means but they would recognize that something something foreign I guess"
  },
  {
    "objectID": "notes/cs11-737-w08-contact/index.html#lifeschool",
    "href": "notes/cs11-737-w08-contact/index.html#lifeschool",
    "title": "Language Contact and Change",
    "section": "lifeschool",
    "text": "lifeschool\n\nThat’s a really good question and I i think basically the knowing about these things could lead you to be able to create better applications so if you think about the if you think about the simplest possible way to have any sort of like life school sharing between languages it’s train a model on lots of different languages and hope that something good happens and that’s actually what we do right now a lot of the times so we have like a character based model or a subword based model and we train it on lots of different languages or we train it and then we do alignment here and I think actually a lot of the stuff that I presented here was more forward-looking in that it’s like this is the way that lexical sharing actually happens in language and there’s not a whole lot of work that actually applies it in an intelligent way to doing these things one example that I i can give is this is a paper where we used explicit morphological and phonological embeddings to try to like do better information sharing between languages this was back when we just used like word to back embeddings or things like this it wasn’t when we used bert or or other things here so I think you know similar language similar methods like this could be applied to learning like multilingual birds or multilingual machine translation models or something like that as well so yeah"
  },
  {
    "objectID": "notes/cs11-737-w08-contact/index.html#papers",
    "href": "notes/cs11-737-w08-contact/index.html#papers",
    "title": "Language Contact and Change",
    "section": "Papers",
    "text": "Papers\n\nGibson 2019: Survey on how efficiency shapes human language.\nDanescu-Niculescu-Mizil et al. 2013: Paper about group identity and prestige.\nItô & Mester ’95: Work on core-periphery lexicon structure.\nKnight & Graehl ’98: Work on FSTs for transliteration models.\nRosca & Breuel’16: Work on LSTMs with attention for transliteration models.\nWu & Cotterell’19: Work on exact hard monotonic attention for character-level transduction.\nMann & Yarowsky ’01, Dellert ’18: Phonologically-weighted Levenshtein distance between phonetic sequences.\nKondrak ’01, Kondrak, Marcu & Knight ’03: Phonetic + semantic distance.\nBouchard-Côté et al. ’09: Log-linear model with Optimality-theoretic features.\nHall & Klein ’10, ’11: Generative models of sound laws and word evolution for cognate identification.\nTsvetkov & Dyer ’16: Optimality-theoretic constraint-based learning for loanword identification.\nSoisalon-Soininen & Granroth-Wilding ’19: Cognate identification using Siamese networks.\nYip ’93, Kang ’03, Kenstowicz & Suchato ’06, Benson ’59, Friesner ’09, Schwarzwald ’98, Ojo ’77, Schadeberg ’09, Johnson ’14, Haspelmath & Tadmor ’09: Case studies of lexical borrowing in language pairs.\nHolden ’76, Van Coetsem ’88, Ahn & Iverson ’04, Kawahara ’08, Hock & Joseph ’09, Calabrese & Wetzels ’09, Kang ’11; Rabeno ’97, Repetti ’06; Whitney ’81, Moravcsik ’78, Myers-Scotton ’02: Case studies of phonological/morphological phenomena in borrowing.\nGuy ’90, McMahon ’94, Sankoff ’02, Appel & Muysken ’05: Case studies of sociolinguistic phenomena in borrowing.\n\nAdditionally, the lecture mentions a paper that uses explicit morphological and phonological embeddings to improve information sharing between languages. There is also a paper presented on a large-scale name transliteration resource with 1.6 million named entities across 180 languages."
  }
]